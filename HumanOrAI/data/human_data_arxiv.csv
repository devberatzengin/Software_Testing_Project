text,label,source,subject,license,title
"In this paper, we introduce a novel framework using inhomogeneous Branching Random Walks (BRWs) to model growth processes, specifically introducing genealogy-dependence in branching rates and displacement distributions to model phenomena like bacterial colony growth. Current stochastic models often either assume independent and identical behavior of individual agents or incorporate only spatiotemporal inhomogeneity, ignoring the effect of genealogy-based inhomogeneity on the long-time behavior of these processes. Such long-time asymptotics are of independent mathematical interest and are crucial in understanding the effect of patterns. We propose several inhomogeneous BRW models in 2D space where displacement distributions and branching rates vary with time, space, and genealogy. A combined model then uses a weighted average of positions given by these separate models to study the shape of the growth patterns. Using computer simulations, we tune parameters from these models, which are based on genealogical and spatiotemporal factors, observe the resulting structures, and compare them with images of real bacterial colonies.",0,arxiv,Evrim,CC-BY/arXiv,Inhomogeneous Branching Random Walks: Incorporating Genealogy and Density Effects
"Recent mosquito-borne outbreaks have revealed vulnerabilities in our abatement programmes, raising concerns about how abatement-districts should choose optimal future control strategies. Spatial dissemination of vector-borne disease is strongly shaped by the movement of both hosts and mosquitoes, creating substantial overlap between vector activity and pathogen spread. We developed a mathematical model for Culex mosquito dynamics in a patchy landscape, integrating entomological observations, weather-driven factors, and the vector control practices of the Northwest Mosquito Abatement District (NWMAD) in Cook County, Illinois. By coupling a temperature-driven multi-patch ODE model with NWMAD's adulticide and larvicide interventions, we investigated how spatial heterogeneity and control timing influence mosquito abundance. We also evaluated how mosquito dispersal modifies intervention effectiveness by comparing single-patch and two-patch model outcomes. Our results showed that models ignoring spatial connectivity can substantially overestimate the impact of interventions or misidentify the thresholds of vector persistence. Through numerical simulations, we analysed continuous and pulsatile control approaches under varying spatial and temporal configurations. These findings provide insight into optimal strategies for managing Culex populations and mitigating mosquito-borne disease risk in weather-driven, spatially connected environments across Cook County, Illinois.",0,arxiv,Evrim,CC-BY/arXiv,A mathematical model of \textit{Culex} population abundance and the impact of vector control interventions in a patchy environment
"Finite and infinite population models are frequently used in population dynamics. However, their interrelationship is rarely discussed. In this work, we examine the limits of large populations of the Moran process (a finite-population birth-death process) and the replicator equation (an ordinary differential equation) as paradigmatic examples of finite and infinite population models, respectively, both of which are extensively used in population genetics. Except for certain degenerate cases, we completely characterize when these models exhibit similar dynamics, i.e., when there is a one-to-one relation between the stable attractors of the replicator equations and the metastable states of the Moran process. To achieve this goal, we first show that the asymptotic expression for the fixation probability in the Moran process, when the population size is large and individual interaction is almost arbitrary (including cases modeled through $d$-player game theory), is a convex combination of the asymptotic approximations obtained in the constant fitness case or 2-player game theory. We discuss several examples and the inverse problem, i.e., how to derive a Moran process that is compatible with a given replicator dynamics. In particular, we prove that modeling a Moran process with an inner metastable state may require the use of $d$-player game theory with possibly large $d$ values, depending on the precise location of the inner equilibrium.",0,arxiv,Evrim,CC-BY/arXiv,Dynamical compatibility for finite and infinite population models used in genetics
"In any ecosystem, the conditions of the environment and the characteristics of the species that inhabit it are entangled, co-evolving in space and time. We introduce a model that couples active agents with a dynamic environment, interpreted as a nutrient source. Agents are persistent random walkers that gather food from the environment and store it in an inner energy depot. This energy is used for self-propulsion, metabolic expenses, and reproduction. The environment is a two-dimensional surface divided into patches, each of them producing food. Thus, population size and resource distribution become emergent properties of the system. Combining simulations and analytical framework to analyze limiting cases, we show that the system exhibits distinct phases separating quasi-static and highly motile regimes. We observe that, in general, population sizes are inversely proportional to the average energy per agent. Furthermore, we find that, counter-intuitively, reduced access to resources or increased metabolic expenditure can lead to a larger population size. The proposed theoretical framework provides a link between active matter and movement ecology, allowing to investigate short vs long-term strategies to resource exploitation and rationing, as well as sedentary vs wandering strategy. The introduced approach may serve as a tool to describe real-world ecological systems and to test environmental strategies to prevent species extinction.",0,arxiv,Evrim,CC-BY/arXiv,Resource and population dynamics in an agent-environment interaction model
"Background: Adolescence is a critical period of brain maturation and heightened vulnerability to cognitive and mental health disorders. Sleep plays a vital role in neurodevelopment, yet the mechanisms linking insufficient sleep to adverse brain and behavioral outcomes remain unclear. The glymphatic system (GS), a brain-wide clearance pathway, may provide a key mechanistic link. Methods: Participants from the Adolescent Brain Cognitive Development (ABCD) Study (n =6,800; age ~ 11 years) were categorized into sleep-sufficient (>=9 h/night) and sleep-insufficient (<9 h/night) groups. Linear models tested associations among sleep, PVS burden, brain volumes, and behavioral outcomes. Mediation analyses evaluated whether PVS burden explained sleep-related effects. Results: Adolescents with insufficient sleep exhibited significantly greater PVS burden, reduced cortical, subcortical, and white matter volumes, poorer cognitive performance across multiple domains (largest effect in crystallized intelligence), and elevated psychopathology (largest effect in general problems). Sleep duration and quality were strongly associated with PVS burden. Mediation analyses revealed that PVS burden partially mediated sleep effects on cognition and mental health, with indirect proportions up to 10.9%. Sequential models suggested a pathway from sleep -> PVS -> brain volume -> behavior as the most plausible route. Conclusions: Insufficient sleep during adolescence is linked to glymphatic dysfunction, reflected by increased PVS burden, which partially accounts for adverse effects on brain structure, cognition, and mental health. These findings highlight the GS as a potential mechanistic pathway and imaging biomarker, underscoring the importance of promoting adequate sleep to support neurodevelopment and mental health.",0,arxiv,Evrim,CC-BY/arXiv,"Sleep effects on brain, cognition, and mental health during adolescence are mediated by the glymphatic system"
"We develop a continuous mathematical model of population dynamics that describes the sequential emergence of new genotypes under limited resources. The framework models genotype density as a nonlinear flow in mutation space, combining transport driven by a time-dependent mutation rate with logistic growth and nonlocal competition. For the advection-reaction regime without reverse mutations, we derive analytical solutions using the method of characteristics and obtain explicit expressions for time-varying carrying capacities and mutation velocities. We analyze how decaying and accelerating mutation rates shape the saturation and propagation of population fronts through level-set geometry. When reverse mutations are included, the system becomes a quasilinear parabolic equation with diffusion in genotype space; numerical experiments show that backward mutation flows stabilize the dynamics and smooth the evolving fronts. The proposed model generalizes classical quasispecies and Crow-Kimura formulations by incorporating logistic regulation, variable mutation rates, and reversible transitions, offering a unified approach to evolutionary processes relevant to virology, bacterial adaptation, and tumor progression.",0,arxiv,Evrim,CC-BY/arXiv,Flow-Based Modelling of Population Dynamics with Consecutive Continuous Mutations
"In an emerging pandemic, policymakers need to make important decisions with limited information, for example choosing between a mitigation, suppression or elimination strategy. These strategies may require trade-offs to be made between the health impact of the pandemic and the economic costs of the interventions introduced in response. Mathematical models are a useful tool that can help understand the consequences of alternative policy options on the future dynamics and impact of the epidemic. Most models have focused on direct health impacts, neglecting the economic costs of control measures. Here, we introduce a model framework that captures both health and economic costs. We use this framework to compare the expected aggregate costs of mitigation, suppression and elimination strategies, across a range of different epidemiological and economic parameters. We find that for diseases with low severity, mitigation tends to be the most cost-effective option. For more severe diseases, suppression tends to be most cost effective if the basic reproduction number $R_0$ is relatively low, while elimination tends to be more cost-effective if $R_0$ is high. We use the example of New Zealand's elimination response to the Covid-19 pandemic in 2020 to anchor our framework to a real-world case study. We find that parameter estimates for Covid-19 in New Zealand put it close to or above the threshold at which elimination becomes more cost-effective than mitigation. We conclude that our proposed framework holds promise as a decision-support tool for future pandemic threats, although further work is needed to account for population heterogeneity and other factors relevant to decision-making.",0,arxiv,Evrim,CC-BY/arXiv,Joint economic and epidemiological modelling of alternative pandemic response strategies
"Bacteriophage-bacteria interactions are central to microbial ecology, influencing evolution, biogeochemical cycles, and pathogen behavior. Most theoretical models assume static environments and passive bacterial hosts, neglecting the joint effects of bacterial traits and environmental fluctuations on coexistence dynamics. This limitation hinders the prediction of microbial persistence in dynamic ecosystems such as soils and oceans.Using a minimal ordinary differential equation framework, we show that the bacterial growth rate and the phage adsorption rate collectively determine three possible ecological outcomes: phage extinction, stable coexistence, or oscillation-induced extinction. Specifically, we demonstrate that environmental fluctuations can suppress destructive oscillations through resonance, promoting coexistence where static models otherwise predict collapse. Counterintuitively, we find that lower bacterial growth rates are helpful in enhancing survival under high infection pressure, elucidating the observed post-infection growth reduction.Our studies reframe bacterial hosts as active builders of ecological dynamics and environmental variation as a potential stabilizing force. Our findings thus bridge a key theory-experiment gap and provide a foundational framework for predicting microbial responses to environmental stress, which might have potential implications for phage therapy, microbiome management, and climate-impacted community resilience.",0,arxiv,Evrim,CC-BY/arXiv,Frequency Locking to Environmental Forcing Suppresses Oscillatory Extinction in Phage-Bacteria Interactions
"Dogs exhibit an exceptional range of morphological diversity as a result of their long-term association with humans. Attempts to identify when dog morphological variation began to expand have been constrained by the limited number of Pleistocene specimens, the fragmentary nature of remains, and difficulties in distinguishing early dogs from wolves on the basis of skeletal morphology. In this study, we used three-dimensional geometric morphometrics to analyze the size and shape of 643 canid crania spanning the past 50,000 years. Our analyses show that a distinctive dog morphology first appeared at about 11,000 calibrated years before present, and substantial phenotypic diversity already existed in early Holocene dogs. Thus, this variation emerged many millennia before the intense human-mediated selection shaping modern dog breeds beginning in the 19th century.",0,arxiv,Evrim,CC-BY/arXiv,The emergence and diversification of dog morphology
"Community science observational datasets are useful in epidemiology and ecology for modeling species distributions, but the heterogeneous nature of the data presents significant challenges for standardization, data quality assurance and control, and workflow management. In this paper, we present a data workflow for cleaning and harmonizing multiple community science datasets, which we implement in a case study using eBird, iNaturalist, GBIF, and other datasets to model the impact of highly pathogenic avian influenza in populations of birds in the subantarctic. We predict population sizes for several species where the demographics are not known, and we present novel estimates for potential mortality rates from HPAI for those species, based on a novel aggregated dataset of mortality rates in the subantarctic.",0,arxiv,Evrim,CC-BY/arXiv,Harmonizing Community Science Datasets to Model Highly Pathogenic Avian Influenza (HPAI) in Birds in the Subantarctic
"We show that spatial extensions of many-species population dynamics models, such as the Lotka-Volterra model with random interactions we focus on in this work, generically exhibit scale-free correlation functions of population sizes in the limit of an infinite number of species. Using dynamical mean-field theory, we describe the many-species system in terms of single-species dynamics with demographic and environmental noises. We show that the single-species model features a random mass term, or equivalently a random space-time averaged growth rate, poising some species very close to extinction. This introduces a hierarchy of ever larger correlation times and lengths as the extinction threshold is approached. In turn, every species, even those far from extinction, are coupled to these near-critical fields which combine to make fluctuations of population sizes generically scale-free. We argue that these correlations are described by exponents derived from those of directed percolation in spatial dimension $d=3$, but not in lower dimensions.",0,arxiv,Evrim,CC-BY/arXiv,Self-organized criticality in complex model ecosystems
"We use evolutionary game theory to examine how conflict-averse centrism can facilitate authoritarian success in polarized political conflicts. Such conflicts are often asymmetric: authoritarian actors can employ norm-breaking or coercive tactics, while democratic resistance faces stronger normative constraints on acceptable behavior. Yet formal models typically treat sides symmetrically and rarely examine conflict-averse behavior. Drawing on empirical research on protest backlash, civility norms, and authoritarian resilience, we model these dynamics as a three-strategy evolutionary game. This framework yields two outcomes -- cyclic authoritarian resurgence through a heteroclinic cycle and a stable centrist--authoritarian coalition excluding resistance -- depending on confrontation responses. We demonstrate how an established dynamical framework with empirically grounded behavioral assumptions clarifies conditions under which conflict aversion can diminish the effectiveness of democratic resistance.",0,arxiv,Evrim,CC-BY/arXiv,How Conflict Aversion Can Enable Authoritarianism: An Evolutionary Dynamics Approach
"We present a computational framework that integrates functional-structural plant modeling (FSPM) with an evolutionary algorithm to optimize three-dimensional maize canopy architecture for enhanced light interception under high-density planting. The optimization revealed an emergent ideotype characterized by two distinct strategies: a vertically stratified leaf profile (steep, narrow upper leaves for penetration; broad, horizontal lower leaves for capture) and a radially tiled azimuthal arrangement that breaks the conventional distichous symmetry of maize to minimize self and mutual shading. Reverse ray-tracing simulations show that this architecture intercepts significantly more photosynthetically active radiation (PAR) than virtual canopies parameterized from high-performing field hybrids, with gains that generalize across multiple U.S. latitudes and planting densities. The optimized trait combinations align with characteristics of modern density-tolerant cultivars, supporting biological plausibility. Because recent gene editing advances enable more independent control of architectural traits, the designs identified here are increasingly feasible. By uncovering effective, non-intuitive trait configurations, our approach provides a scalable, predictive tool to guide breeding targets, improve light-use efficiency, and ultimately support sustainable yield gains.",0,arxiv,Evrim,CC-BY/arXiv,"Algorithmic design of ""smart canopy"" maize architectures that maximize light use efficiency"
"This study introduces a novel epidemiological model that expands upon the Kermack-McKendrick model by incorporating the age of infection and reinfection. By including infection age, we can classify participants, which enables a more targeted analysis within the modeling framework. The reinfection term addresses the real-world occurrences of secondary or recurrent viral infections. In the theoretical part, we apply the contraction mapping principle, the dominated convergence theorem, and the properties of Volterra integral equations to derive analytical expressions for the number of newly infected individuals denoted by $N(t)$. Then, we establish a Volterra integral equation for $N(t)$ and study its initial conditions for both a single cohort and multiple cohorts. From this equation, we derive a method for identifying the effective reproduction number, denoted as $\mathcal{R}(t)$. In the practical aspect, we present two distinct methods and separately apply them to analyze the daily new infection cases from the 2003 SARS outbreak in Singapore and the cumulative number of deaths from the COVID-19 epidemic in China. This work effectively bridges theoretical epidemiology and computational modeling, providing a robust framework for analyzing infection dynamics influenced by infection-age-structured transmission and reinfection mechanisms.",0,arxiv,Evrim,CC-BY/arXiv,The Effective Reproduction Number in the Kermack-McKendrick model with age of infection and reinfection
"Dormancy is a widespread adaptive strategy that enables populations to persist in fluctuating environments, yet how its benefits depend on the temporal structure of environmental variability remains unclear. We examine how dormancy interacts with environmental correlation times using a delayed-logistic model in which dormant individuals reactivate after a fixed lag while birth rates fluctuate under temporally correlated stochasticity. Numerical simulations and analytical calculations show that the combination of demographic memory and colored multiplicative noise generates a strongly non-monotonic dependence of fitness on dormancy duration, with three distinct performance regimes. Very short dormancy maximizes linear growth but amplifies fluctuations and extinction risk. Very long dormancy buffers environmental variability, greatly increasing mean extinction times despite slower growth. Strikingly, we find a broad band of intermediate dormancy durations that is maladaptive, simultaneously reducing both growth and persistence due to a mismatch between delay times and environmental autocorrelation. An evolutionary agent-based model confirms bistability between short- and long-dormancy strategies, which avoid intermediate lag times and evolve toward stable extremes. These results show that dormancy duration is not merely a life-history parameter but an adaptive mechanism tuned to environmental timescales, and that intermediate ""dangerous middle"" strategies can be inherently disfavored. More broadly, this work identifies a generic mechanism by which demographic delays interacting with correlated environmental variability produce a non-monotonic fitness landscape that selects for extreme timing strategies.",0,arxiv,Evrim,CC-BY/arXiv,Fluctuating Environments Favor Extreme Dormancy Strategies and Penalize Intermediate Ones
"The ongoing explosion of genome sequence data is transforming how we reconstruct and understand the histories of biological systems. Across biological scales, from individual cells to populations and species, trees-based models provide a common framework for representing ancestry. Once limited to species phylogenetics, ""tree thinking"" now extends deeply to population genomics and cell biology, revealing the genealogical structure of genetic and phenotypic variation within and across organisms. Recently, there have been great methodological and computational advances on tree-based methods, including methods for inferring ancestral recombination graphs in populations, phylogenetic frameworks for comparative genomics, and lineage-tracing techniques in developmental and cancer biology. Despite differences in data types and biological contexts, these approaches share core statistical and algorithmic challenges: efficiently inferring branching histories from genomic information, integrating temporal and spatial signals, and connecting genealogical structures to evolutionary and functional processes. Recognizing these shared foundations opens opportunities for cross-fertilization between fields that are traditionally studied in isolation. By examining how tree-based methods are applied across cellular, population, and species scales, we identify the conceptual parallels that unite them and the distinct challenges that each domain presents. These comparisons offer new perspectives that can inform algorithmic innovations and lead to more powerful inference strategies across the full spectrum of biological systems.",0,arxiv,Evrim,CC-BY/arXiv,"Tree Thinking in the Genomic Era: Unifying Models Across Cells, Populations, and Species"
"Approximately 1.4 Ga after life first appeared, atmospheric oxygen suddenly jumped by more than an order of magnitude over a 20-50 Ma period. The contrast between these two timescales does not seem to be due to any sudden, large amplitude change in external forcing. However, it could be due to processes intrinsic to the geobiological system itself, namely, positive feedback between atmospheric oxygen and photosynthetic bacteria: More oxygen leads to more photosynthesis, which leads to more oxygen, and so on. Already-published feedbacks include buildup of an ozone shield and nutrient production by oxidative weathering. The feedback proposed here is the 15-fold greater efficiency of aerobic vs anaerobic respiration and the tight coupling of respiration and photosynthesis inside the cell. As in the climate system, feedback leads to tipping points, where a rapid, large amplitude change in the state of the system occurs. For the geobiological system, the GOE is the tipping point, and the long buildup before the GOE is the gradual oxidation of the crust and ocean, due either to burial of organic matter, oxidation of volcanic gases, or escape of hydrogen to space. The feedback hypothesis is a framework for interpreting observations leading to the GOE.",0,arxiv,Evrim,CC-BY/arXiv,The Great Oxidation Event (GOE): Biogeochemical Feedback and Tipping Points
"We analyze a size-structured branching process in which individual cells grow exponentially according to a Feller square-root process and divide under general size-control mechanisms. We obtain exact expressions for the asymptotic population growth rate, the steady-state snapshot distribution of cell sizes, and the fluctuations of the total cell number. Our first result is that the population growth rate is exactly equal to the mean single-cell growth rate, for all noise strengths and for all division and size-regulation schemes that maintain size homeostasis. Thus square-root growth noise is neutral with respect to long-term fitness, in sharp contrast to models with size-independent stochastic growth rates. Second, we show that the steady-state population cell-size distribution is obtained from the deterministic inverse-square-law solution by a one-sided exponential convolution with kernel width set by the strength of growth fluctuations. Third, the mean-rescaled population size $N_t/\left\langle N_t\right\rangle$ converges to a stationary compound Poisson-exponential distribution that depends only on growth noise. This distribution, and hence the long-time shape of population-size fluctuations, is unchanged by division-size noise or asymmetric partitioning. These results identify Feller-type exponential growth with square-root noise as an exactly solvable benchmark for stochastic growth in size-controlled populations and provide concrete signatures that distinguish it from models with size-independent growth-rate noise.",0,arxiv,Evrim,CC-BY/arXiv,Exactly Solvable Population Model with Square-Root Growth Noise and Cell-Size Regulation
"In mathematical phylogenetics, evolutionary relationships are often represented by trees and networks. The latter are typically used whenever the relationships cannot be adequately described by a tree, which happens when so-called reticulate evolutionary events happen, such as horizontal gene transfer or hybridization. But as such events are known to be relatively rare for most species, evolution is sometimes thought of as a process that can be represented by a tree with some additional edges, i.e., with a network that is still ``somewhat treelike''. In this context, different versions of so-called tree-based networks have played a major role in recent phylogenetic literature. Yet, surprisingly little is known about their combinatorial and graph-theoretic properties. In our manuscript, we answer a recently published question concerning the colorability of a specific class of tree-based networks. In particular, we will investigate an even more general class of graphs and show their 3-colorability. This nicely links recent phylogenetic concepts with classical graph theory.",0,arxiv,Evrim,CC-BY/arXiv,Colorings of unrooted tree-based networks and related graphs
"Introduction: Raoellidae are small artiodactyls retrieved from the middle Eocene of Asia (ca. -47 Ma) and closely related to stem Cetacea. Morphological observations of their endocranial structures allow for outlining some of the early steps of the evolutionary history of the cetacean brain. The external features of the brain and associated sinuses of Raoellidae are so far only documented by the virtual reconstruction of the endocast based on specimens of the species Indohyus indirae. These specimens are however too deformed to fully access the external morphology, surface area, and volume measurements of the brain. Methods: We bring here new elements to the picture of the raoellid brain by an investigation of the internal structures of an exceptionally well-preserved cranium collected from the Kalakot area (Jammu and Kashmir, India) referred to the species Khirtharia inflata. Micro-CT scan investigation and virtual reconstruction of the endocast and associated sinuses of this specimen provide crucial additional data about the morphological diversity within Raoellidae as well as reliable linear, surfaces, and volumes measurements, allowing for quantitative studies. Results: We show that, like I. indirae, the brain of K. inflata exhibits a mosaic of features observed in earliest artiodactyls: a small neocortex with simple folding pattern, widely exposed midbrain, and relatively long cerebellum. But, like Indohyus, the brain of Khirtharia shows unique derived characters also observed in stem cetaceans: narrow elongated olfactory bulbs and peduncles, posterior location of the braincase in the cranium, and complex network of blood vessels around the cerebellum. The volume of the brain relative to body mass of K. inflata is markedly small when compared to other early artiodactyls. Conclusion: We show here that cetaceans that nowadays have the second biggest brain after humans derive from a group of animals that had a lower-than-average expected brain size. This is probably a side effect of the adaptation to aquatic life. Conversely, this very small brain size relative to body mass might be another line of evidence supporting the aquatic habits in raoellids.",0,arxiv,Evrim,CC-BY/arXiv,"The Endocranial Cast of Khirtharia (Artiodactyla, Raoellidae) Provides New Insights into the Earliest Evolution of the Cetacean Brain"
"In this paper, we consider an adaptive optimal control problem for an SIR/V epidemic model with human behavioral effects.We develop a model where effective management of infectious diseases are monitored by the means of non pharmaceutical interventions.This study develops an adaptive optimal control function within an SIR/V framework embedding a non cooperative game theoretic mechanism to capture the dynamic interplay between individual vaccination behavior and population level transmission. We derive analytical expression for the optimal control trajectory under resource constrain and heterogeneous susceptibility and we validate our model using numerical simulations,calibrated with the real world epidemic parameters. We find that for the adaptive optimal policy for a generally known SIR/V model depending on the game theoretic epidemic state leads to substantial reduction in expenses compared to non adaptive policies. Moreover, our results demonstrate that, adaptive strategies significantly outperform the static policies by achieving lower peak infections and faster epidemic extinctions while evolutionary game dynamics identify critical behavioral thresholds that drive strategy evolution and inform timely policy adaptation",0,arxiv,Evrim,CC-BY/arXiv,Investigating the effect of adaptive optimal control function in epidemic dynamics: predictions and strategy evolution based on SIR/V game theoretic framework
"Deamination has historically been important for authenticating ancient biomolecules. However, expanding paleogenomic datasets indicate that damage patterns are more influenced by burial hydrology and microstructural context than by molecular age or ancestry. Fossils interact with their environments differently: some form closed, water-restricted compartments that preserve minimally damaged endogenous biomolecules, whereas others serve as open molecular reservoirs in which infiltrated environmental biomolecules undergo extensive deamination from repeated water exposure. Reliance on deamination alone can therefore suppress endogenous signals and complicate the interpretation of exogenous sequences. By introducing the molecular sedimentation model for fossil biomolecules, this Perspective outlines a source tracing framework that integrates fossil microstructure, ecological reference sets, and species-specific fragments to enable more reliable molecular inference across diverse depositional environments.",0,arxiv,Evrim,CC-BY/arXiv,"Sedimentary models of fossil biomolecules, principles and methodological improvements"
"This review explores the integration of Artificial Intelligence into Horizon Scanning, focusing on identifying and responding to emerging threats and opportunities linked to Infectious Diseases. We examine how AI tools can enhance signal detection, data monitoring, scenario analysis, and decision support. We also address the risks associated with AI adoption and propose strategies for effective implementation and governance. The findings contribute to the growing body of Foresight literature by demonstrating the potential and limitations of AI in Public Health preparedness.",0,arxiv,Evrim,CC-BY/arXiv,Artificial Intelligence Applications in Horizon Scanning for Infectious Diseases
"Digital public health monitoring has long relied on data from major social media platforms. Twitter was once an indispensable resource for tracking disease outbreaks and public sentiment in real time. Researchers used Twitter to monitor everything from influenza spread to vaccine hesitancy, demonstrating that social media data can serve as an early-warning system for emerging health threats. However, recent shifts in the social media landscape have challenged this data-driven paradigm. Platform policy changes, exemplified by Twitter's withdrawal of free data access, now restrict the very data that fueled a decade of digital public health research. At the same time, advances in artificial intelligence, particularly large language models (LLMs), have dramatically expanded our capacity to analyze large-scale textual data across languages and contexts. This presents a paradox: we possess powerful new AI tools to extract insights from social media, but face dwindling access to the data. In this viewpoint, we examine how digital public health monitoring is navigating these countervailing trends. We discuss the rise of decentralized social networks like Mastodon and Bluesky as alternative data sources, weighing their openness and ethical alignment with research against their smaller scale and potential biases. Ultimately, we argue that digital public health surveillance must adapt by embracing new platforms and methodologies, focusing on common diseases and broad signals that remain detectable, while advocating for policies that preserve researchers' access to public data in privacy-respective ways.",0,arxiv,Evrim,CC-BY/arXiv,Decentralized Social Media and Artificial Intelligence in Digital Public Health Monitoring
"This study presents an agent-based model (ABM) developed to simulate staff and resident interactions within a synthetic aged care facility, capturing movement, task execution, and proximity-based contact events across three staff shifts and varying levels of resident care. Contacts were defined by spatial thresholds (1.5 m and 3 m) and cumulative duration, enabling the generation of detailed contact matrices. Simulation results showed that low and medium care residents experienced the highest frequency of interactions, particularly with staff on morning and afternoon shifts, while high care residents and night staff had substantially fewer contacts. Contact rates varied significantly by care level and shift, confirmed through Poisson-based regression modelling. Temporal analyses revealed clustering of high-risk contacts during structured daily routines, especially communal and care activities. An integrated airborne transmission module, seeded with a single infectious staff member, demonstrated that infection risk was highest during high-contact shifts and among medium care residents. Vaccination scenarios reduced predicted transmission by up to 68\%, with the greatest impact observed when both staff and residents were vaccinated. These findings highlight the importance of accounting for contact heterogeneity in aged care and demonstrate the utility of ABMs for evaluating targeted infection control strategies in high-risk, enclosed environments.",0,arxiv,Evrim,CC-BY/arXiv,Generating a Contact Matrix for Aged Care Settings in Australia: an agent-based model study
"Epiphytic communities offer an original framework to disentangle the contributions of environmental filters, biotic interactions and dispersal limitations to community structure at fine spatial scales. We determine here whether variations in light, microclimatic conditions and host tree size affect the variation in species composition and phylogenetic structure of epiphytic bryophyte communities, and hence, assess the contribution of environmental filtering, phylogenetic constraints and competition to community assembly.A canopy crane giving access to 1.1 ha of tropical rainforest in Yunnan (China) was employed to record hourly light and microclimatic conditions from 54 dataloggers and epiphytic bryophyte communities from 408 plots. Generalized Dissimilarity Modelling was implemented to analyse the relationship between taxonomic and phylogenetic turnover among epiphytic communities, host-tree characteristics and microclimatic variation.Within-tree vertical turnover of bryophyte communities was significantly about 30% higher than horizontal turnover among-trees. Thus, the sharp vertical variations in microclimatic conditions from tree base to canopy are more important than differences in age, reflecting the likelihood of colonization, area, and habitat conditions between young and old trees, in shaping the composition of epiphytic bryophyte communities.",0,arxiv,Evrim,CC-BY/arXiv,Microclimatic variation in tropical canopies: A glimpse into the processes of community assembly in epiphytic bryophyte communities
"Estimating divergence times from molecular sequence data is central to reconstructing the evolutionary history of lineages. Although Bayesian relaxed-clock methods provide a principled framework for incorporating fossil information, their dependence on repeated evaluations of the full phylogenetic likelihood makes them computationally demanding for large genomic datasets. Furthermore, because disagreements in divergence-time estimates often arise from uncertainty or error in fossil placement and prior specification, there is a need for methods that are both computationally efficient and robust to fossil-calibration uncertainty. In this study, we introduce fast and accurate alternatives based on the phylogenetic pairwise composite likelihood, presenting two adjusted pairwise likelihood (APW) formulations that employ asymptotic moment-matching weights to better approximate the behavior of the full likelihood within a Bayesian MCMC framework. Extensive simulations across diverse fossil-calibration scenarios show that APW methods produce node-age estimates comparable to those obtained from the full likelihood while offering greater robustness to fossil misplacement and prior misspecification, due to the reduced sensitivity of composite likelihoods to local calibration errors. Applied to a genome-scale dataset of modern birds, APW methods recover divergence time patterns consistent with recent studies, while reducing computational cost by more than an order of magnitude. Overall, our results demonstrate that adjusted pairwise likelihoods provide a calibration-robust and computationally efficient framework for Bayesian node dating, especially suited for large phylogenomic datasets and analyses in which fossil priors may be uncertain or imperfectly placed.",0,arxiv,Evrim,CC-BY/arXiv,Fast and Accurate Node-Age Estimation Under Fossil Calibration Uncertainty Using the Adjusted Pairwise Likelihood
"Inferring the phylogenetic relationships among a sample of organisms is a fundamental problem in modern biology. While distance-based hierarchical clustering algorithms achieved early success on this task, these have been supplanted by Bayesian and maximum likelihood search procedures based on complex models of molecular evolution. In this work we describe minimal neural network architectures that can approximate classic phylogenetic distance functions and the properties required to learn distances under a variety of molecular evolutionary models. In contrast to model-based inference (and recently proposed model-free convolutional and transformer networks), these architectures have a small computational footprint and are scalable to large numbers of taxa and molecular characters. The learned distance functions generalize well and, given an appropriate training dataset, achieve results comparable to state-of-the art inference methods.",0,arxiv,Evrim,CC-BY/arXiv,On the Approximation of Phylogenetic Distance Functions by Artificial Neural Networks
"Origins of life research investigates how life could emerge from prebiotic chemistry only. One possible explanation provides the RNA world hypothesis. It states that life could emerge from RNA strands only, storing and transferring biological information, as well as catalyzing reactions as ribozymes. Before this state could have emerged, however, the prebiotic world was probably a purely chemical pool of short RNA strands with random sequences and without biological function performing hybridization and dehybridization, as well as ligation and cleavage. In this context relevant questions are what are the conditions that allow longer RNA strands to be built and how can information carrying in RNA sequence emerge?   In order to investigate such RNA reactors, efficient simulations are needed because the space of possible RNA sequences increases exponentially with the length of the strands, as well as the number of reactions between two strands. In addition, simulations have to be compared to experimental data for validation and parameter calibration. Here, we present the MoRSAIK python package for sequence motif (or k-mer) reactor simulation, analysis and inference. It enables users to simulate RNA sequence motif dynamics in the mean field approximation as well as to infer the reaction parameters from data with Bayesian methods and to analyze results by computing observables and plotting. MoRSAIK simulates an RNA reactor by following the reactions and the concentrations of all strands inside up to a certain length (of four nucleotides by default). Longer strands are followed indirectly, by tracking the concentrations of their containing sequence motifs of that maximum length.",0,arxiv,Evrim,CC-BY/arXiv,"MoRSAIK: Sequence Motif Reactor Simulation, Analysis and Inference Kit in Python"
"We consider a nonlocal bistable reaction-diffusion equation, which serves as a model for a population structured by a phenotypic trait, subject to mutation, trait-dependent fitness, and nonlocal competition. Within this replicator-mutator framework, we further incorporate a ''pseudo-Allee effect'' so that the long time behavior (extinction vs. survival) depends on the size of the initial data. After proving the well-posedness of the associated Cauchy problem, we investigate its long-time behavior. We first show that small initial data lead to extinction. More surprisingly, we then prove that that extinction may also occur for too large initial data, in particular when selection is not strong enough. Finally, we exhibit situations where intermediate initial data lead to persistence, thereby revealing the existence of (at least) two thresholds. These results stand in sharp contrast with the behavior observed in local bistable equations.",0,arxiv,Evrim,CC-BY/arXiv,Existence of two thresholds in a bistable equation with nonlocal competition
"We present a spatially-extended system of chemical reactions exhibiting adaptation to time-dependent influxes of reactants. Here adaptation is defined as improved reproductive success, namely the ability of one of the many locally stable states available to the system to expand in space at the expense of other states. We find that adaptation can arise simply by environmental exposure to sequences of varying influxes. This adaptation is specific to the temporal sequence yet flexible enough to generalize to related sequences. It is enhanced through repeated exposure to the same environmental sequence, representing a form of learning, and through spatial interactions, enabling natural selection to act and representing a form of collective learning. Finally, adaptation benefits from a nearby adapted state, representing a form of teacher-guided learning. By combining environmental drives and reproduction within a stochastic reaction-diffusion dynamics framework, our model lays a foundation for a theory of adaptation grounded in physical principles.",0,arxiv,Evrim,CC-BY/arXiv,Adaptation to time-varying environments in a reaction-diffusion model
"Large animal groups -- bird flocks, fish schools, insect swarms -- are often assumed to form by gradual aggregation of sparsely distributed individuals. Using a mathematically precise framework based on time-varying directed interaction networks, we show that this widely held view is incomplete.   The theory demonstrates that large moving groups do not arise by slow accumulation; instead, they emerge through the rapid merging of multiple pre-existing subgroups that are simultaneously activated under high-density conditions. The key mechanism is topological: the long-term interaction structure of any moving group contains a single dominant strongly connected component (SCC). This dominant SCC determines the collective velocity -- both speed and direction -- of the entire group.   When two subgroups encounter one another, the trailing subgroup aligns with -- and ultimately inherits -- the velocity of the dominant SCC of the leading subgroup. Repeated merging events naturally generate large groups whose speed is predicted to be lower than the mean speed of the original subgroups. The same dynamics explain several universal empirical features: broad neighbour-distance distributions, directional asymmetry in neighbour selection, and the characteristic narrow-front, wide-rear geometry of real flocks.   The framework yields testable predictions for STARFLAG-style 3D datasets, offering a unified explanation for the formation, maintenance, and geometry of coordinated animal groups.",0,arxiv,Evrim,CC-BY/arXiv,"A Theoretical Framework for the Formation of Large Animal Groups: Topological Coordination, Subgroup Merging, and Velocity Inheritance"
"We investigate whether the structural connectivity of urban road networks helps explain dengue incidence in Recife, Brazil (2015--2024). For each neighborhood, we compute the average \emph{communicability curvature}, a graph-theoretic measure capturing the ability of a locality to influence others through multiple network paths. We integrate this metric into Negative Binomial models, fixed-effects regressions, SAR/SAC spatial models, and a hierarchical INLA/BYM2 specification. Across all frameworks, curvature is the strongest and most stable predictor of dengue risk. In the BYM2 model, the structured spatial component collapses ($Ï†\approx 0$), indicating that functional network connectivity explains nearly all spatial dependence typically attributed to adjacency-based CAR terms. The results show that dengue spread in Recife is driven less by geographic contiguity and more by network-mediated structural flows.",0,arxiv,Evrim,CC-BY/arXiv,Correlation-Weighted Communicability Curvature as a Structural Driver of Dengue Spread: A Bayesian Spatial Analysis of Recife (2015-2024)
"Mutualisms are key for structuring ecological communities, but they are sensitive to environmental change and fluctuations in population size. Consequently, how mutualisms achieve stability remains an open question in ecological theory. Motivated by previous results in competitive and predator-prey interactions, we hypothesize that self-organized pattern formation can act as a key stabilizing mechanism of mutualistic interactions. We test this hypothesis using a two-species reaction-diffusion model of a plant-pollinator system that incorporates non-local plant competition and local mutualistic interactions. We first perform a linear stability analysis to determine the conditions under which non-local competition can trigger vegetation pattern formation. We then compute the bifurcation diagrams for both spatial and homogeneous solutions and find that pattern formation enables coexistence at mutualistic strengths below the threshold required in well-mixed populations. This stability gain increases as environmental conditions worsen, because local maxima in vegetation density create the conditions for community persistence despite globally harsh conditions. Moreover, in the strong mutualism limit, the spatial system exhibits multistability between patterned and homogeneous solutions, creating alternative stable configurations that can buffer against fluctuations in population abundance. Spatial self-organization thus stabilizes mutualistic communities through spatial patterns, potentially driving plant-pollinator persistence in stressed environments, including arid ecosystems.",0,arxiv,Evrim,CC-BY/arXiv,Spatial self-organization stabilizes obligate mutualism through pattern formation
"Building upon the eco-evolutionary game dynamics framework established by Tilman et al., we investigate stochastic fluctuations in a two-strategy system incorporating environmental feedback mechanisms, where the payoff matrix exhibits population size dependence. We adopt a systematic approach which is the so-called $Î©$-expansion. When the stochastic factor is integrated, it is shown that the population size for each strategy fluctuates around the interior equilibrium of the macroscopic equations (corresponding to the deterministic model of the eco-evolutionary game) and its variance converges to a constant that is proportional to the environmental carrying capacity if the interior equilibrium is asymptotically stable. The simulation results demonstrate that the $Î©$ expansion provides a valid approximation, and the reliability of the aforementioned conclusions is verified. Therefore, analogous to Fudenberg and Harris' s stochastic replicator dynamics for infinite populations under external noise (\emph{J. Econ. Theory 57, 420-441}), the dynamic stability of the eco-evolutionary game can be extended to the stochastic regime when the environmental carrying capacity is sufficiently large.",0,arxiv,Evrim,CC-BY/arXiv,Stochastic fluctuations in an eco-evolutionary game dynamics with environmental feedbacks
"The genetic evolution of SARS-CoV-2 has caused recurring epidemic waves, understanding its global dispersal patterns is critical for effective surveillance. We developed the Site-based mutation dynamics - Equal Power Sampling (S-EPS) framework, a phylogenetic-free, bias-correcting framework for profiling viral source-sink dynamics. Applying S-EPS to 6.6 million SARS-CoV-2 genomes (March 2020 - June 2024) from 13 regions worldwide, we identified Africa and the Indian subcontinent as the predominant sources of key mutations. Southeast Asia serves as an early transmission hub, while Russia and South America mainly acted as sinks. Key mutations took longer to establish fitness in source regions than externally. Once an amino acid substitution on the receptor-binding domain reached 1% prevalence in major sources, there is an 80% probability it would spread elsewhere, with a 2-month median lead time (IQR: 1-4). Our findings underscore the importance of genetic surveillance, with S-EPS offering enhanced capability for monitoring emerging viral threats.",0,arxiv,Evrim,CC-BY/arXiv,A novel approach to profile global circulation pathway of SARS-CoV-2 variants by site-based mutation dynamics
"Phylogenetic trees and networks are graphs used to model evolutionary relationships, with trees representing strictly branching histories and networks allowing for events in which lineages merge, called reticulation events. While the question of data sufficiency has been studied extensively in the context of trees, it remains largely unexplored for networks. In this work we take a first step in this direction by establishing bounds on the amount of genomic data required to reconstruct binary level-$1$ semi-directed phylogenetic networks, which are binary networks in which reticulation events are indicated by directed edges, all other edges are undirected, and cycles are vertex-disjoint. For this class, methods have been developed recently that are statistically consistent. Roughly speaking, such methods are guaranteed to reconstruct the correct network assuming infinitely long genomic sequences. Here we consider the question whether networks from this class can be uniquely and correctly reconstructed from finite sequences. Specifically, we present an inference algorithm that takes as input genetic sequence data, and demonstrate that the sequence length sufficient to reconstruct the correct network with high probability, under the Cavender-Farris-Neyman model of evolution, scales logarithmically, polynomially, or polylogarithmically with the number of taxa, depending on the parameter regime. As part of our contribution, we also present novel inference rules for quartet data in the semi-directed phylogenetic network setting.",0,arxiv,Evrim,CC-BY/arXiv,Bounds on the sequence length sufficient to reconstruct level-1 phylogenetic networks
"Fixed tree topologies are widely used in phylodynamic analyses to reduce computational burden, yet the consequences of this assumption remain insufficiently understood. Here, we systematically assess the impact of various fixed-topology strategies on phylogenetic and phylodynamic parameter estimates across a diverse set of viral datasets. We compare fully Bayesian joint inference with fixed-topology strategies, including conditioning on maximum likelihood trees subsequently dated with LSD or TreeTime. Our analyses show that global parameters of the substitution and site models are largely robust to the fixed-topology assumption, whereas parameters that depend on the temporal structure of the tree, such as molecular clock rates, node ages, and demographic histories, can exhibit substantial biases. We do treat unconstrained Bayesian analyses as the reference, although we recognize that these too are model-based approximations. Nevertheless, our results highlight serious discordance associated with fixing the topology and underscore the need for faster, time-aware methods that simultaneously integrate topology and parameter estimation. These findings raise important questions about the balance between computational efficiency and inferential accuracy in phylodynamic studies.",0,arxiv,Evrim,CC-BY/arXiv,Assessing the Validity of the Fixed Tree Topology Assumption in Phylodynamic Inference
"Mechanistic models are essential tools across ecology, epidemiology, and the life sciences, but parameter inference remains challenging when likelihood functions are intractable. Approximate Bayesian Computation with Sequential Monte Carlo (ABC-SMC) offers a powerful likelihood-free alternative that requires only the ability to simulate data from mechanistic models. Despite its potential, many researchers remain hesitant to adopt these methods due to perceived complexity. This tutorial bridges that gap by providing a practical, example-driven introduction to ABC-SMC using Python. From predator-prey dynamics to hierarchical epidemic models, we illustrate by example how to implement, diagnose, and interpret ABC-SMC analyses. Each example builds intuition about when and why ABC-SMC works, how partial observability affects parameter identifiability, and how hierarchical structures naturally emerge in Bayesian frameworks. All code leverages PyMC's modern probabilistic programming interface, ensuring reproducibility and easy adaptation to new problems. The code its fully available for download at \href{https://github.com/mariocastro73/ABCSMC_pymc_by_example}{mariocastro73/ABCSMC\_pymc\_by\_example}",0,arxiv,Evrim,CC-BY/arXiv,Approximate Bayesian Computation Made Easy: A Practical Guide to ABC-SMC for Dynamical Systems with \texttt{pymc}
"We considered a model for an infectious disease outbreak, when the depletion of susceptible individuals is negligible, and assumed that individuals adapt their behavior according to the information they receive about new cases. In line with the information index approach, we supposed that individuals react to past information according to a memory kernel that is continuously distributed in the past. We analyzed equilibria and their stability, with analytical results for selected cases. Thanks to the recently developed pseudospectral approximation of delay equations, we studied numerically the long-term dynamics of the model for memory kernels defined by gamma distributions with a general non-integer shape parameter, extending the analysis beyond what is allowed by the linear chain trick. In agreement with previous studies, we showed that behavior adaptation alone can cause sustained waves of infections even in an outbreak scenario, and notably in the absence of other processes like demographic turnover, seasonality, or waning immunity. Our analysis gives a more general insight into how the period and peak of epidemic waves depend on the shape of the memory kernel and how the level of minimal contact impacts the stability of the behavior-induced positive equilibrium.",0,arxiv,Evrim,CC-BY/arXiv,Behavior-induced oscillations in epidemic outbreaks with distributed memory: beyond the linear chain trick using numerical methods
We present an algorithm for computing all evolutionarily stable strategies in nondegenerate normal-form games with three or more players.,0,arxiv,Evrim,CC-BY/arXiv,Computing Evolutionarily Stable Strategies in Multiplayer Games
"Clemmesen's hook refers to a commonly observed slowdown and rebound in breast cancer incidence around the age at menopause. It suggests a shift in the underlying carcinogenic dynamics, but the mechanistic basis remains poorly understood. Building on our previously developed Extended Multistage Clonal Expansion Tumor (MSCE-T) model, we perform a theoretical analysis to determine the conditions under which Clemmesen's hook would occur. Our results show that Clemmesen's hook can be quantitatively explained by time-specific changes in the proliferative and apoptotic balance of early-stage mutated cell populations, corresponding to the decline in progesterone levels and progesterone-driven proliferation due to reduced menstrual cycles preceding menopause, and changing dominant carcinogenic impact from alternative growth pathways post-menopause (e.g., adipose-derived growth signals). In contrast, variation in last-stage clonal dynamics cannot effectively reproduce the observed non-monotonic incidence pattern. Analytical results further demonstrate that midlife incidence dynamics corresponding to the hook are governed primarily by intrinsic proliferative processes rather than detection effects. Overall, this study provides a mechanistic and mathematical explanation for Clemmesen's hook and establishes a quantitative framework linking hormonal transitions during menopause to age-specific breast cancer incidence curve.",0,arxiv,Evrim,CC-BY/arXiv,Hormonal Regulation of Breast Cancer Incidence Dynamics: A Mathematical Analysis Explaining the Clemmesen's Hook
"Originating in evolutionary game theory, the class of ""zero-determinant"" strategies enables a player to unilaterally enforce linear payoff relationships in simple repeated games. An upshot of this kind of payoff constraint is that it can shape the incentives for the opponent in a predetermined way. An example is when a player ensures that the agents get equal payoffs. While extensively studied in infinite-horizon games, extensions to discounted games, nonlinear payoff relationships, richer strategic environments, and behaviors with long memory remain incompletely understood. In this paper, we provide necessary and sufficient conditions for a player to enforce arbitrary payoff relationships (linear or nonlinear), in expectation, in discounted games. These conditions characterize precisely which payoff relationships are enforceable using strategies of arbitrary complexity. Our main result establishes that any such enforceable relationship can actually be implemented using a simple two-point reactive learning strategy, which conditions on the opponent's most recent action and the player's own previous mixed action, using information from only one round into the past. For additive payoff constraints, we show that enforcement is possible using even simpler (reactive) strategies that depend solely on the opponent's last move. In other words, this tractable class is universal within expectation-enforcing strategies. As examples, we apply these results to characterize extortionate, generous, equalizer, and fair strategies in the iterated prisoner's dilemma, asymmetric donation game, nonlinear donation game, and the hawk-dove game, identifying precisely when each class of strategy is enforceable and with what minimum discount factor.",0,arxiv,Evrim,CC-BY/arXiv,Expectation-enforcing strategies for repeated games
"We look at the interaction of dispersal and environmental stochasticity in $n$-patch models. We are able to prove persistence and extinction results even in the setting when the dispersal rates are stochastic. As applications we look at Beverton-Holt and Hassell functional responses. We find explicit approximations for the total population size at stationarity when we look at slow and fast dispersal. In particular, we show that if dispersal is small then in the Beverton-Holt setting, if the carrying capacity is random, then environmental fluctuations are always detrimental and decrease the total population size. Instead, in the Hassell setting, if the inverse of the carrying capacity is made random, then environmental fluctuations always increase the population size. Fast dispersal can save populations from extinction and therefore increase the total population size. We also analyze a different type of environmental fluctuation which comes from switching environmental states according to a Markov chain and find explicit approximations when the switching is either fast or slow - in examples we are able to show that slow switching leads to a higher population size than fast switching.   Using and modifying some approximation results due to Cuello, we find expressions for the total population size in the $n=2$ patch setting when the growth rates, carrying capacities, and dispersal rates are influenced by random fluctuations. We find that there is a complicated interaction between the various terms and that the covariances between the various random parameters (growth rate, carrying capacity, dispersal rate) play a key role in whether we get an increase or a decrease in the total population size. Environmental fluctuations turn to sometimes be beneficial -- this show that not only dispersal, but also environmental stochasticity can lead to an increase in population size.",0,arxiv,Evrim,CC-BY/arXiv,Population size in stochastic multi-patch ecological models
"Collective vigilance describes how animals in groups benefit from the predator detection efforts of others. Empirical observations typically find either a many-eyes strategy with all (or many) group members maintaining a low level of individual vigilance, or a sentinel strategy with one (or a few) individuals maintaining a high level of individual vigilance while others do not. With a general analytical treatment that makes minimal assumptions, we show that these two strategies are alternate solutions to the same adaptive problem of balancing the costs of predation and vigilance. Which strategy is preferred depends on how costs scale with the level of individual vigilance: many-eyes strategies are preferred where costs of vigilance rise gently at low levels but become steeper at higher levels (convex; e.g. an open field); sentinel strategies are preferred where costs of vigilance rise steeply at low levels and then flatten out (concave; e.g. environments with vantage points). This same dichotomy emerges whether individuals act selfishly to optimise their own fitness or cooperatively to optimise group fitness. The model is extended to explain discrete behavioural switching between strategies and differential levels of vigilance such as edge effects.",0,arxiv,Evrim,CC-BY/arXiv,Many-Eyes and Sentinels in Selfish and Cooperative Groups
"Population heterogeneity is a key factor in epidemic dynamics, influencing both transmission and final epidemic size. While heterogeneity is often modeled through age structure, spatial location, or contact patterns, differences in host susceptibility have recently gained attention, particularly during the COVID-19 pandemic. Building on the framework of Diekmann and Inaba (Journal of Mathematical Biology, 2023), we focus on the special case of SEIR-models, which are widely used for influenza and other respiratory infections. We derive the model equations under two distinct assumptions linking susceptibility and infectiousness. Analytical results show that heterogeneity in susceptibility reduces the epidemic final size compared to homogeneous models with the same basic reproduction number $\Ro$. In the case of gamma-distributed susceptibility, we obtain stronger results on the epidemic final size. The resulting model captures population heterogeneity through a single parameter, which makes it practical for fitting epidemic data. We illustrate its use by applying it to seasonal influenza in Italy.",0,arxiv,Evrim,CC-BY/arXiv,SEIR models with host heterogeneity: theoretical aspects and applications to seasonal influenza dynamics
"We present ContagionRL, a Gymnasium-compatible reinforcement learning platform specifically designed for systematic reward engineering in spatial epidemic simulations. Unlike traditional agent-based models that rely on fixed behavioral rules, our platform enables rigorous evaluation of how reward function design affects learned survival strategies across diverse epidemic scenarios. ContagionRL integrates a spatial SIRS+D epidemiological model with configurable environmental parameters, allowing researchers to stress-test reward functions under varying conditions including limited observability, different movement patterns, and heterogeneous population dynamics. We evaluate five distinct reward designs, ranging from sparse survival bonuses to a novel potential field approach, across multiple RL algorithms (PPO, SAC, A2C). Through systematic ablation studies, we identify that directional guidance and explicit adherence incentives are critical components for robust policy learning. Our comprehensive evaluation across varying infection rates, grid sizes, visibility constraints, and movement patterns reveals that reward function choice dramatically impacts agent behavior and survival outcomes. Agents trained with our potential field reward consistently achieve superior performance, learning maximal adherence to non-pharmaceutical interventions while developing sophisticated spatial avoidance strategies. The platform's modular design enables systematic exploration of reward-behavior relationships, addressing a knowledge gap in models of this type where reward engineering has received limited attention. ContagionRL is an effective platform for studying adaptive behavioral responses in epidemic contexts and highlight the importance of reward design, information structure, and environmental predictability in learning.",0,arxiv,Evrim,CC-BY/arXiv,Reward Engineering for Spatial Epidemic Simulations: A Reinforcement Learning Platform for Individual Behavioral Learning
"Collapse Lineage Tree (CLTree) is a software tool that annotates, roots, and evaluates phylogenetic trees by using lineages. A recursive algorithm was designed to annotate the branches by the common taxonomic lineage of its descendants in a rooted tree. For an unrooted tree, it determines the root that best conforms to the taxonomic system based on the aforementioned lineage annotations. Based on the lineage annotations of notes, CLTree infers the monophyly of taxonomic units and quantifies the concordance between the phylogenetic tree and the taxonomic system base on Shannon entropy. The core algorithm of CLTree is highly efficient with linear complexity, capable of processing phylogenetic trees with 17,955 terminal nodes within one second. We believe that CLTree will serve as a powerful tool for study of evolution and taxonomy.",0,arxiv,Evrim,CC-BY/arXiv,"CLTree: A Tool for Annotating, Rooting, and Evaluating Phylogenetic Trees Leveraging Genomic Lineages"
"Understanding how cooperation emerges and persists is a central challenge in evolutionary game theory. Existing models often rely on restricted, hand-picked strategy sets, which can overlook critical behavioural pathways. A recent four-strategy framework showed that cheap talk can promote cooperation through local interactions, yet it remained unclear whether modelled strategies might alter these conclusions. Here, we extend this framework to the complete set of eight strategies that naturally arise from communication and decision-making rules. We show that incorporating the full strategy space dramatically changes the evolutionary landscape. Cooperation becomes both more robust and more versatile, driven by novel pathways absent in the restricted model. In particular, we uncover a previously overlooked mechanism in which suspicious cooperation catalyses a cyclic dynamic that sustains cooperation. Conversely, the assumed role of strategic defection in the biased model is fragile, acting mainly as a spoiler rather than a genuine evolutionary attractor. The complete model further reveals a rich spectrum of long-term behaviours, including stable coexistence among up to seven strategies and time-varying patterns of partial coexistence. These results demonstrate that the full strategy space unlocks hidden routes to cooperative behaviour and highlight the importance of comprehensive modelling when explaining the emergence of cooperation.",0,arxiv,Evrim,CC-BY/arXiv,Complete strategy spaces reveal hidden pathways to cooperation
"Real ecosystems are characterized by sparse and asymmetric interactions, posing a major challenge to theoretical analysis. We introduce a new method to study the generalized Lotka-Volterra model with stochastic dynamics on sparse graphs. By deriving local Fokker-Planck equations and employing a mean-field closure, we can efficiently compute stationary states for both symmetric and asymmetric interactions. We validate our approach by comparing the results with the direct integration of the dynamical equations and by reproducing known results and, for the first time, we map the phase diagram for sparse asymmetric networks. Our framework provides a versatile tool for exploring stability in realistic ecological communities and can be generalized to applications in different contexts, such as economics and evolutionary game theory.",0,arxiv,Evrim,CC-BY/arXiv,Local equations for the generalized Lotka-Volterra model on sparse asymmetric graphs
"We study a model for a mosquito-borne epidemic outbreak in which humans can adopt protective behaviour against vector bites depending on information on the past and present prevalence. Assuming that mosquitoes can also feed on other non-competent hosts (i.e. hosts that cannot infect others), we first review some results from the literature by showing that protective behaviour may either decrease or increase the value of the reproduction number of the epidemic depending on multiple elements. Then, assuming that changes in opinion occur much faster than the spread of the disease, we exploit an approach based on the Geometric Singular Perturbation Theory to reduce the two-group model to a model for a homogeneous host population. Then, we use the resulting model to investigate the effect of information-induced behavioural changes on the transient dynamics of the epidemic, discussing the case when protective measures induced an outbreak with a low attack rate. We illustrate how behavioural changes might either help in containing an epidemic outbreak or make the epidemic last longer, even triggering recurrent damped epidemic waves. We conclude with numerical simulations to illustrate our analytical results.",0,arxiv,Evrim,CC-BY/arXiv,A model for mosquito-borne epidemic outbreaks with information-dependent protective behaviour
"In mathematical phylogenetics, labeled histories describe the sequences by which sets of labeled lineages coalesce to a shared ancestral lineage. We study labeled histories for at-most-$r$-furcating trees. Consider a rooted leaf-labeled tree in which internal nodes each have $i$ offspring, and $i$ is permitted to range from 2 to $r$ across internal nodes, for a specified value of $r$. For labeled topologies with $n$ leaves, we enumerate the total number of labeled histories with at-most-$r$-furcation. We enumerate the labeled histories possessed by a specific at-most-$r$-furcating labeled topology. We then demonstrate that the maximally probable at-most-$r$-furcating unlabeled topology on $n \geq 2$ leaves -- the unlabeled topology whose labelings have the largest number of labeled histories -- is the maximally probable strictly bifurcating unlabeled topology on $n$ leaves. Finally, we enumerate labeled histories for at-most-$r$-furcating labeled topologies in a setting that permits simultaneous branchings. We similarly reduce the problem of identifying the maximally probable at-most-$r$-furcating unlabeled topology on $n \geq 2$ leaves, allowing simultaneity, to that of identifying the maximally probable strictly bifurcating unlabeled topology on $n$ leaves, with simultaneity; we conjecture the shape of this bifurcating unlabeled topology. The computations contribute to the study of multifurcation, which arises in various biological processes, and they connect to analogous mathematical settings involving precedence-constrained scheduling.",0,arxiv,Evrim,CC-BY/arXiv,Labeled histories and maximally probable labeled topologies with multifurcation
"An age structured mathematical model with time dependent parameters is developed to investigate the dynamics of dengue transmission. Its properties are thoroughly analyzed in the first part of this work, as for example its disease free steady state, the corresponding effective reproduction numbers, its basic reproduction number (obtained via the Euler and Lotka equation and the next generation matrix approach). We also provide formulas for the time-varying effective reproduction number, and draw relations with the instantaneous growth rate. In the second part, we apply this model to Brazil and use weekly time series data from this country. Various medical parameters are firstly evaluated from these data, and an extensive numerical simulations for the period 2021 to 2024 is then carried out. Estimation of the transmission rates are derived both from epidemiological data and from environmental data such as temperature and humidity. The time-varying effective reproduction numbers are then estimated on these data, following the theoretical investigations performed in the first part. The sensitive parameters that significantly affect the model dynamics are presented graphically. Model predictions for following year by using different transmission rates are finally presented. Our findings show the importance of population age distribution, vector population dynamics, and climate, contributing to a deeper understanding of dengue transmission dynamics in Brazil.",0,arxiv,Evrim,CC-BY/arXiv,"Age-structured model of dengue transmission dynamics with time-varying parameters, and its application to Brazil"
"We present a new class of models for assessing the cell dynamics characterising muscular dystrophies. The proposed approach comprises a system of integro-differential equations for the statistical distributions, over a large patient cohort, of the densities of muscle fibers and immune cells implicated in muscle inflammation, degeneration, and regeneration, which underpin disease development. Considering an appropriately scaled version of this model, we formally derive, as the corresponding mean-field limit, a system of Fokker-Planck equations, from which we subsequently derive, as a macroscopic model counterpart, a system of differential equations for the mean densities of muscle and immune cells in the cohort of patients and the related variances. Then, we study long-time asymptotics for the mean-field model by determining the quasi-equilibrium cell distribution functions, which are in the form of probability density functions of inverse Gamma distributions, and proving the long-time convergence to such quasi-equilibrium distributions. The analytical results obtained are illustrated by means of a sample of results of numerical simulations. The modeling approach presented here has the potential to offer new insights into the balance between degeneration and regeneration mechanisms in the progression of muscular dystrophies, and provides a basis for future extensions, including the modeling of therapeutic interventions.",0,arxiv,Evrim,CC-BY/arXiv,Kinetic and mean-field modeling of muscular dystrophies
"Consider a population of organisms that harvest free energy from their environment to reproduce. This paper shows that if the organisms' reproductive rates are proportional to the amount of physical free energy that they can convert into reproductive work, then the implicit probabilities that the organisms assign to environmental states are updated according to Bayes' rule.",0,arxiv,Evrim,CC-BY/arXiv,Thermodynamics + Natural Selection = Bayesian Inference
"In temperate regions, respiratory virus epidemics recur on a yearly basis, primarily during the winter season. This is believed to be induced by seasonal forcing, where the rate at which the virus can be transmitted varies cyclically across the course of each year. Seasonal epidemics can place substantial burden upon the healthcare system, with large numbers of infections and hospitalisations occurring across a short time period. However, the interactions between seasonal forcing and the factors necessary for epidemic resurgence - such as waning immunity, antigenic variation or demography - remain poorly understood. In this manuscript, we examine how the dynamics of antibody waning and antigenic variation can shape the seasonal recurrence of epidemics. We develop a susceptible-infectious-susceptible (SIS) immuno-epidemiological model of respiratory virus spread, where the susceptible population is stratified by their antibody level against the currently circulating strain of the virus, with this decaying as both antibody waning and antigenic drift occur. In the absence of seasonal forcing, we demonstrate the existence of two Hopf bifurcations over the effective antibody decay rate, with associated periodic model solutions. When seasonal forcing is introduced, we identify complex interactions between the strength of forcing and the effective antibody decay rate, yielding myriad dynamics including multi-year periodicity, quasiperiodicity and chaos. The timing and magnitude of seasonal epidemics is highly sensitive to this interaction, with the distribution of infection timing (by time of year) varying substantially across the parameter space. Finally, we show that seasonal forcing can produce resonant amplification (or damping) resulting in a cumulative infection burdens that is greater (or lesser) than would otherwise be observed.",0,arxiv,Evrim,CC-BY/arXiv,The role of antibody-mediated immunity in shaping the seasonality of respiratory viruses
"Benthic habitat is challenging due to the environmental complexity of the seafloor, technological limitations, and elevated operational costs, especially in under-explored regions. This generates knowledge gaps for the sustainable management of hydrobiological resources and their nexus with society. We developed ECOSAIC (Ecological Compression via Orthogonal Specialized Autoencoders for Interpretable Classification), an Artificial Intelligence framework for automatic classification of benthic habitats through interpretable latent representations using a customizable autoencoder. ECOSAIC compresses n-dimensional feature space by optimizing specialization and orthogonality between domain-informed features. We employed two domain-informed categories: biogeochemical and hydrogeomorphological, that together integrate biological, physicochemical, hydrological and geomorphological, features, whose constraints on habitats have been recognized in ecology for a century. We applied the model to the Colombian Pacific Ocean and the results revealed 16 benthic habitats, expanding from mangroves to deep rocky areas up to 1000 m depth. The candidate habitats exhibited a strong correspondence between their environmental constraints, represented in latent space, and their expected species composition. This correspondence reflected meaningful ecological associations rather than purely statistical correlations, where the habitat's environmental offerings align semantically with the species' requirements. This approach could improve the management and conservation of benthic habitats, facilitating the development of functional maps that support marine planning, biodiversity conservation and fish stock assessment. We also hope it provides new insights into how ecological principles can inform AI frameworks, particularly given the substantial data limitations that characterize ecological research.",0,arxiv,Evrim,CC-BY/arXiv,An Ecologically-Informed Deep Learning Framework for Interpretable and Validatable Habitat Mapping
"Trained on massive cross-species DNA corpora, DNA large language models (LLMs) learn the fundamental ""grammar"" and evolutionary patterns of genomic sequences. This makes them powerful priors for DNA sequence modeling, particularly over long ranges. However, two major constraints hinder their use in practice: the quadratic computational cost of self-attention and the growing memory required for key-value (KV) caches during autoregressive decoding. These constraints force the use of heuristics such as fixed-window truncation or sliding windows, which compromise fidelity on ultra-long sequences by discarding distant information. We introduce FOCUS (Feature-Oriented Compression for Ultra-long Self-attention), a progressive context-compression module that can be plugged into pretrained DNA LLMs. FOCUS combines the established k-mer representation in genomics with learnable hierarchical compression: it inserts summary tokens at k-mer granularity and progressively compresses attention key and value activations across multiple Transformer layers, retaining only the summary KV states across windows while discarding ordinary-token KV. A shared-boundary windowing scheme yields a stationary cross-window interface that propagates long-range information with minimal loss. We validate FOCUS on an Evo-2-based DNA LLM fine-tuned on GRCh38 chromosome 1 with self-supervised training and randomized compression schedules to promote robustness across compression ratios. On held-out human chromosomes, FOCUS achieves near-lossless fidelity: compressing a 1 kb context into only 10 summary tokens (about 100x) shifts the average per-nucleotide probability by only about 0.0004. Compared to a baseline without compression, FOCUS reduces KV-cache memory and converts effective inference scaling from O(N^2) to near-linear O(N), enabling about 100x longer inference windows on commodity GPUs with near-lossless fidelity.",0,arxiv,Evrim,CC-BY/arXiv,Near-Lossless Model Compression Enables Longer Context Inference in DNA Large Language Models
"In this work, we study the finite-population behaviour of the Reed-Frost epidemic model. Our analysis relies on the exact expression for the final epidemic size, replaced by Monte Carlo simulations in cases where the exact formula becomes numerically unstable. When the initial reproduction number is greater than a critical threshold, the distribution of the final size becomes bimodal. We therefore define the probabilities of small and large outbreaks, providing an intuitive answer to the question posed in the title through simple arguments based on the geometric distribution. Finally, an agent-based simulation confirms that the Reed-Frost model offers a good approximation in the case of the COVID-19 outbreak.",0,arxiv,Evrim,CC-BY/arXiv,How many outbreaks before an epidemic?
"In this work, we integrate theoretical modeling, molecular simulation, and empirical analysis to identify and characterize evolutionary hysteresis. We first show how epistatic interactions create bistable fitness landscapes and structural hysteresis in a two-locus Wright-Fisher model, revealing two distinct hysteresis regimes under cyclic and noisy selection. Notably, an epistatically constrained population achieves maximal average fitness at an intermediate level of environmental stochasticity. We then extend this framework to more complex systems, demonstrating robust hysteresis loops in both a disordered multi-locus model and in biophysically realistic simulation of protein structural flexibility. Finally, we present direct empirical evidence of evolutionary hysteresis. By analyzing two decades of metagenomic time-series data from freshwater C. Nanopelagicaceae experiencing strong seasonal temperature cycles, we find that approximately 65% of seasonally oscillating alleles exhibit statistically significant hysteresis. Together, these results establish hysteresis as a general, measurable feature of evolution and a potential probe of complex fitness landscapes.",0,arxiv,Evrim,CC-BY/arXiv,Evolutionary Hysteresis: Cycling about in a Rugged Landscape
"Ecological processes depend on the flow and balance of essential elements such as carbon (C) and phosphorus (P), and changes in these elements can cause adverse effects to ecosystems. The theory of Ecological Stoichiometry offers a conceptual framework to investigate the impact of elemental imbalances on structured populations while simultaneously considering how ecological structures regulate nutrient cycling and ecosystem processes. While there have been significant advances in the development of stoichiometric food web models, these efforts often consider a homogeneous population and neglect stage-structure. The development of stage-structured population models has significantly contributed to understanding energy flow and population dynamics of ecological systems. However, stage structure models fail to consider food quality in addition to food quantity. We develop a stoichiometric stage-structure producer-grazer model that considers co-limitation of nutrients, and parameterize the model for an algae-Daphnia food chain. Our findings emphasize the impact of stoichiometric constraints on structured population dynamics. By incorporating both food quantity and quality into maturation rates, we demonstrate how stage-structured dynamics can influence outcomes in variable environments. Stage-specific parameters, such as juvenile growth and ingestion rates can drive shifts in equilibria, limit cycles, and bifurcation points. These effects are especially significant in high-light environments where nutrient limitations are most pronounced.",0,arxiv,Evrim,CC-BY/arXiv,Stoichiometric ontogenetic development influences population dynamics: Stage-structured model under nutrient co-limitations
"We investigate stochastic predator-prey dynamics and their spatial phase synchronization using the Rosenzweig-MacArthur model coupled across multiple patches. Combining stochastic simulations based on the Gillespie algorithm with analytical methods inspired by the XY model, we uncover fundamental mechanisms through which demographic noise and dispersal shape synchronization and phase transitions. This study offers a theoretical foundation for understanding and managing large-scale ecological synchrony and ecosystem resilience.",0,arxiv,Evrim,CC-BY/arXiv,Modeling Spatial Synchronization of Predator-Prey Oscillations via the XY Model under Demographic Stochasticity and Migration
"In this paper we study the dynamics of stochastic microorganism flocculation models. Given the strong influence of environmental and seasonal fluctuations that are present in these models, we propose a stochastic model that includes multiple layers of stochasticity, from small Brownian fluctuations, to possibly large changes due to environmental `shifts'. We are able to give a full classification of the asymptotic behavior of these models. New techniques had to be developed to prove the persistence and extinction of the process as the system is not in Kolmogorov form and, as a result, the analysis is significantly more involved.",0,arxiv,Evrim,CC-BY/arXiv,Dynamics of stochastic microorganism flocculation models
"This paper considers large-scale vaccination campaigns, a major platform for vaccine access in a lot of the world, as a recapture estimate of the target population marked by routine immunization. Framing the campaign as a measurement, we learn about its properties, including the campaign's coverage of the target population and some implied sampling properties of post-campaign coverage surveys (PCCSs), the current gold-standard in implementation quality measurement.   We develop this idea in the context of the 2023 measles campaign in Kano State, Nigeria, where we have detailed implementation data collected by vaccination teams involved in that effort. Looking specifically at the teams' tally sheets, the daily records of who they vaccinated, we find significant discrepancies between the recapture estimates and those from the corresponding PCCS. Exploring a variety of bias models applied to both the tally sheets and the PCCS helps clarify how anecdotal issues from the field relate to this discrepancy.   Overall, we find that the tally sheets, despite being an unorthodox population sample, provide a tractable perspective on implementation and measurement, one that's in principle available nearly instantly and at high resolution for any vaccination touchpoint.",0,arxiv,Evrim,CC-BY/arXiv,A mark and recapture perspective on vaccination touchpoints
"Antimicrobial resistance (AMR) poses a mounting global health crisis, requiring rapid and reliable prediction frameworks that capture its complex evolutionary dynamics. Traditional antimicrobial susceptibility testing (AST), while accurate, remains laborious and time-consuming, limiting its clinical scalability. Existing computational approaches, primarily reliant on single nucleotide polymorphism (SNP)-based analysis, fail to account for evolutionary drivers such as horizontal gene transfer (HGT) and genome-level interactions.   This study introduces a novel Evolutionary Mixture of Experts (Evo-MoE) framework that integrates genomic sequence analysis, machine learning, and evolutionary algorithms to model and predict AMR evolution. A Mixture of Experts model, trained on labeled genomic data for multiple antibiotics, serves as the predictive core, estimating the likelihood of resistance for each genome. This model is embedded as a fitness function within a Genetic Algorithm designed to simulate AMR development across generations. Each genome, encoded as an individual in the population, undergoes mutation, crossover, and selection guided by predicted resistance probabilities.   The resulting evolutionary trajectories reveal dynamic pathways of resistance acquisition, offering mechanistic insights into genomic evolution under selective antibiotic pressure. Sensitivity analysis of mutation rates and selection pressures demonstrates the model's robustness and biological plausibility. Validation against curated AMR databases and literature evidence further substantiates the framework's predictive fidelity.   This integrative approach bridges genomic prediction and evolutionary simulation, offering a powerful tool for understanding and anticipating AMR dynamics, and potentially guiding rational antibiotic design and policy interventions.",0,arxiv,Evrim,CC-BY/arXiv,AMR-MoEGA: Antimicrobial Resistance Prediction using Mixture of Experts and Genetic Algorithms
"A correlation between karyotype diversity and species richness was first observed in mammals in 1980, and subsequently confirmed after controlling for phylogenetic signal. The correlation was attributed to submicroscopic factors, presumably operating at the level of the genome. At the same time, an unexpected association between mutation rates and substitution rates has been observed in all eukaryotes so far examined. One hypothesis to explain the latter observation proposed that neutral mutation (dS) and non-neutral (dN) substitution rates in gene codons co-vary according to genomic position, or location in the genome. Later, it was found that mutation and substitution rates in eukaryotes increase with DNA replication timing during the Synthetic phase, or S phase, of the cell cycle. In 1991, Motoo Kimura proposed a molecular theory of non-adaptive radiation (NAR). Accordingly, genetic drift plays a significant role in speciation, albeit in parallel to and in conjunction with natural selection. The following will examine the contribution of DNA replication timing and DNA repair factors to the relationship between species richness and karyotype diversity, and by extension, to the large variation in species richness across the mammalian Tree of Life.",0,arxiv,Evrim,CC-BY/arXiv,"DNA Replication Timing, Genome Stability and Non-adaptive Radiation"
"We study here the social network generated by the asynchronous visits, to a fixed set of sites, of mobile agents modelled as independent random walks on the plane lattice. The social network is constructed by assuming that a group of agents are associated if they have visited the same set of sites within a finite time interval. This construction is an instance of a random intersection graph, and has been used in the literature to study association networks in a number of animal species. We characterize the mathematical structure of these networks, which we view as one-mode projections of suitable bipartite graphs or, equivalently, as 2-sections of the corresponding hypergraphs. We determine analytically the probability distribution of the random bipartite graphs and hypergraphs associated to this construction, and suggest that association networks generated by the use of common resources are better described by hypergraphs rather than simple projected graphs, that miss important information regarding the actual associations among the agents.",0,arxiv,Evrim,CC-BY/arXiv,Animal social networks as intersections graphs of random walks
"Theoretical ecologists have long leveraged empirical data in various forms to advance ecology. Recently increased volumes and access to ecological data present an expanding set of opportunities for theoreticians to inform model development, framing, and interpretation. Whereas statisticians have collective guidance on best practices for data use, theoreticians might lack formal education on how to integrate diverse types of data into a single ecological model. As a group of predominantly early-career theoretical ecologists, we have developed guiding principles and practical tips to support theoretical ecologists in synthesizing multiple types of data at different phases of the modeling process. Our rules fall into three overarching themes: iteration in the data-model integration process, leveraging multiple sources of data), and understanding uncertainty. Across these rules, we emphasize that the data-model integration requires transparent, justifiable, and defensible communication of modeling choices to support readers in appropriately contextualizing the model and its implications.",0,arxiv,Evrim,CC-BY/arXiv,10 simple rules for data-model integration in theoretical ecology
"Stochastic chemical reaction or population dynamics in finite systems often terminates in an absorbing state. Yet in large spatially extended systems, the time to reach species extinction (or fixation) becomes exceedingly long. Tuning control parameters may diminish the survival probability, rendering species coexistence susceptible to stochastic extinction events. In inhomogeneous settings, where a vulnerable subsystem is diffusively coupled to an adjacent stable patch, the former is reanimated through continuous influx from the interfaces, provided the absorbing region sustains spreading activity fronts. We demonstrate this generic elimination of finite-size extinction instabilities via immigration flux in predator-prey, epidemic spreading, and cyclic competition models.",0,arxiv,Evrim,CC-BY/arXiv,Invading activity fronts stabilize excitable systems against stochastic extinction
"In this work we study an age-structured chemostat model with a renewal boundary condition and a coupled substrate equation. The model is nonlinear and consists of a hyperbolic partial differential equation and an ordinary differential equation with nonlinear, nonlocal terms appearing both in the ordinary differential equation and the boundary condition. Both differential equations contain a non-negative control input, while the states of the model are required to be positive. Under an appropriate weak solution framework, we determine the state space and the input space for this model. We prove global existence and uniqueness of solutions for all admissible initial conditions and all allowable control inputs. To this purpose we employ a combination of Banach's fixed-point theorem with implicit solution formulas and useful solution estimates. Finally, we show that the age-structured chemostat model gives a well-defined control system on a metric space.",0,arxiv,Evrim,CC-BY/arXiv,The Age-Structured Chemostat with Substrate Dynamics as a Control System
"Given an evolutionary model, such as Wright--Fisher (WF) or Moran, the n-coalescent problem consists of going backward in time to find for example the time to the most recent common ancestor (MRCA) and the topology of the tree. In the literature, this problem is tackled mostly by computing directly the random variable t, time to reach the MRCA. I show here that by shifting the focus from the random variable t to the joined variable (n,t), where n is the number of ancestors at time t, the problem is greatly simplified. Indeed, P(n,t), the probability of this variable, obeys a simpler master equation that can be solved in a straightforward way for the most general model. This probability can then be used to compute relevant information of the n-coalescent, for both random variables $t_{n}$ (random time to reach a given state n) and $n_{t}$ (random number of ancestors at a given time t). The cumulative distribution function for $t_{1}$ for example is $P(1,t)$. I give in this article the general solution for continuous time models such as Moran and discrete time ones such as WF.",0,arxiv,Evrim,CC-BY/arXiv,A master equation approach to the n-coalescent problem
"We study an infection-age structured epidemic model in which both the infectivity and the rate of loss of immunity depend on the time-since-infection. The model can be equivalently viewed as a nonlinear renewal equation for the incidence of infection or as a partial differential equation for the density of infected individuals. We explicitly consider gamma, rather than Erlang, distributed durations of infection using a combination of ODE approximations and numerical bifurcation methods. We show that the shape of this distribution strongly influences stability of the endemic equilibrium, even when the basic reproduction number $R_0$ and the mean duration of infectiousness are fixed. Moreover, we establish the existence of regions of bistability, where a stable endemic equilibrium coexists with a stable periodic orbit. To our knowledge, this provides the first example of bistability in infection-age structured models with waning immunity alone. Finally, we show how common compartmental modelling approaches, which impose implicit assumptions on the distribution of the duration of infection, can lead to spurious dynamical outcomes. Taken together, our analysis underscores the crucial role of distributional structure in epidemic modelling and provides new insights into the rich dynamics of infection-age structured SIS/SIRS models.",0,arxiv,Evrim,CC-BY/arXiv,Genuine and spurious bistability in a simple epidemic model with waning immunity
"For many taxonomic groups, online biodiversity portals used by naturalists and citizen scientists constitute the primary source of distributional information. Over the last decade, site-occupancy models have been advanced as a promising framework to analyse such loosely structured, opportunistically collected datasets. Current approaches often ignore important aspects of the detection process and do not fully capitalise on the information present in these datasets, leaving opportunities for fine-grained spatiotemporal backcasting untouched. We propose a flexible Bayesian spatiotemporal site-occupancy model that aims to mimic the data-generating process that underlies common citizen science datasets sourced from public biodiversity portals, and yields rich biological output. We illustrate the use of the model to a dataset containing over 3M butterfly records in Belgium, collected through the citizen science data portal Observations.be. We show that the proposed approach enables retrospective predictions on the occupancy of species through time and space at high resolution, as well as inference on inter-annual distributional trends, range dynamics, habitat preferences, phenological patterns, detection patterns and observer heterogeneity. The proposed model can be used to increase the value of opportunistically collected data by naturalists and citizen scientists, and can aid the understanding of spatiotemporal dynamics of species for which rigorously collected data are absent or too costly to collect.",0,arxiv,Evrim,CC-BY/arXiv,Backcasting biodiversity at high spatiotemporal resolution using flexible site-occupancy models for opportunistically sampled citizen science data
"Species-rich ecosystems often exhibit multiple stable states with distinct species compositions. Yet, the factors determining the likelihood of each state's occurrence remain poorly understood. Here, we characterize and explain the landscape of stable states in the random Generalized Lotka-Volterra (GLV) model, in which multistability is widespread. We find that the same pool of species with random initial abundances can result in different stable states, whose likelihoods typically differ by orders of magnitude. A state's likelihood increases sharply with its total biomass, or inverse self-inhibition. We develop a simplified model to predict and explain this behavior, by coarse-graining ecological interactions so that each stable state behaves as a unit. In this setting, we can accurately predict the entire landscape of stable states using only two macroscopic properties: the biomass of each state and species diversity. Our theory also provides insight into the biomass-likelihood relationship: High-biomass states have low self-inhibition and thus grow faster, outcompete others, and become much more likely. These results reveal emergent self-inhibition as a fundamental organizing principle for the attractor landscape of complex ecosystems - and provide a path to predict ecosystem outcomes without knowing microscopic interactions.",0,arxiv,Evrim,CC-BY/arXiv,Emergent self-inhibition governs the landscape of stable states in complex ecosystems
"A novel mathematical framework is proposed to describe the ecological and evolutionary consequences of consumer--resource interactions. Both the consumer and resource are assumed to consist of several (sub)species, which interact between themselves in addition to incorporating the deleterious effects of the consumer on the resource. Separating the ecological and evolutionary time scales, we allow our mathematical model to evolve, with the evolutionary steps chosen according to the (divergent) objective functions of the consumer and the resource. Numerical simulations show that the model, along with the expected outcomes of either consumer or resource winning the evolutionary battle, is capable of producing also the (quasi)stationary state of consumer--resource coexistence with monotone growth of both the consumer and resource fitnesses. Such stable states highlight the importance of the intra-population interactions, which, despite the opposite evolutionary goals of the consumer and the resource, lead to long term ecological stability.",0,arxiv,Evrim,CC-BY/arXiv,A mathematical framework of consumer-resource dynamics: How to incorporate interactions between interactions in evolutionary process
"Growth is a multi-layered phenomenon in human societies, composed of socioeconomic and demographic change at many different scales. Yet, standard macroeconomic indicators average over most of these processes, blurring the spatial and hierarchical heterogeneity driving people's choices and experiences. To address this gap, we introduce here a framework based on the Price equation to decompose aggregate growth exactly into endogenous and selection effects across nested spatial scales. We illustrate this approach with population and income data from the Chicago metropolitan area (2014-2019) and show that both growth rates and spatial selection effects are most intense at local levels, fat-tailed and spatially correlated. We also find that selection, defined as the covariance between prevailing income and relative population change, is concentrated in few spatial units and exhibits scaling behavior when grouped by county. Despite the intensity of local sorting, selection effects largely cancel in the aggregate, implying that fast heterogeneous micro-dynamics can yield deceptively stable macro-trends. By treating local spatial units (neighborhoods) as evolving subpopulations under selection, we demonstrate how methods from complex systems provide new tools to classify residential selection processes, such as abandonment and gentrification, in an urban sociological framework. This approach is general and applies to any other nested economic systems such as networks of production, occupations, or innovation enabling a new mechanistic understanding of compositional change and growth across scales of organization.",0,arxiv,Evrim,CC-BY/arXiv,Spatial Selection and the Multiscale Dynamics of Urban Change
"Waning immunity and reinfection are critical features of many infectious diseases, but epidemiological models often fail to capture the intricate interaction between an individual's history of immunity and their current infection status; when they do, the approach is usually overly simplistic. We develop a novel dual-age structured model that simultaneously tracks immunity age (time since the last recovery from infection) and infection age (time since infection) to analyze epidemic dynamics under conditions of waning immunity and reinfection. The model is formulated as a system of age-structured partial differential equations that describe susceptible and infected populations stratified by both immunity and infection ages. We derive basic reproduction numbers associated with the model and numerically solve the system using a second-order Runge-Kutta scheme along the characteristic lines. We further extend the model to explore vaccination interventions, specifically targeting individuals according to their immunity age. Numerical results reveal that higher contact rates produce larger amplitude oscillations with longer interepidemic periods. The relationship between initial infection levels and long-term epidemic behavior is nonmonotonic. Vaccination efficiency depends critically on the viral load profile across immunity and infection age, with more pronounced viral load distributions requiring higher vaccination rates for disease elimination. Most efficient vaccination strategies begin with intermediate immunity ages rather than targeting only fully susceptible individuals. The structured dual-age framework provides a flexible approach to analyzing the dynamics of reinfection and evaluating targeted vaccination strategies based on the history of immunity.",0,arxiv,Evrim,CC-BY/arXiv,An epidemiological model with waning immunity and reinfection
"RNA-based vaccination has been broadly applied in the COVID pandemic. A characteristic of the immunization was fast waning immunity. However, the time scale of this process varied considerable for virus subtypes and among individuals. Understanding the origin of this variability is crucial in order to improve future vaccination strategies. Here, we introduce a mathematical model of RNA-based vaccination and the kinetics of the induced immune response. In the model, antigens produced following vaccination rise an immune response leading to germinal center reactions and accordingly B-cell differentiation into memory B-cells and plasma cells. In a negative feedback loop, the antibodies synthesized by newly specified plasma cells shut down the germinal center reaction as well as antigen-induced differentiation of memory B-cell into plasma cells. This limits the build-up of long-lasting immunity and thus is accompanied by fast waning immunity. The detailed data available on infection with and vaccination against SARS-CoV-2 enabled computational simulation of essential processes of the immune response. By simulation, we analyzed to which extent a single or double dose vaccination provides protection against infection. We find that variability of the immune response in individuals, originating e.g. in different immune cell densities, results in a broad log-normal-like distribution of the vaccine-induced protection times that peaks around 100 days. Protection times decrease for virus variants with mutated antibody binding sites or increased replication rates. Independent of these virus specifics, our simulations suggest optimal timing of a second dose about 5 weeks after the first in agreement with clinical trials.",0,arxiv,Evrim,CC-BY/arXiv,Modelling variability of the immunity build-up and waning following RNA-based vaccination
"One of the greatest challenges of terrestrial locomotion is resisting gravity. The morphological adaptive features of the limb long-bones of extant elephants, the heaviest living terrestrial animals, have previously been highlighted; however, their bone microanatomy remains largely unexplored. Here we investigate the microanatomy of the six limb long-bones in Elephas maximus and Loxodonta africana, using comparisons of virtual slices as well as robustness analyses, to understand how they were adapted to heavy weight-bearing. We find that the long bones of elephant limbs display a relatively thick cortex and a medullary area almost entirely filled with trabecular bone. This trabecular bone is highly anisotropic with trabecular orientations reflecting the mechanical load distribution along the limb. The respective functional roles of the bones are reflected in their microanatomy through variations of cortical thickness distribution and main orientation of the trabeculae. We find microanatomical adaptations to heavy weight support that are common to other heavy mammals. Despite these shared characteristics, the long bones of elephants are closer to those of sauropods due to their shared columnar posture, which allows a relaxation of morphofunctional constraints, and thus relatively less robust bones with a thinner cortex than would be expected in such massive animals.",0,arxiv,Evrim,CC-BY/arXiv,Long-bone microanatomy in elephants: microstructural insights into gigantic beasts
"Basic and instantaneous reproduction numbers, ""R"" _""0"" and ""R"" _""t"" , are important metrics to assess progress of an epidemic and effectiveness of preventative interventions undertaken, and also to estimate coverage needed for vaccination. Reproduction numbers are related to the daily number of positive cases recorded by the national public health authorities, via the renewal equation. During periods of exponential growth or decay they are linked also to the rate constants by the Lotka-Euler equation. For either application, we need the distribution of generation times between primary and secondary infections. In practice, we use instead the directly observable serial interval between symptoms onset of infector and infectee. Pre-symptomatic transmission that occurs in COVID infection causes serial intervals to extend to negative values, which can be described with a Gaussian distribution. Consistent application of the two approaches requires careful attention to lower limits imposed on the distribution. Allowing Gaussian-distributed serial intervals to extend to minus infinity with the Lotka-Euler equation, as commonly is done, results in lower reproduction numbers than predicted from the discretized renewal equation. Here, we formulate the Lotka-Euler equation for Gaussian distributions including an explicit lower cut-off, and use this to explore the consequences of presymptomatic transmission for COVID-19 infections.",0,arxiv,Evrim,CC-BY/arXiv,"Reproduction Numbers R_0, R_t for COVID-19 Infections with Gaussian Distribution of Generation Times, and of Serial Intervals including Presymptomatic Transmission"
"Evolution occurs in populations of reproducing individuals. In stochastic descriptions of evolutionary dynamics, such as the Moran process, individuals are chosen randomly for birth and for death. If the same type is chosen for both steps, then the reproductive event is wasted, because the composition of the population remains unchanged. Here we introduce a new phenotype, which we call a \textit{replacer}. Replacers are efficient competitors. When a replacer is chosen for reproduction, the offspring will always replace an individual of another type (if available). We determine the selective advantage of replacers in well-mixed populations and on one-dimensional lattices. We find that being a replacer substantially boosts the fixation probability of neutral and deleterious mutants. In particular, fixation probability of a single neutral replacer who invades a well-mixed population of size $N$ is of the order of $1/\sqrt N$ rather than the standard $1/N$. Even more importantly, replacers are much better protected against invasions once they have reached fixation. Therefore, replacers dominate the mutation selection equilibrium even if the phenotype of being a replacer comes at a substantial cost: curiously, for large population size and small mutation rate the relative fitness of a successful replacer can be as low as $1/e$.",0,arxiv,Evrim,CC-BY/arXiv,The selective advantage of neighborhood-aware mutants in Moran process
"Multispecies ecosystems modelled by generalized Lotka-Volterra equations exhibit stationary population abundances, where large number of species often coexist. Understanding the precise conditions under which this is at all feasible and what triggers species extinctions is a key, outstanding problem in theoretical ecology. Using standard methods of random matrix theory, I show that distributions of species abundances are Gaussian at equilibrium, in the weakly interacting regime. One consequence is that feasibility is generically broken before stability, for large enough number of species. I further derive an analytic expression for the probability that $n=0,1,2,...$ species go extinct and conjecture that a single-parameter scaling law governs species extinctions. These results are corroborated by numerical simulations in a wide range of system parameters.",0,arxiv,Evrim,CC-BY/arXiv,Feasibility and Single Parameter Scaling of Extinctions in Multispecies Lotka-Volterra Ecosystems
"Dengue continues to pose a major global threat, infecting nearly 390 million people annually. Recognizing the pivotal role of vector competence (vc), recent research focuses on mosquito parameters to inform transmission modeling and vector control strategies.This study models interactions between Aedes vectors and dengue pathogens, highlighting vc as a key driver of within vector infection dynamics and endemic persistence. Using a predator prey framework, we show that endemic conditions emerge naturally from the biological interplay between the vectors strategies to pathogen pressure and we prove global stability of such conditions. Our results reveal that under tropical and subtropical environmental pressures, the innate immune system of vectors cannot offset high vc during endemic outbreaks, highlighting a fundamental biological trade off, vectors can evolve increased transmission potential but cannot enhance immune capacity. This constraint defines the limits of their evolutionary response to pathogen driven selection and drives instability in disease transmission dynamics.",0,arxiv,Evrim,CC-BY/arXiv,Vector Traits Shape Disease Persistence: A Predator Prey Approach to Dengue
"Habitat fragmentation, often driven by human activities, alters ecological landscapes by disrupting connectivity and reshaping species interactions. In such fragmented environments, habitats can be modeled as networks, where individuals disperse across interconnected patches. We consider an intraspecific competition model, where individuals compete for space while dispersing according to a nonlinear random walk, capturing the heterogeneity of the network. The interplay between asymmetric competition, dispersal dynamics, and spatial heterogeneity leads to nonuniform species distribution: individuals with stronger competitive traits accumulate in central (hub) habitat patches, while those with weaker traits are displaced toward the periphery. We provide analytical insights into this mechanism, supported by numerical simulations, demonstrating how competition and spatial structure jointly influence species segregation. In the large-network limit, this effect becomes extreme, with dominant individuals disappearing from peripheral patches and subordinate ones from central regions, establishing spatial segregation. This pattern may create favorable conditions for speciation, as physical separation can reinforce divergence within the population over time.",0,arxiv,Evrim,CC-BY/arXiv,Habitat fragmentation promotes spatial scale separation under resource competition
"Social contact patterns are a key input to many infectious disease models. Contact surveys, where participants are asked to provide information on their recent close and casual contacts with others, are one of the standard methods to measure contact patterns in a population. Surveys that require detailed sociodemographic descriptions of contacts allow for the specification of fine-grained contact rates between subpopulations in models. However, perception biases affecting a surveyed person's ability to estimate sociodemographic attributes (e.g., age, race, socioeconomic status) of others could affect contact rates derived from survey data. Here, we simulate contact surveys using a synthetic contact network of New Mexico to investigate the impact of these biases on survey accuracy and infectious disease model projections. We found that perception biases affecting the estimation of another individual's age and race substantially decreased the accuracy of the derived contact patterns. Using these biased patterns in a Susceptible-Infectious-Recovered compartmental model lead to an underestimation of cumulative incidence among older people (65+ years) and individuals identifying as races other than White. Our study shows that perception biases can impact contact patterns estimated from surveys in ways that systematically underestimate disease burden in minority populations when used in transmission models.",0,arxiv,Evrim,CC-BY/arXiv,Simulating the impact of perception bias on social contact surveys for infectious disease modelling
"A canonical step in quantifying a system is to measure its entropy. Shannon entropy and other traditional entropy measures capture only the information encoded in the frequencies of a system's elements. Recently, Leinster, Cobbold, and Reeve (LCR) introduced a method that also captures the rich information encoded in the similarities and differences among elements, yielding similarity-sensitive entropy. More recently, the Vendi score (VS) was introduced as an alternative, raising the question of how LCR and VS compare, and which is preferable. Here we address these questions conceptually, analytically, and experimentally, using 53 machine-learning datasets. We show that LCR and VS can differ by orders of magnitude and can capture complementary information about a system, except in limiting cases. We demonstrate that both LCR and VS depend on how similarities are scaled and introduce the concept of ``half distance'' to parameterize this dependence. We prove that VS provides an upper bound on LCR for several values of the RÃ©nyi-Hill order parameter and conjecture that this bound holds for all values. We conclude that VS is preferable only when interpreting elements as linear combinations of a more fundamental set of ``ur-elements'' or when the system or dataset possesses a quantum-mechanical character. In the broader circumstance where one seeks simply to capture the rich information encoded by similarity, LCR is favored; nevertheless, for certain half-distances the two methods can complement each other.",0,arxiv,Evrim,CC-BY/arXiv,Which Similarity-Sensitive Entropy?
"Estimating species and gene trees from sequence data is challenging. Gene tree estimation is often hampered by low phylogenetic signal in alignments, leading to inaccurate trees. Species tree estimation is complicated by incomplete lineage sorting (ILS), where gene histories differ from the species' history. Summary methods like MP-EST, ASTRAL2, and ASTRID infer species trees from gene trees but suffer when gene tree accuracy is low. To address this, the Statistical Binning (SB) and Weighted Statistical Binning (WSB) pipelines were developed to improve gene tree estimation. However, previous studies only tested these pipelines using multi-locus bootstrapping (MLBS), not the BestML approach.   This thesis proposes a novel pipeline, WSB+WQMC, which shares design features with the existing WSB+CAML pipeline but has other desirable properties and is statistically consistent under the GTR+MSC model. This study evaluated WSB+WQMC against WSB+CAML using BestML analysis on various simulated datasets. The results confirmed many trends seen in prior MLBS analyses. WSB+WQMC substantially improved gene tree and species tree accuracy (using ASTRAL2 and ASTRID) on most datasets with low, medium, and moderately high ILS levels. In a direct comparison, WSB+WQMC computed less accurate trees than WSB+CAML under certain low and medium ILS conditions. However, WSB+WQMC performed better or at least as accurately as WSB+CAML on all datasets with moderately high and high ILS. It also proved better for estimating gene trees on some medium and low ILS datasets. Thus, WSB+WQMC is a promising alternative to WSB+CAML for phylogenetic estimation, especially in the presence of low phylogenetic signal.",0,arxiv,Evrim,CC-BY/arXiv,Improving Gene Trees without more data
"Bacteria develop resistance to antibiotics through various mechanisms, with the specific mechanism depending on the drug-bacteria pair. It remains unclear, however, which resistance mechanism best supports favorable treatment outcomes, specifically in clearing infections and inhibiting further resistance. In this study, we use periodic ordinary differential equation models to simulate different antibiotic treatment protocols for bacterial infections. Using stability analysis and numerical simulations, we investigate how different resistance mechanisms, including plasmid-induced and mutation-induced resistance, affect treatment outcomes. Our findings suggest that antibiotic treatments with fixed dosing schedules are more likely to be effective when resistance arises exclusively through plasmid-mediated transmission. Further, when treatment fails, mutation-driven mechanisms tend to favor the selection of fully resistant bacterial strains. We also investigated the efficacy of different treatment strategies based on these mechanisms, finding that a twice-daily regimen consistently outperforms a once-daily regimen in terms of infection clearance. Additionally, our simulations with short half-life antibiotics indicate that the ""catch-up"" strategy outperforms the ""compensatory double-dose"" approach after a missed dose, a finding that aligns with general pharmaceutical advice for short-half-life drugs.",0,arxiv,Evrim,CC-BY/arXiv,Impact of Resistance Development Mechanisms on Antibiotic Treatment Outcomes
"Living systems can be understood as organized entities that capture, transform, and reproduce information. Classical gene-centered models explain adaptation through frequency changes driven by differential fitness, yet they often overlook the higher-order organization and causal closure that characterize living systems. Here we revisit several evolutionary frameworks, from the replicator equation to group selection and holobiont dynamics, and show that evolutionary change in population frequencies can be expressed as a Jeffreys divergence. Building on this foundation, we introduce a categorical model of Information Handlers (IH), entities capable of self-maintenance, mutation, and combination. This abstract architecture illustrates the usefulness of category theory for framing evolutionary processes that range from very simple to highly complex. The same categorical scheme can represent basic allele-frequency change as well as more elaborate scenarios involving reproductive interactions, symbiosis, and other organizational layers. A key feature of the framework is that different levels of evolutionary change can be summarized through a measure that quantifies the information generated, thereby distinguishing diverse types of evolutionary transformation such as individual and sexual selection, mate choice, or even holobiont selection. Finally, we show that the informational partition associated with host-microbiome pairings in holobionts generalizes the information-theoretic structure previously developed for non-random mating, revealing a common underlying architecture across biological scales.",0,arxiv,Evrim,CC-BY/arXiv,Life as a Categorical Information-Handling System: An Evolutionary Information-Theoretic Model of the Holobiont
"Island endemism is shaped by complex interactions among environmental, ecological, and evolutionary factors, yet the relative contributions of topography, climate, and land cover remain incompletely quantified. We investigated the drivers of endemic plant richness across Crete, a Mediterranean biodiversity hotspot, using spatially explicit data on species distributions, topographic complexity, climatic variability, land cover, and soil characteristics. Artificial Neural Network models, a machine learning tool, were employed to assess the relative importance of these predictors and to identify hotspots of endemism. We found that total species richness, elevation range, and climatic variability were the strongest predictors of endemic richness, reflecting the role of biodiversity, topographic heterogeneity, and climatic gradients in generating diverse habitats and micro-refugia that promote speciation and buffer extinction risk. Endemic hotspots only partially overlapped with areas of high total species richness, indicating that total species richness was the optimal from the ones examined, yet an imperfect surrogate. These environmentally heterogeneous areas also provide critical ecosystem services, including soil stabilization, pollination, and cultural value, which are increasingly threatened by tourism, renewable energy development, land-use change, and climate impacts. Our findings underscore the importance of prioritizing mountainous and climatically variable regions in conservation planning, integrating ecosystem service considerations, and accounting for within-island spatial heterogeneity. By explicitly linking the environmental drivers of endemism to both biodiversity patterns and ecosystem function, this study provides a framework for evidence-based conservation planning in Crete and other Mediterranean islands with similar geological and biogeographic contexts.",0,arxiv,Evrim,CC-BY/arXiv,"Topography, climate, land cover, and biodiversity: Explaining endemic richness and management implications on a Mediterranean island"
"In evolutionary models of large populations, it is common to analyze the effects of cyclic or random variation in the parameters that describe selection. It is less common, however, to study how stochasticity in the genetic transmission process itself affects evolutionary outcomes. Suppose that a gene locus has alleles $A$ and $a$ under constant selection. This locus is linked to a modifier locus with alleles $M_1$ and $M_2$, which control the mutation rate from $A$ to $a$. The Reduction Principle states that, near a mutation--selection balance where $M_1$ is fixed with mutation rate $u_1$, a rare allele $M_2$ can invade if its associated rate $u_2$ is lower than $u_1$. This result, valid for both haploids and diploids, assumes constant mutation rates through time. We extend this framework by allowing the mutation rate associated with $M_2$ to fluctuate randomly across generations, denoted as $u_{2,t}$. In this stochastic setting, the condition for invasion by a new modifier allele depends not only on the resident mutation rate $u_1$ and the mean mutation rate $u_2$ associated with the invading allele, but also on the temporal distribution of $u_{2,t}$, the strength of selection at the $A/a$ locus, and the recombination rate between $M_1/M_2$ and $A/a$. The analysis shows how stochasticity and recombination in transmission do not simply modify the magnitude of evolutionary change predicted under deterministic assumptions. Instead, through their interaction with selection and linkage, they can generate conditions under which the direction of modifier evolution is qualitatively reversed relative to the deterministic Reduction Principle.",0,arxiv,Evrim,CC-BY/arXiv,Evolution under Stochastic Transmission: Mutation-Rate Modifiers
"Growing literatures on epidemic and rumor dynamics show that infection and information coevolve. We present a unified framework for modeling the spread of infection and information: a general class of interaction-driven fluid-limit models expressed as coupled ODEs. The class includes the SIR epidemic model, the Daley-Kendall rumor model, and many extensions. For this general class, we derive theoretical results: under explicit graph-theoretic conditions, we obtain a classification of asymptotic behavior and motivate a conjecture of exponential decay for vanishing states. When these conditions are violated, the classification can fail, and decay may become non-exponential (e.g., algebraic). In deriving the main result, we establish asymptotic stability and $L^1$-integrability properties for state variables. Alongside these results, we introduce the dependency graph that captures outflow dependencies and offers a new angle on the structure of this model class. Finally, we illustrate the results with several examples, including a heterogeneous rumor model and a rumor-dependent SIR model, showing how small changes to the dependency graph can flip asymptotic behavior and reshape epidemic trajectories.",0,arxiv,Evrim,CC-BY/arXiv,Asymptotic behavior for a general class of spreading models
"In this paper, we present a stochastic SVEIS epidemic model perturbed by a Black-Karasinski process. Using a Lyapunov functional approach, we derive a sufficient condition, Rs0>1 for the existence of a stationary distribution, which indicates disease persistence. Additionally, we theoretically demonstrate that the disease will die out at an exponential rate if Re0<1 . Our results show that random fluctuations will facilitate disease outbreak.",0,arxiv,Evrim,CC-BY/arXiv,Asymptotic analysis of a stochastic SVEIS epidemic model using Black-Karasinski process
"Transfer RNAs (tRNAs) are universal adaptors of the genetic code, yet their evolutionary dynamics across photosynthetic eukaryotes remain underexplored. Here, we present the largest comparative re-analysis integrating the PlantRNA database with published data to explore tRNA gene evolution. We find that tRNA gene repertoires have been deeply shaped by ecological transitions, genome architecture, and translational demands. Terrestrialization marks a major shift in tRNA evolution, characterized by the loss of selenoproteins and their dedicated selenocysteine tRNAs in land plants compared to algae. Patterns of intron prevalence, position, and structure diverged among lineages, with extensive intron loss occurring around the origin of land plants. Organellar genomes exhibit divergent trajectories: mitochondrial tRNA sets are highly labile due to recurrent gene losses, imports, and horizontal transfers, whereas plastid repertoires are comparatively stable with lineage-specific exceptions. In parallel, angiosperm nuclear tRNA genes exhibit reinforced cis-regulatory elements, consistent with increased and developmentally complex translational demands, and their copy number correlates tightly with codon usage and amino acid composition. Finally, conserved yet family-biased clustering of nuclear tRNA genes reveals contrasting organizational principles in plants versus metazoans. Together, these findings establish tRNA gene evolution as a major determinant of translational capacity and a key driver of photosynthetic diversification.",0,arxiv,Evrim,CC-BY/arXiv,From the RNA world to land plants: Evolutionary insights from tRNA genes
"In this paper we use Species Distribution Models (SDMs) to forecast the future diversity and distribution of orchids in Great Britain and Ireland under scenarios of climate and land-use change. The study analyzes occurrence data for native orchid taxa in the BSBI database at a fine spatial resolution (1 km^2, monads) and incorporates multiple environmental variables including climate, land use, topography, and soil. These SDMs project significant losses in orchid species richness by 2050 and 2070, especially under severe climate and land-use scenarios, with declines expected across most species and regions, including Ireland where historical data previously indicated gains. The models reveal vulnerable species likely to face extinction by 2070, emphasizing the impact of both climate warming and habitat modifications. This approach differs from previous trend-based analyses by integrating future projections, high-resolution spatial data, and dynamic land-use scenarios, thereby providing higher-resolution estimates of orchid range contractions and diversity losses. While current observed orchid trends show some regional increases, particularly in Ireland, the SDM forecasts indicate substantial future risks. The study also discusses uncertainties due to niche truncation from geographic data limits and highlights the need for broader-scale modeling for more robust predictions. Overall, the paper anticipates conservation challenges for orchid biodiversity in response to ongoing environmental changes.",0,arxiv,Evrim,CC-BY/arXiv,The Future Orchid Diversity of Great Britain and Ireland using an SDM Approach
"Infectious disease outbreaks have precipitated a profusion of mathematical models. We introduce a unifying concept of ""epidemic momentum"" -- prevalence weighted by the capacity to infect in the future -- and use it to reveal a common underlying geometry that corresponds to contours of a generic first integral. Exploiting this conserved quantity, we show that it is possible to (i) disentangle the basic reproduction number $R_0$ from the population proportion that was immune before a disease invasion or re-emergence and (ii) infer both from observed data. This separation enables us to revise the classical estimate of the epidemic final size, incorporating prior population immunity. To illustrate the utility of these insights, we present a novel reappraisal of the main wave of the 1918 influenza pandemic.",0,arxiv,Evrim,CC-BY/arXiv,Epidemic Momentum
"Zika fever, a mosquito-borne viral disease with potential severe neurological complications and birth defects, remains a significant public health concern. The epidemiological models often oversimplify the dynamics of Zika transmission by assuming immediate detection of all infected cases. This study provides an enhanced SEIR (Susceptible-Exposed-Infectious-Recovered) model to incorporate partial information by distinguishing between detected and undetected Zika infections (also known as ""dark figures""). By distinguishing the compartments, the model captures the complexities of disease spread by accounting for uncertainties about transmission and the number of undetected infections. This model implements the Kalman filter technique to estimate the hidden states from the observed states. Numerical simulations were performed to understand the dynamics of Zika transmission and real-world data was utilized for parameterization and validation of the model. The study aims to provide information on the impact of undetected Zika infections on disease spread within the population, which will contribute to evidence-based decision making in public health policy and practice.",0,arxiv,Evrim,CC-BY/arXiv,Stochastic Models and Estimation of Undetected Infections in the Transmission of Zika Virus
"In recent years, numerous advances have been made in understanding how epidemic dynamics is affected by changes in individual behaviours. We propose an SIS-based compartmental model to tackle the simultaneous and coupled evolution of an outbreak and of the adoption by individuals of the isolation measure. The compliance with self-isolation is described with the help of the imitation dynamics framework. Individuals are incentivised to isolate based on the prevalence and the incidence rate of the outbreak, and are tempted to defy isolation recommendations depending on the duration of isolation and on the cost of putting social interactions on hold. We are able to derive analytical results on the equilibria of the model under the homogeneous mean-field approximation. Simulating the compartmental model on empirical networks, we also do a preliminary check of the impact of a network structure on our analytical predictions. We find that the dynamics collapses to surprisingly simple regimes where either the imitation dynamics no longer plays a role or the equilibrium prevalence depends on only two parameters of the model, namely the cost and the relative time spent in isolation. Whether individuals prioritise disease prevalence or incidence as an indicator of the state of the outbreak appears to play no role on the equilibria of the dynamics. However, it turns out that favouring incidence may help to flatten the curve in the transient phase of the dynamics. We also find a fair agreement between our analytical predictions and simulations run on an empirical multiplex network.",0,arxiv,Evrim,CC-BY/arXiv,Incentives for self-isolation based on incidence rather than prevalence could help to flatten the curve: a modelling study
"We analyse a series of bacterial growth models with in-built inter-individual variation in rates of cell division. We show that this variation leads to reduced population growth in favorable regimes and reduced population killing in detrimental environments. By treating environmental stress as a model parameter, we then show that the reduction in population growth aggravates with stress. We apply these models to data on growth rates for populations of green algae {\em Clamydomonas reinhardtii}. Specifically, we compare growth rates of two ancestral strains and respective mutation accumulation lines, measured along a stress gradient. The data had previously shown mutants growing consistently slower than ancestors, and this effect aggravating with stress. Here we show that this trend is expected if mutants are more variable than ancestors in individual rates of cell division, even if their means are higher. This can open new prospects for prediction of how populations respond to environmental changes.",0,arxiv,Evrim,CC-BY/arXiv,The impact of nonheritable variation in division rates on population growth across environments
"Aquaculture is pivotal for global food security but faces significant challenges from infectious diseases, particularly those caused by Streptococcus species such as Streptococcus iniae and Streptococcus agalactiae. These pathogens induce severe systemic infections in various fish species, resulting in high morbidity and mortality rates. This review consolidates current knowledge on the epidemiology, pathogenesis, and clinical manifestations of these infections in fish and provides a comprehensive analysis of multifaceted control and prebention strategies. Advancements in genetic engineering and selective breeding are highlighted, demonstrating significant potential in developing disease-resistant fish strains through technologies like CRISPR-Cas9 and genomic selection. We examine the impact of farming practices on disease prevalence, emphasizing the roles of stocking density, feeding regimes, and biosecurity measures. The integration of big data analytics and IoT technologies is shown to revolutionize disease monitoring and management, enabling real-time surveillance and predictive modeling for timely interventions. Progress in vaccine development, including subunit, DNA, and recombinant protein vaccines, highlights the importance of tailored immunoprophylactic strategies. Furthermore, this review emphasizes the One-Health approach and the essential collaboration among industry, academia, and government to address the interconnected health of humans, animals, and the environment. This holistic strategy, supported by advanced technologies and collaborative efforts, promises to enhance the sustainability and productivity of aquaculture systems. Future research directions advocate for continued innovation and interdisciplinary partnerships to overcome the persistent challenges of streptococcal infections in aquaculture.",0,arxiv,Evrim,CC-BY/arXiv,"Streptococcosis in aquaculture: Advances, challenges, and future directions in disease control and prevention"
"Phylogenetic tree shapes capture fundamental signatures of evolution. We consider ``ranked'' tree shapes, which are equipped with a total order on the internal nodes compatible with the tree graph. Recent work has established an elegant bijection of ranked tree shapes and a class of integer matrices, called \textbf{F}-matrices, defined by simple inequalities. This formulation is for isochronous ranked tree shapes, where all leaves share the same sampling time, such as in the study of ancient human demography from present-day individuals. Another important style of phylogenetics concerns trees where the ``timing'' of events is by branch length rather than calendar time. This style of tree, called a rooted phylogram, is output by popular maximum-likelihood methods. These trees are broadly relevant, such as to study the affinity maturation of B cells in the immune system. Discretizing time in a rooted phylogram gives a fully heterochronous ranked tree shape, where leaves are part of the total order. Here we extend the \textbf{F}-matrix framework to such fully heterochronous ranked tree shapes. We establish an explicit bijection between a class of \textbf{F}-matrices and the space of such tree shapes. The matrix representation has the key feature that values at any entry are highly constrained via four previous entries, enabling straightforward enumeration of all valid tree shapes. We also use this framework to develop probabilistic models on ranked tree shapes. Our work extends understanding of combinatorial objects that have a rich history in the literature.",0,arxiv,Evrim,CC-BY/arXiv,Generalizing matrix representations to fully heterochronous ranked tree shapes
"The classical Maximum-Entropy Principle (MEP) based on Shannon entropy is widely used to construct least-biased probability distributions from partial information. However, the Shore-Johnson axioms that single out the Shannon functional hinge on strong system independence, an assumption often violated in real-world, strongly correlated systems. We provide a self-contained guide to when and why practitioners should abandon the Shannon form in favour of the one-parameter Uffink-Jizba-Korbel (UJK) family of generalized entropies. After reviewing the Shore and Johnson axioms from an applied perspective, we recall the most commonly used entropy functionals and locate them within the UJK family. The need for generalized entropies is made clear with two applications, one rooted in economics and the other in ecology. A simple mathematical model worked out in detail shows the power of generalized maximum entropy approaches in dealing with cases where strong system independence does not hold. We conclude with practical guidelines for choosing an entropy measure and reporting results so that analyses remain transparent and reproducible.",0,arxiv,Evrim,CC-BY/arXiv,Generalized Maximum Entropy: When and Why you need it
"We introduce a general diploid population model with self-fertilization and possible overlapping generations, and study the genealogy of a sample of $n$ genes as the population size $N$ tends to infinity. Unlike traditional approach in coalescent theory which considers the unconditional (annealed) law of the gene genealogies averaged over the population pedigree, here we study the conditional (quenched) law of gene genealogies given the pedigree. We focus on the case of high selfing probability and obtain that this conditional law converges to a random probability measure, given by the random law of a system of coalescing random walks on an exchangeable fragmentation-coalescence process of \cite{berestycki04}. This system contains the system of coalescing random walks on the ancestral recombination graph as a special case, and it sheds new light on the site-frequency spectrum (SFS) of genetic data by specifying how SFS depends on the pedigree. The convergence result is proved by means of a general characterization of weak convergence for random measures on the Skorokhod space with paths taking values in a locally compact Polish space.",0,arxiv,Evrim,CC-BY/arXiv,Quenched coalescent for diploid population models with selfing and overlapping generations
"Understanding the dynamics of the spread of diseases within populations is critical for effective public health interventions. We extend the classical SIR model by incorporating additional complexities such as the introduction of a cure and migration between cities. Our framework leverages a system of differential equations to simulate disease transmission across a network of interconnected cities, capturing more realistic patterns. We present theoretical results on the convergence of population sizes in the migration framework (in the absence of deaths). We also run numerical simulations to understand how the timing of the introduction of the cure affects mortality rates. Our numerical results explain how localized interventions affect the spread of the disease across cities. In summary, this work advances the modeling of epidemics to a more local scope, offering a more expressive tool for epidemiological research and public health planning.",0,arxiv,Evrim,CC-BY/arXiv,Explorations of Epidemiological Dynamics across Multiple Population Hubs
"Mapping habitat quality, based on factors like host availability and environmental suitability, is a common approach to determining which locations are important for the spread of a species. Mapping habitat connectivity takes geographic analyses a step further, evaluating the potential roles of locations in biological invasions, pandemics, or species conservation. Locations with high habitat quality may play a minor role in species spread if they are geographically isolated. Yet, a location with lower habitat quality may play a major role in a species' spread if it acts as a bridge between regions that would otherwise be physically fragmented.   Here we introduce the geohabnet R package, which evaluates the potential importance of locations for the spread of species through habitat landscapes. geohabnet incorporates key factors such as dispersal probabilities and habitat availability in a network framework, for better understanding habitat connectivity for host-dependent species, such as pathogens, arthropod pests, or pollinators.   geohabnet uses publicly available or user-provided datasets, six network centrality metrics, and a user-selected geographic scale. We provide examples using geohabnet for surveillance prioritization of emerging plant pests in Africa and the Americas. These examples illustrate how users can apply geohabnet for their species of interest and generate maps of the estimated importance of geographic locations for species spread.   geohabnet provides a quick, open-source, and reproducible baseline to quantify a species' habitat connectivity across a wide range of geographic scales and evaluates potential scenarios for the expansion of a species through habitat landscapes. geohabnet supports biosecurity programs, invasion science, and conservation biology when prioritizing management efforts for transboundary pathogens, pests, or endangered species.",0,arxiv,Evrim,CC-BY/arXiv,geohabnet: An R package for mapping habitat connectivity for biosecurity and conservation
"Evolutionary systems must learn to generalize, often extrapolating from a limited set of selective conditions to anticipate future environmental changes. The mechanisms enabling such generalization remain poorly understood, despite their importance to predict ecological robustness, drug resistance, or design future-proof vaccination strategies. Here, we demonstrate that annealed population heterogeneity, wherein distinct individuals in the population experience different instances of a complex environment over time, can act as a form of implicit regularization and facilitate evolutionary generalization. Mathematically, annealed heterogeneity introduces a variance-weighted demographic noise term that penalizes across-environment fitness variance and effectively rescales the population size, thereby biasing evolution toward generalist solutions. This process is indeed analogous to a variant of the mini-batching strategy employed in stochastic gradient descent, where an effective multiplicative noise produces an inductive bias by triggering noise-induced transitions.   Through numerical simulations and theoretical analysis we discuss the conditions under which variation in how individuals experience environmental selection can naturally promote evolutionary strategies that generalize across environments and anticipate novel challenges.",0,arxiv,Evrim,CC-BY/arXiv,Learning to generalize in evolution through annealed population heterogeneity
"We study the effect of intratumor heterogeneity in the likelihood of cancer cells moving from a primary tumor to other sites in the human body, generating a metastatic process. We model different scenarios of competition between tumor cells using a static evolutionary game in which cells compete for nutrients and oxygen and might choose to stay and proliferate in the primary tumor or opt to a motility strategy in order to find resources in a metastatic site. The theoretical results found in the evolutionarily equilibrium in the mathematical model are in line with the empirical results observed in oncology, namely, the coexistence of both primary and metastatic tumors and the conditions that favor a metastatic process. Particularly, the model finds mathematical support for what is empirically observed in punctuated and branching cancers for the specific case of clear cell renal cell carcinomas: motility of cells is larger in punctuated cancers if the proportion of BAP1 mutations remain below a given cell proportion threshold.",0,arxiv,Evrim,CC-BY/arXiv,Effect of intratumor heterogeneity in managing the go-or-grow dichotomy of cancer cells: a game theory modeling to understand metastasis
"Collective systems that self-organise to maximise the group's ability to collect and distribute information can be successful in environments with high spatial and temporal variation. Such organisations are abundant in nature, as sharing information is a key benefit of many biological collective systems, and have been influential in the design of many artificial collectives such as swarm robotics. Understanding how these systems may be spatially distributed to optimise their collective potential is therefore of importance in both ecology and in collective systems design. Here, we develop a mathematical model which uses an optimisation framework to determine the higher-order spatial structure of a collective that optimises group-level knowledge transfer. The domain of the objective function is a set of weighted simplicial sets, which can fully represent the spatial structure from a topological perspective. By varying the parameters within the objective function and the constraints, we determine how the optimal spatial structure may vary when individuals differ in their information gathering ability and how this variation differs in the context of resource constraints. Our key findings are that the amount of resources in the environment can lead to specific subgroup sizes being optimal for the group as a whole when individuals are homogeneous in their information gathering abilities. Further, when there is variation in information gathering abilities, our model implies that the sharing of space between smaller subgroups of the population, rather than the whole population, is optimal for collective knowledge sharing. Our results have applications across diverse contexts from behavioural ecology to bio-inspired collective systems design.",0,arxiv,Evrim,CC-BY/arXiv,Drivers of Variation in the Optimal Spatial Structure of Collective Information Gatherers
"Rabies continues to pose a significant zoonotic threat, particularly in areas with high populations of domestic dogs that serve as viral reservoirs. This study conducts a comparative analysis of Stochastic Continuous-Time Markov Chain (CTMC) and deterministic models to gain insights into rabies persistence within human and canine populations. By employing a multitype branching process, the stochastic threshold for rabies persistence was determined, revealing important insights into how stochasticity influences extinction probabilities. The stochastic model utilized 10,000 sample paths to estimate the probabilities of rabies outbreaks, offering a rigorous assessment of the variability in disease occurrences. Additionally, the study introduces a novel mathematical formulation of rabies transmission dynamics, which includes environmental reservoirs, free-ranging dogs, and domestic dogs as essential transmission factors. The basic reproduction number ($\mathcal{R}_0$) was derived and analyzed within stochastic frameworks, effectively bridging the gap between these two modeling approaches. Numerical simulations confirmed that the results from the stochastic model closely aligned with those from the deterministic model, while also highlighting the importance of stochasticity in scenarios with low infection rates. Ultimately, the study advocates for a comprehensive approach to rabies control that integrates both the predictable trends identified through deterministic models and the impact of random events emphasized by stochastic models.",0,arxiv,Evrim,CC-BY/arXiv,Evaluating the effectiveness of Stochastic CTMC and deterministic models in correlating rabies persistence in human and dog populations
"The Swadesh approach for determining the temporal separation between two languages relies on the stochastic process of words replacement (when a complete new word emerges to represent a given concept). It is well known that the basic assumptions of the Swadesh approach are often unrealistic due to various contamination phenomena and misjudgments (horizontal transfers, variations over time and space of the replacement rate, incorrect assessments of cognacy relationships, presence of synonyms, and so on). All of this means that the results cannot be completely correct.   More importantly, even in the unrealistic case that all basic assumptions are satisfied, simple mathematics places limits on the accuracy of estimating the temporal separation between two languages. These limits, which are purely probabilistic in nature and which are often neglected in lexicostatistical studies, are analyzed in detail in this article.   Furthermore, in this work we highlight that the evolution of a language's lexicon is also driven by another stochastic process: gradual lexical modification of words. We show that this process equally also represents a major contribution to the reshaping of the vocabulary of languages over the centuries and we also show, from a purely probabilistic perspective, that taking into account this second random process significantly increases the precision in determining the temporal separation between two languages.",0,arxiv,Evrim,CC-BY/arXiv,Evolution of the lexicon: a probabilistic point of view
"In this paper we investigate the asymptotic behavior of some SIR models incorporating demography, bounded random transmission coefficient and a time-dependent vaccination strategy targeting the susceptible population. In this setting, we establish the existence and uniqueness of non-negative global solution of the models and derive conditions under which either the disease is eradicated or becomes endemic. In addition, the theoretical results are further illustrated by several numerical simulations.",0,arxiv,Evrim,CC-BY/arXiv,"SIR models with demography, random transmission coefficient and non-autonomous vaccination rate"
"In this work, we prove the existence of a 2-cycle in an integrodifference equation with a Laplace kernel and logistic growth function, connecting two non-trivial fixed points of the second iterate of the logistic map in the non-chaotic regime. This model was first studied by Kot (1992), and the 2-cycle we establish corresponds to one numerically observed by Bourgeois, Leblanc, and Lutscher (2018) for the Ricker growth function. We provide strong evidence that the 2-cycle for the Ricker growth function can be rigorously proven using a similar approach. Finally, we present numerical results indicating that both 2-cycles exhibit spectral stability.",0,arxiv,Evrim,CC-BY/arXiv,Spatially inhomogeneous two-cycles in an integrodifference equation
"We consider populations evolving according to natural selection, mutation, and recombination, and assume that the genomes of all or a representative selection of individuals are known. We pose the problem if it is possible to infer fitness parameters and genotype fitness order from such data. We tested this hypothesis in simulated populations. We delineate parameter ranges where this is possible and other ranges where it is not.Our work provides a framework for determining when fitness inference is feasible from population-wide, whole-genome, time-stratified data and highlights settings where it is not. We give a brief survey of biological model organisms and human pathogens that fit into this framework.",0,arxiv,Evrim,CC-BY/arXiv,Fitness inference tested by in silico population genetics
"Species distribution models (SDMs), which aim to predict species occurrence based on environmental variables, are widely used to monitor and respond to biodiversity change. Recent deep learning advances for SDMs have been shown to perform well on complex and heterogeneous datasets, but their effectiveness remains limited by spatial biases in the data. In this paper, we revisit deep SDMs from a Bayesian perspective and introduce BATIS, a novel and practical framework wherein prior predictions are updated iteratively using limited observational data. Models must appropriately capture both aleatoric and epistemic uncertainty to effectively combine fine-grained local insights with broader ecological patterns. We benchmark an extensive set of uncertainty quantification approaches on a novel dataset including citizen science observations from the eBird platform. Our empirical study shows how Bayesian deep learning approaches can greatly improve the reliability of SDMs in data-scarce locations, which can contribute to ecological understanding and conservation efforts.",0,arxiv,Evrim,CC-BY/arXiv,BATIS: Bayesian Approaches for Targeted Improvement of Species Distribution Models
"Human bone marrow stromal cells (BMSC) include skeletal stem cells with ground-breaking therapeutic potential. However, BMSC colonies have very heterogeneous in vivo behaviour, due to their different potency; this unpredictability is the greatest hurdle to the development of skeletal regeneration therapies. Colony-level heterogeneity urges a fundamental question: how is it possible that one colony as a collective unit behaves differently from another one? If cell-to-cell variability were just an uncorrelated random process, a million cells in a transplant-bound colony would be enough to yield statistical homogeneity, hence washing out any colony-level traits. A possible answer is that the differences between two originating cells are transmitted to their progenies and collectively persist through an hereditary mechanism. But non-genetic inheritance remains an elusive notion, both at the experimental and at the theoretical level. Here, we prove that heterogeneity in the lineage topology of BMSC clonal colonies is determined by heritable traits that regulate cell-cycle exit. The cornerstone of this result is the definition of a novel entropy of the colony, which measures the hereditary ramifications in the distribution of inactive cells across different branches of the proliferation tree. We measure the entropy in 32 clonal colonies, obtained from single-cell lineage tracing experiments, and show that in the greatest majority of clones this entropy is decisively smaller than that of the corresponding non-hereditary lineage. This result indicates that hereditary epigenetic factors play a major role in determining cycle exit of bone marrow stromal cells.",0,arxiv,Evrim,CC-BY/arXiv,Inheritance entropy quantifies epigenetic regulation of cell-cycle exit in human bone marrow stromal cells
"Many Mendelian randomization (MR) papers have been conducted only in people of European ancestry, limiting transportability of results to the global population. Expanding MR to diverse ancestry groups is essential to ensure equitable biomedical insights, yet presents analytical and conceptual challenges. This review examines the practical challenges of MR analyses beyond the European only context, including use of data from multi-ancestry, mismatched ancestry, and admixed populations. We explain how apparent heterogeneity in MR estimates between populations can arise from differences in genetic variant frequencies and correlation patterns, as well as from differences in the distribution of phenotypic variables, complicating the detection of true differences in the causal pathway.   We summarize published strategies for selecting genetic instruments and performing analyses when working with limited ancestry-specific data, discussing the assumptions needed in each case for incorporating external data from different ancestry populations. We conclude that differences in MR estimates by ancestry group should be interpreted cautiously, with consideration of how the identified differences may arise due to social and cultural factors. Corroborating evidence of a biological mechanism altering the causal pathway is needed to support a conclusion of differing causal pathways between ancestry groups.",0,arxiv,Evrim,CC-BY/arXiv,Mendelian randomization in a multi-ancestry world: reflections and practical advice
"We design a linear chain trick algorithm for dynamical systems for which we have oscillatory time histories in the distributed time delay. We make use of this algorithmic framework to analyse memory effects in disease evolution in a population. The modelling is based on a susceptible-infected-recovered SIR - model and on a susceptible-exposed-infected-recovered SEIR - model through a kernel that dampens the activity based on the recent history of infectious individuals. This corresponds to adaptive behavior in the population or through governmental non-pharmaceutical interventions. We use the linear chain trick to show that such a model may be written in a Markovian way, and we analyze the stability of the system. We find that the adaptive behavior gives rise to either a stable equilibrium point or a stable limit cycle for a close to constant number of susceptibles, i.e.\ locally in time. We also show that the attack rate for this model is lower than it would be without the dampening, although the adaptive behavior disappears as time goes to infinity and the number of infected goes to zero.",0,arxiv,Evrim,CC-BY/arXiv,Memory Effects in Disease Modelling Through Kernel Estimates with Oscillatory Time History
"We investigate the adaptive Ambush strategy in cyclic models following the rules of the spatial rock-paper-scissors game. In our model, individuals of one species possess cognitive abilities to perceive environmental cues and assess the local density of the species they dominate in the spatial competition for natural resources. Based on this assessment, they either initiate a direct attack or, if the local concentration of target individuals does not justify the risk, reposition strategically to prepare an ambush. To quantify the evolutionary consequences of these behavioural strategies, we perform stochastic simulations, analysing emergent spatial patterns and the dependence of species densities on the threshold used by individuals to decide between immediate attack or anticipation. Our findings reveal that, despite being designed to enhance efficiency, cognitive strategies can reduce the abundance of the species due to the constraints of cyclic dominance. We identify an optimal decision threshold: attacking only when the local density of target individuals exceeds 15% provides the best balance between selection risk and long-term persistence. Furthermore, the Ambush strategy benefits low-mobility organisms, increasing coexistence probabilities by up to 53%. These results deepen the understanding of adaptive decision-making in spatial ecology, linking cognitive complexity to ecosystem resilience and extinction risk.",0,arxiv,Evrim,CC-BY/arXiv,Ambush strategy impacts species predominance and coexistence in rock-paper-scissors models
"In a mix of prejudiced and unprejudiced individuals engaged in strategic interactions, the individual intensity of prejudice is expected to have effect on overall level of societal prejudice. High level of prejudice should lead to discrimination that may manifest as unfairness and, perhaps, even spite. In this paper, we investigate this idea in the classical paradigm of the ultimatum game which we theoretically modify to introduce prejudice at the level of players, terming its intensity as prejudicity. The stochastic evolutionary game dynamics, in the regime of replication-selection, reveals the emergence of spiteful behaviour as a dominant behaviour via a first order phase transition -- a discontinuous jump in the frequency of spiteful individuals at a threshold value of prejudicity. The phase transition is quite robust and becomes progressively conspicuous in the limit of large population size where deterministic evolutionary game dynamics, viz., replicator dynamics, approximates the system closely. The emergence of spite driven by prejudice is also found to persist when one considers long-term evolutionary dynamics in the mutation-selection dominated regime.",0,arxiv,Evrim,CC-BY/arXiv,Prejudice driven spite: A discontinuous phase transition in ultimatum game
"Chikungunya virus is a mosquito-borne arbovirus with the potential to establish sustained transmission in subtropical regions like Florida, where climatic and ecological conditions support vector proliferation. In this study, we develop a Continuous Time Markov Chain model to assess the probability of long-term Chikungunya establishment in Miami-Dade County following repeated introductions of external infectious individuals. This work aims to identify seasonal windows of heightened endemic risk and evaluates the impact of vector control strategies on mitigating the likelihood of persistent transmission. These results generate insights into the dynamics of Chikungunya virus infections and inform targeted interventions to prevent its transition from minor sporadic outbreaks to endemic circulation.",0,arxiv,Evrim,CC-BY/arXiv,Quantifying the Risk of Long Term Chikungunya Persistence in Miami Dade County
"The Paradox of Enrichment (PoE) predicts that increasing resources, such as nutrient inputs like fertilizers or food availability, should destabilize ecological systems, such as crop-pest dynamics, leading to population cycles that can increase the risk of crop failure during environmental shocks. Yet, since the Green Revolution, fertilizer use has surged without widespread evidence of yield instability, challenging the PoE's relevance to modern agriculture. Here, we propose and test a novel resolution: that insecticides, frequently co-applied with fertilizers, act as stabilizing agents that counterbalance enrichment-induced instability. Using a modified PoE model with empirically grounded parameters for three major crop-pest systems-soybean-aphid, wheat-aphid, and cabbage-diamondback moth-we find that fertilizer increases yields, but destabilizes dynamics, whereas insecticides restore stability and ensure more predictable harvests. These findings reveal that insecticides may suppress pests but also play a critical role in stabilizing crop yields in nutrient-enriched agroecosystems, with implications for ecosystem management, eutrophication, conservation biology, and pesticide policy.",0,arxiv,Evrim,CC-BY/arXiv,"Fertilizers Fuel, Insecticides Stabilize: Resolving the Paradox of Enrichment in Agriculture"
"The Generic Population Concept - Agent-Based Model, henceforth short, GEPOC ABM, is one of the models within GEPOC, a generic concept to model a country's population and its dynamics using causal modelling approaches. The model is well established and had already proven its worth in various use cases from evaluation of MMR vaccination rates to SARS-CoV-2 epidemics modelling. In this work we will reproducibly specify the base model, to be specific, version 2.2 of it, and several extensions. The base model GEPOC ABM depicts the population of a country with the features sex and age. It uses a co-simulation-inspired time-update, where person-level discrete-event simulators are synchronised by a simulation layer at macro-steps, making the approach amenable to parallelization. A core design choice is structuring person agents around the life-year rather than the calendar year; accordingly, each agent schedules demographic events annually on their birthday. To expand the model's capabilities beyond basic demographic features, GEPOC ABM Geography adds a residence feature in the form of geographical coordinates. Further extensions include GEPOC ABM IM, which adds internal migration processes in three variants, and GEPOC ABM CL, which models locations where agents may have contacts with each other. In this definition we solely specify the conceptual models and do not go into any details with respect to implementation or gathering/processing of parametrisation data.",0,arxiv,Evrim,CC-BY/arXiv,"GEPOC ABM, Generic Population Concept -- Agent-Based Model, Version 2.2"
"Cumulants and moments are closely related to the basic mathematics of continuous and discrete selection (respectively). These relationships generalize Fisher's fundamental theorem of natural selection and also make clear some of its limitation. The relationship between cumulants and continuous selection is especially intuitive and also provides an alternative way to understand cumulants. We show that a similarly simple relationship exists between moments and discrete selection. In more complex scenarios, we show that thinking of selection over discrete generations has significant advantages. For a simple mutation model, we find exact solutions for the equilibrium moments of the fitness distribution. These solutions are surprisingly simple and have some interesting implications including: a necessary and sufficient condition for mutation selection balance, a very simple formula for mean fitness and the fact that the shape of the equilibrium fitness distribution is determined solely by mutation (whereas the scale is determined by the starting fitness distribution).",0,arxiv,Evrim,CC-BY/arXiv,"Cumulants, Moments and Selection: The Connection Between Evolution and Statistics"
"We propose an epidemic model for the spread of vector-borne diseases. The model, which is built extending the classical susceptible-infected-susceptible model, accounts for two populations -- humans and vectors -- and for cross-contagion between the two species, whereby humans become infected upon interaction with carrier vectors, and vectors become carriers after interaction with infected humans. We formulate the model as a system of ordinary differential equations and leverage monotone systems theory to rigorously characterize the epidemic dynamics. Specifically, we characterize the global asymptotic behavior of the disease, determining conditions for quick eradication of the disease (i.e., for which all trajectories converge to a disease-free equilibrium), or convergence to a (unique) endemic equilibrium. Then, we incorporate two control actions: namely, vector control and incentives to adopt protection measures. Using the derived mathematical tools, we assess the impact of these two control actions and determine the optimal control policy.",0,arxiv,Evrim,CC-BY/arXiv,A Human-Vector Susceptible-Infected-Susceptible Model for Analyzing and Controlling the Spread of Vector-Borne Diseases
"Viruses are microscopic infectious agents that require a host cell for replication. Viral replication occurs in several stages, and the completion time for each stage varies due to differences in the cellular environment. Thus, the time to complete each stage in viral replication is a random variable. However, no analytic expression exists for the viral population at the cellular level when the completion time for each process constituting viral replication is a random variable. This paper presents a simplified model of viral replication, treating each stage as a renewal process with independently and identically distributed completion times. Using the proposed model, we derive an analytical formula for viral populations at the cellular level, based on viewing viral replication as a birth-death process. The mean viral count is expressed via probability density functions representing the completion time for each step in the replication process. This work validates the results with stochastic simulations. This study provides a new quantitative framework for understanding viral infection dynamics.",0,arxiv,Evrim,CC-BY/arXiv,"Viral population dynamics at the cellular level, considering the replication cycle"
"We study the propagation speed of bistable traveling waves in the classical two-component diffusive Lotka-Volterra system under strong competition. From an ecological perspective, the sign of the propagation speed determines the long-term outcome of competition between two species and thus plays a central role in predicting the success or failure of invasion of an alien species into habitats occupied by a native species. Using comparison arguments, we establish sufficient conditions determining the sign of the propagation speed, which refine previously known results. In particular, we show that in the symmetric case, where the two species differ only in their diffusion rates, the faster diffuser prevails over a substantially broader parameter range than previously established. Moreover, we demonstrate that when the interspecific competition coefficients differ significantly, the outcome of competition cannot be reversed by adjusting diffusion or growth rates. These findings provide a rigorous theoretical framework for analyzing invasion dynamics, offering sharper mathematical criteria for invasion success or failure.",0,arxiv,Evrim,CC-BY/arXiv,Propagation speed of traveling waves for diffusive Lotka-Volterra system with strong competition
"In this study, we provide a relatively simple simulation framework for constructing artificial life (ALife) with both autonomous and evolutionary aspects by extending chemoton model. While the original chemoton incorporates metabolism, membrane, and genetic templates, it lacks a mechanism for phenotypic variation, preventing true evolutionary dynamics. To address this, we introduced a genotype-phenotype coupling by linking templates to a second autocatalytic cycle, enabling mutations to affect phenotype and be subject to selection. Using a genetic algorithm, we simulated populations of chemotons over generations. Results showed that chemotons without access to the new cycle remained in a stable but complexity-limited regime, while lineages acquiring the additional metabolic set evolved longer templates. These findings demonstrate that even simple replicator systems can achieve primitive evolvability, highlighting structural thresholds and rare innovations as key drivers. Our framework provides a tractable model for exploring autonomy and evolution in ALife.",0,arxiv,Evrim,CC-BY/arXiv,Evolvable Chemotons: Toward the Integration of Autonomy and Evolution
"Physics-informed neural networks (PINNs) are neural networks that embed the laws of dynamical systems modeled by differential equations into their loss function as constraints. In this work, we present a PINN framework applied to oncology. Here, we seek to learn time-varying interactions due to a combination therapy in a tumor microenvironment. In oncology, experimental data are often sparse and composed of a few time points of tumor volume. By embedding inductive biases derived from prior information about a dynamical system, we extend the physics-informed neural networks (PINN) and incorporate observed biological constraints as regularization agents. The modified PINN algorithm is able to steer itself to a reasonable solution and can generalize well with only a few training examples. We demonstrate the merit of our approach by learning the dynamics of treatment applied intermittently in an ordinary differential equation (ODE) model of a combination therapy. The algorithm yields a solution to the ODE and time-varying forms of some of the ODE model parameters. We demonstrate a strong convergence using metrics such as the mean squared error (MSE), mean absolute error (MAE), and mean absolute percentage error (MAPE).",0,arxiv,Evrim,CC-BY/arXiv,Modeling Adoptive Cell Therapy in Bladder Cancer from Sparse Biological Data using PINNs
"Phylogenetic trees capture evolutionary relationships among species and reflect the forces that shaped them. While many studies rely on branch length information, the topology of phylogenetic trees (particularly their degree of imbalance) offers a robust framework for inferring evolutionary dynamics when timing data is uncertain. Classical metrics, such as the Colless and Sackin indices, quantify tree imbalance and have been extensively used to characterize phylogenies. Empirical phylogenies typically show intermediate imbalance, falling between perfectly balanced and highly skewed trees. This regime is marked by a power-law relationship between subtree sizes and their cumulative sizes, governed by a characteristic exponent. Although a recent niche-size model replicates this scaling, its mathematical origin and the exponent's value remain unclear. We present a generative model inspired by Kingman's coalescent that incorporates niche-like dynamics through preferential node coalescence. This process maps to Smoluchowski's coagulation kinetics and is described by a generalized Smoluchowski equation. Our model produces imbalanced trees with power-law exponents matching empirical and numerical observations, revealing the mathematical basis of observed scaling laws and offering new tools to interpret tree imbalance in evolutionary contexts.",0,arxiv,Evrim,CC-BY/arXiv,Power-laws in phylogenetic trees and the preferential coalescent
"Encounters between individuals underlie key ecological processes such as predation, mating, and disease transmission, making encounter rates a direct link between individual movement behavior and population-level outcomes. We investigate how two common features of animal movement--directional persistence and range residency--jointly shape encounter rates. Using the Ornstein-Uhlenbeck with foraging (OUF) model, which integrates these two properties of animal movement, we derive exact analytical expressions for encounter rates and show that, for range-resident animals, the effect of persistence depends strongly on the degree of home-range overlap. Based on this theoretical result, we then introduce a new encounter-based metric that quantifies the spatial organization of home ranges at scales relevant to animal encounters. We finally apply this metric to movement data from lowland tapirs ($\textit{Tapirus terrestris}$) in Brazil's Pantanal region and find a significant level of home-range spatial segregation, consistent with the solitary behavior of this species.",0,arxiv,Evrim,CC-BY/arXiv,Using transient encounter rates to quantify spatial patterns of home-range organization
"Phylogenetic inference, the task of reconstructing how related sequences evolved from common ancestors, is a central objective in evolutionary genomics. The current state-of-the-art methods exploit probabilistic models of sequence evolution along phylogenetic trees, by searching for the tree maximizing the likelihood of observed sequences, or by estimating the posterior of the tree given the sequences in a Bayesian framework. Both approaches typically require to compute likelihoods, which is only feasible under simplifying assumptions such as independence of the evolution at the different positions of the sequence, and even then remains a costly operation. Here we present the first likelihood-free inference method for posterior distributions over phylogenies. It exploits a novel expressive encoding for pairs of sequences, and a parameterized probability distribution factorized over a succession of subtree merges. The resulting network provides accurate estimates of the posterior distribution outperforming both state-of-the-art maximum likelihood methods and a previous likelihood-free method for point estimation. It opens the way to fast and accurate phylogenetic inference under models of sequence evolution beyond those amenable to current likelihood-based inference methods.",0,arxiv,Evrim,CC-BY/arXiv,Likelihood-free inference of phylogenetic tree posterior distributions
"Accurate epidemic forecasting requires models that account for the layered and heterogeneous nature of real social interactions. The basic reproduction number $\mathcal R_0$ calculated from models that assume homogeneous mixing or single-layer contact structures have limited applicability to complex social systems. Here, we propose an expression of $\mathcal R_0$ in the context of multiplex networks, enabling the analysis of disease transmission across multiple social layers.   We adapt the Degree-Based Mean-Field (DBMF) SIR model for single-layered complex networks to the multiplex setting, where each layer has its own degree distribution and infection rate. Using the Next Generation Matrix method, we derive an analytical expression for the basic reproduction number $\mathcal R_0$. Numerical integration of the multiplex DBMF equations shows that $\mathcal R_0 = 1$ marks the epidemic threshold and governs the functional dependence of key outbreak indicators. In addition to the exact result for the $\mathcal R_0$, we provide an approximation denoted as $Ï„$, which is easier to compute and more straightforward to interpret in terms of the parameters of the system, and shares most of the expected properties of the basic reproduction number.   Stochastic agent-based simulations confirm these results, demonstrating a direct correspondence between $Ï„$ and the average number of secondary infections in the early epidemic phase, in line with the interpretation of $\mathcal R_0$.   This research provides a robust generalization of $\mathcal R_0$ for layered contact structures, offering a more realistic basis for epidemic forecasting and the design of intervention strategies.",0,arxiv,Evrim,CC-BY/arXiv,Modeling Epidemics on Multiplex Networks: Epidemic Threshold and Basic Reproduction Number
"Reconstructing evolutionary histories and estimating the rate of evolution from molecular sequence data is of central importance in evolutionary biology and infectious disease research. We introduce a flexible Bayesian phylogenetic inference framework that accommodates changing evolutionary rates over time by modeling sequence character substitution processes as inhomogeneous continuous-time Markov chains (ICTMCs) acting along the unknown phylogeny, where the rate remains as an unknown, positive and integrable function of time. The integral of the rate function appears in the finite-time transition probabilities of the ICTMCs that must be efficiently computed for all branches of the phylogeny to evaluate the observed data likelihood. Circumventing computational challenges that arise from a fully nonparametric function, we successfully parameterize the rate function as piecewise constant with a large number of epochs that we call the polyepoch clock model. This makes the transition probability computation relatively inexpensive and continues to flexibly capture rate change over time. We employ a Gaussian Markov random field prior to achieve temporal smoothing of the estimated rate function. Hamiltonian Monte Carlo sampling enabled by scalable gradient evaluation under this model makes our framework computationally efficient. We assess the performance of the polyepoch clock model in recovering the true timescales and rates through simulations under two different evolutionary scenarios. We then apply the polyepoch clock model to examine the rates of West Nile virus, Dengue virus and influenza A/H3N2 evolution, and estimate the time-varying rate of SARS-CoV-2 spread in Europe in 2020.",0,arxiv,Evrim,CC-BY/arXiv,Inhomogeneous continuous-time Markov chains to infer flexible time-varying evolutionary rates
"The evolutionary mechanisms of cooperative behavior represent a fundamental topic in complex systems and evolutionary dynamics. Although recent advances have introduced real-world stochasticity in nonlinear public goods game (PGG), such stochasticity remains static, neglecting its origin in the external environment as well as the coevolution of system stochasticity and cooperative behavior driven by environmental dynamics. In this work, we introduce a dynamic environment feedback mechanism into the stochastic nonlinear PGG framework, establishing a coevolutionary model that couples environmental states and individual cooperative strategies. Our results demonstrate that the interplay among environment feedback, nonlinear effects, and stochasticity can drive the system toward a wide variety of steady-state structures, including full defection, full cooperation, stable coexistence, and periodic limit cycles. Further analysis reveals that asymmetric nonlinear parameters and environment feedback rates exert significant regulatory effects on cooperation levels and system dynamics. This study not only enriches the theoretical framework of evolutionary game theory, but also provides a foundation for the management of ecological systems and the design of cooperative mechanisms in society.",0,arxiv,Evrim,CC-BY/arXiv,Nonlinear Public Goods Game in Dynamical Environments
"We address the problem of localizing the source of infection in an undirected, tree-structured network under a susceptible-infected outbreak model. The infection propagates with independent random time increments (i.e., edge-delays) between neighboring nodes, while only the infection times of a subset of nodes can be observed. We show that a reduced set of observers may be sufficient, in the statistical sense, to localize the source and characterize its identifiability via the joint Laplace transform of the observers' infection times. Using the explicit form of these transforms in terms of the edge-delay probability distributions, we propose scale-invariant least-squares estimators of the source. We evaluate their performance on synthetic trees and on a river network, demonstrating accurate localization under diverse edge-delay models. To conclude, we highlight overlooked technical challenges for observer-based source localization on networks with cycles, where standard spanning-tree reductions may be ill-posed.",0,arxiv,Evrim,CC-BY/arXiv,Observer-Based Source Localization in Tree Infection Networks via Laplace Transforms
"We employ an n-player coordination game to model mutualism emergence and abandonment. We illustrate our findings in the context of the host--host interactions among plants in plant-mycorrhizal fungi (MF) mutualisms. The coordination game payoff structure captures the insight that mutualistic strategies lead to robust advantages only after such ""biological markets"" reach a certain scale. The game gives rise to three types of Nash equilibria, which correspond to the states derived in studies of the ancestral reconstruction of the mycorrhizal symbiosis in seed plants. We show that all types of Nash equilibria correspond to steady states of a dynamical system describing the underlying evolutionary process. We then employ methods from large deviation theory on discrete-time Markov processes to study stochastic evolutionary dynamics. We provide a sharp analytical characterization of the stochastic steady states and of the transition dynamics across Nash equilibria and employ simulations to illustrate these results in special cases. We find that the mutualism is abandoned and re-established several times through evolutionary time, but the mutualism may persist the majority of time. Changes that reduce the benefit-to-cost ratio associated with the symbiosis increase the likelihood of its abandonment. While the mutualism establishment and abandonment could result from direct transitions across the mutualistic and non-mutualistic states, it is far more likely for such transitions to occur indirectly through intermediate partially mutualistic states. The MF-plant mutualism might be (partially or fully) abandoned by plants even if it provides overall superior fitness.",0,arxiv,Evrim,CC-BY/arXiv,Symbiosis emergence and abandonment in nature: a coordination game approach
"Positive selection drives the emergence of adaptive mutations in Mycobacterium tuberculosis, shaping drug resistance, transmissibility, and virulence. Phylogenetic trees capture evolutionary relationships among isolates and provide a natural framework for detecting such adaptive signals. We present a phylogeny-guided graph attention network (GAT) approach, introducing a method for converting SNP-annotated phylogenetic trees into graph structures suitable for neural network analysis. Using 500 M. tuberculosis isolates from four major lineages and 249 single-nucleotide variants (84 resistance-associated and 165 neutral) across 61 drug-resistance genes, we constructed graphs where nodes represented isolates and edges reflected phylogenetic distances. Edges between isolates separated by more than seven internal nodes were pruned to emphasise local evolutionary structure. Node features encoded SNP presence or absence, and the GAT architecture included two attention layers, a residual connection, global attention pooling, and a multilayer perceptron classifier. The model achieved an accuracy of 0.88 on a held-out test set and, when applied to 146 WHO-classified ""uncertain"" variants, identified 41 candidates with convergent emergence across multiple lineages, consistent with adaptive evolution. This work demonstrates the feasibility of transforming phylogenies into GNN-compatible structures and highlights attention-based models as effective tools for detecting positive selection, aiding genomic surveillance and variant prioritisation.",0,arxiv,Evrim,CC-BY/arXiv,Decoding Positive Selection in Mycobacterium tuberculosis with Phylogeny-Guided Graph Attention Models
"The origin of microbial cells required the emergence of metabolism, an autocatalytic network of roughly 400 enzymatically catalyzed chemical reactions that synthesize the building blocks of life: amino acids, nucleotides and cofactors. Proposals for metabolic origin are theoretical in nature [1-9], empirical studies addressing the origin and early evolution of the 400-reaction chemical network itself are lacking. Here we identify intermediate states in the primordial assembly of metabolism from its inorganic origins, using structure-refined clusters for metabolic enzymes of prokaryotic genomes. We show that metabolism in the last universal common ancestor (LUCA) was enzymatically incomplete, undergoing final assembly independently in the lineages leading to bacteria and archaea, with metal catalysts that predated both enzymes and cofactors providing essential functions. Over half of modern core metabolism corresponds to laboratory reactions catalyzed by native transition metals--Fe(0), Co(0), Ni(0) and their alloys--under conditions of serpentinizing hydrothermal vents. As the hitherto elusive source of primordial aqueous phosphorylation, we show that phosphite, a constituent of serpentinizing systems [10], phosphorylates AMP [11] to ADP using native metals in water. Seventeen cofactors that transfer electrons, nitrogen, and carbon units to substrates in modern metabolism [12] can be functionally replaced by environmental transition metals [13-19]. The data reveal that cofactors are synthesized late in enzymatic metabolism and are required in reactions preceding their synthesis, specifying the existence at origins of simpler precursors, which we identify here as native metals. Cofactors liberated metabolism from a requirement for solid state catalysis at a phosphorylating hydrothermal vent, engendering its autocatalytic state.",0,arxiv,Evrim,CC-BY/arXiv,Gradual assembly of metabolism at a phosphorylating hydrothermal vent
"Predicting species distributions using occupancy models accounting for imperfect detection is now commonplace in ecology. Recently, modelling spatial and temporal autocorrelation was proposed to alleviate the lack of replication in occupancy data, which often prevents model identifiability. However, how such models perform in highly heterogeneous datasets where missing or single-visit data dominates remains an open question. Motivated by an heterogeneous fine-scale butterfly occupancy dataset, we evaluate the performance of a multi-season occupancy model with spatial and temporal random effects to a skewed (Poisson) distribution of the number of surveys per site, overlap of covariates between occupancy and detection submodels, and spatiotemporal clustering of observations. Results showed that the model is robust to heterogeneous data and covariate overlap. However, when spatiotemporal gaps were added, site occupancy was biased towards the average occupancy, itself overestimated. Random effects did not correct the influence of gaps, due to identifiability issues of variance and autocorrelation parameters. Occupancy analysis of two butterfly species further confirmed these results. Overall, multi-season occupancy models with autocorrelation are robust to heterogeneous data and covariate overlap, but still present identifiability issues and are challenged by severe data gaps, which compromise predictions even in data-rich areas.",0,arxiv,Evrim,CC-BY/arXiv,Evaluating multi-season occupancy models with autocorrelation fitted to heterogeneous datasets
"Environmental feedback mechanisms are ubiquitous in real-world complex systems. In this study, we incorporate a homogeneous environment into the evolutionary dynamics of a three-state system comprising cooperators, defectors, and empty nodes. Both coherence resonance and equilibrium states, resulting from the tightly clustering of cooperator agglomerates, enhance population survival and environmental quality. The resonance phenomenon arises at the transition between cooperative and defective payoff parameters in the prisoner's dilemma game.",0,arxiv,Evrim,CC-BY/arXiv,Three-state coevolutionary game dynamics with environmental feedback
"We define symmetric and asymmetric branching trees, a class of processes particularly suited for modeling genealogies of inhomogeneous populations where individuals may reproduce throughout life. In this framework, a broad class of Crump-Mode-Jagers processes can be constructed as (a)symmetric Sevast'yanov processes, which count the branches of the tree. Analogous definitions yield reduced (a)symmetric Sevast'yanov processes, which restrict attention to branches that lead to extant progeny. We characterize their laws through generating functions. The genealogy obtained by pruning away branches without extant progeny at a fixed time is shown to satisfy a branching property, which provides distributional characterizations of the genealogy.",0,arxiv,Evrim,CC-BY/arXiv,Inhomogeneous branching trees with symmetric and asymmetric offspring and their genealogies
"Insect species subject to infection, predation, and anisotropic environmental conditions may exhibit preferential movement patterns. Given the innate stochasticity of exogenous factors driving these patterns over short timescales, individual insect trajectories typically obey overdamped stochastic dynamics. In practice, data-driven modeling approaches designed to learn the underlying Fokker-Planck equations from observed insect distributions serve as ideal tools for understanding and predicting such behavior. Understanding dispersal dynamics of crop and silvicultural pests can lead to a better forecasting of outbreak intensity and location, which can result in better pest management. In this work, we extend weak-form equation learning techniques, coupled with kernel density estimation, to learn effective models for lepidopteran larval population movement from highly sparse experimental data. Galerkin methods such as the Weak form Sparse Identification of Nonlinear Dynamics (WSINDy) algorithm have recently proven useful for learning governing equations in several scientific contexts. We demonstrate the utility of the method on a sparse dataset of position measurements of fall armyworms (Spodoptera frugiperda) obtained in simulated agricultural conditions with varied plant resources and infection status.",0,arxiv,Evrim,CC-BY/arXiv,Weak Form Learning for Mean-Field Partial Differential Equations: an Application to Insect Movement
"We investigate predator-prey school interactions in aquatic environments using a stochastic differential equation (SDE)-based, particle-level model that incorporates attraction, repulsion, alignment, and environmental noise. Two predation strategies-center attack and nearest attack-are examined to assess their effects on prey survival, predator efficiency, and group dynamics. Simulations reveal diverse emergent behaviors such as prey dispersal and regrouping, oscillatory predation with collective defense, and predator encirclement. Results show that collective hunting enhances capture efficiency compared to solitary attacks, but benefits diminish beyond a critical predator group size due to intra-predator competition. This work provides new insights into cooperative predation and introduces a generalizable SDE framework for analyzing predator-prey interactions.",0,arxiv,Evrim,CC-BY/arXiv,Modeling Predator-Prey Dynamics with Stochastic Differential Equations: Patterns of Collective Hunting and Nonlinear Predation Effects
"Why do human populations remain vulnerable to collapse, even when they are large? Classical demographic theory predicts that volatility in growth should decline rapidly with size due to the averaging effects of the law of large numbers. As such, while small-scale societies may be demographically fragile, large-scale societies should be much more stable. Using a large census dataset of 228 indigenous societies from Brazil, we show that this prediction does not hold. Instead of volatility declining as the square root of population size, it falls much more slowly. This means that individuals within communities do not behave as independent demographic units as their lives are correlated through cooperation, shared subsistence practices, overlapping land use, and exposure to common shocks such as disease outbreaks or failed harvests. These correlations build demographic synchrony, drastically reducing the effective demographic degrees of freedom in a population, keeping volatility higher than expected at all scales. As a result, large-scale populations fluctuate as if they were much smaller, increasing their vulnerability to collapse. This helps explain why human societies of all sizes seem vulnerable to collapse, and why the archaeological and historical record is filled with examples of large, complex societies collapsing despite their size. We suggest demographic synchrony provides a general mechanism for understanding why human populations remain vulnerable across all scales: Scale still stabilizes synchronous populations via density increases, but synchrony ensures that stability grows only slowly with size, leaving large populations more volatile, and more vulnerable, than classical demographic theory predicts.",0,arxiv,Evrim,CC-BY/arXiv,Demographic synchrony increases the vulnerability of human societies to collapse
"Zoonotic pathogens represent a growing global risk, yet the speed of adaptive immune activation across mammalian species remains poorly understood. Despite orders-of-magnitude differences in size and metabolic rate, we show that the time to initiate adaptive immunity is remarkably consistent across species. To understand this invariance, we analyse empirical data showing how the numbers and sizes of lymph nodes scale with body mass, finding that larger animals have both more and larger lymph nodes. Using scaling theory and our mathematical model, we show that larger lymph nodes enable faster search times, conferring an advantage to larger animals that otherwise face slower biological times. This enables mammals to maintain, or even accelerate, the time to initiate the adaptive immune response as body size increases. We validate our analysis in simulations and compare it to empirical data.",0,arxiv,Evrim,CC-BY/arXiv,Bigger is Faster in the Adaptive Immune Response
"Aging is a universal consequence of life, yet researchers have identified no universal theme. This manuscript considers aging from the perspective of entropy, wherein things fall apart. We first examine biological information change as a mutational distance, analogous to physical distance. In this model, informational change over time is fitted to an advection-diffusion equation, a normal distribution with a time component. The solution of the advection-diffusion equation provides a means of measuring the entropy of diverse biological systems. The binomial distribution is also sufficient to demonstrate that entropy increases as mutations or epimutations accumulate. As modeled, entropy scales with lifespans across the tree of life. This perspective provides potential mechanistic insights and testable hypotheses as to how evolution has attained enhanced longevity: entropy management. We find entropy is an inclusive rather than exclusive aging theory.",0,arxiv,Evrim,CC-BY/arXiv,Entropy and diffusion characterize mutation accumulation and biological information loss
"Ancestral recombination graphs (ARGs) encode the complete genealogical history of a population of recombining lineages. ARGs, and their succinct representation, tree sequences, are increasingly central to modern population genetics methods, yet building an intuition for ARGs remains challenging. This is particularly true when analyzing ancestry in a geographic context, as there is a critical lack of dedicated, interactive tools capable of visualizing ARGs as spatiotemporal objects. To address this gap, we introduce ARGscape, an interactive platform for simulating, analyzing, and visualizing ARGs across space and time. ARGscape provides a user-friendly graphical interface featuring dynamic 2- and 3-dimensional visualizations to explore ARGs through space and time, as well as a novel ""spatial diff"" visualization for quantitative comparison of geographic inference methods. ARGscape is an innovative, unified framework that seamlessly integrates leading command-line, Python, and R-based tools for ARG simulation, manipulation, and use in spatiotemporal inference into both graphical and command-line interfaces. By integrating these various functionalities, ARGscape facilitates novel data exploration and hypothesis generation, while lowering the barrier to entry for spatiotemporal ARG analysis in both research and education use-cases. ARGscape is built with a Python FastAPI backend and a React/TypeScript frontend. It is freely available as a live demo at https://www.argscape.com and as a Python package on PyPI (pip install argscape). The source code and documentation are available on GitHub at https://github.com/chris-a-talbot/argscape.",0,arxiv,Evrim,CC-BY/arXiv,"ARGscape: A modular, interactive tool for manipulation of spatiotemporal ancestral recombination graphs"
"Additional food sources for an introduced predator are known to increase its efficiency on a target pest. In this context, inhibiting factors such as interference, predator competition, and the introduction of temporally dependent quantity and quality of additional food are all known to enable pest extinction. As climate change and habitat degradation have increasing effects in enhancing patchiness in ecological systems, the effect of additional food in patch models has also been recently considered. However, the question of complete pest extinction in such patchy systems remains open. In the current manuscript, we consider a biological control model where additional food drives competition among predators in one patch, and they subsequently disperse to a neighboring patch via drift or dispersal. We show that complete pest extinction in both patches is possible. Further, this state is proved to be globally asymptotically stable under certain parametric restrictions. We also prove a codimension-2 Bogdanov-Takens bifurcation. We discuss our results in the context of designing pest management strategies under enhanced climate change and habitat fragmentation. Such strategies are particularly relevant to control invasive pests such as the Soybean aphid (\emph{Aphis glycines}), in the North Central United States.",0,arxiv,Evrim,CC-BY/arXiv,"An additional food driven biological control patch model, incorporating generalized competition"
"Biological control strategies against mosquito-borne diseases--such as the sterile insect technique (SIT), RIDL, and Wolbachia-based releases--require reliable estimates of dispersal and survival of released males. We propose a mechanistic--statistical framework for mark--release--recapture (MRR) data linking an individual-based 2D diffusion model with its reaction--diffusion limit. Inference is based on solving the macroscopic system and embedding it in a Poisson observation model for daily trap counts, with uncertainty quantified via a parametric bootstrap. We validate identifiability using simulated data and apply the model to an urban MRR campaign in El Cano (Havana, Cuba) involving four weekly releases of sterile Aedes aegypti males. The best-supported model suggests a mean life expectancy of about five days and a typical displacement of about 180 m. Unlike empirical fits of survival or dispersal, our mechanistic approach jointly estimates movement, mortality, and capture, yielding biologically interpretable parameters and a principled framework for designing and evaluating SIT-based interventions.",0,arxiv,Evrim,CC-BY/arXiv,Mechanistic-statistical inference of mosquito dynamics from mark-release-recapture data
"From the formation of ice in small clusters of water molecules to the mass raids of army ant colonies, the emergent behavior of collectives depends critically on their size. At the same time, common wisdom holds that such behaviors are robust to the loss of individuals. This tension points to the need for a more systematic study of how number influences collective behavior. We initiate this study by focusing on collective behaviors that change abruptly at certain critical numbers of individuals. We show that a subtle modification of standard bifurcation analysis identifies such critical numbers, including those associated with discreteness- and noise-induced transitions. By treating them as instances of the same phenomenon, we show that critical numbers across physical scales and scientific domains commonly arise from competing feedbacks that scale differently with number. We then use this idea to find overlooked critical numbers in past studies of collective behavior and explore the implications for their conclusions. In particular, we highlight how deterministic approximations of stochastic models can fail near critical numbers. We close by distinguishing these qualitative changes from density-dependent phase transitions and by discussing how our approach could generalize to broader classes of collective behaviors.",0,arxiv,Evrim,CC-BY/arXiv,How many more is different?
"Phylogenetic trees represent certain species and their likely ancestors. In such a tree, present-day species are leaves and an edge from u to v indicates that u is an ancestor of v. Weights on these edges indicate the phylogenetic distance. The phylogenetic diversity (PD) of a set of species A is the total weight of edges that are on any path between the root of the phylogenetic tree and a species in A. Selecting a small set of species that maximizes phylogenetic diversity for a given phylogenetic tree is an essential task in preservation planning, where limited resources naturally prevent saving all species. An optimal solution can be found with a greedy algorithm [Steel, Systematic Biology, 2005; Pardi and Goldman, PLoS Genetics, 2005]. However, when a food web representing predator-prey relationships is given, finding a set of species that optimizes phylogenetic diversity subject to the condition that each saved species should be able to find food among the preserved species is NP-hard [Spillner et al., IEEE/ACM, 2008]. We present a generalization of this problem, where, inspired by biological considerations, the food web has weighted edges to represent the importance of predator-prey relationships. We show that this version is NP-hard even when both structures, the food web and the phylogenetic tree, are stars. To cope with this intractability, we proceed in two directions. Firstly, we study special cases where a species can only survive if a given fraction of its prey is preserved. Secondly, we analyze these problems through the lens of parameterized complexity. Our results include that finding a solution is fixed-parameter tractable with respect to the vertex cover number of the food web, assuming the phylogenetic tree is a star.",0,arxiv,Evrim,CC-BY/arXiv,Weighted Food Webs Make Computing Phylogenetic Diversity So Much Harder
"Progress of the COVID-19 pandemic was quantified, in the first instance, using the daily number of positive cases recorded by the national public health authorities. Averaged over a seven-day window, the daily incidence of COVID-19 in Germany reveals clear sections of exponential growth or decay in propagation of infection. Comparing with incidence profiles according to onset-of-symptoms shows that reporting of cases involves variable delays. Observed changes in exponential rates come from growing public awareness, governmental restrictions and their later relaxation, annual holidays, seasonal variation, emergence of new viral variants, and from mass vaccination. Combining the measured rates with epidemiological parameters established for SARS-CoV-2 yields the dynamics of change in disease transmission. Combined with the distribution of serial intervals (or generation times), the rate gives basic and instantaneous values of the reproduction number that govern development and ultimate outcome of the epidemic. Herd immunity requires vaccination of approximately seventy percent of the population, but this increases to circa eighty percent for the more transmissible Alpha-variant. Beyond this point, progressive vaccination reduces the susceptible population, and competes with the emergence of new variants. By the first Omicron wave, circa seventy percent were doubly vaccinated, with the target then standing at circa eighty percent. Combined with the distribution of times-to-death, incidence rates from onset of symptoms predict the daily profile of COVID-associated deaths and estimated case-fatality ratio. Cases are under-reported in the first wave and reflect age heterogeneity in fatalities at the second wave. In periods of low incidence, COVID mortality was one percent or less of detected infection.",0,arxiv,Evrim,CC-BY/arXiv,"Daily Profile of COVID-19 Infections in Germany, throughout the Pandemic"
"Evolutionary game theory offers a general framework to study how behaviors evolve by social learning in a population. This body of theory can accommodate a range of social dilemmas, or games, as well as real-world complexities such as spatial structure or behaviors conditioned on reputations. Nonetheless, this approach typically assumes a deterministic payoff structure for social interactions. Here, we extend evolutionary game theory to account for random changes in the social environment, so that mutual cooperation may bring different rewards today than it brings tomorrow, for example. Even when such environmental noise is unbiased, we find it can have a qualitative impact on the behaviors that evolve in a population. Noisy payoffs can permit the stable co-existence of cooperators and defectors in the prisoner's dilemma, for example, as well as bistability in snowdrift games and stable limit cycles in rock-paper-scissors games -- dynamical phenomena that cannot occur in the absence of noise. We conclude by discussing the relevance of our framework to scenarios where the nature of social interactions is subject to external perturbations.",0,arxiv,Evrim,CC-BY/arXiv,Evolution of social behaviors in noisy environments
"Climate change and fisheries jointly shape the resilience of the Barents Sea marine ecosystem, yet the recovery of key fish populations to climate and anthropogenic disturbances requires further investigation. This study examines how fishing pressure and climate change, driven by the NEMO-MEDUSA Earth system model, influence the recovery times of Demersal and Planktivorous fish in the Barents Sea. We used the StrathE2EPolar end-to-end ecosystem model to simulate transient dynamics under increasing fishing pressure scenarios, and quantified recovery times for Demersal, Planktivorous, and ecosystem-wide groups relative to a shifting unfished baseline. Recovery times increased with both fishing intensity and climate change, by as much as 18 years for Demersal fish and 54 years for Planktivorous fish across all fishing scenarios. At the ecosystem level, recovery was constrained by the slow rebound of top predators, many of which experienced biomass collapse under climate change, preventing recovery to a shifting baseline. Our results suggest that fishing pressure in tandem with climate change substantially reduces ecosystem resilience, highlighting the importance of sustainable harvest strategies in a changing climate.",0,arxiv,Evrim,CC-BY/arXiv,Ecosystem Recovery to Historical Targets Becomes Unattainable Under Modelled Fishing and Climate in the Barents Sea
"In this paper, we introduce gonosomic algebras to algebraically translate the phenomenon of genetic sterility. Gonosomic algebras extend the concept of gonosomal algebras used as algebraic model of genetic phenomena related to sex-determination and sex-linked gene transmission by allowing genetic sterility to be taken into account. Conditions under which gonosomic algebras are not gonosomal and several algebraic constructions of gonosomic algebras are given. To each gonosomic algebra, an evolution operator noted W is associated that gives the state of the offspring population at the birth stage. Next from W we define the operator V which gives the frequency distribution of genetic types. We show that the various stability notions of equilibrium points are preserved by passing from W to V .",0,arxiv,Evrim,CC-BY/arXiv,Gonosomic algebras: an extension of gonosomal algebras
"We investigate traveling wave solutions in the two-species reaction-diffusion Lotka-Volterra competition system under weak competition. For the strict weak competition regime $(b<a<1/c,\,d>0)$, we construct refined upper and lower solutions combined with the Schauder fixed point theorem to establish the existence of traveling waves for all wave speeds $s\geq s^*:=\max\{2,2\sqrt{ad}\}$, and provide verifiable sufficient conditions for the emergence of non-monotone waves. Such conditions for non-monotonic waves have not been explicitly addressed in previous studies. It is interesting to point out that our result for non-monotone waves also hold for the critical speed case $s=s^*$. In addition, in the critical weak competition case $(b<a=1/c,\,d>0)$, we rigorously prove, for the first time, the existence of front-pulse traveling waves.",0,arxiv,Evrim,CC-BY/arXiv,Non-Monotone Traveling Waves of the Weak Competition Lotka-Volterra System
"Metagenomic data has significantly advanced microbiome research by employing ecological models, particularly in personalised medicine. The generalised Lotka-Volterra (gLV) model is commonly used to understand microbial interactions and predict ecosystem dynamics. However, gLV models often fail to capture complex interactions, especially when data is limited or noisy. This study critically assesses the effectiveness of gLV and similar models using Bayesian inference and a model reduction method based on information theory. We found that ecological data often leads to non-interpretability and overfitting due to limited information, noisy data, and parameter sloppiness. Our results highlight the need for simpler models that align with the available data and propose a distribution-based approach to better capture ecosystem diversity, stability, and competition. These findings challenge current bottom-up ecological modelling practices and aim to shift the focus toward a Statistical Mechanics view of ecology based on distributions of parameters.",0,arxiv,Evrim,CC-BY/arXiv,"Scarce Data, Noisy Inferences, and Overfitting: The Hidden Flaws in Ecological Dynamics Modelling"
"This study investigates the impact of time since fire on bird community composition in Southern California chaparral ecosystems. We surveyed avian richness and abundance across 14 sites representing a 0 to 25 year post-fire chronosequence in Los Angeles County. Sites burned within the last five years supported fewer species, primarily dominated by generalists, while mid- to late-successional sites exhibited greater richness and a higher proportion of specialists. These patterns corresponded with increases in vegetation structural complexity over time. However, no consistent relationships were found between bird communities and abiotic variables, such as weather, temperature, and elevation, likely due to the single-visit sampling design. Our results align with successional theory and underscore the ecological importance of fire return intervals that allow full chaparral recovery. Restoration and management should prioritize long-term structural development, invasive grass control, and post-fire heterogeneity to support diverse and resilient avian communities.",0,arxiv,Evrim,CC-BY/arXiv,The Effects of Time Since Fire On Bird Community Composition in Chaparral Ecosystems Across Los Angeles County
"Everyone can see that over the last 150 years, theoretical ecology has become considerably more mathematical. But what is the nature of this phenomenon? Are mathematics applied, as in the use of statistical tests, for example, or are they involved, as in physics, where laws cannot be expressed without them? Through the history of the {\em Competitive Exclusion Principle} formulated at the very beginning of the 20th century by the naturalist Grinnell concerning the distribution of brown-backed chickadees, up to its modern integration into what is known in mathematics as population dynamics, I highlight the effectiveness of what could be called the mathematical novel in clarifying certain concepts in theoretical ecology.",0,arxiv,Evrim,CC-BY/arXiv,Evolution du Principe d'Exclusion CompÃ©titive : Le rÃ´le des mathÃ©matiques
"To promote climate adaptation and mitigation, it is crucial to understand stakeholder perspectives and knowledge gaps on land use and climate changes. Stakeholders across 21 European islands were consulted on climate and land use change issues affecting ecosystem services. Climate change perceptions included temperature, precipitation, humidity, extremes, and wind. Land use change perceptions included deforestation, coastal degradation, habitat protection, renewable energy facilities, wetlands, and others. Additional concerns such as invasive species, water or energy scarcity, infrastructure problems, and austerity were also considered. Climate and land use change impact perceptions were analysed with machine learning to quantify their influence. The predominant climatic characteristic is temperature, and the predominant land use characteristic is deforestation. Water-related problems are top priorities for stakeholders. Energy-related problems, including energy deficiency and issues with wind and solar facilities, rank high as combined climate and land use risks. Stakeholders generally perceive climate change impacts on ecosystem services as negative, with natural habitat destruction and biodiversity loss identified as top issues. Land use change impacts are also negative but more complex, with more explanatory variables. Stakeholders share common perceptions on biodiversity impacts despite geographic disparity, but they differentiate between climate and land use impacts. Water, energy, and renewable energy issues pose serious concerns, requiring management measures.",0,arxiv,Evrim,CC-BY/arXiv,The land use-climate change-biodiversity nexus in European islands stakeholders
"Our adaptive immune system relies on the persistence over long times of a diverse set of antigen-experienced B cells to encode our memories of past infections and to protect us against future ones. While longitudinal repertoire sequencing promises to track the long-term dynamics of many B cell clones simultaneously, sampling and experimental noise make it hard to draw reliable quantitative conclusions. Leveraging statistical inference, we infer the dynamics of memory B cell clonal dynamics and conversion to plasmablasts, which includes clone creation, degradation, abundance fluctuations, and differentiation. We find that memory B cell clones degrade slowly, with a half-life of 10 years. Based on the inferred parameters, we predict that it takes about 50 years to renew 50\% of the repertoire, with most observed clones surviving for a lifetime. We infer that, on average, 1 out of 100 memory B cells differentiates into a plasmablast each year, more than expected from purely antigen-stimulated differentiation, and that plasmablast clones degrade with a half-life of about one year in the absence of memory imports. Our method is general and could be applied to other longitudinal repertoire sequencing B cell subsets.",0,arxiv,Evrim,CC-BY/arXiv,Dynamics of memory B cells and plasmablasts in healthy individuals
"Public opinion on environmental issues remains polarized in many countries, posing a significant barrier to the implementation of effective policies. Behind this polarization, empirical studies have identified social susceptibility, personal prejudice, and personal experience as dominant factors in opinion formation on environmental issues. However, current coupled human-environment models have not yet incorporated all three factors in polarized populations. We developed a stylized coupled human-environment model to investigate how social susceptibility, personal prejudice, and personal experience shape opinion formation and the environment in polarized populations. Using analytical and numerical methods, we characterized the conditions under which polarization, consensus, opinion changes, and cyclic dynamics emerge depending on the costs of mitigation, environmental damage, and the factors influencing opinion formation. Our model shows that prejudice is the key driver of persistent polarization, with even slightly prejudiced populations maintaining indefinite polarization independent of their level of objectivity. We predict that polarization can be reduced by decreasing the role of prejudice or increasing the willingness to consider opposing opinions. Finally, our model shows that cost reduction methods are less effective at reducing environmental impact in prejudiced populations. Our model generates thresholds for when reducing costs or emissions is more useful depending on the factors which influence the population's opinion formation. Overall, our model provides a framework for investigating the importance of cognitive and social structures in determining human-environment dynamics.",0,arxiv,Evrim,CC-BY/arXiv,Coupled opinion-environmental dynamics in polarized and prejudiced populations
"Infected individuals in some epidemics can remain asymptomatic while still carrying and transmitting the infection. These individuals contribute to the spread of the epidemic and pose a significant challenge to public health policies. Identifying asymptomatic individuals is critical for measuring and controlling an epidemic, but periodic and widespread testing of healthy individuals is often too costly. This work tackles the problem of identifying asymptomatic individuals considering a classic SI (Susceptible-Infected) network epidemic model where a fraction of the infected nodes are not observed as infected (i.e., their observed state is identical to susceptible nodes). In order to classify healthy nodes as asymptomatic or susceptible, a Graph Neural Network (GNN) model with supervised learning is adopted where a set of node features are built from the network with observed infected nodes. The approach is evaluated across different network models, network sizes, and fraction of observed infections. Results indicate that the proposed methodology is robust across different scenarios, accurately identifying asymptomatic nodes while also generalizing to different network sizes and fraction of observed infections.",0,arxiv,Evrim,CC-BY/arXiv,Identifying Asymptomatic Nodes in Network Epidemics using Graph Neural Networks
"Climate change is a global emergency, as was the COVID-19 pandemic. Why was our collective response to COVID-19 so much stronger than our response to the climate emergency, to date? We hypothesize that the answer has to do with the scale of the systems, and not just spatial and temporal scales but also the `altruistic scale' that measures whether an action must rely upon altruistic motives for it to be adopted. We treat COVID-19 and climate change as common pool resource problems that exemplify coupled human-environment systems. We introduce a framework that captures regimes of containment, mitigation, and failure to control. As parameters governing these three scales are varied, it is possible to shift from a COVID-like system to a climate-like system. The framework replicates both inaction in the case of climate change mitigation, as well as the faster response that we exhibited to COVID-19. Our cross-system comparison also suggests actionable ways that cooperation can be improved in large-scale common pool resources problems, like climate change. More broadly, we argue that considering scale and incorporating human-natural system feedbacks are not just interesting special cases within non-cooperative game theory, but rather should be the starting point for the study of altruism and human cooperation.",0,arxiv,Evrim,CC-BY/arXiv,"Space, time and altruism in pandemics and the climate emergency"
"The COVID-19 pandemic underscored a critical need for intervention strategies that balance disease containment with socioeconomic stability. We approach this challenge by designing a framework for modeling and evaluating disease-spread prevention strategies. Our framework leverages multi-objective reinforcement learning (MORL) - a formulation necessitated by competing objectives - combined with a new stochastic differential equation (SDE) pandemic simulator, calibrated and validated against global COVID-19 data. Our simulator reproduces national-scale pandemic dynamics with orders of magnitude higher fidelity than other models commonly used in reinforcement learning (RL) approaches to pandemic intervention. Training a Pareto-Conditioned Network (PCN) agent on this simulator, we illustrate the direct policy trade-offs between epidemiological control and economic stability for COVID-19. Furthermore, we demonstrate the framework's generality by extending it to pathogens with different epidemiological profiles, such as polio and influenza, and show how these profiles lead the agent to discover fundamentally different intervention policies. To ground our work in contemporary policymaking challenges, we apply the model to measles outbreaks, quantifying how a modest 5% drop in vaccination coverage necessitates significantly more stringent and costly interventions to curb disease spread. This work provides a robust and adaptable framework to support transparent, evidence-based policymaking for mitigating public health crises.",0,arxiv,Evrim,CC-BY/arXiv,Learning Pareto-Optimal Pandemic Intervention Policies with MORL
"Antimicrobial resistance (AMR) threatens global health. A promising and underexplored strategy to tackle this problem are sequential therapies exploiting collateral sensitivity (CS), whereby resistance to one drug increases sensitivity to another. Here, we develop a four-genotype stochastic birth-death model with two bacteriostatic antibiotics to identify switching periods that maximize bacterial extinction under subinhibitory concentrations. We show that extinction probability depends nonlinearly on switching period, with stepwise increases aligned to discrete switch events: fast sequential therapies are suboptimal as they do not allow for the evolution of resistance, a key ingredient in these therapies. A geometric distribution framework accurately predicts cumulative extinction probabilities, where the per-switch extinction probability rises with switching period. We further derive a heuristic approximation for the extinction probability based on times to fixation of single-resistant mutants. Sensitivity analyses reveal that strong reciprocal CS is required for this strategy to work, and we explore how increasing antibiotic doses and higher mutation rates modulate extinction in a nonmonotonic manner. Finally, we discuss how longer therapies maximize extinction but also cause higher resistance, leading to a Pareto front of optimal switching periods. Our results provide quantitative design principles for in vitro and clinical sequential antibiotic therapies, underscoring the potential of CS-guided regimens to suppress resistance evolution and eradicate infections.",0,arxiv,Evrim,CC-BY/arXiv,Optimization of sequential therapies to maximize extinction of resistant bacteria through collateral sensitivity
"Chikungunya (CHIK) is a viral disease transmitted to humans through the bites of {\it Aedes} mosquitoes infected with the chikungunya virus (CHIKV). CHIKV has been imported annually to Florida in the last decade due to Miami's crucial location as a hub for international travel, particularly from Central and South America including Brazil, where CHIK is endemic. This work addresses to the geographic spread of CHIK, incorporating factors such as human movement, temperature dependency, as well as vertical transmission, and incubation periods, for different patches. Central to the model is the integration of a multi-patch framework, and in the numerical analysis it is considered human movement between endemic Brazilian states and Florida. We establish crucial correlations between the mosquito reproduction number $\mathcal{R}_{m}$ and the disease reproduction number $\mathcal{R}_{0}$ with the disease dynamics in a multi-patch environment, encompassing not only a numerical analysis but also from a theoretical perspective. Through numerical simulations, validated with real population and temperature data, it is possible to understand the disease dynamics under many different scenarios and make future projections, offering insights for potential effective control strategies, as well as addressing the timing for these strategies to be adopted.",0,arxiv,Evrim,CC-BY/arXiv,On the Geographic Spread of Chikungunya between Brazil and Florida: A Multi-patch Model with Time Delay
"We study a bi-virus susceptible-infected-susceptible (SIS) epidemic model in which individuals are either susceptible or infected with one of two virus strains, and consider mutation-driven transitions between strains. The general case of bi-directional mutation is first analyzed, where we characterize the disease-free equilibrium and establish its global asymptotic stability, as well as the existence, uniqueness, and stability of an endemic equilibrium. We then present a game-theoretic framework where susceptible individuals strategically choose whether to adopt protection or remain unprotected, to maximize their instantaneous payoffs. We derive Nash strategies under bi-directional mutation, and subsequently consider the special case of unidirectional mutation. In the latter case, we show that coexistence of both strains is impossible when mutation occurs from the strain with lower reproduction number and transmission rate to the other strain. Furthermore, we fully characterize the stationary Nash equilibrium (SNE) in the setting permitting coexistence, and examine how mutation rates influence protection adoption and infection prevalence at the SNE. Numerical simulations corroborate the analytical results, demonstrating that infection levels decrease monotonically with higher protection adoption, and highlight the impact of mutation rates and protection cost on infection state trajectories.",0,arxiv,Evrim,CC-BY/arXiv,Bi-Virus SIS Epidemic Propagation under Mutation and Game-theoretic Protection Adoption
A much-cited 2022 paper by Pekar et al. claimed that Bayesian analysis of the molecular phylogeny of early SARS-CoV-2 cases indicated that it was more likely that two successful introductions to humans had occurred than that just one had. Here I show that after correcting a fundamental error in Bayesian reasoning the results in that paper give larger likelihood for a single introduction than for two.,0,arxiv,Evrim,CC-BY/arXiv,Bayesian Re-Analysis of the Phylogenetic Topology of Early SARS-CoV-2 Case Sequences
"In this work, we investigate the population dynamics of tumor cells under therapeutic pressure. Although drug treatment initially induces a reduction in tumor burden, treatment failure frequently occurs over time due to the emergence of drug resistance, ultimately leading to cancer recurrence. To model this process, we employ a two-type branching process with state-dependent growth rates. The model assumes an initial tumor population composed predominantly of drug-sensitive cells, with a small subpopulation of resistant cells. Sensitive cells may acquire resistance through mutation, which is coupled to a change in cellular fitness. Furthermore, the growth rates of resistant cells are modulated by the overall tumor burden. Using stochastic differential equation techniques, we establish a functional law of large numbers for the scaled populations of sensitive cells, resistant cells, and the initial resistant clone. We then define the stochastic recurrence time as the first time the total tumor population regrows to its initial size following treatment. For this recurrence time, as well as for measures of clonal diversity and the size of the largest resistant clone at recurrence, we derive corresponding law of large number limits. These asymptotic results provide a theoretical foundation for constructing statistically consistent estimators for key biological parameters, including the cellular growth rates, the mutation rate, and the initial fraction of resistant cells.",0,arxiv,Evrim,CC-BY/arXiv,Parameter Estimation in Recurrent Tumor Evolution with Finite Carrying Capacity
"Early detection of African swine fever virus (ASFV) is critical to preventing widespread epidemics. However, the effectiveness of within-farm sampling remains to be examined, particularly during the early phase of an outbreak when disease prevalence is low, animals may be asymptomatic, or clinical signs are masked by co-circulating diseases. This study assessed four sampling strategies for detecting ASFV-infected animals in suspected growing pig farms within the first 14 days of the introduction of either a high- or moderate-virulence ASFV strain. Pens were selected using three methods: random sampling, targeted sampling of pens with clinical animals, and informative sampling based on estimated pen infection probabilities. The informative sampling method was further divided into sequential method, which ranked pens by descending viral load probability, and cluster and random method, which selected pens at random from high and low viral load clusters. Each pen-selection method was examined with different sample sizes. We calculated the sensitivity of each approach as the probability of detecting at least one ASFV-positive pig per farm. Results showed that sampling 30 pens with one pig per pen using the target and random pen-selection method yielded the highest detection sensitivity, even in the presence of other co-circulating diseases that interfere with the accurate identification of clinical ASFV cases. In contrast, sampling five pens resulted in the lowest sensitivity. These findings provide valuable insights for improving ASFV surveillance strategies in the U.S. and can inform preparedness efforts for other foreign animal diseases.",0,arxiv,Evrim,CC-BY/arXiv,Evaluating sampling strategies for effective detection of African swine fever in the growing pig population in the U.S
"Road mortality may be a significant factor in the global decline of amphibian populations, yet rigorous assessments of its effect on long-term population persistence are lacking.   Here, we investigate population persistence through a field study and mathematical model of a western toad ({\textit{Anaxyrus Boreas}} {\RR(Baird and Girard, 1852)}) population within a highway corridor in the Selkirk Mountains of British Columbia.   The analysis shows traffic levels strongly correlate with toad mortality, with each additional vehicle causing a 3.1\% $\pm$ 1.3\% ($p=0.020$) increase in toad deaths.   Although the current risk of the population becoming threatened or endangered is low, it rises to 50\% if baseline road mortality increases from 10\% to 30\%. Gravid female mortality is higher than the baseline mortality and can increase the probability of endangerment by nearly two-fold at higher baseline mortality levels.   We make the case that a small increase in vehicle traffic resulting from future development and recreational pressures   could destabilize this apparently healthy toad population. The high sensitivity to traffic levels and rapid transition from healthy to endangered raises concerns for similar populations worldwide. Compensatory structures such as amphibian underpasses (toad tunnels) should be given high priority.",0,arxiv,Evrim,CC-BY/arXiv,Modelling road mortality risks to persistence to a Western Toad ({\it Anaxyrus boreas}) population in British Columbia
"We extend a formal framework that previously derived time from the multifractal structure of biological lineages (Hudnall \& D'Souza, 2025). That work showed that time itself is multifractal -- not a universal background dimension, but an observer-dependent geometry. Here we develop the corresponding theory of measurement: showing that a multifractal conception of time not only permits measurement, but grounds it more rigorously in the structure of biology. The tree of life is modeled as the outcome of stochastic, convex branching, and we show how information-theoretic and fractal measures render its multifractal geometry into measurable, observer-relative time intervals. At the core is a dilation equation that expresses relative time elapse between entities as dimensionless ratios. Operational standards such as the SI second remain valid, but our framework makes explicit their lineage-dependence. This framework unifies measurement theory with biological form, preserves full compatibility with established science, and provides a biologically grounded theory of observation. It enables comparative analyses of duration and kinematics across lineages, with predictions that are directly open to experimental validation.",0,arxiv,Evrim,CC-BY/arXiv,Information and the living tree of life: A theory of measurement grounded in biology
"We analyze a variant of the Noisy $K$-Branching Random Walk, a population model that evolves according to the following procedure. At each time step, each individual produces a large number of offspring that inherit the fitness of their parents up to independent and identically distributed fluctuations. The next generation consists of a random sample of all the offspring so that the population size remains fixed, where the sampling is made according to a parameterized Gibbs measure of the fitness of the offspring. Our model interpolates between classical models of fitness waves and exhibits a novel phase transition in the propagation of the wave. By employing a stochastic Hopf-Cole transformation, we show that as we increase the population size, the random dynamics of the model can be described by deterministic operations acting on the limiting population densities. We then show that for fitness fluctuations with exponential tails, these operations admit a unique traveling wave solution with local stability. The traveling wave solution undergoes a phase transition when changing selection pressure, revealing a complex interaction between evolution and natural selection.",0,arxiv,Evrim,CC-BY/arXiv,$K$-Branching Random Walk with Noisy Selection: Large Population Limits and Phase Transitions
"By unifying three foundational principles of modern biology, we develop a mathematical framework to analyze the growing tree of life. Contrary to the static case, where the analogy between phylogenetic trees and the tree that grows in soil is drawn, our framework shows that the living tree of life is analogous to a Cantor dust where each branch is a distinct fractal curve. The system as a whole is therefore multifractal in the sense that it consists of many unique fractals. The three foundational principles for the mathematical framework are that phylogeny is nested, phylogeny is dualistic (i.e., transitive between singularities and populations), and phylogeny is stochastic. Integrating these three principles, we model the dynamic (i.e., living) tree of life as a random iterated function system that generates unique convexly related sequences of branching random variables (visualized in Animation 1). The multifractal nature of this dynamic tree of life implies that, for any two living entities, the time interval from their last common ancestor to the present moment is a distinct fractal curve for each. Thus, the length of a time interval along each distinct branch is unique, so that time is also multifractal and not an ultrametric on the tree of life.",0,arxiv,Evrim,CC-BY/arXiv,What does the tree of life look like as it grows? Evolution and the multifractality of time
"Lattice-based random walk models are widely used to study populations of migrating cells with motility bias and proliferation. Crowding is typically represented by volume exclusion, where each lattice site can be occupied by at most one agent and conflicting moves are aborted. This framework enables simulations that yield both population-level spatiotemporal agent density profiles and individual agent trajectories, comparable to experimental cell-tracking data. Previous continuum models for tagged-agent trajectories captured trajectory information only, and overlooked any measure of variability. This is an important limitation since trajectory data is inherently variable. To address this limitation, here we derive partial differential equations for the probability density function of tagged-agent trajectories. This continuum description has a clear physical interpretation, agrees well with distributional data from stochastic simulations, reveals the role of stochasticity in different contexts, and generalises to multiple subpopulations of distinct agents.",0,arxiv,Evrim,CC-BY/arXiv,Continuum models describing probabilistic motion of tagged agents in exclusion processes
"In network-based SIS models of infectious disease transmission, infection can only occur between directly connected individuals. This constraint naturally gives rise to spatial correlations between the states of neighboring nodes, as the infection status of connected individuals becomes interdependent. Although mean-field approximations are commonly invoked to simplify disease forecasting on networks, they fail to account for these correlations by assuming that infectious individuals are well-mixed within a population, leading to inaccurate predictions of infection numbers over time. As such, the development of mathematical frameworks that account for spatially correlated infections is of great interest, as they offer a compromise between accurate disease forecasting and analytic tractability. Here, we use existing corrections to mean-field theory on the regular lattice to construct a more general framework for equivalent corrections on regular random graph topologies. We derive and simulate a system of ordinary differential equations for the time evolution of the spatial correlation function at various geodesic distances on random networks, and use solutions to this hierarchy of ordinary differential equations to predict the global infection density as a function of time, finding good agreement with corresponding numerical simulations. Our results constitute a substantial development on existing corrections to mean-field theory for infectious individuals in SIS processes and provide an in-depth characterization of how structural randomness in networks affects the dynamical trajectories of infectious diseases on networks.",0,arxiv,Evrim,CC-BY/arXiv,Spatial correlations in SIS processes on regular random graphs
"In complex ecological communities, species may self-organize into clusters or clumps where highly similar species can coexist. The emergence of such species clusters can be captured by the interplay between neutral and niche theories. Based on the generalized Lotka-Volterra model of competition, we propose a minimal model for ecological communities in which the steady states contain self-organized clusters. In this model, species compete only with their neighbors in niche space through a common interaction strength. Unlike many previous theories, this model does not rely on random heterogeneity in interactions. By varying only the interaction strength, we find an exponentially large set of cluster patterns with different sizes and combinations. There are sharp phase transitions into the formation of clusters. There are also multiple phase transitions between different sets of possible cluster patterns, many of which accumulate near a small number of critical points. We analyze such a phase structure using both numerical and analytical methods. In addition, the special case with only nearest neighbor interactions is exactly solvable using the method of transfer matrices from statistical mechanics. We analyze the critical behavior of these systems and make comparisons with existing lattice models.",0,arxiv,Evrim,CC-BY/arXiv,A minimal model of self-organized clusters with phase transitions in ecological communities
"In recent decades, European forests have faced an increased incidence of fire disturbances. This phenomenon is likely to persist, given the rising frequency of extreme events expected in the future. Estimating canopy recovery time after disturbance serves as a critical assessment for understanding forest resilience, which can ultimately help determine the ability of forests to regain their capacity to provide essential ecosystem services. This study estimated fire severity and post-disturbance recovery in European forests using a remote sensing--based time series approach. MODIS Leaf Area Index (LAI) time series data were used to track the evolution of vegetation cover over burned areas from 2001 to 2024. Fire severity was defined relative to pre-disturbance conditions by comparing vegetation status before and after fire events. Recovery intervals were determined from temporal evolution of vegetation greening as the duration required to reach the pre-disturbance LAI baseline. Furthermore, this study analyzed the severity and recovery indicators in relation to forest species diversity and landscape heterogeneity metrics across Europe, offering valuable insights into the spatial variability of forest response dynamics across diverse forest ecosystems across Europe. Results revealed a consistent pattern across vegetation cover types: higher forest species diversity and greater landscape shape complexity were associated with lower fire severity and, notably, shorter recovery times following fire disturbance.",0,arxiv,Evrim,CC-BY/arXiv,Fire severity and recovery across Europe: insights from forest diversity and landscape metrics
"This study develops and analyzes an extended Susceptible, Infected, Hospitalized and Recovered (SIHR) model incorporating time dependent control functions to capture preventive measures (e.g., distancing, mask use) and resource limited therapeutic interventions. This formulation provides a realistic mathematical framework for modeling public health responses beyond classical uncontrolled epidemic models. The control design is integrated into the model via an optimal control framework, solved numerically using the Forward Backward Sweep method, enabling the exploration of intervention strategies on epidemic dynamics, including infection prevalence, hospitalization burden, and the effective reproduction number. To assess the robustness of these strategies under uncertainty, we employ Polynomial Chaos Expansion combined with Sobol sensitivity indices, quantifying the influence of key epidemiological parameters (transmission, recovery, hospitalization rates) on model outcomes. Numerical simulations, calibrated to Djiboutian COVID 19 data, show that combined preventive and therapeutic interventions substantially mitigate epidemic burden, though their effectiveness depends critically on transmission related uncertainties. The originality of this work lies in combining optimal control theory with global sensitivity analysis, thus bridging numerical methods, optimization, and epidemic modeling. This integrated approach offers a general mathematical framework for designing and evaluating control strategies in infectious disease outbreaks, with applications to low resource settings and beyond.",0,arxiv,Evrim,CC-BY/arXiv,Mathematical Framework for Epidemic Dynamics: Optimal Control and Global Sensitivity Analysis
"The majority of analysis of interacting systems is done for weak and well-balanced interactions, when in fact topology and rare event factors often result in strong and sign-biased interactions when considering real systems. We analyse the impact of strong mutualistic interactions in a uniformly weighted ErdÅ‘s-RÃ©nyi network under generalised Lotka-Volterra dynamics. In difference to the typical case we show the interaction topology and system dynamics combine to produce power law abundance distributions in a critical region of the phase diagram, identifiable as a Griffiths phase. We find asymptotic expressions for the fixed point solution in this region, and establish the boundary of this region as when topology alone determines the abundance distribution. We show that the Griffiths phase persists for strong mutualistic interactions more generally, and survives when combined with weak all-to-all competition.",0,arxiv,Evrim,CC-BY/arXiv,Griffiths phase emerging from strong mutualistic disorder in high-dimensional interacting systems
"West Nile virus (WNV) is a mosquito-borne virus of the genus Flaviviridae circulating between mosquitoes and birds, while humans, equids, and other mammals are dead-end hosts. Several preventive measures are recommended to reduce the WNV burden among different hosts. In this work, we develop a mathematical framework for evaluating the theoretical effectiveness of various WNV control methods in Germany. We consider mosquito reduction methods such as the physical removal and destruction of potential mosquito breeding sites, the use of larvicides, and the use of adulticides. We also evaluate the usage of personal protective equipment (PPE) that aims to reduce human-mosquito contact. Adopting PPEs may not happen instantly due to different perceptions, social influence, and the perceived inconvenience and/or frustration that come with using PPEs. Thus, we model a dynamic adoption of PPEs by considering the perceived risk of infection, perceived inconvenience of using PPEs, and the imitation dynamics due to social influence. Furthermore, the model captures vaccination of equids. We formulate and study an optimal control problem, where mosquito controls are temperature-dependent and the decision to start or stop applying the control methods is influenced by the changes in temperature. The optimal control model supports the development and seasonal timing of cost-effective mosquito control methods. For example, results from the optimal control study show that mosquito control efforts in Germany should be initiated during early spring and stopped at the end of June or early July, during the first year of control, to avoid overuse and unnecessary costs. Finally, we developed a Shiny app that allows users to test how different combinations of interventions could reduce WNV cases.",0,arxiv,Evrim,CC-BY/arXiv,"Modeling the control of West Nile virus using mosquito reduction methods, vaccination of equids, and human behavioral adaptation to the usage of personal protective equipment"
"Recent studies on indirect reciprocity with private assessment on complete graphs suggest the possibility that one can continuously modulate the degree of segregation by controlling how to judge a good person helping a bad one. A well-known social norm called L6 judges it as bad, which eventually segregates the society into two antagonistic clusters, but if it is judged as good, the system reaches paradise where everyone likes each other. In this work, we numerically study this transition between segregation and paradise in two different settings. Firstly, in a uniform population of size $N$ where everyone regards such a donor as good with probability $p$ and bad with $1-p$, we observe paradise when $Np$ is sufficiently greater than $O(1)$. In contrast, in a heterogeneous setting where only $k$ individuals judge such a donor as good, the size difference of the clusters increases almost linearly as $k$ increases, so paradise can only be reached as $k \to N$ in a large population. Therefore, when an urgent change is needed to overcome the segregation due to L6, a small change in each and every individual's behavior is more efficient than a radical change in a fraction of the population.",0,arxiv,Evrim,CC-BY/arXiv,Controlling sternness in judging a good person who helps the bad
"The use of ad-hoc engineered viruses in the fight against tumours is one of the greatest ideas in cancer therapeutics within the last three decades. Together with other strategies such as immunotherapies, nanoparticles and adjunct therapies, the use of viral vectors in clinical trials and in the clinics has been and is still widely studied and pursued. The ability of those vectors to infiltrate and infect tumours represents one of the key attributes that regulates the success of such a strategy. Although some remarkable successes have been obtained, it is still not entirely clear how to achieve reliable protocols that can be routinely employed with confidence on a significant range of tumours. In this work, we thus concentrate on the study of different mathematical descriptions of virotherapy with the aim of better understanding the role of viral infectivity and viral dynamics in positive therapeutic outcomes. In particular, we compare probabilistic, individual approaches with continuous, spatially inhomogeneous models and investigate the importance of different tumour motility and different mathematical representations of viral infectivity. These formulations also allow us to arrive at better analytical characterisation of how waves of viral infections arise and propagate in tumours, providing interesting insights into therapy dynamics. Similarly to previous studies, oscillatory behaviours, stochasticity and cancers' diffusivities are all central to the eradication or the escape of tumours under virotherapy. Here, though, our results also show that the ability of viruses to infect tumours seems, in certain cases, more important to a final positive outcome than tumours' motility or even reproducibility. This could hopefully represent a first step into better insights into viral dynamics that may help clinicians to achieve consistently better outcomes.",0,arxiv,Evrim,CC-BY/arXiv,The role of viral dynamics and infectivity in models of oncolytic virotherapy for tumours with different motility
"Ecological interactions can dramatically alter evolutionary outcomes in complex communities. Yet, the classic theoretical results of population genetics (e.g., Kimura's fixation formula) largely ignore ecological effects. Here, we address this shortcoming by using dynamical mean-field theory to integrate ecology into classical population genetics models. We show that ecological interactions between parents and mutants result in frequency-dependent selection and can be characterized by a single emergent parameter that measures the strength of ecological feedbacks. We derive an explicit analytic expression for the fixation probability that generalizes Kimura's famous result and analyze it in various ecological and evolutionary limits. We find that ecological interactions suppress fixation probabilities for moderately beneficial mutants when compared to Kimura's predictions, with the strength of suppression increasing with larger effective population sizes, greater selective differences between parent and mutant, and for ecosystems with a large number of ""open niches"" (i.e., ecosystems well below the packing bound). Frequency-dependent selection also gives rise to prolonged parent-mutant coexistence in complex communities, a phenomenon absent in classical population genetics. Our study establishes a framework for integrating ecological interactions into population genetics models and helps illuminate how ecology can affect evolutionary outcomes.",0,arxiv,Evrim,CC-BY/arXiv,Population genetics in complex ecological communities
"The host's odor serves as a critical biological tracking signal in the host-seeking process of mosquitoes, and its heterogeneity significantly influences the transmission of mosquito-borne diseases. In this study, we propose a mosquito-borne disease model incorporating odor-baited traps to examine the impact of odor on disease transmission. Following recent experimental evidence, we also assume that infected humans are more attractive than susceptible or recovered ones.   The value of the basic reproduction number, $R_0$, depends on the attractiveness of traps, adjusted relative to infected individuals; the dependence on the relative attractiveness of susceptibles is non-monotone, suggesting that there exists an optimal mosquito preference that maximizes disease transmission. When $R_0>1$, there exists an endemic equilibrium which, under certain conditions, is proved to be globally stable. An endemic equilibrium may also exist when $R_0 < 1$, due to a backward bifurcation occurring when infected humans incur significant mortality. The phenomenon of backward bifurcation is reduced when odor-baited traps are more abundant.   Analytical results and simulations show that deploying traps and enhancing their lethality for mosquitoes can help reduce disease prevalence and the risk of an outbreak. However, the attenuation of odor in highly attractive traps may lead to a rebound in the epidemic, especially when the traps gradually lose their attractiveness compared to susceptible hosts.",0,arxiv,Evrim,CC-BY/arXiv,On the effectiveness of odor-baited traps on mosquito-borne infections
"The Vendi score (VS), a diversity metric recently conceived in the context of machine learning, with applications in a wide range of fields, has a few distinct advantages over the metrics commonly used in ecology. It is classification-independent, incorporates abundance information, and has a tunable sensitivity to rare/abundant types. Using rich COVID-19 sequence data as a paradigm, we develop methods for applying the VS to time-resolved sequence data. We show how the VS allows for characterization of the overall diversity of circulating viruses and for discernment of emerging variants prior to formal identification. Furthermore, applying the VS to phylogenetic trees provides a convenient overview of within-clade diversity which can aid viral variant detection.",0,arxiv,Evrim,CC-BY/arXiv,Applications of the Vendi score in genomic epidemiology
"We consider a phenotype-structured reaction-diffusion model of avascular glioma growth. The model describes the interaction dynamics between tumour cells and oxygen, and takes into account anisotropic cell movement and oxygen diffusion related to structural anisotropy of the brain's extracellular environment. In this model, phenotypic heterogeneity of tumour cells is captured by a continuous phenotype-structuring variable, the value of which evolves due to phenotypic changes. We first analyse a one-dimensional version of the model and formally show, through a Hopf-Cole transformation, that it admits, in appropriate asymptotic regimes, phenotypically heterogeneous travelling wave solutions, wherein the locally prevailing cell phenotype varies across the wave due to the presence of oxygen gradients. This provides a mathematical formalisation for the emergence of intratumour phenotypic heterogeneity driven by differences in oxygen availability across the tumour. We then report on the results of both 1D simulations, which corroborate the results of formal asymptotic analyses, and 2D simulations, which also demonstrate the impact of anisotropy in cell movement and oxygen diffusion on tumour growth and on the phenotypic composition of the tumour edge. These results are complemented with additional results of 3D simulations, which are carried out on the geometry of the brain by using a hybrid finite difference-finite element method and integrating patient-specific magnetic resonance imaging data with diffusion tensor imaging data.",0,arxiv,Evrim,CC-BY/arXiv,A phenotype-structured reaction-diffusion model of avascular glioma growth
"Spatial structure can play an important role in the evolution of cooperative behavior and the achievement of collective success of a population. In this paper, we explore the role of random and directed motion on spatial pattern formation and the payoff achieved by populations in both stochastic and deterministic models of spatial populations who engage in social interactions following a hawk-dove game. For the case of purely diffusive motion, both a stochastic spatial model and a partial differential equation model show that Turing patterns can emerge when hawks have a greater movement rate than doves, and in both models hawks and doves see an increase in population size and average payoff as hawk mobility increases. For the case of the payoff-driven motion, the stochastic model shows an overall decrease in population size and average payoff, but the PDE model displays more subtle behavior in this setting and will depend on the relative diffusivities of the two strategies. The PDE model also displays a biologically infeasible short-wave instability in the case of payoff-driven motion and equal diffusivities, indicating that we need to be careful about the mathematical properties of PDE models with payoff-driven directed motion and indicating potential use for nonlocal PDE models for spatial patterns in evolutionary games with directed motion.",0,arxiv,Evrim,CC-BY/arXiv,Pattern Formation in Agent-Based and PDE Models for Evolutionary Games with Payoff-Driven Motion
"Extinction times in resampling processes are fundamental yet often intractable, as previous formulas scale as $2^M$ with the number of states $M$ present in the initial probability distribution. We solve this by treating multinomial updates as independent square-root diffusions of zero drift, yielding a closed-form law for the first-extinction time. We prove that the mean coincides exactly with the Wright-Fisher result of Baxter et al., thereby replacing exponential-cost evaluations with a linear-cost expression, and we validate this result through extensive simulations. Finally, we demonstrate predictive power for model collapse in a simple self-training setup: the onset of collapse coincides with the resampling-driven first-extinction time computed from the model's initial stationary distribution. These results hint to a unified view of resampling extinction dynamics.",0,arxiv,Evrim,CC-BY/arXiv,First-Extinction Law for Resampling Processes
"Developmental trajectories are known to be canalized, or robust to both environmental and genetic perturbations. However, even when these trajectories are decanalized by an environmental perturbation outside of the range of conditions to which they are robust, they often produce phenotypes similar to known mutants called phenocopies. This correspondence between the effect of environmental and genetic perturbations has received little theoretical attention. Here, we study an abstract regulatory model which is evolved to follow a specific trajectory. We then study the effect of both small and large perturbations to the trajectory both by changing parameters and by perturbing the state in a timed manner. We find, surprisingly, that the phenomenon of phenocopying emerges in evolved trajectories even though the alternative trajectories are not selected for. Our results suggest that evolution simplifies the structure of high-dimensional phenotypic landscapes which can simultaneously show robustness and phenocopying.",0,arxiv,Evrim,CC-BY/arXiv,Evolution of phenocopying in a dynamical model of developmental trajectories
"Motivated by observations in sequence data of herpesviruses, we introduce a multi-locus model for the joint evolution of different genotypes in a virus population that is distributed across a population of hosts. In the model, virus particles replicate, recombine, and mutate within their hosts at rates that act on different time scales. Furthermore, virus particles are exchanged between hosts at reinfection events and hosts are replaced by primary infected hosts when they die. We determine the asymptotic type distribution observed in a single host in the limit of large host and virus populations under asymptotic rate assumptions by tracing back the ancestry of the sample. The proposed model may serve as a null model for the evolution of virus populations that are capable of persistence and can be used to estimate the strengths of different evolutionary forces driving genetic diversity, see also [4].",0,arxiv,Evrim,CC-BY/arXiv,On a Neutral Host-Virus Model with Recombination
"Biofilm infections on medical implants are difficult to eradicate because insufficient nutrient availability promotes antibiotic-tolerant persister cells that survive treatment and reseed growth. Existing mathematical models usually omit nutrient-dependent phenotypic switching between proliferative and persister states. Without this mechanism, models cannot capture how environmental conditions control the balance between active growth and dormancy, which is central to biofilm persistence. We present a continuum model that couples nutrient transport with the dynamics of proliferative bacteria, persisters, dead cells, and extracellular polymeric substances. The switching rates between proliferative and persister phenotypes depend on local nutrient concentration through two thresholds, enabling adaptation across nutrient-poor, intermediate, and nutrient-rich regimes. Simulations show that nutrient limitation produces a high and sustained proportion of persister cells even when biomass is reduced, whereas nutrient-rich conditions support reversion to proliferative growth and lead to greater biomass. The model also predicts that persister populations peak at times that vary with nutrient availability, and these peaks coincide with turning points in biofilm growth, identifying critical intervention windows. By directly linking nutrient availability to phenotypic switching, our model reveals mechanisms of biofilm persistence that earlier models could not capture, and it points toward strategies that target nutrient-driven adaptation as a means to improve the control of implant-associated infections.",0,arxiv,Evrim,CC-BY/arXiv,Mathematical modelling of nutrient-dependent biofilm growth on medical implants
"Cooperation is fundamental to the functioning of biological and social systems in both human and animal populations, with the structure of interactions playing a crucial role. Previous studies have used networks to describe interactions and explore the evolution of cooperation, but with limited transposability to social settings due to biologically relevant assumptions. Exogenous processes -- that affect the individual and are not derived from social interactions -- even if unbiased, have a role in supporting cooperation over defection, and this role has been largely overlooked in the context of network-based interactions. Here, we show that selection can favor either cooperation or defection depending on the frequency of exogenous, even if neutral, processes in any population structure. Our framework allows for deriving analytically the conditions for favoring a specific behavior in any network structure strongly affected by non-social environments (frequent exogenous forcing, FEF), which contrasts with previous computationally prohibitive methods. Our results demonstrate that the requirements for favoring cooperation under FEF do not match those in the rare-mutation limit, establishing that underlying neutral processes can be considered a mechanism for cooperation. We reveal that, under FEF, populations are less cooperative, and network heterogeneity can provide an advantage only if targeting specific network properties, clarifying seemingly contradictory experimental results and evolutionary predictions. While focused on cooperation, our assumptions generalize to any decision-making process involving a choice between alternative options. Our framework is particularly applicable to non-homogeneous human populations, offering a new perspective on cooperation science in the context of cultural evolution, where neutral and biased processes within structured interactions are abundant.",0,arxiv,Evrim,CC-BY/arXiv,Social influence on complex networks as a perturbation to individual behavior
"Many biological populations exhibit diversity in their strategy for survival and reproduction in a given environment, and microbes are an example. We explore the fate of different strategies under sustained environmental change by considering a mathematical model for a large population of asexual organisms. Fitness is a bimodal function of a quantitative trait, with two local optima, separated by a local minimum, i.e., a mixture of stabilising and disruptive selection. The optima represent two locally `best' trait values. We consider regimes where, when the environment is unchanging, the equilibrium distribution of the trait is bimodal. A bimodal trait distribution generally requires, for its existence, mutational coupling between the two peaks, and it indicates two coexisting clones with distinct survival and reproduction strategies. When subject to persistent environmental change, the population adapts by utilising mutations that allow it to track the changing environment. The faster the rate of change of the environment, the larger the effect of the mutations that are utilised. Under persistent environmental change, the distribution of trait values takes two different forms. At low rates of change, the distribution remains bimodal. At higher rates, the distribution becomes unimodal. This loss of a clone/biodiversity is driven by a novel mechanism where environmental change decouples a class of mutations.",0,arxiv,Evrim,CC-BY/arXiv,Effective decoupling of mutations and the resulting loss of biodiversity caused by environmental change
"In response to increasing threats to biodiversity, conservation objectives have been set at national and international level, with the aim of halting biodiversity decline by reducing direct anthropogenic pressures on species. However, the potential effects of conservation policies derived from these objectives on common species remain rarely studied. Common species are often not the primary species targeted by conservation measures and can be distributed across a wide range of habitats that may be affected differently by these measures. We analyse the effect of a range of pressures related to climate, land use and land use intensity, on 263 common bird and 144 common butterfly species from more than 20,000 sites between 2000 and 2021 across 26 European countries. We use land-use and land-use-intensity change scenarios produced previously using the IPBES Nature Futures Framework to support the achievement of conservation objectives, as well as climate change scenarios in order to project the future of biodiversity pressures in Europe up to 2050. To project the future of common biodiversity in these scenarios, we translate these pressure changes into expected variations of abundances for all common bird and butterfly species, as well as for the multi-species indicators used to monitor common biodiversity status in Europe. The projected trends are improved, while still declining, for birds in particular farmland species under the scenarios that meet the conservation objectives, with few effects on butterflies. No scenario shows a stop or a reversal in the decline in abundance of bird and butterfly species that are currently common, on the time scale considered. Our results therefore call into question the fate of common biodiversity under the current conservation policies and the need for other anticipatory frameworks that do not implicitly require a growing need for natural resources.",0,arxiv,Evrim,CC-BY/arXiv,Predicted decline in common bird and butterfly species despite conservation policies in Europe
"To ensure faithful information transmission, cells utilize nonequilibrium drives to reduce errors. Kinetic proofreading is a classic mechanism known to sharpen ligand discrimination by T lymphocytes. It remains an open question whether the adaptive immune system relies solely on kinetic proofreading to boost fidelity. Here, we suggest an alternative: an enhanced form of mechanical proofreading (MPR) in which adaptive force exertion via dynamic cell-cell contact allows faithful selection of high-affinity B lymphocytes. Using a minimal model validated with experiment, we show that adaptive MPR, characterized by mechanical feedback between force generation and contact formation, enables robust discrimination of receptor quality regardless of ligand quantity. Although MPR generically balances the tradeoffs between speed and fidelity, a negative scaling of contact duration with ligand abundance indicates the presence of feedback. Due to its ability to modulate interactions of distinct ligands that share load at membrane contacts, adaptive MPR can be harnessed to mitigate autoimmunity or enhance multivalent vaccines. Overall, this work suggests a generalization of the proofreading mechanism to encompass cellular designs that act across scales to enable competing functionalities.",0,arxiv,Evrim,CC-BY/arXiv,Adaptive mechanical proofreading toward faithful clonal selection
"In this paper, we proposed new predator-prey models that take into account memory and kicks. Memory is understood as the dependence of current behavior on the history of past behavior. The equations of these proposed models are generalizations of the Lotka-Volterra and Kolmogorov equations by using the Caputo fractional derivative of non-integer order and periodic kicks. This fractional derivative allows us to take into account memory with power-law fading. The periodic kicks, which are described by Dirac delta-functions, take into account short duration of interaction between predators and prey. For the proposed equations, which are fractional differential equations with kicks, we obtain exact solutions that describe behaviors of predator and prey with power-law fading memory. Using these exact solutions, we derive, without using any approximations, new discrete maps with memory that represent the proposed predator-prey models with memory.",0,arxiv,Evrim,CC-BY/arXiv,Predator-prey models with memory and kicks: Exact solution and discrete maps with memory
"Urban ecosystems exhibit complex predator-prey dynamics increasingly disrupted by anthropogenic disturbances (e.g., noise, habitat fragmentation). Classical Lotka-Volterra (LV) models fail to capture these human-induced stressors, and integrated frameworks incorporating functional responses, stochasticity, and spatial dynamics remain scarce. We develop a comprehensive stochastic model to quantify how human disturbance reshapes predator-prey interactions in urban landscapes, using rat-cat systems as a case study. Our framework extends the LV model to incorporate: (i) human disturbance as an external mortality factor, (ii) Holling Type III functional responses to model predation saturation and prey refugia, (iii) multiplicative noise and periodic forcing to capture stochastic disturbance regimes, and (iv) spatial diffusion across fragmented habitats. We non-dimensionalize the system to generalize dynamics and analyze stability, bifurcations, and noise-induced transitions. Numerical simulations (MATLAB) reveal three key outcomes: (1) Human disturbance disrupts classical oscillations, inducing quasi-periodic cycles and elevating extinction risks; (2) Stochasticity lowers collapse thresholds by 25% compared to deterministic predictions; (3) Spatial diffusion drives pattern formation (e.g., disturbance shadows, prey hotspots) through habitat coupling. Results highlight the extreme vulnerability of urban wildlife to anthropogenic pressures, demonstrating how disturbance intensity governs system stability. The model provides a predictive framework for conservation strategies, emphasizing refuge enhancement and phased interventions synchronized with population cycles.",0,arxiv,Evrim,CC-BY/arXiv,Stochastic Dynamics of Urban Predator-Prey Systems: Integrating Human Disturbance and Functional Responses
"Navigation through narrow passages during colony relocation by the tandem-running ants, $\textit{Diacamma}$ $\textit{indicum}$, is a tour de force of biological traffic coordination. Even on one-lane paths, the ants tactfully manage a bidirectional flow: Informed individuals (termed leaders) guide nest-mates (termed followers) from a suboptimal nest to a new optimal nest, and then return to recruit additional followers. We propose that encounters between the ants moving in opposite directions can be modelled within the framework of game theory leading to an understanding of the mechanism behind observed behaviours. Our experiments reveal that, upon encountering a tandem pair (a leader and its follower) on a narrow path, the returning leader reverses her direction and proceeds toward the new nest again as if she becomes the leader guiding a follower. This observed behaviour is consistent with game-theoretic predictions, provided the assumption of perfect rationality is relaxed in favour of bounded rationality -- specifically, procedural rationality. In other words, the experimental outcomes are consistent with sampling equilibrium but not with Nash equilibrium. Our work, which strives to induct the essence of behavioural game theory into the world of ants, is first ever report of realizing sampling equilibrium in scenarios not involving human players.",0,arxiv,Evrim,CC-BY/arXiv,A game played by tandem-running ants: Hint of procedural rationality
"Antibiotic resistance presents a growing global health threat by diminishing the effectiveness of treatments and allowing once-manageable bacterial infections to persist. This study develops and analyzes an optimization-based mathematical model to investigate the impact of varying antibiotic dosages on bacterial resistance, incorporating the role of the immune system. Additionally, to capture the effects of over and underdosing, a floor function is newly introduced into the model as a switch function. The model is examined both analytically and numerically. As part of the analytical solution, the validity of the model through the existence and uniqueness theorem, stability at the equilibrium points, and characteristics of equilibrium points in relation to state variables have been investigated. Numerical simulations, performed using the Runge Kutta 4th order method, reveal that while antibiotics effectively reduce sensitive bacteria, they simultaneously increase resistant strains and suppress immune cell levels. The results also demonstrate that underdosing antibiotics increases the risk of resistance through bacterial mutation, while overdosing weakens the immune system by disrupting beneficial microbes. These findings emphasize that improper dosing whether below or above the prescribed level can accelerate the development of antibiotic resistance, underscoring the need for carefully regulated treatment strategies that preserve both antimicrobial effectiveness and immune system integrity.",0,arxiv,Evrim,CC-BY/arXiv,Modeling the Effects of Over and Under Doses Antibiotic Treatment to Bacterial Resistance in Presence of Immune System
"Recently, a new vector encoding, Ordered Leaf Attachment (OLA), was introduced that represents $n$-leaf phylogenetic trees as $n-1$ length integer vectors by recording the placement location of each leaf. Both encoding and decoding of trees run in linear time and depend on a fixed ordering of the leaves. Here, we investigate the connection between OLA vectors and the maximum acyclic agreement forest (MAAF) problem. A MAAF represents an optimal breakdown of $k$ trees into reticulation-free subtrees, with the roots of these subtrees representing reticulation events. We introduce a corrected OLA distance index over OLA vectors of $k$ trees, which is easily computable in linear time. We prove that the corrected OLA distance corresponds to the size of a MAAF, given an optimal leaf ordering that minimizes that distance. Additionally, a MAAF can be easily reconstructed from optimal OLA vectors. We expand these results to multifurcated trees: we introduce an $O(kn \cdot m\log m)$ algorithm that optimally resolves a set of multifurcated trees given a leaf-ordering, where $m$ is the size of a largest multifurcation, and show that trees resolved via this algorithm also minimize the size of a MAAF. These results suggest a new approach to fast computation of phylogenetic networks and identification of reticulation events via random permutations of leaves. Additionally, in the case of microbial evolution, a natural ordering of leaves is often given by the sample collection date, which means that under mild assumptions, reticulation events can be identified in polynomial time on such datasets.",0,arxiv,Evrim,CC-BY/arXiv,Ordered Leaf Attachment (OLA) Vectors can Identify Reticulation Events even in Multifurcated Trees
"We show that a simple mechanistic model of spatial dispersal for settling organisms, subject to parameter variability, can generate heavy-tailed radial probability density functions. The movement of organisms in the model consists of a two-dimensional diffusion that ceases after a random time, where the parameters that characterize each of these stages have been randomized. Our findings show that these minimal assumptions can yield heavy-tailed dispersal patterns, providing a simplified framework that increases the understanding of long-distance dispersal events in movement ecology.",0,arxiv,Evrim,CC-BY/arXiv,Parameter variability can produce heavy tails in a model for the spatial distribution of settling organisms
"Despite the critical role of functional diversity (FD) in understanding ecological systems and processes, its robust quantification remains a significant challenge. A long-held view in the field is that it is not possible to capture its three facets -- functional richness, functional divergence, and functional evenness -- in a single index. This perspective has prompted recent proposals for FD measurement to use three separate indices, one for each aspect. Here, we challenge this paradigm by demonstrating that the probability-weighted Vendi Score (pVS), first introduced by Friedman and Dieng (2023), can serve as a powerful functional diversity index that can capture its three facets. We adapt pVS to functional ecology by defining it as the exponential of the RÃ©nyi entropy of the eigenvalues of the abundance-weighted trait similarity matrix. This formulation allows pVS to be applicable at any biological level. It can be defined at the species level, at which most existing FD metrics are defined, and at the individual level to naturally incorporate intraspecific trait variation (ITV) when detailed data are available. We theoretically and empirically demonstrate the robustness of pVS. We first mathematically prove it satisfies several essential desiderata for FD metrics, including invariance to functional redundancy, set monotonicity, distance monotonicity, and concavity. We then show that pVS consistently exhibits the expected ground-truth behavior on simulated ecosystem scenarios under which many FD metrics fail. By integrating abundances and trait similarities within a single, theoretically sound framework, pVS provides a generally applicable index for ecology.",0,arxiv,Evrim,CC-BY/arXiv,A Unified and Predictive Measure of Functional Diversity
"The evolutionary biology of aging is fundamental to understanding the mechanisms of aging and how to develop anti-aging treatments. Thus far most evolutionary theory concerns the genetics of aging with limited physiological integration. Here we present an intuitive evolutionary framework built on how physiology is regulated and how this regulation itself is then predicted to age. Life has evolved to secure reproduction and avoid system failure in early life, and it is the physiological regulation that evolves in response to those early life selection pressures that leads to the emergence of aging. Importantly, asymmetrical regulation of physiology will evolve as the Darwinian fitness costs of loss of regulation will not be symmetrical. When asymmetrical regulatory systems break during aging, they cause physiological function to drift towards the physiological range where costs of dysregulation are lowest, rendering aging directional. Our model explains many puzzling aspects of the biology of aging. These include why aging appears (but is not) programmed, why aging is gradual yet heterogeneous, why cellular and hormonal signaling are closely related to aging, the compensation law of mortality, why trade-offs between reproduction and aging remain elusive, why longer-lived organisms show more signs of aging during their natural lifespans, and why longer-lived organisms can be less responsive to treatments of aging that work well in short-lived organisms. We provide predictions of our theory that are empirically testable. By incorporating physiological regulation into evolutionary models of aging, we provide a novel perspective to guide empirical research in this still growing field.",0,arxiv,Evrim,CC-BY/arXiv,The evolution of asymmetrical regulation of physiology is central to aging
"Plants come with sophisticated strategies to survive within a highly competing environment. In addition, they need to resist frequent attacks from a variety of herbivores acting alone, in small groups, or in swarms. Since the amount of energy a plant might invest in defense and reproduction is limited, a complex optimization problem emerges. In a shared habitat, plants fight herbivores by shape and camouflage, by the release of specific toxins, or by attracting predators of herbivores. Furthermore, plants alert their surrounding field by signaling substances in the event of an assault. Transported by air or through a network of roots, signaling substances reach neighbors to trigger their defense. The offsprings of a plant commonly grow within a certain distance to benefit from symbiotic protection. We introduce a grid-based visual simulation software for detailed configuration and subsequent processing of the behavior of the resulting system in time and space. In terms of solution to a computational optimization problem inspired by nature, settings with low energy need and long life able to cope with different patterns of attack can be figured out and analyzed. Applications include novel techniques for efficient construction and secure operation of sensor networks.",0,arxiv,Evrim,CC-BY/arXiv,A Visual Discrete Event-based Simulator for Protection of Plants against Herbivores Employed as Computational Optimization Game
"Reconciling increasing food production with biodiversity conservation is critical yet challenging, particularly given global declines in beneficial insects driven by monoculture intensification. Intercropping, the simultaneous or sequential cultivation of multiple crops, has been proposed as a viable strategy to enhance beneficial insect services and suppress pests, yet global evidence regarding optimal spatiotemporal intercropping configurations remains fragmented. Here, we synthesize results from 7,584 field experiments spanning six continents and 22 Koppen climate regions, evaluating effects of spatial (row, strip, mixed, agroforestry) and temporal (additive, replacement, relay) intercropping configuations on beneficial insect (predators, parasitoids, pollinators) abundance and pest suppression using the Management Efficiency Ratio (MER; log ratio of abundance in intercropping versus monoculture). Relay intercropping, characterized by temporally staggered planting, emerged as the universally optimal temporal configuration, substantially increasing predator (MER = 0.473) and parasitoid populations (MER = 0.512) and effectively suppressing pests (MER = -0.611) globally. At regional scales, identical spatiotemporal configurations simultaneously optimized beneficial insect predator abundance and pest suppression in 57% of regions, while other regions required distinct, insect-specific approaches. Our findings highlight relay intercropping as a globally generalizable solution, but underscore regional variation that calls for targeted policies to simultaneously secure food production and biodiversity conservation.",0,arxiv,Evrim,CC-BY/arXiv,Temporally staggered cropping co-benefits beneficial insects and pest control globally
"Infectious pathogens often propagate by superspreading, which focusses onward transmission on disproportionately few infected individuals. At the same time, infector-infectee pairs tend to have more similar transmission potentials than expected by chance, as risk factors assort among individuals who frequently interact. A key problem for infectious disease epidemiology, and in the dynamics of complex systems, is to understand how structured variation in individual transmission will scale to impact epidemic dynamics. Here we introduce a framework that reveals how population structure shapes epidemic thresholds, through autocorrelation of individual reproductive numbers along chains of transmission. We show that chains of superspreading can sustain epidemics even when the average transmission rate in the host population is below one, and derive a mathematical threshold beyond which correlated superspreading allows epidemics in otherwise subcritical systems. Empirical analysis of 47 transmission trees for 13 human pathogens indicate self-organizing bursts of superspreading are common and that many trees are near the critical boundary. Vaccination campaigns that proceed up assortative hierarchies of transmission are predicted to sustain the force of infection until herd immunity is reached, providing a mechanistic basis for threshold dynamics observed in real-world settings. Conversely, modulating correlations in transmission, rather than mean or variance, could enable cities and other complex systems to develop immune-like capacities that suppress contagion while preserving core functions.",0,arxiv,Evrim,CC-BY/arXiv,Epidemic amplification by correlated superspreading
"This study compares the predictive capacity of the Dynamic Habitat Index (DHI) - a remote sensing (RS)-based measure of habitat productivity and variability - against traditional land-use/land-cover (LULC) metrics in species distribution modelling (SDM) applications. RS and LULC-based SDMs were built using distribution data for eleven bird, amphibian, and mammal species in ÃŽle-de-France. Predictor variables were derived from Sentinel-2 RS data and LULC classifications, with the latter incorporating Euclidean distance to habitat types. Ensemble SDMs were built using nine algorithms and evaluated with the Continuous Boyce Index (CBI) and a calibrated AUC. Habitat suitability scores and their binary transformations were assessed using niche overlap indices (Schoener, Warren, and Spearman rank correlation coefficient). Both RS and LULC approaches exhibited similar predictive accuracy overall. After binarisation however, the resulting niche maps diverged significantly. While LULC-based models exhibited spatial constraints (habitat suitability decreased as distance from recorded occurrences increased), RS-based models, which used continuous data, were not affected by geographic bias or distance effects. These results underscore the need to account for spatial biases in LULC-based SDMs. The DHI may offer a more spatially neutral alternative, making it a promising predictor for modelling species niches at regional scales.",0,arxiv,Evrim,CC-BY/arXiv,Modelling species distributions using remote sensing predictors: Comparing Dynamic Habitat Index and LULC
"From genomes and ecosystems to bureaucracies and cities, the growth of complex systems occurs by adding new types of functions and expanding existing ones. We present a simple generative model that generalizes the Yule-Simon process by including: (i) a size-dependent probability of introducing new functions, and (ii) a generalized preferential attachment mechanism for expanding existing ones. We uncover a shared underlying structure that helps explain how function diversity evolves in empirical observations, such as prokaryotic proteomes, U.S. federal agencies, and urban economies. We show that real systems are often best represented as having non-Zipfian rank-frequency distributions, driven by sublinear preferential attachment, whilst still maintaining power-law scaling in their abundance distributions. Furthermore, our analytics explain five distinct phases of the organization of functional elements across complex systems. The model integrates empirical findings regarding the logarithmic growth of diversity in cities and the self-similarity of their rank-frequency distributions. Self-similarity previously observed in the rank-frequency distributions of cities is not observed in cells and federal agencies -- however, under a rescaling relative to the total diversity, all systems admit self-similar structures predicted by our theory.",0,arxiv,Evrim,CC-BY/arXiv,A generative model of function growth explains hidden self-similarities across biological and social systems
"UK woodlands, forests, and urban treescapes are under threat from invasive species, exacerbated by climate change, trade, and transport. Invasive tree pests debilitate their host and disrupt forest ecosystems, thus it is imperative to quantitatively model and predict their spread. Addressing this, we model the spread of an invasive pest using a spatiotemporal reaction-diffusion equation, representing the spatial distribution as a population density field. We solve this intractable equation numerically and, from the solution, we determine first arrival times of the pest at locations in the field. The adopted model permits us to obtain the expansion rate of pest spread directly from the model parameters, which we infer in the Bayesian paradigm, using a Markov chain Monte Carlo scheme. We apply our framework to the ongoing spread of oak processionary moth in the UK, an outbreak which continues to grow despite management efforts. We demonstrate that our approach effectively captures the spread of the pest and that this has occurred at a non-constant expansion rate. The proposed framework is a powerful tool for quantitatively modelling the spread of an invasive tree pest and could underpin future prediction and management approaches.",0,arxiv,Evrim,CC-BY/arXiv,Reaction-diffusion models of invasive tree pest spread: quantifying the expansion of oak processionary moth in the UK
"Chest X-rays (CXRs) are the most commonly performed imaging investigation. In the UK, many centres experience reporting delays due to radiologist workforce shortages. Artificial intelligence (AI) tools capable of distinguishing normal from abnormal CXRs have emerged as a potential solution. If normal CXRs could be safely identified and reported without human input, a substantial portion of radiology workload could be reduced.   This article examines the feasibility and implications of autonomous AI reporting of normal CXRs. Key issues include defining normal, ensuring generalisability across populations, and managing the sensitivity-specificity trade-off. It also addresses legal and regulatory challenges, such as compliance with IR(ME)R and GDPR, and the lack accountability frameworks for errors. Further considerations include the impact on radiologists practice, the need for robust post-market surveillance, and incorporation of patient perspectives. While the benefits are clear, adoption must be cautious.",0,arxiv,Evrim,CC-BY/arXiv,Autonomous Reporting of Normal Chest X-rays by Artificial Intelligence in the United Kingdom. Can We Take the Human Out of the Loop?
"We propose a new dynamic SIR model that, in contrast with the available model on time scales, is biological relevant. For the new SIR model we obtain an explicit solution, we prove the asymptotic stability of the extinction and disease-free equilibria, and deduce some necessary conditions for the monotonic behavior of the infected population. The new results are illustrated with several examples in the discrete, continuous, and quantum settings.",0,arxiv,Evrim,CC-BY/arXiv,A consistent SIR model on time scales with exact solution
"Epidemic control frequently relies on adjusting interventions based on prevalence. But designing such policies is a highly non-trivial problem due to uncertain intervention effects, costs and the difficulty of quantifying key transmission mechanisms and parameters. Here, using exact mathematical and computational methods, we reveal a fundamental limit in epidemic control in that prevalence feedback policies are outperformed by a single optimally chosen constant control level. Specifically, we find no incentive to use prevalence based control under a wide class of cost functions that depend arbitrarily on interventions and scale with infections. We also identify regimes where prevalence feedback is beneficial. Our results challenge the current understanding that prevalence based interventions are required for epidemic control and suggest that, for many classes of epidemics, interventions should not be varied unless the epidemic is near the herd immunity threshold.",0,arxiv,Evrim,CC-BY/arXiv,Fundamental limits on taming infectious disease epidemics
"How regional heterogeneity in social and cultural processes drive--and respond to--climate dynamics is little studied. Here we present a coupled social-climate model stratified across five world regions and parameterized with geophysical, economic and social survey data. We find that support for mitigation evolves in a highly variable fashion across regions, according to socio-economics, climate vulnerability, and feedback from changing temperatures. Social learning and social norms can amplify existing sentiment about mitigation, leading to better or worse global warming outcomes depending on the region. Moreover, mitigation in one region, as mediated by temperature dynamics, can influence other regions to act, or just sit back, thus driving cross-regional heterogeneity in mitigation opinions. The peak temperature anomaly varies by several degrees Celsius depending on how these interactions unfold. Our model exemplifies a framework for studying how global geophysical processes interact with population-scale concerns to determine future sustainability outcomes.",0,arxiv,Evrim,CC-BY/arXiv,Implications of regional variations in climate change vulnerability and mitigation behaviour for social-climate dynamics
"Tumor-immune interactions are shaped by both antigenic heterogeneity and stochastic perturbations in the tumor microenvironment, yet the mathematical mechanisms underlying immune phase transitions remain poorly understood. We propose a four-compartment dynamical model that incorporates antigen accumulation and immune escape mutations. Bifurcation analysis reveals bistability between immune surveillance and immune escape states, providing a mechanistic explanation for heterogeneous immune outcomes during tumor progression. In the multistable regime, the stable manifold of a saddle point partitions the state space into distinct basins of attraction, determining the long-term fate of the system. We further analyze how stochastic fluctuations in the tumor microenvironment perturb these separatrices, potentially triggering irreversible state transitions. By characterizing the critical noise intensity and estimating the tipping time, we establish a mathematical framework for assessing noise-induced transitions. The model further predicts that increasing tumor cell death can improve system resilience to stochastic perturbations, whereas stronger immune pressure may facilitate immune escape-highlighting the nonlinear and non-monotonic nature of tumor-immune dynamics.",0,arxiv,Evrim,CC-BY/arXiv,Bistability and Noise-Induced Evasion in Tumor-Immune Dynamics with Antigen Accumulation and Immune Escape
"A central question in evolutionary biology is how to quantitatively understand the dynamics of genetically diverse populations. Modeling the genotype distribution is challenging, as it ultimately requires tracking all correlations (or cumulants) among alleles at different loci. The quasi-linkage equilibrium (QLE) approximation simplifies this by assuming that correlations between alleles at different loci are weak -- i.e., low linkage disequilibrium -- allowing their dynamics to be modeled perturbatively. However, QLE breaks down under strong selection, significant epistatic interactions, or weak recombination. We extend the multilocus QLE framework to allow cumulants up to order $K$ to evolve dynamically, while higher-order cumulants ($>K$) are assumed to equilibrate rapidly. This extended QLE (exQLE) framework yields a general equation of motion for cumulants up to order $K$, which parallels the standard QLE dynamics (recovered when $K = 1$). In this formulation, cumulant dynamics are driven by the gradient of average fitness, mediated by a geometrically interpretable matrix that stems from competition among genotypes. Our analysis shows that the exQLE with $K=2$ accurately captures cumulant dynamics even when the fitness function includes higher-order (e.g., third- or fourth-order) epistatic interactions, capabilities that standard QLE lacks. We also applied the exQLE framework to infer fitness parameters from temporal sequence data. Overall, exQLE provides a systematic and interpretable approximation scheme, leveraging analytical cumulant dynamics and reducing complexity by progressively truncating higher-order cumulants.",0,arxiv,Evrim,CC-BY/arXiv,A High-Order Cumulant Extension of Quasi-Linkage Equilibrium
"Human social life is shaped by repeated interactions, where past experiences guide future behavior. In evolutionary game theory, a key challenge is to identify strategies that harness such memory to succeed in repeated encounters. Decades of research have identified influential one-step memory strategies (such as Tit-for-Tat, Generous Tit-for-Tat, and Win-Stay Lose-Shift) that promote cooperation in iterated pairwise games. However, these strategies occupy only a small corner of the vast strategy space, and performance in isolated pairwise contests does not guarantee evolutionary success. The most effective strategies are those that can spread through a population and stabilize cooperation. We propose a general framework for repeated-interaction strategies that encompasses arbitrary memory lengths, diverse informational inputs (including both one's own and the opponent's past actions), and deterministic or stochastic decision rules. We analyze their evolutionary dynamics and derive general mathematical results for the emergence of cooperation in any network structure. We then introduce a unifying indicator that quantifies the contribution of repeated-interaction strategies to population-level cooperation. Applying this indicator, we show that long-memory strategies evolve to promote cooperation more effectively than short-memory strategies, challenging the traditional view that extended memory offers no advantage. This work expands the study of repeated interactions beyond one-step memory strategies to the full spectrum of memory capacities. It provides a plausible explanation for the high levels of cooperation observed in human societies, which traditional one-step memory models cannot account for.",0,arxiv,Evrim,CC-BY/arXiv,Evolutionary dynamics of memory-based strategies in repeated and structured social interactions
"Dengue, a mosquito-borne viral disease common in tropical areas, is spread by Aedes aegypti and Aedes albopictus. Temperature changes driven by climate affect vector ecology and expand regions of species coexistence. The combined effect of temperature and larval competition on mosquito dynamics and dengue transmission is unclear. We built a deterministic model with temperature-dependent parameters to study larval-stage interactions, linked with a SEIR framework for human infection. We assessed invasion potential, coexistence, and infection peaks. The basic reproduction number (R0) was calculated using the Next Generation Matrix, and the effective reproduction number (Rt) came from simulations with larval competition. Aedes albopictus invades aegypti-dominated systems when the aegypti competition coefficient is below 0.47, with neutral equilibrium from 0.47 to 0.60 and exclusion above 0.60 in stable conditions. In temperature-dependent settings, invasion extends to a coefficient of 0.75. Coexistence analysis showed aegypti dominance (~87% abundance) in stable settings, while temperature-dependent conditions led to ~50% abundance for both species. Dengue cases peaked at 156-168 in stable conditions and 195-220 in temperature-dependent ones. Stronger albopictus competition lowered peaks in both cases. Temperature boosts albopictus invasion and coexistence, while aegypti drives higher infection peaks. Balanced species abundances raise transmission risks, emphasizing the need to factor temperature and competition into vector control.",0,arxiv,Evrim,CC-BY/arXiv,Temperature and Competition: Drivers in the Ecological Dynamics of Aedes Mosquitoes and Dengue Spread
"While monitoring biodiversity through camera traps has become an important endeavor for ecological research, identifying species in the captured image data remains a major bottleneck due to limited labeling resources. Active learning -- a machine learning paradigm that selects the most informative data to label and train a predictive model -- offers a promising solution, but typically focuses on uncertainty in the individual predictions without considering uncertainty across the entire dataset. We introduce a new active learning policy, Vendi information gain (VIG), that selects images based on their impact on dataset-wide prediction uncertainty, capturing both informativeness and diversity. We applied VIG to the Snapshot Serengeti dataset and compared it against common active learning methods. VIG needs only 3% of the available data to reach 75% accuracy, a level that baselines require more than 10% of the data to achieve. With 10% of the data, VIG attains 88% predictive accuracy, 12% higher than the best of the baselines. This improvement in performance is consistent across metrics and batch sizes, and we show that VIG also collects more diverse data in the feature space. VIG has broad applicability beyond ecology, and our results highlight its value for biodiversity monitoring in data-limited environments.",0,arxiv,Evrim,CC-BY/arXiv,Vendi Information Gain for Active Learning and its Application to Ecology
"This paper investigates a behavioral-feedback SIR model in which the infection rate adapts dynamically based on the fractions of susceptible and infected individuals. We introduce an invariant of motion and we characterize the peak of infection. We further examine the system under a threshold constraint on the infection level. Based on this analysis, we formulate an optimal control problem to keep the infection curve below a healthcare capacity threshold while minimizing the economic cost. For this problem, we study a feasible strategy that involves applying the minimal necessary restrictions to meet the capacity constraint and characterize the corresponding cost.",0,arxiv,Evrim,CC-BY/arXiv,Behavioral-feedback SIR epidemic model: analysis and control
"We develop a framework for non-Markovian SIR and SIS models beyond mean field, utilizing the continuous-time random walk formalism. Using a gamma distribution for the infection and recovery inter-event times as a test case, we derive asymptotical late-time master equations with effective memory kernels and obtain analytical predictions for the final outbreak size distribution in the SIR model, and quasistationary distribution and disease lifetime in the SIS model. We show that increasing memory can greatly widen the outbreak size distribution and reduce the disease lifetime. We also show that rescaled Markovian models fail to capture fluctuations in the non-Markovian case. Overall, our findings, confirmed against numerical simulations, demonstrate that memory strongly shapes epidemic dynamics and paves the way for extending such analyses to structured populations.",0,arxiv,Evrim,CC-BY/arXiv,Large deviations in non-Markovian stochastic epidemics
"Co-occurrence network inference algorithms have significantly advanced our understanding of microbiome communities. However, these algorithms typically analyze microbial associations within samples collected from a single environmental niche, often capturing only static snapshots rather than dynamic microbial processes. Previous studies have commonly grouped samples from different environmental niches together without fully considering how microbial communities adapt their associations when faced with varying ecological conditions. Our study addresses this limitation by explicitly investigating both spatial and temporal dynamics of microbial communities. We analyzed publicly available microbiome abundance data across multiple locations and time points, to evaluate algorithm performance in predicting microbial associations using our proposed Same-All Cross-validation (SAC) framework. SAC evaluates algorithms in two distinct scenarios: training and testing within the same environmental niche (Same), and training and testing on combined data from multiple environmental niches (All). To overcome the limitations of conventional algorithms, we propose fuser, an algorithm that, while not entirely new in machine learning, is novel for microbiome community network inference. It retains subsample-specific signals while simultaneously sharing relevant information across environments during training. Unlike standard approaches that infer a single generalized network from combined data, fuser generates distinct, environment-specific predictive networks. Our results demonstrate that fuser achieves comparable predictive performance to existing algorithms such as glmnet when evaluated within homogeneous environments (Same), and notably reduces test error compared to baseline algorithms in cross-environment (All) scenarios.",0,arxiv,Evrim,CC-BY/arXiv,Fused Lasso Improves Accuracy of Co-occurrence Network Inference in Grouped Samples
"Indirect reciprocity promotes cooperation by allowing individuals to help others based on reputation rather than direct reciprocation. Because it relies on accurate reputation information, its effectiveness can be undermined by information gaps. We examine two forms of incomplete information: incomplete observation, in which donor actions are observed only probabilistically, and reputation fading, in which recipient reputations are sometimes classified as ""Unknown"". Using analytical frameworks for public assessment, we show that these seemingly similar models yield qualitatively different outcomes. Under incomplete observation, the conditions for cooperation are unchanged, because less frequent updates are exactly offset by higher reputational stakes. In contrast, reputation fading hinders cooperation, requiring higher benefit-to-cost ratios as the identification probability decreases. We then evaluate costly punishment as a third action alongside cooperation and defection. Norms incorporating punishment can sustain cooperation across broader parameter ranges without reducing efficiency in the reputation fading model. This contrasts with previous work, which found punishment ineffective under a different type of information limitation, and highlights the importance of distinguishing between types of information constraints. Finally, we review past studies to identify when punishment is effective and when it is not in indirect reciprocity.",0,arxiv,Evrim,CC-BY/arXiv,Incomplete Reputation Information and Punishment in Indirect Reciprocity
"The evolutionary origins of ageing and age-associated diseases continue to pose a fundamental question in biology. This study is concerned with a recently proposed framework, which conceptualises development and ageing as a continuous process, driven by genetically encoded epigenetic changes in target sets of cells. According to the Evolvable Soma Theory of Ageing (ESTA), ageing reflects the cumulative manifestation of epigenetic changes that are predominantly expressed during the post-reproductive phase. These late-acting modifications are not yet evolutionarily optimised but are instead subject to ongoing selection, functioning as somatic ""experiments"" through which evolution explores novel phenotypic variation. These experiments are often detrimental, leading to progressive physical decline and eventual death, while a small subset may produce beneficial adaptations, that evolution can exploit to shape future developmental trajectories. According to ESTA, ageing can be understood as evolution in action, yet old age is also the strongest risk factor for major diseases such as cardiovascular diseases, cancer, neurodegenerative disorders, and metabolic syndrome. We argue that this association is not merely correlational but causal: the same epigenetic process that drive development and ageing also underlie age-associated diseases. Growing evidence points to epigenetic regulation as a central factor in these pathologies, since no consistent patterns of genetic mutations have been identified, whereas widespread regulatory and epigenetic disruptions are observed. From this perspective, evolution is not only the driver of ageing but also the ultimate source of the diseases that accompany it, making it the root cause of most age-related pathologies.",0,arxiv,Evrim,CC-BY/arXiv,"Evolution, the mother of age-related diseases"
"Timely and robust influenza incidence forecasting is critical for public health decision-making. This paper presents MAESTRO (Multi-modal Adaptive Estimation for Temporal Respiratory Disease Outbreak), a novel, unified framework that synergistically integrates advanced spectro-temporal modeling with multi-modal data fusion, including surveillance, web search trends, and meteorological data. By adaptively weighting heterogeneous data sources and decomposing complex time series patterns, the model achieves robust and accurate forecasts. Evaluated on over 11 years of Hong Kong influenza data (excluding the COVID-19 period), MAESTRO demonstrates state-of-the-art performance, achieving a superior model fit with an R-square of 0.956. Extensive ablations confirm the significant contributions of its multi-modal and spectro-temporal components. The modular and reproducible pipeline is made publicly available to facilitate deployment and extension to other regions and pathogens, presenting a powerful tool for epidemiological forecasting.",0,arxiv,Evrim,CC-BY/arXiv,Multi-modal Adaptive Estimation for Temporal Respiratory Disease Outbreak
"On May 15, 2025, Brazil reported its first highly pathogenic avian influenza (HPAI) outbreak in a commercial poultry breeder farm in Montenegro, Rio Grande do Sul. This study presents the outbreak timeline, control measures, along with spatial risk assessment and epidemiological model used to simulate detection delays. The transmission model considered Susceptible Exposed Infected Recovered Dead farm statuses to simulate within farm and between farm dynamics under 3 day, 5 day, and 10 day detection delays. The single infected commercial farm lost 15,650 birds, with 92% mortality due to HPAI, and additional culling of the remaining birds on Day 5 post-notification to the state animal health officials. Based on the mortality and outbreak response data, the introduction likely occurred 3 10 days before its official detection. Our field investigations suggested that wild birds were the most likely source of introduction, although biosecurity breaches could not be ruled out. Control measures implemented included movement restrictions and a control zone, from which 4,197 vehicles were inspected upon entry. Risk analysis classified 64.4% of municipalities as low risk, 35.0% as medium risk, and 0.6% as high risk. Our HPAI disease simulation results showed that the number of secondary infections would increase from a median of 4 farms (IQR 2 5) with a 3 day delay to 6 (IQR 3 22) and 34 (IQR 12 47) farms with 5 day and 10 day delays, respectively. The rapid veterinary response eliminated the outbreak within 32 days of detection, highlighting the critical role of early detection and prompt response.",0,arxiv,Evrim,CC-BY/arXiv,"First highly pathogenic avian influenza outbreak in a commercial farm in Brazil: outbreak timeline, control actions, risk analysis, and transmission modeling"
"By generalizing a class of models recently introduced to account for protracted transients in biological systems, we identify a novel mechanism for hyperuniformity. In this model, competition of particles over a shared resource guides the population towards a critical steady state with prolonged individual life time. We show that, in its spatially extended form, this many-particle model exhibits hyperuniform density fluctuations. Through explicit coarse-graining, we develop a hydrodynamic theory that conforms closely with the results of stochastic simulations. Unlike previous models for non-equilibrium hyperuniform states, our model does not exhibit conservation laws, even when approaching criticality. Instead, hyperuniformity arises from the divergence of the interaction range as the system approaches the critical point. These findings may find applications in engineering, cellular population dynamics, and ecology.",0,arxiv,Evrim,CC-BY/arXiv,Self-organized hyperuniformity in a minimal model of population dynamics
"Vaccination against the SARS-CoV-2 disease has significantly reduced its mortality rate and spread. However, despite its availability, a considerable proportion of the public has either refused or delayed getting vaccinated. This reluctance is known as vaccine hesitancy. The aim of this paper is to present a mathematical model to investigate how social interaction can impact vaccine hesitancy. The model describes the temporal transitions between different vaccination classes of the population (those vaccinated, those who are not yet vaccinated but agree to be vaccinated, and those who refuse). We apply the model to state and national survey data from the USA to estimate model parameters that quantify the rates at which public opinion on vaccination changes. Moreover, we investigate how political trends and demographic factors, such as age and education, impact these parameters. Our results show that state-level political affiliation, age, and educational level shape opinions on vaccination and have a strong influence on the temporal dynamics of attitude changes.",0,arxiv,Evrim,CC-BY/arXiv,A mathematical model of vaccine hesitancy: Analysing the impact of political trends and the interaction across age and education groups in the USA
"Indirect reciprocity is a key mechanism for large-scale cooperation. This mechanism captures the insight that in part, people help others to build and maintain a good reputation. To enable such cooperation, appropriate social norms are essential. They specify how individuals should act based on each others' reputations, and how reputations are updated in response to individual actions. Although previous work has identified several norms that sustain cooperation, a complete analytical characterization of all evolutionarily stable norms remains lacking, especially when assessments or actions are noisy. In this study, we provide such a characterization for the public assessment regime. This characterization reproduces known results, such as the leading eight norms, but it extends to more general cases, allowing for various types of errors and additional actions including costly punishment. We also identify norms that impose a fixed payoff on any mutant strategy, analogous to the zero-determinant strategies in direct reciprocity. These results offer a rigorous foundation for understanding the evolution of cooperation through indirect reciprocity and the critical role of social norms.",0,arxiv,Evrim,CC-BY/arXiv,Exact conditions for evolutionary stability in indirect reciprocity under noise
"Mutualistic interactions, where individuals from different species can benefit from each other, are widespread across ecosystems. This study develops a general deterministic model of mutualism involving two populations, assuming that mutualism may involve both costs and benefits for the interacting individuals, leading to density-dependent effects on the dynamics of the two species. This framework aims at generalizing pre-existing models, by allowing the ecological interactions to transition from mutualistic to parasitic when the respective densities of interacting species change. Through ordinary differential equations and phase portrait analysis, we derive general principles governing these systems, identifying sufficient conditions for the emergence of certain dynamic behaviors. In particular, we show that limit cycles can arise when interactions include parasitic phases but are absent in strictly mutualistic regimes. This framework provides a general approach for characterizing the population dynamics of interacting species and highlights the effect of the transitions from mutualism to parasitism due to density dependence.",0,arxiv,Evrim,CC-BY/arXiv,Dynamics of Two Species with Density-Dependent Interactions in a Mutualistic Context
"The sterile insect technique has emerged recently as a biologically secure and effective tool for suppressing wild mosquito pests. To improve the performance of this strategy, understanding the interaction between wild and sterile mosquitoes is critical. Although the common models for this biological problem are scalar equations, they are remarkably resistant to the mathematical analysis. In a series of papers, DueÃ±as, NuÃ±ez, and Obaya have developed a powerful approach to describe the dynamical behavior of scalar equations with d-concave nonlinearities, a property typically related to the sign of the third derivative. In this paper, we show that, for periodic equations coming from population dynamics, this condition is typically associated with the positive sign of the third derivative of the inverse of the PoincarÃ© map. This remark allows us to simplify some arguments in the periodic case and obtain a deep geometrical understanding of the global bifurcation patterns. Consequently, the dynamical behavior of the models is analyzed in terms of simple and testable conditions. Our methodology allows us to describe precisely the dynamical behavior of the common mosquito population suppression models, even incorporating seasonality. This paper generalizes and improves many recent results in the literature.",0,arxiv,Evrim,CC-BY/arXiv,Mosquito population suppression models with seasonality and d-concave equations
"Calibrating agent-based epidemic models is computationally demanding. We present a supervised machine learning calibrator that learns the inverse mapping from epidemic time series to SIR parameters. A three-layer bidirectional LSTM ingests 60-day incidence together with population size and recovery rate, and outputs transmission probability, contact rate, and R0. Training uses a composite loss with an epidemiology-motivated consistency penalty that encourages R0 \* recovery rate to equal transmission probability \* contact rate.   In a 1000-scenario simulation study, we compare the calibrator with Approximate Bayesian Computation (likelihood-free MCMC). The method achieves lower error across all targets (MAE: R0 0.0616 vs 0.275; transmission 0.0715 vs 0.128; contact 1.02 vs 4.24), produces tighter predictive intervals with near nominal coverage, and reduces wall clock time from 77.4 s to 2.35 s per calibration. Although contact rate and transmission probability are partially nonidentifiable, the approach reproduces epidemic curves more faithfully than ABC, enabling fast and practical calibration. We evaluate it on SIR agent based epidemics generated with epiworldR and provide an implementation in R.",0,arxiv,Evrim,CC-BY/arXiv,Machine Generalize Learning in Agent-Based Models: Going Beyond Surrogate Models for Calibration in ABMs
"We propose a multi-patch model of cholera transmission integrating environmental contamination, human mobility, and nutritional vulnerability. The population is stratified by food security status, and transmission occurs via human contact, bacteria in the environment and contaminated food. We derive the basic reproduction number $\mathcal{R}_0$ analyze the stability of the disease-free equilibria and show a forward bifurcation. Numerical simulations illustrate how food insecurity amplifies outbreak severity and mortality. The model highlights the role of spatial heterogeneity and socio-environmental factors in shaping cholera dynamics. Moreover, results show the impact of sinks inside starting epidemic.",0,arxiv,Evrim,CC-BY/arXiv,Modeling Cholera Dynamics Under Food Insecurity and Environmental Contamination: A Multi-Patch Approach
"This work introduces a physics-informed neural networks (PINNs)-based model predictive control (MPC) framework for susceptible-infected-recovered ($SIR$) spreading models. Existing studies in MPC design for epidemic control often assume either 1) measurable states of the dynamics, where the parameters are learned, or 2) known parameters of the model, where the states are learned. In this work, we address the joint real-time estimation of states and parameters within the MPC framework using only noisy infected states, under the assumption that 1) only the recovery rate is known, or 2) only the basic reproduction number is known. Under the first assumption, we propose MPC-PINNs and two novel PINNs algorithms, all of which are integrated into the MPC framework. First, we introduce MPC-PINNs, which are designed for $SIR$ models with control. We then propose log-scaled PINNs (MPC-LS-PINNs), which incorporate a log-scaled loss function to improve robustness against noise. Next, we present split-integral PINNs (MPC-SI-PINNs), which leverage integral operators and state coupling in the neural network training process to effectively reconstruct the complete epidemic state information. Building upon these methods, we further extend our framework for the second assumption. We establish the necessary conditions and extend our PINNs algorithms, where MPC-SI-PINNs are simplified as split-PINNs (MPC-S-PINNs). By incorporating these algorithms into the MPC framework, we simultaneously estimate the epidemic states and parameters while generating optimal control strategies. Experiment results demonstrate the effectiveness of the proposed methods under different settings.",0,arxiv,Evrim,CC-BY/arXiv,A Physics-Informed Neural Networks-Based Model Predictive Control Framework for $SIR$ Epidemics
"During the COVID-19 pandemic, estimating the total deaths averted by vaccination has been of great public health interest. Instead of estimating total deaths averted by vaccination among both vaccinated and unvaccinated individuals, some studies empirically estimated only ""directly averted"" deaths among vaccinated individuals, typically suggesting that vaccines prevented more deaths overall than directly due to the indirect effect. Here, we define the causal estimand to quantify outcomes ""directly averted"" by vaccination$\unicode{x2014}$i.e., the impact of vaccination for vaccinated individuals, holding vaccination coverage fixed$\unicode{x2014}$for vaccination at multiple time points, and show that this estimand is a lower bound on the total outcomes averted when the indirect effect is non-negative. We develop an unbiased estimator for the causal estimand in a one-stage randomized controlled trial (RCT) and explore the bias of a popular ""hazard difference"" estimator frequently used in empirical studies. We show that even in an RCT, the hazard difference estimator is biased if vaccination has a non-null effect, as it fails to incorporate the greater depletion of susceptibles among the unvaccinated individuals. In simulations, the overestimation is small for averted deaths when infection-fatality rate is low, as for many important pathogens. However, the overestimation can be large for averted infections given a high basic reproduction number. Additionally, we define and compare estimand and estimators for avertible outcomes (i.e., outcomes that could have been averted by vaccination, but were not due to failure to vaccinate). Future studies can explore the identifiability of the causal estimand in observational settings.",0,arxiv,Evrim,CC-BY/arXiv,Defining and Estimating Outcomes Directly Averted by a Vaccination Program when Rollout Occurs Over Time
"Over the past decades, more than 25 phylogenetic tree balance indices and several families of such indices have been proposed in the literature -- some of which even contain infinitely many members. It is well established that different indices have different strengths and perform unequally across application scenarios. For example, power analyses have shown that the ability of an index to detect the generative model of a given phylogenetic tree varies significantly between indices. This variation in performance motivates the ongoing search for new and possibly \enquote{better} (im)balance indices. An easy way to generate a new index is to construct a compound index, e.g., a linear combination of established indices.   Two of the most prominent and widely used imbalance indices in this context are the Sackin index and the Colless index. In this study, we show that these classic indices are themselves compound in nature: they can be decomposed into more elementary components that independently satisfy the defining properties of a tree (im)balance index. We further show that the difference Colless minus Sackin results in another imbalance index that is minimized (amongst others) by all Colless minimal trees. Conversely, the difference Sackin minus Colless forms a balance index.   Finally, we compare the building blocks of which the Sackin and the Colless indices consist to these indices as well as to the stairs2 index, which is another index from the literature. Our results suggest that the elementary building blocks we identify are not only foundational to established indices but also valuable tools for analyzing disagreement among indices when comparing the balance of different trees.",0,arxiv,Evrim,CC-BY/arXiv,Revealing the building blocks of tree balance: fundamental units of the Sackin and Colless Indices
"Human cooperation persists among strangers despite theoretical predictions of difficulties in large, well-mixed populations, leaving a fundamental evolutionary puzzle. While upstream (pay-it-forward: helping others because you were helped) and downstream (rewarding-reputation: helping those with good reputations) indirect reciprocity have been independently considered as solutions, their joint dynamics in multiplayer contexts remain unexplored. We study N-player giving games with benefit b and cost c and analyze evolutionary dynamics for three strategies: unconditional cooperation (X), unconditional defection (Y), and an integrated reciprocal strategy (Z) combining unconditional forwarding with reputation-based discrimination. We show that integrating upstream and downstream reciprocity can yield a globally asymptotically stable mixed equilibrium of unconditional defectors and integrated reciprocators whenever the benefit-to-cost ratio exceeds a threshold (b/c > 2). Counterintuitively, introducing small complexity costs, rather than destabilizing, stabilizes the equilibrium by preventing not only unconditional cooperators (viewed as second-order freeloaders) but also alternative conditional strategies from invading. While the equilibrium frequency of integrated reciprocators decreases with group size N, it remains positive for any finite N. Rather than requiring uniformity, our model reveals one pathway to stable cooperation through strategic diversity. Defectors serve as ""evolutionary shields"" preventing system collapse while integrated reciprocators flexibly combine open and discriminative responses. This framework demonstrates how pay-it-forward chains and reputation systems can jointly maintain social polymorphism including cooperation despite cognitive limitations and group size challenges, offering a potential evolutionary foundation for behavioral diversity in human societies.",0,arxiv,Evrim,CC-BY/arXiv,Integrating upstream and downstream reciprocity stabilizes cooperator-defector coexistence in N-player giving games
"The COVID-19 pandemic forced the rapid development of vaccines and the implementation of mass vaccination programs around the world. However, many hesitated to take the vaccine due to concerns about its effectiveness. By looking at an ordinary differential equation (ODE) model of disease spread that incorporates a mass vaccination program, this study aims to determine the sensitivity of the cumulative count of infected individuals ($W$) and the cumulative death count ($D$) to the following model parameters: disease transmission rate ($Î²$), reciprocal of the disease latency period ($Îº$), reciprocal of the infectious period ($Î³$), death ratio ($Î±$), vaccine efficacy rate ($r$), and vaccine rollout rate ($Î´$). This was implemented using Latin hypercube sampling and partial rank correlation coefficient. Results show that $D$ is highly sensitive to $Î±$ and shows increasing sensitivity to $Î´$ in the long run. On the other hand, $W$ is highly sensitive to $Îº$ at the beginning of the simulation, but this weakens over time. In contrast, $W$ is not very sensitive to $Î´$ initially but becomes very significant in the long run. This supports the importance of the vaccine rollout rate over the vaccine efficacy rate in curbing the spread of the disease in the population. It is also worthwhile to reduce the death ratio by developing a cure for the disease or improving the healthcare system as a whole.",0,arxiv,Evrim,CC-BY/arXiv,Sensitivity analysis of an epidemic model with a mass vaccination program of a homogeneous population
"Improved hygiene and infant vaccinations have led to age specific variations in hepatitis A antibody prevalence in Korea, with lower rates among individuals in their 20s to 40s. Given that the fatality rate of hepatitis A increases for those aged 50 and older, the low immunity level among younger adults indicates a future risk of increased deaths in older age groups without additional preventive measures.We developed an age structured transmission model to assess the impact of adult vaccination, assuming it begins in 2025. The 20s age group was modeled with an additional compartment to account for hepatitis A vaccination administered to military recruits. Vaccination strategies targeting the 20s to 30s and 40s to 50s age groups were compared, considering antibody testing costs for the latter in Korea and focusing on projected deaths over approximately 50 years. When the total vaccination cost is fixed, targeting the 40s to 50s group covers 20% fewer individuals than the 20s to 30s group but yields a 1.3 to 1.5 fold greater reduction in deaths. When the total vaccine supply is fixed, targeting the 40s to 50s group is 1.2 times more expensive but yields a 1.7 to 1.8 fold greater reduction in deaths than the 20s to 30s group. Moreover, including a second dose for military personnel in the 20s to 30s group has minimal impact on reducing deaths. Preventive measures for adults may be necessary to reduce future hepatitis A related deaths. Vaccinating the 40s to 50s group would be more effective in reducing deaths than vaccinating the 20s to 30s group.",0,arxiv,Evrim,CC-BY/arXiv,Comparison of the effectiveness and costs of hepatitis A vaccination strategies to mitigate future deaths in the Republic of Korea
"Understanding the spatio-temporal evolution of epidemics with multiple pathogens requires not only new theoretical models but also careful analysis of their practical consequences. Building on the Multiplex Bi-Virus Reaction-Diffusion framework (MBRD) introduced in our companion paper, we investigate how the super-infection model (MBRD-SI) and the co-infection model (MBRD-CI) behave under different epidemiological and network conditions. Through numerical experiments, we study the effects of pathogen virulence, diffusion rates, and cross-diffusion on epidemic hotspot formation and long-term prevalence. Our results highlight the role of multiplex structure in amplifying or suppressing co-circulating infections, and provide quantitative insight into conditions that drive persistent epidemic patterns. Beyond epidemiology, these findings have broader implications for multiplex contagion processes such as information diffusion and malware propagation.",0,arxiv,Evrim,CC-BY/arXiv,Dynamics of Infection Spread and Hotspot Growth in Bi-Pathogen Networks
"Evaluating the timing and trajectory of sensory system innovations is crucial for understanding the increase in phylogenetic, behavioural, and ecological diversity during the Ediacaran-Cambrian transition. Elucidation of sensory adaptations has relied on either body-fossil evidence based on anatomical features or qualitative descriptions of trace-fossil morphology, leaving a gap in the record of sensory system innovations between the development of basic sensory capacities and that of more advanced sensory organs and brains. Here, we examine fossil movement trajectories of Ediacaran and Cambrian grazers for the presence of autocorrelation. Our analysis reveals a lack of temporal correlation in the studied Ediacaran trajectories and its presence in both analysed Cambrian trajectories, indicating time-tuned behaviours were in place by the early Cambrian. These results support the Cambrian Information Revolution hypothesis and indicates that increases in cognitive complexity and behavioural strategies were yet another important evolutionary innovation that occurred during the Ediacaran Cambrian transition.",0,arxiv,Evrim,CC-BY/arXiv,Remember when? Deciphering Ediacaran-Cambrian Metazoan behaviour and temporal memory using fossil movement paths
"When developers of artificial intelligence (AI) products need to decide between profit and safety for the users, they likely choose profit. Untrustworthy AI technology must come packaged with tangible negative consequences. Here, we envisage those consequences as the loss of reputation caused by media coverage of their misdeeds, disseminated to the public. We explore whether media coverage has the potential to push AI creators into the production of safe products, enabling widespread adoption of AI technology. We created artificial populations of self-interested creators and users and studied them through the lens of evolutionary game theory. Our results reveal that media is indeed able to foster cooperation between creators and users, but not always. Cooperation does not evolve if the quality of the information provided by the media is not reliable enough, or if the costs of either accessing media or ensuring safety are too high. By shaping public perception and holding developers accountable, media emerges as a powerful soft regulator -- guiding AI safety even in the absence of formal government oversight.",0,arxiv,Evrim,CC-BY/arXiv,Can Media Act as a Soft Regulator of Safe AI Development? A Game Theoretical Analysis
"Many ecosystems can undergo important qualitative changes, including sudden transitions to alternative stable states, in response to perturbations or increments in conditions. Such 'tipping points' are often preceded by declines in aspects of ecosystem resilience, namely the capacity to recover from perturbations, that leave various spatial and temporal signatures. These so-called 'early warning signals' have been used to anticipate transitions in diverse real systems, but many of the high-throughput, autonomous monitoring technologies that are transforming ecology have yet to be fully leveraged to this end. Acoustic monitoring in particular is a powerful tool for quantifying biodiversity, tracking ecosystem health, and facilitating conservation. By deploying acoustic recorders in diverse environments, researchers have gained insights from the calls and behaviour of individual species to higher-level soundscape features that describe habitat quality and even predict species occurrence. Here, we draw on theory and practice to advocate for using acoustics to probe ecosystem resilience and identify emerging and established early warning signals of tipping points. With a focus on pragmatic considerations, we emphasise that despite limits to tipping point theory and the current scale and transferability of data, acoustics could be instrumental in understanding resilience and tipping potential across distinct ecosystems and scales.",0,arxiv,Evrim,CC-BY/arXiv,Prospects for acoustically monitoring ecosystem tipping points
"Successful collective action on issues from climate change to the maintenance of democracy depends on societal properties such as cultural tightness and social cohesion. How these properties evolve is not well understood because they emerge from a complex interplay between beliefs and behaviors that are usually modeled separately. Here we address this challenge by developing a game-theoretical framework incorporating norm-utility models to study the coevolutionary dynamics of cooperative action, expressed belief, and norm-utility preferences. We show that the introduction of evolving beliefs and preferences into the Snowdrift game and Prisoner's Dilemma leads to a proliferation of evolutionary stable equilibria, each with different societal properties. In particular, we find that a declining material environment can simultaneously be associated with increased cultural tightness (defined as the degree to which individuals behave in accordance with widely held beliefs) and reduced social cohesion (defined as the degree of social homogeneity i.e. the extent to which individuals belong to a single well-defined group). Loss of social homogeneity occurs via a process of evolutionary branching, in which a population fragments into two distinct social groups with strikingly different characteristics. The groups that emerge differ not only in their willingness to cooperate, but also in their beliefs about cooperation and in their preferences for conformity and coherence of their actions and beliefs. These results have implications for our understanding of the resilience of cooperation and collective action in times of crisis.",0,arxiv,Evrim,CC-BY/arXiv,Cultural tightness and social cohesion under evolving norms
"Microbiomes are complex systems comprised of many interacting species. Species can survive harsh or changing conditions by rapid adaptation, a process accelerated by the exchange of genetic material between different species through horizontal gene transfer. Conjugative plasmids are ubiquitous mobile genetic elements that mediate such exchanges both within and between species. Therefore, predicting whether a plasmid can invade and be maintained by a microbial community is critical, for example when assessing the risks of antimicrobial resistance gene spread in commensal or environmental microbiomes. However, existing theory developed to assist such predictions has generally focused on the balance among plasmid costs, benefits, and infection rates, overlooking other relevant factors such as the inherent dynamics and diversity of microbiomes. Here, we hypothesize that plasmid persistence in the absence of positive selection can arise purely from the heterogeneity present in large and diverse microbial communities. We introduce a generic model that integrates population-level dynamics with plasmid conjugation. Using this model, we show that we can predict plasmid maintenance, and that the probability for a plasmid to be maintained depends on traits of the plasmid, most importantly the conjugation rate, and the species abundance distribution of the community. Then, using both empirical abundance data and extensive numerical simulations, we demonstrate that the inherent randomness of ecological interactions and conjugation rates enables plasmid persistence -- even in the absence of positive selection. Our findings thus suggest that natural microbial communities are likely to maintain plasmids indefinitely, offering a new perspective on the spread, maintenance, and ubiquity of plasmids.",0,arxiv,Evrim,CC-BY/arXiv,Heterogeneity drives plasmid maintenance in large microbial communities
"A phylogenetic tree is an important way in Bioinformatics to find the evolutionary relationship among biological species. In this research, a proposed model is described for the estimation of a phylogenetic tree for a given set of data. To estimate a phylogenetic tree there are certain necessary steps have to be considered. Gene sequences are useful data resources to estimate the relationship among species at the molecular level. In this research, the approach is to create a fusion between the existing models for the estimation of phylogenetic trees and a population-based meta-heuristics approach, i.e., Genetic Algorithm. NCBI's Entrez databases are used for the acquisition of gene sequencing data. A modular parallel approach is applied to handle this dataset efficiently. This paper illustrates a proposed model to create an independent platform for phylogenetic tree estimation. Existing benchmark approaches for tree population that are used for population-based meta-heuristic search techniques to find the best possible phylogenetic tree estimation for a given set of gene sequence data.",0,arxiv,Evrim,CC-BY/arXiv,Estimation of Phylogenetic Tree using Gene Sequencing Data
"Bacteria possess diverse mechanisms to regulate their motility in response to environmental and physiological signals, enabling them to navigate complex habitats and adapt their behavior. Among these mechanisms, interspecies recognition enables cells to modulate their movement based on the ecological identity of neighboring species. Here, we introduce a model in which we assume bacterial species recognizes each other and interact via local signals that either enhance or suppress the motility of neighboring cells. Through large-scale simulations and a coarse-grained stochastic model, we demonstrate the emergence of a sharp transition driven by nucleation processes: increasing the density of motility-suppressing interactions drives the system from a fully mixed, motile phase to a state characterized by large, stationary bacterial clusters. Remarkably, in systems with a large number of interacting species, this transition can be triggered solely by altering the structure of the motility-regulation interaction matrix while maintaining species and interaction densities constant. In particular, we find that heterogeneous and modular interactions promote the transition more readily than homogeneous random ones. These results contribute to the ongoing effort to understand microbial interactions, suggesting that structured, non-random ones may be key to reproducing commonly observed spatial patterns in microbial communities.",0,arxiv,Evrim,CC-BY/arXiv,Structured Interactions Drive Abrupt Transitions in the Spatial Organization of Microbial Communities
"Cooperation often depends on individuals avoiding exploitation and interacting preferentially with other cooperators. We explore how context-dependent migration influences the evolution of cooperation in spatially structured populations. Individuals interact in small groups through public goods games and reproduce with possible dispersal. Cooperators migrate more frequently when surrounded by defectors, while defectors disperse uniformly. This behavioral asymmetry reflects realistic differences in mobility and social responsiveness. Our results show that conditional migration can promote cooperation by enabling cooperators to escape defector-rich environments and cluster together. The effectiveness of this mechanism depends on baseline migration rates, group size, and the sensitivity of cooperators to local conditions. We identify parameter ranges where cooperation is favored even under conditions that would typically hinder its evolution. These findings highlight how behavioral plasticity and dispersal strategies can interact with population structure to support the emergence of cooperation.",0,arxiv,Evrim,CC-BY/arXiv,Sensitivity-Driven Migration and the Evolution of Cooperation in Multi-Player Games on Structured Populations
"The Savannah Hypothesis and the Cambrian Information Revolution invoke the development of spatially heterogeneous resource distribution during the Ediacaran-Cambrian transition as a key driver of infaunalization and sensory evolution of mobile bilaterians, respectively. However, difficulties in detecting historical resource distribution hinders the ability to tests these theories. If external conditions crucial to organism fitness (e.g. nutrient distribution, oxygen availability) became increasingly heterogeneous across the Ediacaran-Cambrian transition, then it follows that benthic organisms dependent on these conditions would demonstrate a similar increase in the spatial variability of their movement trajectories. To investigate Ediacaran resource distribution on the seafloor we examined the morphology of Helminthoidichnites tenuis, a simple unbranched horizontal bilaterian trail, from the Ediacara Member of Southern Australia for spatial trends. Our analysis reveals a as-yet undiscovered variability in the behaviour of the Helminthoidichnites tenuis tracemaker and confirmed heterogeneity in external conditions relevant to the tracemaker in the latest Ediacaran.",0,arxiv,Evrim,CC-BY/arXiv,Spatial trends in Ediacaran Bilaterian trails
"The trace-fossil record serves as a rich dataset to examine fossil behaviour, ecologic interactions at community level, and evolutionary trends in behaviour across geological time. Behavioural adaptations are often invoked in a variety of evolutionary hypotheses; however, few methods to quantitatively compare fossil behaviour exist. Movement paths, such as trails and trackways, are well-studied in extant-organism research where they are discretized and mathematically analyzed for behavioural strategies and trends. Here, we reference modern movement ecology research and present a methodology to discretize horizontal movement paths in the fossil record. We then demonstrate the utility of this methodology and the spatiotemporal data it collects via an analysis of the trilobite trace fossil Cruziana semiplicata and assess our results in light of three previous assertions about its recorded behaviour. Our analysis reveals the presence of three morphotypes, interpreted as three distinct behavioural variations, which persisted across multiple geographic localities and are interpreted to reflect changes in external conditions, internal states, or a combination of the two. Our research highlights the immense potential of this methodology to test behavioural hypotheses and provides an open-source groundwork for future research.",0,arxiv,Evrim,CC-BY/arXiv,Quantifying movement: Expanding the Ichnologist toolkit
"Modern life largely transmits genetic information from mother to daughter through the duplication of single physically intact molecules that encode information. However, copying an extended molecule requires complex copying machinery and high fidelity that scales with the genome size to avoid the error catastrophe. Here, we explore these fidelity requirements in an alternative architecture, the virtual circular genome, in which no one physical molecule encodes the full genetic information. Instead, information is encoded and transmitted in a collective of overlapping and interacting segments. Using a model experimental system of a complex mixture of DNA oligomers that can partly anneal and extend off each other, we find that mutant oligomers are suppressed relative to a model without collective encoding. Through simulations and theory, we show that this suppression of mutants can be explained by competition for productive binding partners. As a consequence, information can be propagated robustly in a virtual circular genome even at mutation rates expected under prebiotic conditions.",0,arxiv,Evrim,CC-BY/arXiv,Suppression of errors in collectively coded information
"Chronic Wasting Disease (CWD) is a neurological disease impacting deer, elk, moose, and other cervid populations and is caused by a misfolded protein known as a prion. CWD is difficult to control due to the persistence of prions in the environment. Prions can remain infectious for more than a decade and have been found in soil as well as other environmental vectors, such as ticks and plants. Here, we provide a bifurcation analysis of a mathematical model of CWD spread in a cervid population, and use a modification of the Gillespie algorithm to explore if wolves can be used as an ecological control strategy to limit the spread of the disease in several relevant scenarios. We then analytically compute the probability that the disease spreads given one infected member enters a fully healthy population and the probability of elimination, given a fully susceptible population and remaining prions in the environment. From our analysis, we conclude that wolves can be used as an effective control strategy to limit the spread of CWD in cervid populations, and hunting or other means of lowering the susceptible population are beneficial to controlling the spread of CWD, although it is important to note that inferring biologically relevant parameters from the existing data is an ongoing challenge for this system.",0,arxiv,Evrim,CC-BY/arXiv,The effect of predation on the dynamics of Chronic Wasting Disease in deer
"During fieldwork in Thailand we observed nearly identical frequencies of co-located flashing fireflies and chirping crickets. Motivated by this, we perform a meta-analysis and show an abundance of evolutionarily distinct species that communicate isochronously at ~0.5-4 Hz, suggesting that this might be a frequency ""hotspot."" We hypothesize that this timescale may have a universal basis in the biophysics of the receiver's neurons. We test this by demonstrating that small receiver circuits constructed from elements representing typical neurons will be most responsive in the observed frequency range.",0,arxiv,Evrim,CC-BY/arXiv,A universal animal communication tempo resonates with the receiver's brain
"Fish migration is a dynamic phenomenon observed in many surface water bodies on the earth, while its understanding is still insufficient. Particularly, the biological mechanism behind fish migration is not fully understood. Moreover, its observation is often conducted visually and hence manually, raising questions of accuracy and interpretation of the data sampled. We address the two issues, mechanism and observation, of fish migration based on a recently developed mathematical model. The results obtained in this short paper show that fish migration can be characterized through a minimization principle and evaluate the error of its manual observations. The minimization principle we hypothesize is an optimal control problem where the migrating fish population dynamically changes its size and fluctuation. We numerically investigate alternating and intensive observation schemes as case studies, demonstrating that in some realistic conditions the estimate of total fish count is not reliable. We believe that this paper contributes to a deeper understanding of fish migration.",0,arxiv,Evrim,CC-BY/arXiv,Two Issues in Modelling Fish Migration
"We investigate the impact of Hebbian learning on the contact process, a paradigmatic model for infection spreading, which has been also proposed as a simple model to capture the dynamics of inter-regional brain activity propagation as well as population spreading. Each of these contexts calls for an extension of the contact process with local learning. We introduce Hebbian learning as a positive or negative reinforcement of the activation rate between a pair of sites after each successful activation event. Learning can happen either in both directions motivated by social distancing (mutual learning model), or in only one of the directions motivated by brain and population dynamics (source or target learning models). Hebbian learning leads to a rich class of emergent behavior, where local incentives can lead to the opposite global effects. In general, positive reinforcement (increasing activation rates) leads to a loss of the active phase, while negative reinforcement (reducing activation rates) can turn the inactive phase into a globally active phase. In two dimensions and above, the effect of negative reinforcement is twofold: it promotes the spreading of activity, but at the same time gives rise to the appearance of effectively immune regions, entailing the emergence of two distinct critical points. Positive reinforcement can lead to Griffiths effects with non-universal power-law scaling, through the formation of random loops of activity, a manifestation of the ``ant mill"" phenomenon.",0,arxiv,Evrim,CC-BY/arXiv,Activity propagation with Hebbian learning
"The contact process is a simple infection spreading model showcasing an out-of-equilibrium phase transition between a macroscopically active and an inactive phase. Such absorbing state phase transitions are often sensitive to the presence of quenched disorder. Traditionally, a phase transition in the disordered contact process is either triggered by dilution or by locally varying the infection rate. However, when both factors play an important role, a multicritical point emerges that remains poorly understood. Here, we study the multicritical contact process by large-scale Monte Carlo simulations in two and three dimensions. The multicritical behavior is found to be universal and exhibits ultra-slow, activated dynamical scaling, with exponents consistent with those predicted by the strong disorder renormalization group method. This finding indicates that the multicritical contact process belongs to the same universality class as the multicritical quantum Ising model, opening future directions to measure quantum entanglement properties via classical simulations.",0,arxiv,Evrim,CC-BY/arXiv,Multicritical Infection Spreading
"The mitigation of stress is a key challenge for all biological systems. Conditions of unresolvable stress have been associated with a diverse array of pathologies, from cancer to post-traumatic stress disorder (PTSD). Here, I unify insights from evolutionary and developmental biology with trauma psychology to present a novel framework for tumorigenesis which synthesizes stress-perception, tissue dysfunction, and the hallmarks of neoplastic growth. This view carries therapeutic implications, suggesting a reintegrative approach that seeks to return cancer cells to the homeostatic control of the surrounding tissue.",0,arxiv,Evrim,CC-BY/arXiv,Tumorigenesis as a trauma response: the fragmentation of morphogenetic memory drives neoplastic dissociation
"Vector-borne diseases arise from the coupled dynamics of human mobility and mosquito ecology, producing outbreaks shaped by both spatial distributions and temporal patterns of movement. Here we develop a coarse-grained hub--leaf reduction that isolates the universal principles governing epidemic vulnerability in interconnected populations. By deriving and analyzing the epidemic vulnerability equation, we show how human and vector population ratios, together with mobility parameters that regulate time spent in hub and leaf locations, jointly determine the conditions for outbreak persistence. The analysis reveals that balanced flows of individuals between patches consistently minimize vulnerability, whereas disproportionate concentrations of vectors can shift the dominant risk to specific locations. Across parameter regimes, compensatory mobility emerges as a stabilizing mechanism, while skewed host--vector ratios elevate epidemic risk. These results establish general principles for the spatiotemporal determinants of vector-borne disease spread and provide a theoretical foundation for extending minimal models to more complex epidemiological settings.",0,arxiv,Evrim,CC-BY/arXiv,Spatiotemporal Determinants of Vector-Borne Disease Outbreaks
"Bacteria often develop distinct phenotypes to adapt to environmental stress. In particular, they can produce biofilms, dense communities of bacteria that live in a complex extracellular matrix. Bacterial biofilms provide a safe haven from environmental conditions by distributing metabolic workload and allowing them to perform complex multicellular processes. While previous studies have investigated how bacterial biofilms are regulated under laboratory conditions, they have not considered (1) the data requirements necessary to estimate model parameters and (2) how bacteria respond to recurring stressors in their natural habitats. To address (1), we adapted a mechanistic population model to explore the dynamics of biofilm formation in the presence of predator stress, using synthetic data. We used a Maximum Likelihood Estimation framework to measure crucial parameters underpinning the biofilm formation dynamics. We used genetic algorithms to propose an optimal data collection schedule that minimised parameter identifiability confidence interval widths. Our sensitivity analysis revealed that we could simplify the binding dynamics and eliminate biofilm detachment. To address (2), we proposed a structured version of our model to capture the long-term behaviour and evolutionary selection. In our extended model, the subpopulations feature different maximal rates of biofilm formation. We compared the selection under different predator types and amounts and identified key parameters that affected the speed of selection via sensitivity analysis.",0,arxiv,Evrim,CC-BY/arXiv,"Identifiability, Sensitivity, and Genetic Algorithms in Bacterial Biofilm Selection Models"
"Group synchrony in the animal kingdom is usually associated with mating. Being in sync is likely advantageous, as it may help in luring the opposite sex. Yet there are also disadvantages -- such as the homogenization of the group -- which make it harder for individuals to stand-out. Here we address this tradeoff, bringing together the Kuramoto model with concepts from evolutionary game theory. We focus on the existence of self-interested ''cheaters,'' which have been extensively studied in a variety of species. In our scenario, cheating individuals take part in the synchronous group display but position themselves (in terms of phase) slightly ahead or behind the pack. This allows them to enjoy both the group benefit of advertisement and the individual benefit of being unique. But a group can only tolerate a limited number of such individuals while still achieving synchrony. We therefore incorporate a from of ''policing'' into our model: if an individual strays too far from the group's synchronous phase, they reveal themselves as dishonest and are punished. Our model offers testable predictions regarding natural population compositions, and will hopefully spur further investigation into not only how, but also why, natural systems synchronize.",0,arxiv,Evrim,CC-BY/arXiv,Modeling the evolution of collective synchrony
"Artificial chemistry simulations produce many intriguing emergent behaviors, but they are often difficult to steer or control. This paper proposes a method for steering the dynamics of a classic artificial chemistry model, known as AlChemy (Algorithmic Chemistry), which is based on untyped lambda calculus. Our approach leverages features that are endogenous to AlChemy without constructing an explicit external fitness function or building learning into the dynamics. We demonstrate the approach by synthesizing non-trivial lambda functions, such as Church addition and succession, from simple primitives. The results provide insight into the possibility of endogenous selection in diverse systems such as autocatalytic chemical networks and software systems.",0,arxiv,Evrim,CC-BY/arXiv,Prebiotic Functional Programs: Endogenous Selection in an Artificial Chemistry
"Species interactions (ranging from direct predator prey relationships to indirect effects mediated by the environment) are central to ecosystem balance and biodiversity. While empirical methods for measuring these interactions exist, their interpretability and limitations remain unclear. Here we examine the empirical matrix of pairwise interactions, a widely used tool, and analyze its temporal variability. We show that apparent fluctuations in interaction strength (and even shifts in interaction signs, often interpreted as transitions between competition and facilitation) can arise intrinsically from population dynamics with fixed ecological roles. Experimental protocols further shape these estimates: the duration of observation and the type of setup in microbial growth studies (e.g., chemostats, batch cultures, or resource conditions) systematically affect measured interactions. Considering interactions across timescales enhances interpretability: short-term measurements primarily capture direct species couplings, whereas long-term observations increasingly reflect indirect community feedback. Taken together, these results establish short duration inferences, obtained either directly or extrapolated, as a principled way to disentangle direct from indirect interactions. Building on this insight, we propose a model inference approach that leverages multiple short time series rather than extended longitudinal datasets.",0,arxiv,Evrim,CC-BY/arXiv,Unraveling the temporal dependence of ecological interaction measures
"In many biological processes, the size of a population changes stochastically with time, and recent work in the context of cancer and bacterial growth have focused on the situation when the mean population size grows exponentially. Here, motivated by the evolutionary process of genetic hitchhiking in a selectively neutral population, we consider a model in which the mean size of the population increases linearly. We are interested in understanding how the fluctuations in the population size impact the first passage statistics, and study the fixation probability that a mutant reaches frequency one by a given time in a population whose size follows a conditional Wright-Fisher process. We find that at sufficiently short and long times, the fixation probability can be approximated by a model that ignores temporal correlations between the inverse of the population size, but at intermediate times, it is significantly smaller than that obtained by neglecting the correlations. Our analytical and numerical study of the correlation functions show that the conditional Wright-Fisher process of interest is neither a stationary nor a Gaussian process; we also find that the variance of the inverse population size initially increases linearly with time $t$ and then decreases as $t^{-2}$ at intermediate times followed by an exponential decay at longer times. Our work emphasizes the importance of temporal correlations in populations with fluctuating size that are often ignored in population-genetic studies of biological evolution.",0,arxiv,Evrim,CC-BY/arXiv,Dynamics of fixation probability in a population with fluctuating size
"Microbial adaptation to extreme stress, such as starvation, antimicrobial exposure, or freezing often reveals fundamental trade-offs between survival and proliferation. Understanding how populations navigate these trade-offs in fluctuating environments remains a central challenge. We develop a quantitative model to investigate the adaptation of populations of yeast (Saccharomyces cerevisiae) subjected to cycles of growth and extreme freeze-thaw stress, focusing on the role of quiescence as a mediator of survival. Our model links key life-history traits: growth rate, lag time, quiescence probability, and stress survival, to a single underlying phenotype, motivated by the role of intracellular trehalose in the adaptation of yeast to freeze-thaw stress. Through stochastic population simulations and analytical calculation of the long-term growth rate, we identify the evolutionary attractors of the system. We find that the strength of the growth-survival trade-off depends critically on environmental parameters, such as the duration of the growth phase. Crucially, our analysis reveals that populations optimized for growth-stress cycles can often maintain viability alongside growth-optimized populations even in the absence of stress. This demonstrates that underlying physiological trade-offs do not necessarily translate into fitness trade-offs at the population level, providing general insights into the complex interplay between environmental fluctuations, physiological constraints, and evolutionary dynamics.",0,arxiv,Evrim,CC-BY/arXiv,Adaptation to extreme stress under the growth-survival fitness trade-off
"Omnivory, where species feed across multiple trophic levels, is a widespread feature of ecological networks. A key mechanism underlying such complexity is intraguild predation (IGP), in which a top predator consumes both an intermediate predator and a shared resource. Here, we show that Shilnikov homoclinic orbits emerge in a minimal intraguild predation model, triggering a cascade of homoclinic bifurcations near a saddle-focus equilibrium that culminates in chaos. Numerical simulations and Lyapunov spectrum analysis reveal multiple coexistence modes, ranging from regular oscillations to Shilnikov homoclinic orbits and chaos. Our model quantitatively reproduces patterns observed in natural omnivore networks, providing mechanistic insights into complex population fluctuations in ecological systems.",0,arxiv,Evrim,CC-BY/arXiv,A homoclinic route to chaos in omnivore communities
"A central goal in ecology is to understand how biodiversity is maintained. Previous theoretical works have employed the rock-paper-scissors (RPS) game as a toy model, demonstrating that population mobility is crucial in determining the species' coexistence. One key prediction is that biodiversity is jeopardized and eventually lost when mobility exceeds a certain value--a conclusion at odds with empirical observations of highly mobile species coexisting in nature. To address this discrepancy, we introduce a reinforcement learning framework and study a spatial RPS model, where individual mobility is adaptively regulated via a Q-learning algorithm rather than held fixed. Our results show that all three species can coexist stably, with extinction probabilities remaining low across a broad range of baseline migration rates. Mechanistic analysis reveals that individuals develop two behavioral tendencies: survival priority (escaping from predators) and predation priority (remaining near prey). While species coexistence emerges from the balance of the two tendencies, their imbalance jeopardizes biodiversity. Notably, there is a symmetry-breaking of action preference in a particular state that is responsible for the divergent species densities. Furthermore, when Q-learning species interact with fixed-mobility counterparts, those with adaptive mobility exhibit a significant evolutionary advantage. Our study suggests that reinforcement learning may offer a promising new perspective for uncovering the mechanisms of biodiversity and informing conservation strategies.",0,arxiv,Evrim,CC-BY/arXiv,Species coexistence in the reinforcement learning paradigm
"Understanding interactions between the spread of multiple pathogens during an epidemic is crucial to assessing the severity of infections in human communities. In this paper, we introduce two new Multiplex Bi-Virus Reaction-Diffusion models (MBRD) on multiplex metapopulation networks: the super-infection model (MBRD-SI) and the co-infection model (MBRD-CI). These frameworks capture two-pathogen dynamics with spatial diffusion and cross-diffusion, allowing the prediction of infection clustering and large-scale spatial distributions. We establish conditions for Turing and Turing-Hopf instabilities in both models and provide experimental evidence of epidemic pattern formation. Beyond epidemiology, we discuss applications of the MBRD framework to information propagation, malware diffusion, and urban transportation networks.",0,arxiv,Evrim,CC-BY/arXiv,Spatial Super-Infection and Co-Infection Dynamics in Networks
"Downsizing the US public health workforce throughout 2025 amplifies potential risks during public health crises. Expert judgment from public health officials represents a vital information source, distinct from traditional surveillance infrastructure, that should be valued -- not discarded. Understanding how expert knowledge functions under constraints is essential for understanding the potential impact of reduced capacity. To explore expert forecasting capabilities, 114 public health officials at the 2024 CSTE workshop generated 103 predictions plus 102 rationales of peak hospitalizations and 114 predictions of influenza H3 versus H1 dominance in Pennsylvania for the 2024/25 season. We compared expert predictions to computational models and used rationales to analyze reasoning patterns using Latent Dirichlet Allocation. Experts better predicted H3 dominance and assigned lower probability to implausible scenarios than models. Expert rationales drew on historical patterns, pathogen interactions, vaccine data, and cumulative experience. Expert public health knowledge constitutes a critical data source that should be valued equally with traditional datasets. We recommend developing a national toolkit to systematically collect and analyze expert predictions and rationales, treating human judgment as quantifiable data alongside surveillance systems to enhance crisis response capabilities.",0,arxiv,Evrim,CC-BY/arXiv,Beyond Traditional Surveillance: Harnessing Expert Knowledge for Public Health Forecasting
"Phylogenetic trees represent evolutionary relationships and can be uniquely defined by sets of finite-state biological characteristics. Despite prior work showing that sufficiently large trees can be determined by $r$-state character sets, the minimal leaf thresholds $n_r$ remain largely unknown. In this work, we establish the 3-state case as $n_3 = 8$, providing a concrete base for higher-state analyses. We then resolve the 5-state problem by constructing a counterexample for $n=15$ and proving that for $n \geq 16$, $\lceil (n-3)/4 \rceil$ 5-state characters suffice to uniquely define any tree. Our approach relies on rigorous mathematical induction with complete verification of base cases and logically consistent inductive steps, offering new insights into the minimal conditions for character-based tree identification.",0,arxiv,Evrim,CC-BY/arXiv,Defining a phylogenetic tree with the minimum number of small-state characters
"Previous pandemics, including influenza pandemics and Covid-19, have disproportionately impacted MÄori and Pacific populations in Aotearoa New Zealand. The reasons for this are multi-faceted, including differences in socioeconomic deprivation, housing conditions and household size, vaccination rates, access to healthcare, and prevalence of pre-existing health conditions. Many mathematical models that were used to inform the response to the Covid-19 pandemic did not explicitly include ethnicity or other socioeconomic variables. This limited their ability to predict, understand and mitigate inequitable impacts of the pandemic. Here, we extend a model that was developed during the Covid-19 pandemic to support the public health response by stratifying the population into four ethnicity groups: MÄori, Pacific, Asian and European/other. We include three ethnicity-specific components in the model: vaccination rates, clinical severity parameters, and contact patterns. We compare model results to ethnicity-specific data on Covid-19 cases, hospital admissions and deaths between 1 January 2022 and 30 June 2023, under different model scenarios in which these ethnicity-specific components are present or absent. We find that differences in vaccination rates explain only part of the observed disparities in outcomes. While no model scenario is able to fully capture the heterogeneous temporal dynamics, our results suggest that differences between ethnicities in the per-infection risk of clinical severe disease is an important factor. Our work is an important step towards models that are better able to predict inequitable impacts of future pandemic and emerging disease threats, and investigate the ability of interventions to mitigate these.",0,arxiv,Evrim,CC-BY/arXiv,Modelling the transmission and impact of Omicron variants of Covid-19 in different ethnicity groups in Aotearoa New Zealand
"Despite growth being fundamental to all aspects of cell biology, we do not yet know its organizing principles in eukaryotic cells. Classic models derived from the bacteria E. coli posit that protein-synthesis rates are set by mass-action collisions between charged tRNAs produced by metabolic enzymes and mRNA-bound ribosomes. These models show that faster growth is achieved by simultaneously raising both ribosome content and peptide elongation speed. Here, we test if these models are valid for eukaryotes by combining single-molecule tracking, spike-in RNA sequencing, and proteomics in 15 carbon- and nitrogen-limited conditions using the budding yeast S. cerevisiae. Ribosome concentration increases linearly with growth rate, as in bacteria, but the peptide elongation speed remains constant (~9 amino acids/s) and charged tRNAs are not limiting. Total mRNA concentration rises in direct proportion to ribosomes, driven by enhanced RNA polymerase II occupancy of the genome. We show that a simple kinetic model of mRNA-ribosome binding predicts both the fraction of active ribosomes, the growth rate, and responses to transcriptional perturbations. Yeast accelerate growth by coordinately and proportionally co-up-regulating total mRNA and ribosome concentrations, not by speeding elongation. Taken together, our work establishes a new framework for eukaryotic growth control and resource allocation.",0,arxiv,Evrim,CC-BY/arXiv,Yeast growth is controlled by the proportional scaling of mRNA and ribosome concentrations
"West Nile virus (WNV) is a climate-sensitive mosquito-borne arbovirus circulating between mosquitoes of the genus Culex and birds, with a potential spillover to humans and other mammals. Recent trends in climatic change, characterized by early and/or prolonged summer seasons, increased temperatures, and above-average rainfall, probably facilitated the spread of WNV in Europe, including Germany. In this work, we formulate a spatial WNV model consisting of a system of parabolic partial differential equations (PDEs), using the concept of diffusion and advection in combination with temperature-dependent parameters, i.e., mosquito biting rate, extrinsic incubation, and mortality rate. Diffusion represents the random movement of both mosquitoes and hosts across space, while advection captures the directed movement of migratory birds. The model is first studied mathematically, and we show that it has non-negative, unique, and bounded solutions in time and space. Numerical simulations of the PDE model are performed using temperature data for Germany (2019 - 2024). Results obtained from the simulation showed a high agreement with the reported WNV cases among birds and equids in Germany. The observed spreading patterns from the year 2018 to 2022 and the year 2024 were mainly driven by temperature in combination with diffusion processes of hosts and vectors. Only during the year 2023, the additional inclusion of advection for migratory birds was important to correctly predict new hotspots in new locations in Germany.",0,arxiv,Evrim,CC-BY/arXiv,Modeling the impact of temperature and bird migration on the spread of West Nile virus
"We study a size--structured population model of proliferating cells in which biomass accumulation and binary division occur at rates modulated by fluctuating internal phenotypes. We quantify how fluctuations in internal variables that influence both growth and division shape the distribution of population phenotypes. We derive conditions under which the distributions of size and internal state decouple. Under this decoupling, population--level expectations are obtained from lineage-level expectations by an exponential tilting given by the Feynman--Kac formula. We further characterize weaker (ensemble-specific) versions of decoupling that hold in the lineage or the population ensemble but not both. Finally, we provide a more general interpretation of the tilted expectations in terms of the mass-weighted phenotype distribution.",0,arxiv,Evrim,CC-BY/arXiv,Size-structured populations with growth fluctuations: Feynman--Kac formula and decoupling
"Using past behaviors to guide future actions is essential for fostering cooperation in repeated social dilemmas. Traditional memory-based strategies that focus on recent interactions have yielded valuable insights into the evolution of cooperative behavior. However, as memory length increases, the complexity of analysis grows exponentially, since these strategies need to map every possible action sequence of a given length to subsequent responses. Due to their inherent reliance on exhaustive mapping and a lack of explicit information processing, it remains unclear how individuals can handle extensive interaction histories to make decisions under cognitive constraints. To fill this gap, we introduce coordinated reciprocity strategies ($CORE$), which incrementally evaluate the entire game history by tallying instances of consistent actions between individuals without storing round-to-round details. Once this consistency index surpasses a threshold, $CORE$ prescribes cooperation. Through equilibrium analysis, we derive an analytical condition under which $CORE$ constitutes an equilibrium. Moreover, our numerical results show that $CORE$ effectively promotes cooperation between variants of itself, and it outperforms a range of existing strategies including memory-$1$, memory-$2$, and those from a documented strategy library in evolutionary dynamics. Our work thus underscores the pivotal role of cumulative action consistency in enhancing cooperation, developing robust strategies, and offering cognitively low-burden information processing mechanisms in repeated social dilemmas.",0,arxiv,Evrim,CC-BY/arXiv,Evolutionary dynamics under coordinated reciprocity
"Determining associations among different species from citizen science databases is challenging due to observer behavior and intrinsic density variations that give rise to correlations that do not imply species associations. This paper introduces a method that can efficiently analyze large datasets to extract likely species associations. It tiles space into small blocks chosen to be of the accuracy of the data coordinates, and reduces observations to presence/absence per tile, in order to compute pairwise overlaps. It compares these overlaps with a spatial Poisson process that serves as a null model. For each species $i$, an expected overlap $Î¼_i$ is estimated by averaging normalized overlaps over other species in the same vicinity. This gives a $z$-score for significance of a species-species association and a correlation index for the strength of this association. This was tested on $874,263$ iNaturalist observations spanning $15,975$ non-avian taxa in the Santa Cruz, California region ($\approx 4.68\times10^{6}$ tiles). The method recovers well-known insect host-plant obligate relationships, particularly many host-gall relationships, as well as the relationship between Yerba Santa Beetles and California Yerba Santa. This approach efficiently finds associations on $\sim10^{8}$ species pairs on modest hardware, filtering correlations arising from heterogeneous spatial prevalence and user artifacts. It produces a ranked shortlist of ecological interactions that can be further pursued. Extensions to this method are possible, such as investigating the effects of time and elevation. It could also be useful in the determination of microhabitats and biomes.",0,arxiv,Evrim,CC-BY/arXiv,Finding Inter-species Associations on Large Citizen Science Datasets
"Multilevel selection occurs when short-term individual-level reproductive interests conflict with longer-term group-level fitness effects. Detecting and quantifying this phenomenon is key to understanding evolution of traits ranging from multicellularity to pathogen virulence. Multilevel selection is particularly important in artificial life research due to its connection to major evolutionary transitions, a hallmark of open-ended evolution. Bonetti Franceschi & Volz (2024) proposed to detect multilevel selection dynamics by screening for mutations that appear more often in a population than expected by chance (due to individual-level fitness benefits) but are ultimately associated with negative longer-term fitness outcomes (i.e., smaller, shorter-lived descendant clades). Here, we use agent-based modeling with known ground truth to assess the efficacy of this approach. To test these methods under challenging conditions broadly comparable to the original dataset explored by Bonetti Franceschi & Volz (2024), we use an epidemiological framework to model multilevel selection in trade-offs between within-host growth rate and between-host transmissibility. To achieve success on our in silico data, we develop an alternate normalization procedure for identifying clade-level fitness effects. We find the method to be sensitive in detecting genome sites under multilevel selection with 30% effect sizes on fitness, but do not see sensitivity to smaller 10% mutation effect sizes. To test the robustness of this methodology, we conduct additional experiments incorporating extrinsic, time-varying environmental changes and adaptive turnover in population compositions, and find that screen performance remains generally consistent with baseline conditions. This work represents a promising step towards rigorous generalizable quantification of multilevel selection effects.",0,arxiv,Evrim,CC-BY/arXiv,Extending a Phylogeny-based Method for Detecting Signatures of Multi-level Selection for Applications in Artificial Life
"A normal (phylogenetic) network with $k$ reticulations displays $2^k$ phylogenetic trees. In this paper, we establish an analogous result for tree-child (phylogenetic) networks with no underlying $3$-cycles. In particular, we show that a tree-child network with $k\ge 2$ reticulations and no underlying $3$-cycles displays at least $2^{k/2}$ phylogenetic trees if $k$ is even and at least $\frac{3}{2\sqrt{2}}2^{k/2}$ if $k$ is odd. Moreover, we show that these bounds are sharp and characterise the tree-child networks that attain these bounds.",0,arxiv,Evrim,CC-BY/arXiv,A sharp lower bound for the number of phylogenetic trees displayed by a tree-child network
"Natural selection can create information. In particular, because of the action of natural selection, we can often learn something about an environment by examining local organisms, and vice versa. For example, the characteristics of a cactus suggest that the local environment is relatively dry, and if a natural terrestrial environment is dry, then we will generally have an enhanced probability of finding drought-resistant plants (like cacti). Here, we propose a measure that can be used to quantify the information that is created by natural selection. We call the proposed quantity reproductive information, and we show that it has an intuitively satisfying relationship to standard quantitative definitions of information. Reproductive information is also approximately equal to a previously defined measure of biological adaptation. In addition, we explain how reproductive information can be measured using phenotypic characters, instead of genotypes. This could facilitate the measurement of reproductive information, and it could also allow for the quantification of the information that is created by natural selection on groups of organisms, instead of just selection on individuals. Thus, the concept of reproductive information has the potential to advance research on the ""units of selection"", the ""major transitions in evolution"", and the emergence of ""superorganisms"" via cooperation among group members.",0,arxiv,Evrim,CC-BY/arXiv,How can we measure the information created by natural selection?
"Infectious disease spread is a multi-scale process composed of within-host (biological) and between-host (social) drivers and disentangling them from each other is a central challenge in epidemiology. Here, we introduce VIBES, a multi-scale modeling framework that explicitly integrates viral dynamics based on patient-level data with population-level transmission on a data-driven network of social contacts. Using SARS-CoV-2 as a case study, we analyze three emergent epidemic properties, namely the generation time, serial interval, and pre-symptomatic transmission. First, we established a purely biological baseline, thus independent of the reproduction number (R), from the within-host model, estimating a generation time of 6.3 days for symptomatic individuals and 43.1% presymptomatic transmission. Then, using the full model incorporating social contacts, we found a shorter generation time (5.4 days at R=3.0) and an increase in pre-symptomatic transmission (52.8% at R=3.0), disentangling the impact of social drivers from a purely biological baseline. We further show that as pathogen transmissibility increases (R from 1.3 to 6), competition among infectious individuals shortens the generation time and serial interval by up to 21% and 13%, respectively. Conversely, a social intervention, like isolation, increases the proportion of pre-symptomatic transmission by about 30%. Our framework also estimates metrics that are challenging to obtain empirically, such as the generation time for asymptomatic individuals (5.6 days; 95%CI: 5.1-6.0 at R=1.3). Our findings establish multi-scale modeling as a powerful tool for mechanistically quantifying how pathogen biology and human social behavior shape epidemic dynamics as well as for assessing public health interventions.",0,arxiv,Evrim,CC-BY/arXiv,VIBES: A Multi-Scale Modeling Approach Integrating Within-Host and Between-Hosts Dynamics in Epidemics
"The regularity of ecosystem size spectra is one of the most intriguing and relevant phenomena on our planet. Aquatic size spectra generally show a log-linearly downtrending shape, following a power-law distribution. A constant log-linear slope has been reported for many marine pelagic ecosystems, often being approximately b = -1. Conversely, there are variable trophic-level-biomass relationships (trophic pyramids). The contrasting observations of a constant biomass (M) spectrum slope and highly variable biomass pyramids may be defined as the constant size spectra-variable trophic dynamics paradox. Here, a 'mass-specific predator-prey-efficiency theory of size spectra' (PETS) is presented and discussed. A thorough analysis of available data, literature and models result in the conclusion that most pelagic marine ecosystems are controlled by trophic processes such as resource-limit stress (bottom-up control) and top-down regulation, with a key role of the maximum carrying capacity of large-sized organisms. This has relevant consequences for the prediction and interpretation of size spectra and the context of fisheries, whaling, and the introduction of exotic predators (e.g., lionfish). The complete size spectrum obtained in situ, including living organisms and non-living particles (e.g., for data from LOPC and UVP) is discussed. This paper is intended as a plea for the integration of modeling approaches, to understand and integrate data and processes across communities including bacteria, phytoplankton, fish and mammals, considering the effect of non-organismic particles.",0,arxiv,Evrim,CC-BY/arXiv,Towards a compleat theory of ecosystem size spectra
"The Eigen model is a prototypical toy model of evolution that is synonymous with the so-called error catastrophe: when mutation rates are sufficiently high, the genetic variant with the largest replication rate does not occupy the largest fraction of the total population because it acts as a source for the other variants. Here we show that, in the stochastic version of the Eigen model, there is also a fidelity catastrophe. This arises due to the state-dependence of fluctuations and occurs when rates of mutation fall beneath a certain threshold, which we calculate. The result is a type of noise-induced multistability whereupon the system stochastically switches between short-lived regimes of effectively clonal behavior by different genetic variants. Most notably, when the number of possible variants -- typically $\sim4^L$, with $L\gg 1$ the length of the genome -- is significantly larger than the population size, there is only a vanishingly small interval of mutation rates for which the Eigen model is neither in the fidelity- nor error-catastrophe regimes, seemingly subverting traditional expectations for evolutionary systems.",0,arxiv,Evrim,CC-BY/arXiv,Stochastic Multistability of Clonallike States in the Eigen Model: a Fidelity Catastrophe
"Phytophagous mites in the genus Brevipalpus (Tenuipalpidae) can be major pests, causing direct damage to their host plants or vectoring viruses. However, a phylogeny-based classification to understand their evolution and predict their bioecological aspects is lacking. Accurate species identification is crucial for studying pathosystem interactions, implementing quarantine measures, and developing control strategies. This study revisited the classification of Brevipalpus based on phylogenetic relationships, identifying cryptic species and determining genetic distance boundaries. A multi-tool exploration of DNA datasets, including mitochondrial (two COI regions) and nuclear (ITS2 and D1-D3) data combined with a detailed morphological study using light and scanning microscopy, was performed. Specimens were collected from 20 host plant families from South America, Europe, and the Middle East. Species were discriminated using three different approaches, namely, Automatic Barcode Gap Discovery (ABGD), Assemble Species by Automatic Partition (ASAP), and a local regression to establish consistent genetic distance thresholds. Results indicate that the current species-group classification only partially matches that based on genetic lineages. Some species currently classified as belonging to the phoenicis and cuneatus species groups were found to be polyphyletic and related to species placed in other species groups. A rearrangement of the species groups to align with the observed genetic lineages is proposed, and phylogenetically informative morphological traits are defined. Cryptic diversity in certain taxa was consistently confirmed by various methods. Nuclear and mitochondrial markers were essential for identifying candidate lineages when morphology alone was insufficient. Nuclear markers detected host-associated lineages within certain species. Inconsistencies between markers and methods suggest the presence of ongoing speciation processes, and hypotheses about speciation events were explored. Intra- and interspecific genetic distances and threshold values were determined for the four studied genomic fragments and can be used for routine taxonomic identification and uncovering cryptic lineages. Further research should combine genetic data, carefully selected markers, and thorough morphological analyses before proposing new species.",0,arxiv,Evrim,CC-BY/arXiv,"Revisiting the systematics of Brevipalpus flat mites (Tenuipalpidae): phylogeny, species groups and cryptic diversity"
"Population size is a key metric for management and conservation. This is especially true for large carnivore populations for which management decisions are often based on population size estimates. In France, gray wolves (Canis lupus) have been monitored for more than two decades using non-invasive genetic sampling and capture-recapture models. Population size estimates directly inform the annual number of wolves that can be killed legally. It is therefore key to use appropriate methods to obtain robust population size estimates. To track the recent numerical and geographical expansion of the population, a substantial increase in sample collection was performed during the winter 2023/24 within the entire wolf distribution range in France. A total of 1964 samples were genotyped and assigned to 576 different individuals using microsatellites genetic markers. During the winter 2023/24, spatial capture-recapture models estimated the wolf population size in France to be likely between 920 and 1125 individuals (95% credible interval). Detection probability varied spatially and was positively influenced by snow cover and accessibility. Wolf density was strongly associated with historical presence, reflecting the ongoing recolonization process from the Alps. This work illustrates the usefulness of non-invasive genetic data and spatial capture-recapture for large-scale population assessment. It also lays the ground for future improvements in monitoring to fully exploit the potential of spatial capture-recapture models.",0,arxiv,Evrim,CC-BY/arXiv,Estimating wolf population size in France using non-invasive genetic sampling and spatial capture recapture models
"Highly pathogenic avian influenza (HPAI), especially the H5N1 strain, remains a major threat to animal health, food security, and public health. Recent spillover events in dairy cattle in the United States, linked to wild birds, highlight the critical importance of understanding transmission pathways at the cattle--wild bird--environment interface. In this work, we formulate and analyze a deterministic compartmental model that captures the transmission of HPAI between dairy cattle and wild birds, incorporating both direct and indirect (environmental) routes. The model combines an $SEIR$ framework for cattle with an $SIR$ structure for wild birds, coupled through an environmental compartment. We derive the basic reproduction number, $\mathcal{R}_{0}$, using the next-generation matrix approach, decomposing it into cattle-to-cattle, bird-to-bird, and environmental contributions. Qualitative analysis establishes positivity, boundedness, and global stability of equilibria through Lyapunov functions. Numerical simulations confirm the results of the theoretical analyses, illustrating outbreak trajectories, extinction thresholds, and persistence dynamics. A global sensitivity analysis, based on Latin hypercube sampling and partial rank correlation coefficients, identifies key parameters, particularly transmission among cattle, environmental contamination, and recovery rate as critical drivers of epidemic outcomes. Our results show that disease elimination is achievable when $\mathcal{R}_{0} < 1$, while persistence is inevitable for $\mathcal{R}_{0} > 1$. These findings provide a comprehensive mathematical framework for assessing HPAI risks and offer guidance for biosecurity strategies aimed at mitigating spillover and controlling outbreaks in livestock populations.",0,arxiv,Evrim,CC-BY/arXiv,A mathematical model of HPAI transmission between dairy cattle and wild birds with environmental effects
"Accurate phylogenetic inference from biological sequences depends critically on the quality of multiple sequence alignments, yet optimal alignment for many sequences is computationally intractable and sensitive to scoring choices. In this work we introduce MOEA/D-ADF, a novel variant of MOEA/D that adaptively adjusts subproblem weight vectors based on fitness variance to improve the exploration-exploitation trade-off. We combine MOEA/D-ADF with PMAO (PASTA with many application-aware optimization criteria) to form PMAO++, where PMAO-generated solutions are used to seed MOEA/D-ADF, which then evolves a population using 30 weight vectors to produce a diverse ensemble of alignment-tree pairs. PMAO++ outperforms the original PMAO on a majority of benchmark cases, achieving better false-negative (FN) rates on 12 of 17 BAliBASE-derived datasets and producing superior best-case trees, including several instances with zero FN rate. Beyond improving single best alignments, the rich set of alignment-tree pairs produced by PMAO++ is especially valuable for downstream summary methods (for example, consensus and summary-tree approaches), allowing more robust phylogenetic inference by integrating signal across multiple plausible alignments and trees. Certain dataset features, such as large terminal N/C extensions found in the RV40 group, remain challenging, but overall PMAO++ demonstrates clear advantages for sequence-based phylogenetic analysis. Future work will explore parameter tuning, larger benchmark suites, and tighter integration with summary-tree pipelines to further enhance applicability for biological sequence studies.",0,arxiv,Evrim,CC-BY/arXiv,Improving MSA Estimation through Adaptive Weight Vectors in MOEA/D
"Many models of population dynamics are formulated as deterministic iterated maps although real populations are stochastic. This is justifiable in the limit of large population sizes, as the stochastic fluctuations are negligible then. However, this also makes it challenging to use the same models for small populations where finite size effects like demographic noise and extinction cannot be ignored. Moreover, adding noise to the equations does not solve this problem as it can only represent the environmental stochasticity. An approach, sometimes used in ecological literature, but surprisingly uncommon in dynamical systems community, is \emph{Binomial maps}, which allow stochastic evolution of deterministic iterated map models of population. Here we present their formulation in a way so as to make their connection to the agent-based models explicit, and demonstrate it for the Logistic and Ricker maps. We also show that the Binomial maps are not completely equivalent to their deterministic counterparts, and derive sufficient conditions under which the equivalence holds. This approach enables rigorous finite-population analysis within familiar map-based models, bridging the deterministic map models and stochastic agent-based models.",0,arxiv,Evrim,CC-BY/arXiv,Binomial maps: stochastically evolving iterated integer maps for finite populations
"Fisher's fundamental theorem of natural selection states that the rate of change in a population's mean fitness equals its additive genetic variance in fitness. This implies that mean fitness should not decline in a constant environment, thereby positioning it as an indicator of evolutionary progression. However, this theorem has been shown to lack universality. Here, we derive the Fokker-Planck equation that describes the stochastic frequency dynamics of two phenotypes in a large population under weak selection and genetic drift, and develop a path integral formulation that characterizes the probability density of phenotypic frequency. Our formulation identifies that, under both selection and genetic drift, the ratio of the probability density of adaptive traits (e.g., phenotypic frequency) to that under neutrality represents a time-invariant evolutionary path characteristic. This ratio quantifies the cumulative effect of directional selection on the evolutionary process compared to genetic drift. Importantly, the expected value of this ratio does not decline over time. In the presence of fitness variance, the effect of directional selection on expected phenotypic changes accumulates over time, diverging progressively from paths shaped solely by genetic drift. The expectation of this time-invariant ratio thus offers a robust and informative alternative to mean fitness as a measure of progression in stochastic evolutionary dynamics.",0,arxiv,Evrim,CC-BY/arXiv,The invariance and non-decreasing expectation of an evolutionary path characteristic under weak selection
"B cells and the antibodies they produce are vital to health and survival, motivating research on the details of the mutational and evolutionary processes in the germinal centers (GC) from which mature B cells arise. It is known that B cells with higher affinity for their cognate antigen (Ag) will, on average, tend to have more offspring. However the exact form of this relationship between affinity and fecundity, which we call the ``affinity-fitness response function'', is not known. Here we use deep learning and simulation-based inference to learn this function from a unique experiment that replays a particular combination of GC conditions many times. All code is freely available at https://github.com/matsengrp/gcdyn, while datasets and inference results can be found at https://doi.org/10.5281/zenodo.15022130.",0,arxiv,Evrim,CC-BY/arXiv,Inference of germinal center evolutionary dynamics via simulation-based deep learning
"Models for epidemic spread typically account for variable risk factors but do not account for the correlation between behavior and risk. Here we extend these models to account for such correlations. We find that a positive correlation between behavior and risk, i.e., voluntary risk aversion by individuals at high risk and risk-taking behavior by individuals at low risk, leads to a linear reduction in morbidity and mortality and load on healthcare services compared to the uncorrelated case. We show that increasing caution in response to news from countries with a preceding outbreak leads to a more graded response to a lock-down. We also show that if vaccinated individuals are less cautious an increase in herd immunity threshold ensues.",0,arxiv,Evrim,CC-BY/arXiv,"Coupling between risk and cautious behavior affect epidemic morbidity, mortality and dynamics"
"Population cycles are important components of many natural systems. Most studied in short-lived and small-bodied species, cycles frequently appear to be driven by density-dependent feedbacks. However, compelling evidence of cycles -- often more qualitative than quantitative -- also exists in large mammals. Among ungulates, both density-dependent vital rates and 'cohort effects' (lasting impacts of birth conditions on fecundity and survival) exist, but the implications of such feedbacks for oscillatory population dynamics have not been explored. Here, we present a synthetic model of ungulate population dynamics, parameterized for barren-ground caribou (Rangifer tarandus groenlandicus) and motivated by extensive Indigenous knowledge suggesting decades-long fluctuations in abundance. Caribou herds are theorized to be subject to both cohort effects and density dependence, and we linked these endogenous factors with environmental stochasticity to understand cycling. Using wavelet analysis, we characterized periodic phenomena and performed sensitivity analyses to clarify the drivers and characteristics of population cycles. We found that cohort effects, predominantly those impacting survival, can produce long-period oscillatory behavior across a wide range of environments and demographic structures. Our modeling framework is generalizable to other long-lived, large-bodied species with complex demography, and collectively, these efforts broaden the scope of inquiry into proximal drivers of population cycling.",0,arxiv,Evrim,CC-BY/arXiv,"Drivers of periodicity in population dynamic models of long-lived, large mammals"
"Vaccines not only directly protect vaccinated individuals but also contribute to protect the entire population via indirect herd-immunity benefits. However, researchers have long struggled to quantify these indirect effects at the population level, hindering assessment of vaccination program effectiveness. We developed a new method to estimate these effects, thereby markedly improving measures of the number of infections, hospitalizations, and deaths averted by vaccination. Our population-based analysis of 6,440,000 residents of Victoria, Australia reveal strong indirect effects during the Delta outbreak (September-November 2021). By modelling a non-vaccination counterfactual, we conservatively estimate 316,000 infections were averted (95\% BCI: 232k-406k), as well as 33,500 hospitalizations (95\% BCI: 22.2k-46.2k), and 4,900 deaths (95\% BCI: 2.9k-7.3k). These are 4.0, 7.5, and 8.0 times higher, respectively, than observed. Half of the averted infections and around one-quarter of hospitalizations and deaths were attributable to indirect protection. Homogeneous vaccination across LGAs could have reduced outcomes by approximately 25\%.",0,arxiv,Evrim,CC-BY/arXiv,"Quantifying the direct and indirect impact of COVID-19 vaccination: evidence from Victoria, Australia"
"State-space models are dynamical systems defined by a latent and an observed process. In ecology, stochastic state-space models in discrete time are most often used to describe the imperfectly observed dynamics of population sizes or animal movement. However, several studies have observed identifiability issues when state-space models are fitted to simulated or real data, and it is not currently clear whether those are due to data limitations or more fundamental model non-identifiability. To investigate such theoretical identifiability, a suitable exhaustive summary is required, defined as a vector of parameter combinations which fully determines the model. Previous work on exhaustive summaries has used expectations of the stochastic process, so that noise parameters are unaccounted for. In this paper, we build an exhaustive summary using the spectral density of the observed process, which fully accounts for all mean and variance parameters. This diagnostic is applied to contrasted ecological models and we show that they are generally theoretically identifiable, unless some model compartements are unobserved. This suggest that issues encountered while fitting models are mostly due to practical identifiability.",0,arxiv,Evrim,CC-BY/arXiv,Identifiability of linear stochastic state-space models with application to ecology
"Calhoun's Rat Utopia experiments demonstrated a puzzling population trajectory: initial growth, plateau, and eventually a total collapse of the rat population despite abundant resources. This paper proposes a hypothesis that the enclosure's design enabled full visibility of the social hierarchy (pecking order), leading to entropy degeneration: progressive loss of uncertainty in rats' perceived ranks over generations. High initial uncertainty drives engagement in dominance, reproduction, and care; as visibility solidifies the hierarchy over the generations, uncertainty vanishes, nullifying perceived gains from social activities. Simulations reproduce the experimental arc which rely on a game theoretic matrix that is parameterized by the uncertainty (entropy) in the hierarchy which changes over rat generations.",0,arxiv,Evrim,CC-BY/arXiv,Hierarchy Entropy Degeneration Explains the Rat Utopia Population Collapse: The Role of Full Visibility and Isolation
"A consensus tree is a phylogenetic tree that synthesizes a given collection of phylogenetic trees, all of which share the same leaf labels but may have different topologies, typically obtained through bootstrapping. Our research focuses on creating a consensus tree from a collection of phylogenetic trees, each detailed with branch-length data. We integrate branch lengths into the consensus to encapsulate the progression rate of genetic mutations. However, traditional consensus trees, such as the strict consensus tree, primarily focus on the topological structure of these trees, often neglecting the informative value of branch lengths. This oversight disregards a crucial aspect of evolutionary study and highlights a notable gap in traditional phylogenetic approaches. In this paper, we extend \textit{PrimConsTree}\footnote{A preliminary version of this article was presented at \emph{the Fifteenth International Conference on Bioscience, Biochemistry, and Bioinformatics (ICBBB~2025)}~(reference~\cite{torquet2005icbbb}).}, a graph-based method for constructing consensus trees. This algorithm incorporates topological information, edge frequency, clade frequency, and branch length to construct a more robust and comprehensive consensus tree. Our adaptation of the well-known Prim algorithm efficiently identifies the maximum frequency branch and maximum frequency nodes to build the optimal consensus tree. This strategy was pre-processed with clustering steps to calibrate the robustness and accuracy of the consensus tree.\\ \textbf{Availability and implementation:} The source code of PrimConsTree is freely available on GitHub at https://github.com/tahiri-lab/PrimConsTree.",0,arxiv,Evrim,CC-BY/arXiv,Graph-based method for constructing consensus trees
"There is a growing need to integrate sustainability into tertiary mathematics education given the urgency of addressing global environmental challenges. This paper presents four case studies from Australian university courses that incorporate ecological and environmentally-conscious concepts into the mathematics curriculum. These case studies cover topics such as population dynamics, sustainable fisheries management, statistical inference for endangered species assessment, and mathematical modelling of climate effects on marine ecosystems. Each case demonstrates how fundamental mathematical methods, including calculus, statistics and operations research, can be applied to real-world ecological issues. These examples are ready-to-implement problems for integrating ecological thinking into mathematics classes, providing educators with practical tools to help students develop interdisciplinary problem-solving skills and prepare for the challenges of sustainability in their future careers.",0,arxiv,Evrim,CC-BY/arXiv,Embedding Sustainability in Undergraduate Mathematics with Actionable Case Studies
"Studies that predict species extinction have focused on a range of flora and fauna but in regard to Homo sapiens there are, with one notable exception, no predictive studies, only considerations of possible ways this may occur. The exception believes extinction of Homo sapiens will happen in 10,000 years. We agree that extinction will happen, but we disagree on the timing: The work we present here suggests that if the current decline in birth rates continues, humans could be extinct by 2394. If we consider the absence of working-age people and the accompanying collapse of services, the survivorship rates would most likely be lower. Given this, it is plausible that extinction could occur around 2359. We also examined a scenario in which births ended in 2024, which revealed that Homo sapiens would become extinct in 2134. Given societal collapse, extinction under the zero births scenario could occur around 2089.",0,arxiv,Evrim,CC-BY/arXiv,Human Extinction A Demographic Perspective
"Modelling disease outbreak models remains challenging due to incomplete surveillance data, noise, and limited access to standardized datasets. We have created BIGBOY1.2, an open synthetic dataset generator that creates configurable epidemic time series and population-level trajectories suitable for benchmarking modelling, forecasting, and visualisation. The framework supports SEIR and SIR-like compartmental logic, custom seasonality, and noise injection to mimic real reporting artifacts. BIGBOY1.2 can produce datasets with diverse characteristics, making it suitable for comparing traditional epidemiological models (e.g., SIR, SEIR) with modern machine learning approaches (e.g., SVM, neural networks).",0,arxiv,Evrim,CC-BY/arXiv,BIGBOY1.2: Generating Realistic Synthetic Data for Disease Outbreak Modelling and Analytics
"Phylogenetic trees are ubiquitous and central to biology, but most published trees are available only as visual diagrams and not in the machine-readable newick format. There are thus thousands of published trees in the scientific literature that are unavailable for follow-up analyses, comparisons, supertree construction, etc. Experts can easily read such diagrams, but the manual construction of a newick string is prohibitively laborious. Previous attempts to semi-automate the reading of tree images relied on image processing techniques. These quickly encounter difficulties with typical published tree diagrams that contain various graphical elements that overlap the branches, such as error bars on internal nodes. Here we introduce Treemble, a user-friendly desktop application for generating newick strings from tree images. The user simply clicks to mark node locations, and Treemble algorithmically assembles the tree from the node coordinates alone. Tip nodes can be automatically detected and marked. Treemble also facilitates the automatic reading of tip name labels and can handle both rectangular and circular trees. Treemble is a native desktop application for both MacOS and Windows, and is freely available and fully documented at treemble.org.",0,arxiv,Evrim,CC-BY/arXiv,Treemble: A Graphical Tool to Generate Newick Strings from Phylogenetic Tree Images
"Several international agreements have called for the rapid expansion of protected areas to halt biodiversity declines. However, recent research has shown that expanding protected areas may be less cost-effective than redirecting resources towards threat management in existing reserves. These findings often assume that threats are homogeneously distributed in the landscape. In some cases, threats are more concentrated near the edge of protected areas. As protected areas expand, core habitat in the centre expands more rapidly than its edge, potentially creating a refuge from threats. In this paper, we present a framework linking protected area expansion and threat management to extinction risk, via their impact on population carrying capacity and growth rate within core and edge habitats. We demonstrate the framework using a simple population model where individuals are uniformly distributed in a circular protected area threatened by poachers who penetrate the protected area to a fixed distance. We parameterise the model for Peter's Duiker (Cephalophus callipygus) harvested for food in the dense undergrowth of African forests using snares. Expanding protected areas can reduce extinction risk more effectively compared to an equivalent investment in snare removal for larger protected areas that already sustain core unhunted habitat. Our results demonstrate the importance of protected area expansion in buffering susceptible populations from fixed hunting pressure restricted to protected area edges. However, for cases where threats, wildlife, and managers respond to each other strategically in space, the relative importance of expansion versus increased management remains a significant open problem.",0,arxiv,Evrim,CC-BY/arXiv,Expand or better manage protected areas: a framework for minimizing extinction risk when threats are concentrated near edges
"Phylogenetic trees summarize evolutionary relationships. The Billera-Holmes-Vogtmann (BHV) space for comparing phylogenetic trees has many elegant mathematical properties, but it does not encompass trees with differing leaf sets. To overcome this, we introduce Towering space: a complete metric space that extends BHV space to trees with non-identical leaf sets. Towering space is a structured collection of BHV spaces connected via pruning and regrafting operations. We study the geometry of paths in Towering space and present an algorithm for computing metric distances. By addressing a major limitation of BHV space, Towering space facilitates the analysis of modern phylogenetic datasets such as multi-domain gene trees.",0,arxiv,Evrim,CC-BY/arXiv,Geometry of the space of phylogenetic trees with non-identical leaves
"Speciation is often associated with geographical barriers that limit gene flow. However, species can also emerge in continuous homogeneous environments through isolation by distance. When the environment is not homogeneous, natural selection contributes to differentiation by local adaptation and tends to facilitate speciation. To explore how isolation by distance and adaptation combine to determine species diversity, we implemented a model regulated by these two components. The first is implemented via mating restrictions on spatial proximity and genetic similarity. The second is realized by an ecological phenotype subjected to adaption and natural selection. We consider scenarios where the environment is either homogeneous, with a single ecological optimum, or heterogeneous with two distinct optima. In homogeneous environments no speciation is observed under permissive mating restrictions, independent on the strength of natural selection. On the other hand, if reproduction requires close genetic similarity (strict mating restrictions), speciation occurs only under strong selection. In contrast, in environments with two local optima and strong selection, species well adapted to each of the niches emerge along the spatial structure, leading to the formation of groups with distinct phenotypes. Permissive mating restrictions leads to the formation of only two species, each occupying one of the niches; strong mating restriction leads to several species per niche, in a much faster speciation process. Interestingly, when selection is weak and reproduction requires strong genetic similarity, several species form, but the process is slow. Moreover, species average phenotypes do not remain constant over generations, causing the phenotypic distribution to oscillate, never reaching a stationary pattern.",0,arxiv,Evrim,CC-BY/arXiv,Speciation by local adaptation and isolation by distance in extended environments
"Trap cropping is a pest management strategy where a grower plants an attractive ""trap crop"" alongside the primary crop to divert pests away from it. We propose a simple framework for optimizing the proportion of a grower's field or greenhouse allocated to a main crop and a trap crop to maximize agricultural yield. We implement this framework using a model of pest movement governed by trap crop attractiveness, the potential yield threatened by pests, and functional relationships between yield loss and pest density drawn from the literature. Focusing on a simple case in which pests move freely across the field and are attracted to traps solely by their relative attractiveness, we find that allocating 5-20 percent of the landscape to trap plants is typically required to maximize yield and achieve effective pest control in the absence of pesticides. For highly attractive trap plants, growers can devote less space because they are more effective; less attractive plants are ineffective even in large numbers. Intermediate attractiveness warrants the greatest investment in trap cropping. Our framework offers a transparent and tractable approach for exploring trade-offs in pest management and can be extended to incorporate more complex pest behaviors, crop spatial configurations, and economic considerations.",0,arxiv,Evrim,CC-BY/arXiv,Optimal trap cropping investments to maximize agricultural yield
"Identifiability of phylogenetic models is a necessary condition to ensure that the model parameters can be uniquely determined from data. Mixture models are phylogenetic models where the probability distributions in the model are convex combinations of distributions in simpler phylogenetic models. Mixture models are used to model heterogeneity in the substitution process in DNA sequences. While many basic phylogenetic models are known to be identifiable, mixture models in generality have only been shown to be identifiable in certain cases. We expand the main theorem of [Rhodes, Sullivant 2012] to prove identifiability of mixture models in equivariant phylogenetic models, specifically the Jukes-Cantor, Kimura 2-parameter model, Kimura 3-parameter model and the Strand Symmetric model.",0,arxiv,Evrim,CC-BY/arXiv,Identifiability of Large Phylogenetic Mixtures for Many Phylogenetic Model Structures
"The growing number of infectious disease outbreaks, like the one caused by the SARS-CoV-2 virus, underscores the necessity of actuarial models that can adapt to epidemic-driven risks. Traditional life insurance frameworks often rely on static mortality assumptions that fail to capture the temporal and behavioral complexity of disease transmission. In this paper, we propose an integrated actuarial framework based on the SEIARD epidemiological model. This framework enables the explicit modeling of incubation periods and disease-induced mortality. We derive key actuarial quantities, including the present value of annuity benefits, payment streams, and net premiums, based on SEIARD dynamics. We formulate a prospective reserve function and analyze its evolution throughout the course of an epidemic. Additionally, we examine the forces of infection, mortality, and removal to assess their impact on epidemic-adjusted survival probabilities. Numerical simulations implemented via a nonstandard finite difference (NSFD) scheme illustrate the model's applicability under various parameter settings and insurance policy assumptions.",0,arxiv,Evrim,CC-BY/arXiv,Actuarial Analysis of an Infectious Disease Insurance based on an SEIARD Epidemiological Model
"While voluntary participation is a key mechanism that enables altruistic punishment to emerge, its explanatory power typically rests on the common assumption that non-participants have no impact on the public good. Yet, given the decentralized nature of voluntary participation, opting out does not necessarily preclude individuals from influencing the public good. Here, we revisit the role of voluntary participation by allowing non-participants to exert either positive or negative impacts on the public good. Using evolutionary analysis in a well-mixed finite population, we find that positive externalities from non-participants lower the synergy threshold required for altruistic punishment to dominate. In contrast, negative externalities raise this threshold, making altruistic punishment harder to sustain. Notably, when non-participants have positive impacts, altruistic punishment thrives only if non-participation is incentivized, whereas under negative impacts, it can persist even when non-participation is discouraged. Our findings reveal that efforts to promote altruistic punishment must account for the active role of non-participants, whose influence can make or break collective outcomes.",0,arxiv,Evrim,CC-BY/arXiv,Non-participant externalities reshape the evolution of altruistic punishment
"Humans are bounded rational at best and this, we argue, has worked in their favour in the hunter-gatherer society where emergence of a coordinated action, leading to cooperation, is otherwise the standard stag-hunt dilemma (when individuals are rational). In line with the fact the humans strive for developing self-reputation by having less propensity to cheat than to be cheated, we observe that the payoff structure of the stag-hunt game appropriately modifies to that of coordination-II game. Subsequently, within the paradigm of evolutionary game theory, we establish that a population -- consisting of procedural rational players (a type of bounded rationality) -- is unequivocally evolutionarily stable against emergence of more rational strategies in coordination-II game. The cooperation is, thus, shown to have been established by evolutionary forces picking less rational individuals.",0,arxiv,Evrim,CC-BY/arXiv,Coordinating cooperation in stag-hunt game: Emergence of evolutionarily stable procedural rationality
"Humans stand alone in terms of their potential to collectively and cumulatively improve technologies in an open-ended manner. This open-endedness provides societies with the ability to continually expand their resources and to increase their capacity to store, transmit and process information at a collective-level. Here, we propose that the production of resources arises from the interaction between technological systems (a society's repertoire of interdependent skills, techniques and artifacts) and search spaces (the aggregate collection of needs, problems and goals within a society). Starting from this premise we develop a macro-level model wherein both technological systems and search spaces are subject to cultural evolutionary dynamics. By manipulating the extent to which these dynamics are characterised by stochastic or selection-like processes, we demonstrate that open-ended growth is extremely rare, historically contingent and only possible when technological systems and search spaces co-evolve. Here, stochastic factors must be strong enough to continually perturb the dynamics into a far-from-equilibrium state, whereas selection-like factors help maintain effectiveness and ensure the sustained production of resources. Only when this co-evolutionary dynamic maintains effective technological systems, supports the ongoing expansion of the search space and leads to an increased provision of resources do we observe open-ended technological evolution.",0,arxiv,Evrim,CC-BY/arXiv,Modelling the emergence of open-ended technological evolution
"Although the descriptive epidemiology of primary breast cancer is well characterized in the US, breast cancer recurrence rates have not been measured in an unselected population. The number of breast cancer survivors at risk for recurrence is growing each year, so recurrence surveillance is a pressing need. We used missing data methods to impute breast cancer recurrence and estimate the risk of recurrence in the Cancer Recurrence Information and Surveillance Program (CRISP) cohort in the Georgia Cancer Registry. The imputation model was based on an internal validation substudy and indicators recorded in the registry (e.g., pathology reports, imaging claims), prognostic variables (e.g., stage at diagnosis), and characteristics associated with missing data (e.g., insurance coverage). We pooled hazard ratios (HR) and 95% Confidence Intervals (CI) across 1000 imputed datasets, adjusted for age, stage, grade, subtype, race and ethnicity, marital status, and urban/rural county at diagnosis. There were 1,606 patients with a validated outcome (75% with breast cancer recurrence) and we imputed the outcome for the remaining 23,439 patients. We estimated an overall 7.2% incidence of recurrence between at least 1 year after diagnosis and up to 5 years of follow up. When comparing the hazards pooled across imputations, we found that some patterns differed from established patterns in mortality or survival, notably by race and ethnicity, underscoring the need for continued research on the descriptive epidemiology of breast cancer recurrence. These results provide new insights into surveillance for breast cancer survivors in Georgia, especially those with higher stage and grade tumors, of Hispanic ethnicity, and who may be lacking social support.",0,arxiv,Evrim,CC-BY/arXiv,"Estimating breast cancer recurrence in a population-based registry in Georgia, US"
"The concept of the mutual influence that awareness and disease may exert on each other has recently presented significant challenges. The actions individuals take to prevent contracting a disease and their level of awareness can profoundly affect the dynamics of its spread. Simultaneously, disease outbreaks impact how people become aware. In response, we initially propose a null model that couples two Susceptible-Infectious-Recovered (SIR) dynamics and analyze it using a mean-field approach. Subsequently, we explore the parameter space to quantify the effects of this mutual influence on various observables. Finally, based on this null model, we conduct an empirical analysis of Twitter data related to COVID-19 and confirmed cases within American states. Our findings indicate that in specific regions of the parameter space, it is possible to suppress the epidemic by increasing awareness, and we investigate phase transitions. Furthermore, our model demonstrates the ability to alter the dominant population group by adjusting parameters throughout the course of the outbreak. Additionally, using the model, we assign a set of parameters to each state, revealing that these parameters change at different pandemic peaks. Notably, a robust correlation emerges between the ranking of states' Twitter activity, as gathered from empirical data, and the immunity parameters assigned to each state using our model. This observation underscores the pivotal role of sustained awareness transitioning from the initial to the subsequent peaks in the disease progression.",0,arxiv,Evrim,CC-BY/arXiv,Tweets vs Pathogen Spread: A Case Study of COVID-19 in American States
"The Central Dogma of molecular biology, as originally proposed by Crick, asserts that information passed into protein cannot flow back out. This principle has been interpreted as underpinning modern understandings of heredity and evolution, implying the unidirectionality of information flow from nucleic acids to proteins. Here, we propose a generalisation of the Central Dogma as a division of labour between the transmission and expression of information: the transmitter (nucleic acids) perpetuates information across generations, whereas the expressor (protein) enacts this information to facilitate the transmitter's function without itself perpetuating information. We argue that this generalisation offers two benefits. First, it provides a unifying perspective for comparing the Central Dogma to analogous divisions of labour observed at vastly different biological scales, including multicellular organisms, eukaryotic cells, organelles, and bacteria. Second, it offers a theoretical framework to explain the Central Dogma as an outcome of evolution. Specifically, we review a mathematical model suggesting that the Central Dogma originates through spontaneous symmetry breaking driven by evolutionary conflicts between different levels of selection. By reframing the Central Dogma as an informational relationship between components of a system, this generalisation underscores its broader relevance across the biological hierarchy and sheds light on its evolutionary origin.",0,arxiv,Evrim,CC-BY/arXiv,Generalising the Central Dogma as a cross-hierarchical principle of biology
"The effective reproduction number is an important descriptor of an infectious disease epidemic. In small populations, ideally we would estimate the effective reproduction number using a Markov Jump Process (MJP) model of the spread of infectious disease, but in practice this is computationally challenging. We propose a computationally tractable approximation to an MJP which tracks only latent and infectious individuals, the EI model, an MJP where the time-varying immigration rate into the E compartment is equal to the product of the proportion of susceptibles in the population and the transmission rate. We use an analogue of the central limit theorem for MJPs to approximate transition densities as normal, which makes Bayesian computation tractable. Using simulated pathogen RNA concentrations collected from wastewater data, we demonstrate the advantages of our stochastic model over its deterministic counterpart for the purpose of estimating effective reproduction number dynamics, and compare against a state of the art method. We apply our new model to inference of changes in the effective reproduction number of SARS-CoV-2 in several college campus communities that were put under wastewater pathogen surveillance in 2022.",0,arxiv,Evrim,CC-BY/arXiv,The signal is not flushed away: Inferring the effective reproduction number from wastewater data in small populations
"Summary: Ancestral recombination graphs (ARGs) are a complete representation of the genetic relationships between recombining lineages and are of central importance in population genetics. Recent breakthroughs in simulation and inference methods have led to a surge of interest in ARGs. However, understanding how best to take advantage of the graphical structure of ARGs remains an open question for researchers. Here, we introduce tskit_arg_visualizer, a Python package for programmatically drawing ARGs using the interactive D3.js visualization library. We highlight the usefulness of this visualization tool for both teaching ARG concepts and exploring ARGs inferred from empirical datasets. Availability and implementation: The latest stable version of tskit_arg_visualizer is available through the Python Package Index (https://pypi.org/project/tskit-arg-visualizer, currently v0.1.0). Documentation and the development version of the package are found on GitHub (https://github.com/kitchensjn/tskit_arg_visualizer). Supplementary materials: Methods for creating the example ARGs seen in Figure 1 can be found in the supplementary materials.",0,arxiv,Evrim,CC-BY/arXiv,tskit_arg_visualizer: interactive plotting of ancestral recombination graphs
"The depiction of populations - of humans or animals - as ""population pyramids"" is a useful tool for the assessment of various characteristics of populations at a glance. Although these visualisations are well-known objects in various communities, formalised and algorithmic approaches to gain information from these data are less present. Here, we present an algorithm-based classification of population data into ""pyramids"" of different shapes ([normal and inverted] pyramid / plunger / bell, [lower / middle / upper] diamond, column, hourglass) that are linked to specific characteristics of the population. To develop the algorithmic approach, we used data describing global zoo populations of mammals from 1970-2024. This algorithm-based approach delivers plausible classifications, in particular with respect to changes in population size linked to specific series of, and transitions between, different ""pyramid"" shapes. We believe this approach might become a useful tool for analysing and communicating historical population developments in multiple contexts and is of broad interest. Moreover, it might be useful for animal population management strategies.",0,arxiv,Evrim,CC-BY/arXiv,A semi-automatic approach to study population dynamics based on population pyramids
"We present a mathematical model to study the transmission dynamics of soil-transmitted helminth (STH) infections and to assess the impact of community-based water, sanitation, and hygiene (WASH) program interventions. STH infections are a pressing public health issue in vulnerable populations, impairing children's growth and development. Our model explicitly incorporates WASH coverage and effectiveness as dynamic parameters, enabling analysis of their effects on the basic and effective reproduction numbers and the stability of disease-free and endemic equilibria. Through saddle-node bifurcation analysis, we identify the critical thresholds in the intervention parameters necessary for infection elimination. Numerical simulations show these thresholds and delineate the conditions under which WASH interventions alone may or may not suffice to eliminate transmission, even under widespread coverage. Our findings provide a mathematical framework to optimize helminth control strategies and provide evidence-based insights for public health policies aligned with the World Health Organization's Global Strategy for the elimination of STH infections by 2030.",0,arxiv,Evrim,CC-BY/arXiv,A Mathematical Model of Helminth Transmission Dynamics under WASH Program Interventions
"A common assumption in evolutionary thought is that adaptation drives an increase in biological complexity. However, the rules governing evolution of complexity appear more nuanced. Evolution is deeply connected to learning, where complexity is much better understood, with established results on optimal complexity appropriate for a given learning task. In this work, we suggest a mathematical framework for studying the relationship between evolved organismal complexity and enviroenmntal complexity by leveraging a mathematical isomorphism between evolutionary dynamics and learning theory. Namely, between the replicator equation and sequential Bayesian learning, with evolving types corresponding to competing hypotheses and fitness in a given environment to likelihood of observed evidence. In Bayesian learning, implicit regularization prevents overfitting and drives the inference of hypotheses whose complexity matches the learning challenge. We show how these results naturally carry over to the evolutionary setting, where they are interpreted as organism complexity evolving to match the complexity of the environment, with too complex or too simple organisms suffering from \textit{overfitness} and \textit{underfitness}, respectively. Other aspects, peculiar to evolution and not to learning, reveal additional trends. One such trend is that frequently changing environments decrease selected complexity, a result with potential implications to both evolution and learning. Together, our results suggest that the balance between over-adaptation to transient environmental features, and insufficient flexiblity in responding to environmental challenges, drives the emergence of optimal complexity, reflecting environmental structure. This framework offers new ways of thinking about biological complexity, suggesting new potential causes for it to increase or decrease in different environments.",0,arxiv,Evrim,CC-BY/arXiv,Fitness and Overfitness: Implicit Regularization in Evolutionary Dynamics
"This paper develops and analyzes a diffusion-advection model coupling population dynamics with toxicant transport, incorporating a boundary protection zone. For both upstream and downstream protection zone configurations, we investigate the combined influence of protected zones and key ecological factors on population persistence or extinction. Employing monotone dynamical system theory and eigenvalue analysis, we establish the global dynamics of the population-toxicant coexistence equilibrium. Furthermore, we characterize the parameter dependence governing the stability of the toxicant-only steady state, specifically examining the protected zone length, toxicant effect coefficient on population growth, per-unit contaminant discharge rate, toxicant input rate, diffusion/advection rates, and population natural growth rate. Finally, numerical simulations reveal the complex interplay between the protection zone and toxicant advection rate in significantly shaping population persistence domains.",0,arxiv,Evrim,CC-BY/arXiv,Effect of protection zone on the dynamics of a diffusion-advection population-toxicant model
"Populations evolving in fluctuating environments face the fundamental challenge of balancing adaptation to current conditions against preparation for uncertain futures. Here, we study microbial evolution in partially predictable environments using proteome allocation models that capture the trade-off between growth rate and lag time during environmental transitions. We demonstrate that evolution drives populations toward an evolutionary stable allocation strategy that minimizes resource depletion time, thereby balancing faster growth with shorter adaptation delays. In environments with temporal structure, populations evolve to learn the statistical patterns of environmental transitions through proteome pre-allocation, with the evolved allocations reflecting the transition probabilities between conditions. Our framework reveals how microbial populations can extract and exploit environmental predictability without explicit neural computation, using the proteome as a distributed memory system that encodes environmental patterns. This work demonstrates how information-theoretic principles govern cellular resource allocation and provides a mechanistic foundation for understanding learning-like behavior in evolving biological systems.",0,arxiv,Evrim,CC-BY/arXiv,Evolutionary learning of microbial populations in partially predictable environments
"Learning from experience is a key feature of decision-making in cognitively complex organisms. Strategic interactions involving Bayesian inferential strategies can enable us to better understand how evolving individual choices to be altruistic or selfish can affect collective outcomes in social dilemmas. Bayesian strategies are distinguished, from their reactive opponents, in their ability to modulate their actions in the light of new evidence. We investigate whether such strategies can be resilient against reactive strategies when actions not only determine the immediate payoff but can affect future payoffs by changing the state of the environment. We use stochastic games to mimic the change in environment in a manner that is conditioned on the players' actions. By considering three distinct rules governing transitions between a resource-rich and a resource-poor states, we ascertain the conditions under which Bayesian tit-for-tat strategy can resist being invaded by reactive strategies. We find that the Bayesian strategy is resilient against a large class of reactive strategies and is more effective in fostering cooperation leading to sustenance of the resource-rich state. However, the extent of success of the Bayesian strategies depends on the other strategies in the pool and the rule governing transition between the two different resource states.",0,arxiv,Evrim,CC-BY/arXiv,Bayesian tit-for-tat fosters cooperation in evolutionary stochastic games
"This study builds upon our previously proposed stochastic differential equation (SDE)-based model to further investigate fish school fragmentation under predation. Specifically, we explore structural dynamics by incorporating graph-theoretic metrics--namely, the number of connected components--to quantify changes in prey school organization. Two quantitative indicators, first split time and final component count, are introduced to assess the timing and extent of group disintegration. Sensitivity analyses are performed on key parameters to evaluate their influence on group stability under nearest attack and center attack strategies. We independently examine the effect of environmental noise on fish school cohesion. Simulation results show that parameter changes impact fish school fragmentation differently under the two predation strategies. High environmental noise also makes it difficult for the school to stay cohesive. This framework provides a structured and quantitative basis for assessing how fish schools respond to different predation strategies and environmental noise levels.",0,arxiv,Evrim,CC-BY/arXiv,Quantifying Fish School Fragmentation under Predation Using Stochastic Differential Equations
"A population of individuals with the same genes can present heterogeneous traits (phenotypes). The prevalence of this heterogeneity can be explained as a bet-hedging strategy that improves the population proliferation rate (fitness) in fluctuating environments. The phenotype distribution is influenced by factors such as competition between phenotypes, the duration of environmental states, and the rate of phenotype-switching. We illustrate these effects in a system where both the environment and the phenotype can adopt two states. This system includes scenarios such as symmetric bet-hedging and dormant-proliferating phenotypes. We examine how environmental and phenotypic states share mutual information, measured in bits, and explore the relationship between this information and population fitness. We propose that when fitness is measured relative to the case where phenotype and environment are independent, information and fitness can be treated as equivalent measures. We investigate strategies that individuals can use to improve this information, such as adjusting the rates of proliferation and phenotype-switching relative to the environmental fluctuation rate. Through these strategies, with fixed marginal distributions, an increase in information implies an increase in population fitness. We also identify limits to the maximum achievable fitness and information and discuss the value of the information in terms of this new normalized fitness. Our framework offers new insights into how organisms adapt to fluctuating environmental conditions.",0,arxiv,Evrim,CC-BY/arXiv,Information and fitness in two-state systems: self-replicating individuals in a fluctuating environment
"Complex spatial structure, with partially isolated subpopulations, and environment heterogeneity, such as gradients in nutrients, oxygen, and drugs, both shape the evolution of natural populations. We investigate the impact of environment heterogeneity on mutant fixation in spatially structured populations with demes on the nodes of a graph. When migrations between demes are frequent, we demonstrate that environment heterogeneity can amplify natural selection and simultaneously accelerate mutant fixation and extinction, thereby fostering the quick fixation of beneficial mutants. We evidence this effect in the star graph, more strongly in the line graph, and also in a more general class of graphs. We show that for amplification to occur, mutants must have a stronger fitness advantage in demes with stronger migration outflow. In circulation graphs, where migration inflow and outflow are equal in each deme, we find that environment heterogeneity has no impact in a first approximation, but increases the fixation probability of beneficial mutants to second order. When migrations between demes are rare, we show that environment heterogeneity can also foster amplification of selection, by allowing demes with sufficient mutant advantage to become refugia for mutants.",0,arxiv,Evrim,CC-BY/arXiv,Environment heterogeneity creates fast amplifiers of natural selection in graph-structured populations
"The release of Wolbachia-infected mosquitoes into Aedes aegypti infested areas is a promising strategy for localised eradication of dengue infection. Ae aegypti mosquitoes favour urban environments as breeding habitats, so are often found in and around houses. Therefore, it is likely that they will infect members of the households that they reside around. Since population groupings within households are small, stochastic effects become important. Despite this, little work has been carried out to investigate the outcome of releasing Wolbachia-infected mosquitoes at a household scale, either from an empirical and theoretical stand point. In previous work, we developed and analysed a stochastic (continuous time Markov chain) model for the invasion of Wolbachia-infected mosquitoes into a single household containing a population of wildtype mosquitoes. In the present study, we extend our framework to a connected community of households coupled by the movement of mosquitoes. We use numerical results obtained via Gillespie's stochastic simulation algorithm to investigate optimal strategies for the release of Wolbachia-infected mosquitoes carried out at either the community or the household scale. We find that household scale releases can facilitate rapid and successful invasion of the Wolbachia-infected mosquitoes into the household population and then into the wider community. We further explore the impact of regular household scale releases of Wolbachia-infected mosquitoes for a range of compositions for the release population, time intervals between releases and proportion of households participating in the releases. We find that a single release household can provide sufficient protection to the entire community of households if releases are carried out frequently for a number of years and a sufficient number of females are released on each occasion.",0,arxiv,Evrim,CC-BY/arXiv,Household scale Wolbachia release strategies for effective dengue control
"Background:Chronic kidney disease is one of the most prevalent non-communicable health issues globally, and high body mass index plays a significant role in the onset and progression of chronic kidney disease. Methods: Data on the disease burden attributable to high body mass index were retrieved from the 2021 Global Burden of Disease, Injuries, and Risk Factors Study . The global cases, age-standardized mortality rate , and age-standardized disability-adjusted life years attributable to high body mass index were estimated based on age, sex, geographic location, and the Social-demographic Index (SDI). The estimated annual percentage change was calculated to quantify trends in ASMR and ASDR from 1990 to 2019. Decomposition and frontier analyses were conducted to understand the drivers behind changes in burden and to identify top-performing countries. Inequality analysis was performed to assess disparities in burden across different SDI levels. The Bayesian age-period-cohort model was used to predict the disease burden up to 2035.Results: In 2021, there were 4,643.41 global deaths and 2,514,227.16 DALYs attributable to high body mass index-related CKD, more than triple the figures from 1990. Additionally, from 1990 to 2021, the ASMR and ASDR accelerated, with EAPCs of 2.25 (95% CI: 2.13 to 2.37) and 1.98 (95% CI: 1.89 to 2.08), respectively, particularly among males, in High-income North America, and in Low-middle SDI regions. In terms of SDI, the Low-middle SDI region had the highest ASMR and ASDR related to CKD in 2021. Conclusion: From 1990 to 2021, there was a significant increase in global deaths and DALYs attributable to high high body mass index related CKD. As a major public health issue for CKD patients, high BMI urgently requires targeted measures to address it.",0,arxiv,Evrim,CC-BY/arXiv,"Global, Regional, and National Burden of Chronic Kidney Disease Attributable to High Body Mass Index (BMI) among Individuals Aged 20-54 Years from 1990 to 2021: An Analysis of the Global Burden of Disease Study"
"This piece serves two purposes. Firstly, it aims at elucidating the role of epistasis in shaping, at a molecular level, the evolutionary paths of proteins, as well as the extent to which these epistatic effects are the outcome of an as-yet-unidentified epistatic force. Second, it seeks to ascertain the extent to which the principle of least action will enable us to identify which of all potential trajectories has the highest evolutionary efficiency, as well as how variations in factors such as protein robustness and folding rates, resulting from the unavoidability of destabilizing mutations, might influence this critical evolutionary process. The initial findings suggest that protein evolution, at a molecular level, may be more predictable than previously thought, as epistasis and the principle of least action collectively impose constraints on evolutionary paths and trajectories, and consequently, on protein evolvability. Thus, this work should advance our understanding of the main molecular mechanisms that underlie the evolution of mutation-driven proteins and also provide grounds to answer a fundamental evolutionary question: how does Darwinian selection regard all potential trajectories available?",0,arxiv,Evrim,CC-BY/arXiv,"Factors controlling protein evolvability, at the molecular scale"
"The displayed tree phylogenetic network model is shown to sit as a natural submodel of the graphical model associated to a directed acyclic graph (DAG). This representation allows to derive a number of results about the displayed tree model. In particular, the concept of a local modification to a DAG model is developed and applied to the displayed tree model. As an application, some nonidentifiability issues related to the displayed tree models are highlighted as they relate to reticulation edges and stacked reticulations in the networks. We also derive rank conditions on flattenings of probability tensors for the displayed tree model, generalizing classic results for phylogenetic tree models.",0,arxiv,Evrim,CC-BY/arXiv,Phylogenetic network models as graphical models
"Explaining the emergence of self-organized biodiversity and species abundance distribution patterns remians a fundamental challenge in ecology. While classical frameworks, such as neutral theory and models based on pairwise species interactions, have provided valuable insights, they often neglect higher-order interactions (HOIs), whose role in stabilizing ecological communities is increasingly recognized. Here, we extend the Generalized Lotka-Volterra framework to incorporate HOIs and demonstrate that these interactions can enhance ecosystem stability and prevent collapse. Our model exhibits a diverse range of emergent dynamics, including self-sustained oscillations, quasi-periodic (torus) trajectories, and intermittent chaos. Remarkably, it also reproduces empirical species abundance distributions observed across diverse natural communities. These results underscore the critical role of HOIs in structuring biodiversity and offer a broadly applicable theoretical framework for capturing complexity in ecological systems",0,arxiv,Evrim,CC-BY/arXiv,Self-organized biodiversity and species abundance distribution patterns in ecosystems with higher-order interactions
"Biological systems commonly exhibit complex spatiotemporal patterns whose underlying generative mechanisms pose a significant analytical challenge. Traditional approaches to spatiodynamic inference rely on dimensionality reduction through summary statistics, which sacrifice complexity and interdependent structure intrinsic to these data in favor of parameter identifiability. This imposes a fundamental constraint on reliably extracting mechanistic insights from spatiotemporal data, highlighting the need for analytical frameworks that preserve the full richness of these dynamical systems. To address this, we developed a simulation-based inference framework that employs vision transformer-driven variational encoding to generate compact representations of the data, exploiting the inherent contextual dependencies. These representations are subsequently integrated into a likelihood-free Bayesian approach for parameter inference. The central idea is to construct a fine-grained, structured mesh of latent representations from simulated dynamics through systematic exploration of the parameter space. This encoded mesh of latent embeddings then serves as a reference map for retrieving parameter values that correspond to observed data. By integrating generative modeling with Bayesian principles, our approach provides a unified inference framework to identify both spatial and temporal patterns that manifest in multivariate dynamical systems.",0,arxiv,Evrim,CC-BY/arXiv,Spatiodynamic inference using vision-based generative modelling
"Effective public health decisions require early reliable inference of the infectious disease properties. In this paper we assess the ability to infer infectious disease attributes from population-level stochastic epidemic trajectories. In particular, we construct stochastic Kermack-McKendrick model trajectories, sample them with and without measurement error, and evaluate inversions for the population mean infectiousness as a function of time since infection, the infection duration distribution, and its complementary cumulative distribution, the infection survival distribution. Based on an integro-differential equation formulation we employ a natural regression approach to fit the corresponding integral kernels and show that these disease attributes are recoverable from both un-regularized multi-trajectory inversions and regularized single trajectory inversions. Moreover, we demonstrate that the infection duration distributions (or alternatively the infection survival distributions) and population mean infectiousness kernels recovered can be used to solve for the individual infectiousness profile, the infectiousness of an individual over the duration of their infection. The work suggests that, aggressive monitoring of the stochastic evolution of a novel infectious disease outbreak in a single local well-mixed population can allow determination of the underlying disease attributes that characterize its spread.",0,arxiv,Evrim,CC-BY/arXiv,Determining disease attributes from epidemic trajectories
"The investigation of allele frequency trajectories in populations evolving under controlled environmental pressures has become a popular approach to study evolutionary processes on the molecular level. Statistical models based on well-defined evolutionary concepts can be used to validate different hypotheses about empirical observations. Despite their popularity, classic statistical models like the Wright-Fisher model suffer from simplified assumptions such as the independence of selected loci along a chromosome and uncertainty about the parameters. Deep generative neural networks offer a powerful alternative known for the integration of multivariate dependencies and noise reduction. Due to their high data demands and challenging interpretability they have, so far, not been widely considered in the area of population genomics. To address the challenges in the area of Evolve and Resequencing experiments (E&R) based on pooled sequencing (Pool-Seq) data, we introduce a deep generative neural network that aims to model a concept of evolution based on empirical observations over time. The proposed model estimates the distribution of allele frequency trajectories by embedding the observations from single nucleotide polymorphisms (SNPs) with information from neighboring loci. Evaluation on simulated E&R experiments demonstrates the model's ability to capture the distribution of allele frequency trajectories and illustrates the representational power of deep generative models on the example of linkage disequilibrium (LD) estimation. Inspecting the internally learned representations enables estimating pairwise LD, which is typically inaccessible in Pool-Seq data. Our model provides competitive LD estimation in Pool-Seq data high degree of LD when compared to existing methods.",0,arxiv,Evrim,CC-BY/arXiv,Deep Generative Models of Evolution: SNP-level Population Adaptation by Genomic Linkage Incorporation
"Pre-exposure prophylaxis (PrEP) has been established as an effective tool for preventing HIV infection among men who have sex with men (MSM). However, there is the possibility of PrEP usage leading to increased sexual partners and increased transmission of non-HIV sexually transmitted infections such as syphilis. We take here a network perspective to examine this possibility using data on sexual partnerships, demographic data, PrEP usage, and syphilis among MSM in Columbus, Ohio. We use a recently developed community detection algorithm, an adaptation of the community detection algorithm InfoMap to absorbing random walks, to identify clusters of people (`communities') that may drive syphilis transmission. Our community detection approach takes into account both sexual partnerships as well as syphilis treatment rates when detecting communities. We apply this algorithm to sexual networks fitted to empirical data from the Network Epidemiology of Syphilis Transmission (NEST) study in Columbus, Ohio. We assume that PrEP usage is associated with regular visits to a sexual health provider, and thus is correlated with syphilis detection and treatment rates. We examine how PrEP usage can affect community structure in the sexual networks fitted to the NEST data. We identify two types of PrEP users, those belonging to a large, highly connected community and tending to have a large number of sexual partners, versus those with a small number of sexual partners and belonging to smaller communities. A stochastic syphilis model indicates that PrEP users in the large community may play an important role in sustaining syphilis transmission.",0,arxiv,Evrim,CC-BY/arXiv,Pre-exposure prophylaxis and syphilis in men who have sex with men: a network analysis
"Let x and y be two length n DNA sequences, and suppose we would like to estimate the divergence time T. A well known simple but crude estimate of T is p := d(x,y)/n, the fraction of mutated sites (the p-distance). We establish a posterior concentration bound on T, showing that the posterior distribution of T concentrates within a logarithmic factor of p when d(x,y)log(n)/n = o(1). Our bounds hold under a large class of evolutionary models, including many standard models that incorporate site dependence. As a special case, we show that T exceeds p with vanishingly small posterior probability as n increases under models with constant mutation rates, complementing the result of Mihaescu and Steel (Appl Math Lett 23(9):975--979, 2010). Our approach is based on bounding sequence transition probabilities in various convergence regimes of the underlying evolutionary process. Our result may be useful for improving the efficiency of iterative optimization and sampling schemes for estimating divergence times in phylogenetic inference.",0,arxiv,Evrim,CC-BY/arXiv,Posterior bounds on divergence time of two sequences under dependent-site evolutionary models
"Populations interact non-linearly and are influenced by environmental fluctuations. In order to have realistic mathematical models, one needs to take into account that the environmental fluctuations are inherently stochastic. Often, environmental stochasticity is modeled by systems of stochastic differential equations. However, this type of stochasticity is not always the best suited for ecological modeling. Instead, biological systems can be modeled using piecewise deterministic Markov processes (PDMP). For a PDMP the process follows the flow of a system of ordinary differential equations for a random time, after which the environment switches to a different state, where the dynamics is given by a different system of differential equations. Then this is repeated. The current paper is devoted to the study of the dynamics of $n$ populations described by $n$-dimensional Kolmogorov PDMP. We provide sharp conditions for persistence and extinction, based on the invasion rates (Lyapunov exponents) of the ergodic probability measures supported on the boundary of the positive orthant. In order to showcase the applicability of our results, we apply the theory in some interesting ecological examples.",0,arxiv,Evrim,CC-BY/arXiv,Population dynamics under random switching
"In evolutionary biology, phylogenetic networks are graphs that provide a flexible framework for representing complex evolutionary histories that involve reticulate evolutionary events. Recently phylogenetic studies have started to focus on a special class of such networks called semi-directed networks. These graphs are defined as mixed graphs that can be obtained by de-orienting some of the arcs in some rooted phylogenetic network, that is, a directed acyclic graph whose leaves correspond to a collection of species and that has a single source or root vertex. However, this definition of semi-directed networks is implicit in nature since it is not clear when a mixed-graph enjoys this property or not. In this paper, we introduce novel, explicit mathematical characterizations of semi-directed networks, and also multi-semi-directed networks, that is, mixed graphs that can be obtained from directed phylogenetic networks that may have more than one root. In addition, through extending foundational tools from the theory of rooted networks into the semi-directed setting - such as cherry picking sequences, omnians, and path partitions - we characterize when a (multi-)semi-directed network can be obtained by de-orienting some rooted network that is contained in one of the well-known classes of tree-child, orchard, tree-based or forest-based networks. These results address structural aspects of (multi-)semi-directed networks and pave the way to improved theoretical and computational analyses of such networks, for example, within the development of algebraic evolutionary models that are based on such networks.",0,arxiv,Evrim,CC-BY/arXiv,Characterizing semi-directed phylogenetic networks and their multi-rootable variants
"Although maturation delays are frequently included in population models, researchers rarely account for mortality between birth and maturity. Previous discrete population models have included mortality of immature individuals during the maturation delay finding that increasing the delay decreases the equilibrium population size, eventually leading to extinction. Since maturation delays beyond one breeding cycle are often found in nature, they must also have a benefit leading to a trade-off. We derive a class of models to explore the trade-off between the benefit of a longer maturation delay on fecundity due to larger body sizes at maturity and the down-side on survival. We examine two scenarios: density independent survival and cohort density dependent survival of immature individuals. For the mature and immature individuals, we consider two different, but popular, survival functions: the Beverton--Holt model and the Ricker model. Across all models, we identify a positive maturation delay that maximizes the population size that we refer to as the ``optimal maturation delay'' and a critical delay threshold that results in extinction. We also find oscillatory dynamics with the Ricker survival function for certain ranges of maturation delay. Overall, our delay model sets up a useful phenomenological framework to test multiple combinations of trade-offs in parent survival, offspring survival, and reproductive investment.",0,arxiv,Evrim,CC-BY/arXiv,Adding a fecundity-survival trade-off to a discrete population model with maturation delay
"Recurrence plots and their associated quantifiers provide a robust framework for detecting and characterising complex patterns in non-linear time-series. In this paper, we employ recurrence quantification analysis to investigate the dynamics of the cyclic, non-hierarchical May-Leonard model, also referred to as rock--paper--scissors systems, that describes competitive interactions among three species. A crucial control parameter in these systems is the species' mobility $m$, which governs the spatial displacement of individuals and profoundly influences the resulting dynamics. By systematically varying $m$ and constructing suitable recurrence plots from numerical simulations, we explore how recurrence quantifiers reflect distinct dynamical features associated with different ecological states. We then introduce an ensemble-based approach that leverages statistical distributions of recurrence quantifiers, computed from numerous independent realisations, allowing us to identify dynamical outliers as significant deviations from typical system behaviour. Through detailed numerical analyses, we demonstrate that these outliers correspond to divergent ecological regimes associated with specific mobility values, providing also a robust manner to infer the mobility parameter from observed numerical data. Our results highlight the potential of recurrence-based methods as diagnostic tools for analysing spatial ecological systems and extracting ecologically relevant information from their non-linear dynamical patterns.",0,arxiv,Evrim,CC-BY/arXiv,Investigating Mobility in Spatial Biodiversity Models through Recurrence Quantification Analysis
"Diverse learning algorithms, optimization methods, and natural selection share a common mathematical structure, despite their apparent differences. Here I show that a simple notational partitioning of change by the Price equation reveals a universal force-metric-bias (FMB) law: $Î”\mathbfÎ¸ = \mathbf{M}\,\mathbf{f} + \mathbf{b} + \mathbfÎ¾$. The force $\mathbf{f}$ drives improvement in parameters, $Î”\mathbfÎ¸$, in proportion to the slope of performance with respect to the parameters. The metric $\mathbf{M}$ rescales movement by inverse curvature. The bias $\mathbf{b}$ adds momentum or changes in the frame of reference. The noise $\mathbfÎ¾$ enables exploration. This framework unifies natural selection, Bayesian updating, Newton's method, stochastic gradient descent, stochastic Langevin dynamics, Adam optimization, and most other algorithms as special cases of the same underlying process. The Price equation also reveals why Fisher information, Kullback-Leibler divergence, and d'Alembert's principle arise naturally in learning dynamics. By exposing this common structure, the FMB law provides a principled foundation for understanding, comparing, and designing learning algorithms across disciplines.",0,arxiv,Evrim,CC-BY/arXiv,The Price equation reveals a universal force-metric-bias law of algorithmic learning and natural selection
"The origin of life on Earth via the spontaneous emergence of a protocell prior to Darwinian evolution remains a fundamental open question in physics and chemistry. Here, we develop a conceptual framework based on information theory and algorithmic complexity. Using estimates grounded in modern computational models, we evaluate the difficulty of assembling structured biological information under plausible prebiotic conditions. Our results highlight the formidable entropic and informational barriers to forming a viable protocell within the available window of Earth's early history. While the idea of Earth being terraformed by advanced extraterrestrials might violate Occam's razor from within mainstream science, directed panspermia -- originally proposed by Francis Crick and Leslie Orgel -- remains a speculative but logically open alternative. Ultimately, uncovering physical principles for life's spontaneous emergence remains a grand challenge for biological physics.",0,arxiv,Evrim,CC-BY/arXiv,"The unreasonable likelihood of being: origin of life, terraforming, and AI"
"Probabilistic modeling over the combinatorially large space of tree topologies remains a central challenge in phylogenetic inference. Previous approaches often necessitate pre-sampled tree topologies, limiting their modeling capability to a subset of the entire tree space. A recent advancement is ARTree, a deep autoregressive model that offers unrestricted distributions for tree topologies. However, its reliance on repetitive tree traversals and inefficient local message passing for computing topological node representations may hamper the scalability to large datasets. This paper proposes ARTreeFormer, a novel approach that harnesses fixed-point iteration and attention mechanisms to accelerate ARTree. By introducing a fixed-point iteration algorithm for computing the topological node embeddings, ARTreeFormer allows fast vectorized computation, especially on CUDA devices. This, together with an attention-based global message passing scheme, significantly improves the computation speed of ARTree while maintaining great approximation performance. We demonstrate the effectiveness and efficiency of our method on a benchmark of challenging real data phylogenetic inference problems.",0,arxiv,Evrim,CC-BY/arXiv,ARTreeFormer: A Faster Attention-based Autoregressive Model for Phylogenetic Inference
"This work explores the relationship between altruism and the genetic system of arrhenotoky through an evolutionary game theory (EGT)-inspired lens, using a dynamic model of beehive populations consisting of three castes: workers, drones, and the queen. Arrhenotoky is a form of asexual reproduction in which unfertilized eggs become males while fertilized eggs develop into females, leading to unusual patterns of genetic relatedness between family members. This mode of reproduction occurs in insects such as the Hymenoptera, including bees. In the hive environment, bees often display altruistic behavior, or actions taken by an organism that reduce its own fitness to increase the fitness of others. Eusociality, an elaborate form of social organization characterized by complex and altruistic social behaviors, is also observed in the Hymenoptera. To explore the interplay between altruism and the reproductive patterns of arrhenotoky, we employ a population dynamics model to simulate beehive populations over a range of parameters, controlling for altruism in workers and the queen. Our results show that altruistic behaviors are essential for beehive success, with optimal worker altruism corresponding to the division of labor observed in eusocial species. Furthermore, we find that modest altruism from the queen is also vital for hive survival, emphasizing the delicate balance that can exist in these complex social systems. Overall, our findings shed light on the co-evolution of altruism, arrhenotoky, and eusociality in the natural world.",0,arxiv,Evrim,CC-BY/arXiv,Altruism and energy flow in dynamic beehive models
"The inference of phylogenetic networks, which model complex evolutionary processes including hybridization and gene flow, remains a central challenge in evolutionary biology. Until now, statistically consistent inference methods have been limited to phylogenetic level-1 networks, which allow no interdependence between reticulate events. In this work, we establish the theoretical foundations for a statistically consistent inference method for a much broader class: semi-directed level-2 networks that are outer-labeled planar and galled. We precisely characterize the features of these networks that are distinguishable from the topologies of their displayed quartet trees. Moreover, we prove that an inter-taxon distance derived from these quartets is circular decomposable, enabling future robust inference of these networks from quartet data, such as concordance factors obtained from gene tree distributions under the Network Multispecies Coalescent model. Our results also have novel identifiability implications across different data types and evolutionary models, applying to any setting in which displayed quartets can be distinguished.",0,arxiv,Evrim,CC-BY/arXiv,Distinguishing Phylogenetic Level-2 Networks with Quartets and Inter-Taxon Quartet Distances
"Wildlife-vehicle collisions (WVC) threaten both biodiversity and human safety worldwide. Despite empirical efforts to characterize the major determinants of WVC risk and optimize mitigation strategies, we still lack a theoretical framework linking traffic, landscape, and individual movement features to collision risk. Here, we introduce such a framework by leveraging recent advances in movement ecology and reaction-diffusion stochastic processes with partially absorbing boundaries. Focusing on range-resident terrestrial mammals -- responsible for most fatal WVCs -- we model interactions with a single linear road and derive exact expressions for key survival statistics, including mean collision time and road-induced lifespan reduction. These quantities are expressed in terms of measurable parameters, such as traffic intensity or road width, and movement parameters that can be robustly estimated from relocation data, such as home-range crossing times, home-range sizes, or distance between home-range center and road. Therefore, our work provides an effective theoretical framework integrating movement and road ecology, laying the foundation for data-driven, evidence-based strategies to mitigate WVCs and promote safer, more sustainable transportation networks.",0,arxiv,Evrim,CC-BY/arXiv,How animal movement influences wildlife-vehicle collision risk: a mathematical framework for range-resident species
"This study proposes a new method for computing transpiration across an eddy covariance footprint using field observations of plant sap flow, phytomorphology sampling, uncrewed aerial system (UAS), deep learning-based digital image processing, and eddy covariance micrometeorological measurements. The method is applied to the Jornada Experimental Range, New Mexico, where we address three key questions: (1) What are the daily summer transpiration rates of Mesquite (Prosopis glandulosa) and Creosote (Larrea tridentata) individuals, and how do these species contribute to footprint-scale evapotranspiration? (2) How can the plant-level measurements be integrated for terrain-wide transpiration estimates? (3) What is the contribution of transpiration to total evapotranspiration within the eddy covariance footprint? Data collected from June to October 2022, during the North American Monsoon season, include hourly evapotranspiration and precipitation rates from the Ameriflux eddy covariance system (US Jo-1 Bajada site) and sap flux rates from heat-balance sensors. We used plant biometric measurements and supervised classification of multispectral imagery to upscale from the patch to footprint-scale estimations. A proportional relationship between the plant's horizontal projected area and the estimated number of water flow conduits was extended to the eddy covariance footprint via UAS data. Our results show that Mesquite's average daily summer transpiration is 2.84 mm/d, while Creosote's is 1.78 mm/d (a ratio of 1.6:1). The summer footprint integrated transpiration to evapotranspiration ratio (T/ET) was 0.50, decreasing to 0.44 during dry spells and increasing to 0.63 following significant precipitation. Further testing of this method is needed in different regions to validate its applicability. With appropriate adjustments, it could be relevant for other areas with similar ecological conditions.",0,arxiv,Evrim,CC-BY/arXiv,"Partitioning of Eddy Covariance Footprint Evapotranspiration Using Field Data, UAS Observations and GeoAI in the U.S. Chihuahuan Desert"
"A major locus of musicological activity-increasingly in the digital domain-is the cataloguing of sources, which requires large-scale and long-lasting research collaborations. Yet, the databases aiming at covering and representing musical repertoires are never quite complete, and scholars must contend with the question: how much are we still missing? This question structurally resembles the 'unseen species' problem in ecology, where the true number of species must be estimated from limited observations. In this case study, we apply for the first time the common Chao1 estimator to music, specifically to Gregorian chant. We find that, overall, upper bounds for repertoire coverage of the major chant genres range between 50 and 80 %. As expected, we find that Mass Propers are covered better than the Divine Office, though not overwhelmingly so. However, the accumulation curve suggests that those bounds are not tight: a stable ~5% of chants in sources indexed between 1993 and 2020 was new, so diminishing returns in terms of repertoire diversity are not yet to be expected. Our study demonstrates that these questions can be addressed empirically to inform musicological data-gathering, showing the potential of unseen species models in musicology.",0,arxiv,Evrim,CC-BY/arXiv,"Knowing when to stop: insights from ecology for building catalogues, collections, and corpora"
"The myriad microscopic interactions among the individual organisms that constitute an ecological system collectively give rise, at the macroscopic scale, to evolutionary trends. The ability to detect the directionality of such trends is crucial for understanding and managing the dynamics of natural systems. Nevertheless, identifying the key observable quantities that capture such directional behaviour poses a major challenge. In this study, we propose that translating ecological data into a network framework is a valuable strategy to measure system stability and evolution. We examine the Tangled Nature model as a test case, evaluating network entropy, species diversity, and the clustering coefficient as metrics of network stability and directionality.",0,arxiv,Evrim,CC-BY/arXiv,Directionality measures in evolutionary ecological networks: Insights from the Tangled Nature model
"Recently, Sturma, Drton, and Leung proposed a general-purpose stochastic method for hypothesis testing in models defined by polynomial equality and inequality constraints. Notably, the method remains theoretically valid even near irregular points, such as singularities and boundaries, where traditional testing approaches often break down. In this paper, we evaluate its practical performance on a collection of biologically motivated models from phylogenetics. While the method performs remarkably well across different settings, we catalogue a number of issues that should be considered for effective application.",0,arxiv,Evrim,CC-BY/arXiv,Methodological considerations for semialgebraic hypothesis testing with incomplete U-statistics
"Social media is transforming various aspects of offline life, from everyday decisions such as dining choices to the progression of conflicts. In this study, we propose a coupled modelling framework with an online social network layer to analyse how engagement on a specific topic spills over into offline protest activities. We develop a stochastic model and derive several mean-field models of varying complexity. These models allow us to estimate the reproductive number and anticipate when surges in activity are likely to occur. A key factor is the transmission rate between the online and offline domains; for offline outbursts to emerge, this rate must fall within a critical range, neither too low nor too high. Additionally, using synthetic networks, we examine how network structure influences the accuracy of these approximations. Our findings indicate that low-density networks need more complex approximations, whereas simpler models can effectively represent higher-density networks. When tested on two real-world networks, however, increased complexity did not enhance accuracy.",0,arxiv,Evrim,CC-BY/arXiv,Modelling the spillover from online engagement to offline protest: stochastic dynamics and mean-field approximations on networks
"There has been a long debate on how new levels of organization have evolved. It might seem unlikely, as cooperation must prevail over competition. One well-studied example is the emergence of autocatalytic sets, which seem to be a prerequisite for the evolution of life. Using a simple model, we investigate how varying bias toward cooperation versus antagonism shapes network dynamics, revealing that higher-order organization emerges even amid pervasive antagonistic interactions. In general, we observe that a quantitative increase in the number of elements in a system leads to a qualitative transition.   We present a random threshold-directed network model that integrates node-specific traits with dynamic edge formation and node removal, simulating arbitrary levels of cooperation and competition. In our framework, intrinsic node values determine directed links through various threshold rules. Our model generates a multi-digraph with signed edges (reflecting support/antagonism, labeled ``help''/``harm''), which ultimately yields two parallel yet interdependent threshold graphs. Incorporating temporal growth and node turnover in our approach allows exploration of the evolution, adaptation, and potential collapse of communities and reveals phase transitions in both connectivity and resilience.   Our findings extend classical random threshold and ErdÅ‘s-RÃ©nyi models, offering new insights into adaptive systems in biological and economic contexts, with emphasis on the application to Collective Affordance Sets. This framework should also be useful for making predictions that will be tested by ongoing experiments of microbial communities in soil.",0,arxiv,Evrim,CC-BY/arXiv,Life Finds A Way: Emergence of Cooperative Structures in Adaptive Threshold Networks
"Effective epidemic control is crucial for mitigating the spread of infectious diseases, particularly when pharmaceutical interventions such as vaccines or treatments are limited. Non-pharmaceutical strategies, including mobility restrictions, are key in reducing transmission rates but require careful optimization to balance public health benefits and socioeconomic costs. Quantum computing is emerging as a powerful tool for solving complex optimization problems that are intractable for classical methods and can thus be leveraged to handle mobility restrictions. This article presents a new approach to optimizing epidemic control strategies using quantum computing techniques. We focus on non-pharmaceutical interventions, particularly mobility restriction, modeled as a discrete-time network epidemic process based on the susceptible-infected-susceptible and susceptible-infected-removed frameworks. The control problem is formulated as a combinatorial optimization task, inherently NP-hard due to the binary nature of intervention decisions. To tackle this computational complexity, we derive a Quadratic Unconstrained Binary Optimization representation of the control problem, enabling its efficient solution via quantum computing resources. Our methodology is validated through numerical simulations on realistic case studies, showcasing the potential of quantum algorithms for enhancing epidemic control strategies. These findings pave the way for leveraging quantum optimization in broader applications of networked dynamical systems, demonstrating its viability for complex decision-making processes in public health management.",0,arxiv,Evrim,CC-BY/arXiv,A Quantum Approach for Optimal Transient Control in Network-Based Epidemic Models
"Emerging in December 2019, the COVID-19 pandemic caused widespread health, economic, and social disruptions. Rapid global transmission overwhelmed healthcare systems, resulting in high infection rates, hospitalisations, and fatalities. To minimise the spread, governments implemented several non-pharmaceutical interventions like lockdowns and travel restrictions. While effective in controlling transmission, these measures also posed significant economic and societal challenges. Although the WHO declared COVID-19 no longer a global health emergency in May 2023, its impact persists, shaping public health strategies. The vast amount of data collected during the pandemic offers valuable insights into disease dynamics, transmission, and intervention effectiveness. Leveraging these insights can improve forecasting models, enhancing preparedness and response to future outbreaks while mitigating their social and economic impact. This paper presents a large-scale case study on COVID-19 forecasting in Cyprus, utilising a two-year dataset that integrates epidemiological data, vaccination records, policy measures, and weather conditions. We analyse infection trends, assess forecasting performance, and examine the influence of external factors on disease dynamics. The insights gained contribute to improved pandemic preparedness and response strategies.",0,arxiv,Evrim,CC-BY/arXiv,Investigating Forecasting Models for Pandemic Infections Using Heterogeneous Data Sources: A 2-year Study with COVID-19
"The population biology model holds a significant position within ecosystems. Introducing stochastic perturbations into the model can more accurately depict real biological processes. In this paper, we primarily investigate the most probable transition phenomenon in a three-dimensional nutrient-phytoplankton-zooplankton (NPZ) plankton model. With appropriate parameter values, the system coexists with a stable equilibrium point and a stable limit cycle. Under noise perturbations, transitions occur between these two steady states. Based on the Onsager-Machlup action functional and the neural shooting method, we have studied the most probable transition time, the most probable transition pathway and the most probable transition probability of the NPZ system. The transition between these metastable states plays a crucial role in stochastic ecosystems, providing guidance for a better understanding of complex biological processes.",0,arxiv,Evrim,CC-BY/arXiv,Detecting the most probable transition phenomenon of a nutrient-phytoplankton-zooplankton system
"Microbiomes, which are collections of interacting microbes in an environment, often substantially impact the environmental patches or living hosts that they occupy. In microbiome models, it is important to consider both the local dynamics within an environment and exchanges of microbiomes between environments. One way to incorporate these and other interactions across multiple scales is to employ metacommunity theory. Metacommunity models commonly assume continuous microbiome dispersal between the environments in which local microbiome dynamics occur. Under this assumption, a single parameter between each pair of environments controls the dispersal rate between those environments. This metacommunity framework is well-suited to abiotic environmental patches, but it fails to capture an essential aspect of the microbiomes of living hosts, which generally do not interact continuously with each other. Instead, living hosts interact with each other in discrete time intervals. In this paper, we develop a modeling framework that encodes such discrete interactions and uses two parameters to separately control the interaction frequencies between hosts and the amount of microbiome exchange during each interaction. We derive analytical approximations of models in our framework in three parameter regimes and prove that they are accurate in those regimes. We compare these approximations to numerical simulations for an illustrative model. We demonstrate that both parameters in our modeling framework are necessary to determine microbiome dynamics. Key features of the dynamics, such as microbiome convergence across hosts, depend sensitively on the interplay between interaction frequency and strength.",0,arxiv,Evrim,CC-BY/arXiv,Interacting Hosts with Microbiome Exchange: An Extension of Metacommunity Theory for Discrete Interactions
"Ordered leaf attachment, Phylo2Vec, and HOP are three recently introduced vector representations for rooted phylogenetic trees where the representation is determined by an ordering of the underlying leaf set X. Comparing the vectors of two rooted phylogenetic X-trees T and T' for a fixed ordering on X leads to polynomial-time computable measure for the dissimilarity of T and T', albeit dependent on the choice of the leaf ordering. For each of ordered leaf attachment, Phylo2Vec, and HOP, we compare this measure with the rooted subtree prune and regraft distance (rSPR), the hybrid number, and the temporal tree-child hybrid number of T and T'. Although there is no direct relationship between rSPR and any of the three vector-based measures, we show that, when minimized over all orderings, the hybrid number is equivalent to HOP, and an upper bound on the other two. Moreover, when minimized over all orderings induced by common cherry-picking sequences of T and T', the temporal tree-child hybrid number of T and T' is equivalent to each of the three vector-based measures.",0,arxiv,Evrim,CC-BY/arXiv,Order-Dependent Dissimilarity Measures on Phylogenetic Trees
"During the COVID-19 pandemic, Aotearoa followed an elimination strategy followed by a mitigation strategy, which saw high success and kept health impact low. However, there were inequities in health outcomes, notably that MÄori and Pacific Peoples had lower vaccine coverage and experienced higher age-standardised rates of hospitalisation and death. Models provide predictions of disease spread and burden, which can effectively inform policy, but are often less good at including inequities/heterogeneity. Despite the differences in health outcomes, most models have not explicitly considered ethnic heterogeneities as factors. We developed such a model to investigate the first Omicron wave of the COVID-19 pandemic in Aotearoa, which was the first widespread community transmission of SARS-CoV-2. We analysed three models for contact patterns within and between ethnicities: proportionate, assortative, and unconstrained mixing, which were fit using ethnicity-specific data on reported cases and spatially disaggregated population counts. We found that MÄori, Pacific, and Asian transmission rates were between 1.08-2.46, 1.50-3.89, and 0.80-0.92 times the European rates, respectively. We then found that from the parameters considered in the model, the disparity in ethnic transmission rates explained the majority of the observed ethnic disparity in attack rates, while assortativity and vaccination rates explained comparatively less.",0,arxiv,Evrim,CC-BY/arXiv,Modelling the interaction between ethnicity and infectious disease transmission dynamics in Aotearoa New Zealand during the first Omicron wave of the COVID-19 pandemic
"Understanding the dynamics of antibody levels is crucial for characterizing the time-dependent response to immune events: either infections or vaccinations. The sequence and timing of these events significantly influence antibody level changes. Despite extensive interest in the topic in the recent years and many experimental studies, the effect of immune event sequences on antibody levels is not well understood. Moreover, disease or vaccination prevalence in the population are time-dependent. This, alongside the complexities of personal antibody kinetics, makes it difficult to analyze a sample immune measurement from a population. As a solution, we design a rigorous mathematical characterization in terms of a time-inhomogeneous Markov chain model for event-to-event transitions coupled with a probabilistic framework for the post-event antibody kinetics of multiple immune events. We demonstrate that this is an ideal model for immune event sequences, referred to as personal trajectories. This novel modeling framework surpasses the susceptible-infected-recovered (SIR) characterizations by rigorously tracking the probability distribution of population antibody response across time. To illustrate our ideas, we apply our mathematical framework to longitudinal severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) data from individuals with multiple documented infection and vaccination events. Our work is an important step towards a comprehensive understanding of antibody kinetics that could lead to an effective way to analyze the protective power of natural immunity or vaccination, predict missed immune events at an individual level, and inform booster timing recommendations.",0,arxiv,Evrim,CC-BY/arXiv,Probabilistic Modeling of Antibody Kinetics Post Infection and Vaccination: A Markov Chain Approach
"Fear is a critical brain function for detecting danger and learning to avoid specific stimuli that can lead to danger. While fear is believed to have evolved under pressure from predators, experimentally reproducing the evolution is challenging. To investigate the relationship between environmental conditions, the evolution of fear, and the evolution of other rewards, such as food reward and social reward, we developed a distributed evolutionary simulation. In our simulation, prey and predator agents co-evolve their innate reward functions, including a possibly fear-like term for observing predators, and learn behaviors via reinforcement learning. Surprisingly, our simulation revealed that social reward for observing the same species is more important for prey to survive, and fear-like negative reward for observing predators evolves only after acquiring social reward. We also found that the predator with increased hunting ability (larger mouth) amplified fear emergence, but also that fear evolution is more stable with non-evolving predators that are bad at chasing prey. Additionally, unlike for predators, we found that positive rewards evolve in opposition to fear for stationary threats, as areas with abundant leftover food develop around them. These findings suggest that fear and social reward have had a complex interplay with each other through evolution, along with the nature of predators and threats.",0,arxiv,Evrim,CC-BY/arXiv,Evolution of Fear and Social Rewards in Prey-Predator Relationship
"This study presents a seasonally forced cholera model that incorporates imperfect vaccination as a control strategy. The model captures the temporal dynamics of susceptible, vaccinated, infected, and recovered individuals, as well as the environmental pathogen concentration. A key focus is the instantaneous reproduction number, which serves as a threshold indicator for outbreak persistence or elimination. When reproduction number, the disease-free equilibrium is attainable; otherwise, endemic conditions persist. We conduct a sensitivity analysis to evaluate the influence of two critical parameters: the vaccination rate and the waning rate of immunity. Results show that increasing the vaccination rate and reducing the waning rate significantly decrease reproduction number, reinforcing the importance of sustained vaccine efficacy. Seasonal forcing amplifies the complexity of cholera dynamics, revealing the need for timely public health interventions, especially before high-transmission periods. This model demonstrates practical applicability in informing vaccination strategies, especially in resource-limited settings prone to seasonal outbreaks. It offers a flexible framework for public health planning, adaptable to other waterborne diseases. The findings suggest that integrated approaches combining vaccination, improved sanitation, and targeted education are essential to reducing cholera transmission and achieving long-term control.",0,arxiv,Evrim,CC-BY/arXiv,Modeling Cholera Dynamics with Vaccination as the Control Strategy and Seasonal-forcing Transmission
"Habitats integrate the abiotic conditions, vegetation composition and structure that support biodiversity and sustain nature's contributions to people. Most habitats face mounting pressures from human activities, which requires accurate, high-resolution habitat mapping for effective conservation and restoration. Yet, current habitat maps often fall short in thematic or spatial resolution because they must (1) model several mutually exclusive habitat types that co-occur across landscapes and (2) cope with severe class imbalance that complicates exhaustive multi-class training. Here, we evaluated how high-resolution remote sensing (RS) data and Artificial Intelligence (AI) tools can improve habitat mapping across large geographical extents at fine spatial and thematic resolution. Using vegetation plots from the European Vegetation Archive, we modelled the distribution of Level 3 EUNIS habitat types across Europe and assessed multiple modelling strategies against independent validation datasets. Strategies that exploited the hierarchical nature of habitat classifications resolved classification ambiguities, especially in fragmented habitats. Integrating satellite-borne multispectral and radar imagery, particularly through Earth Observation (EO) Foundation models (EO-FMs), enhanced within-formation discrimination and overall performance. Finally, ensemble machine learning that corrects class imbalance boosted predictive accuracy even further. Our methodological framework is transferable beyond Europe and adaptable to other classification systems. Future research should advance temporal modelling of habitat dynamics, extend to habitat segmentation and quality assessment, and exploit next-generation EO data paired with higher-quality in situ observations.",0,arxiv,Evrim,CC-BY/arXiv,Continental-scale habitat distribution modelling with multimodal earth observation foundation models
"The transmission dynamics of the common enteric pathogen Lawsonia intracellularis are not fully understood. To evaluate the transmission parameters of this pathogen, one and two conventional weaned pigs, were inoculated with a pure culture of L. intracellularis, then placed among respective groups (#1 and #2) of a sentinel cohort of pigs, 9 days later. The two experimentally exposed groups (n = 31 pigs each) and a control group (n = 5 pigs) were separately housed. Fecal shedding, seroconversion, oral fluid detection, and clinical signs were monitored throughout a 38-day exposure period following the introduction of the experimentally inoculated pigs. Transmission rates were estimated based on the number of infected pigs detected over time. Quantitative PCR first detected L. intracellularis in the two experimental groups 14 days post-exposure. By day 38, 63% of Group 1 and 86% of Group 2 sentinel pigs were infected. The estimated basic reproduction number (R0) was 3.35 (95% CI: 1.62 - 7.03), and the transmission rate was 0.096 (0.046 - 0.2). These results demonstrate that introducing a single infectious source of L. intracellularis into a group of susceptible nursery age pigs is sufficient to sustain transmission over a prolonged period, resulting in infection in the majority of pigs within a five-week period. These findings underscore the ability of L. intracellularis to persist and propagate within a group, reinforcing the importance of detection and control measures.",0,arxiv,Evrim,CC-BY/arXiv,Evaluation of the transmission dynamics of Lawsonia intracellularis in swine
"An autoencoder (AE) is a neural network that, using self-supervised training, learns a succinct parameterized representation, and a corresponding encoding and decoding process, for all instances in a given class. Here, we introduce the concept of a meta-autoencoder (MAE): an AE for a collection of autoencoders. Given a family of classes that differ from each other by the values of some parameters, and a trained AE for each class, an MAE for the family is a neural net that has learned a compact representation and associated encoder and decoder for the class-specific AEs. One application of this general concept is in research and modeling of natural evolution -- capturing the defining and the distinguishing properties across multiple species that are dynamically evolving from each other and from common ancestors. In this interim report we provide a constructive definition of MAEs, initial examples, and the motivating research directions in machine learning and biology.",0,arxiv,Evrim,CC-BY/arXiv,Meta-autoencoders: An approach to discovery and representation of relationships between dynamically evolving classes
"We study how environmental stochasticity influences the long-term population size in certain one- and two-species models. The difficulty is that even when one can prove that there is persistence, it is usually impossible to say anything about the invariant probability measure which describes the persistent species. We are able to circumvent this problem for some important ecological models by noticing that the per-capita growth rates at stationarity are zero, something which can sometimes yield information about the invariant probability measure. For more complicated models we use a recent result by Cuello to explore how small noise influences the population size. We are able to show that environmental fluctuations can decrease, increase, or leave unchanged the expected population size. The results change according to the dynamical model and, within a fixed model, also according to which parameters (growth rate, carrying capacity, etc) are affected by environmental fluctuations. Moreover, we show that not only do things change if we introduce noise differently in a model, but it also matters what one takes as the deterministic `no-noise' baseline for comparison.",0,arxiv,Evrim,CC-BY/arXiv,Population size in stochastic discrete-time ecological dynamics
"There is no much doubt that biotic interactions shape community assembly and ultimately the spatial co-variations between species. There is a hope that the signal of these biotic interactions can be observed and retrieved by investigating the spatial associations between species while accounting for the direct effects of the environment. By definition, biotic interactions can be both symmetric and asymmetric. Yet, most models that attempt to retrieve species associations from co-occurrence or co-abundance data internally assume symmetric relationships between species. Here, we propose and validate a machine-learning framework able to retrieve bidirectional associations by analyzing species community and environmental data.   Our framework (1) models pairwise species associations as directed influences from a source to a target species, parameterized with two species-specific latent embeddings: the effect of the source species on the community, and the response of the target species to the community; and (2) jointly fits these associations within a multi-species conditional generative model with different modes of interactions between environmental drivers and biotic associations. Using both simulated and empirical data, we demonstrate the ability of our framework to recover known asymmetric and symmetric associations and highlight the properties of the learned association networks. By comparing our approach to other existing models such as joint species distribution models and probabilistic graphical models, we show its superior capacity at retrieving symmetric and asymmetric interactions. The framework is intuitive, modular and broadly applicable across various taxonomic groups.",0,arxiv,Evrim,CC-BY/arXiv,Uncovering symmetric and asymmetric species associations from community and environmental data
"When a new infectious disease (or a new strain of an existing one) emerges, as in the recent COVID-19 pandemic, different types of mobility restrictions are considered to slow down or mitigate the spread of the disease. The measures to be adopted require carefully weighing the social cost against their impact on disease control. In this work, we analyze, in a context of mobility restrictions, the role of frequent versus occasional contacts in epidemic spread. We develop an individual-based mathematical model where frequent contacts among individuals (at home, work, schools) and occasional contacts (at stores, transport, etc.) are considered. We define several contact structures by changing the relative weight between frequent and occasional contacts while keeping the same initial effective rate of spread. We find the remarkable result that the more frequent contacts prevail over occasional ones, the higher the epidemic peak, the sooner it occurs, and the greater the final number of individuals affected by the epidemic. We conduct our study using an SIR model, considering both exponential and deterministic recovery from infection, and obtain that this effect is more pronounced under deterministic recovery. We find that the impact of relaxation measures depends on the relative importance of frequent and occasional contacts within the considered social structures. Finally, we assess in which of the considered scenarios the homogeneous mixing approximation provides a reasonable description of the epidemic dynamics.",0,arxiv,Evrim,CC-BY/arXiv,Epidemic spread: limiting contacts to regular circles is not necessarily the safest option
"We investigate the evolution of quiescence within the framework of Adaptive Dynamics for an SIQS (Susceptible - Infected - Quiescent) model with constant environment. In the first part of the paper, the competition of two strains which have the same basic fitness (same reproduction number) but different timing in quiescence is analyzed. Thereto, the complexity of the model is reduced: By a time scale argument, we approximate the SQIS model by an SIS model under the assumption of rapid switching between I and Q. Furthermore, using dimension reduction methods for the van Kampen expansion of the models, we replace the multi-dimensional SDE by an effective one dimensional SDE on the center manifold of the models. Finally, the fixation probabilities of the strains are derived.   In the second part of the paper, we use concepts from adaptive dynamics to analyze the stochastic random walk on the trait space. We find that quiescence is favored by intrinsic stochastic effects. In the end, a more intuitive explanation is added to the mathematical arguments.   The analysis suggests a new paradigm for the evolution of quiescence in parasites: In the present context, quiescence is not a bed-hedging strategy to escape detrimental conditions in a fluctuating environment, but a simple and efficient method to continuously control and slow down the time scale of life-history traits. It turns out that evolution favors slowness, with the analysis suggesting that this effect may be widespread in systems where species interact only indirectly through competition for resources.",0,arxiv,Evrim,CC-BY/arXiv,Quiescence wins: The Discovery Of Slowness
"Branch-specific substitution models are popular for detecting evolutionary change-points, such as shifts in selective pressure. However, applying such models typically requires prior knowledge of change-point locations on the phylogeny or faces scalability issues with large data sets. To address both limitations, we integrate branch-specific substitution models with shrinkage priors to automatically identify change-points without prior knowledge, while simultaneously estimating distinct substitution parameters for each branch. To enable tractable inference under this high-dimensional model, we develop an analytical gradient algorithm for the branch-specific substitution parameters where the computation time is linear in the number of parameters. We apply this gradient algorithm to infer selection pressure dynamics in the evolution of the BRCA1 gene in primates and mutational dynamics in viral sequences from the recent mpox epidemic. Our novel algorithm enhances inference efficiency, achieving up to a 90-fold speedup per iteration in maximum-likelihood optimization when compared to central difference numerical gradient method and up to a 360-fold improvement in computational performance within a Bayesian framework using Hamiltonian Monte Carlo sampler compared to conventional univariate random walk sampler.",0,arxiv,Evrim,CC-BY/arXiv,Detecting Evolutionary Change-Points with Branch-Specific Substitution Models and Shrinkage Priors
"Compartmental models like the Susceptible-Infected-Recovered (SIR)\cite{Kermack1927} and its extensions such as the Susceptible-Exposed-Infected-Recovered (SEIRS)\cite{Ottar2020,Ignazio2021,Grimm2021,Paoluzzi2021} are commonly used to model the spread of infectious diseases. We propose here, a modified SEIRS, namely, an SEIRSD model which comprises of (i) a reverse transmission from exposed to susceptible compartment to account for the probabilistic character of disease transmission seen in nature, and (ii) inclusion of mortality caused by infection in addition to death by other causes. We observed that, a reverse flow from exposed to susceptible class, has a significant impact on the height of infection peaks and their time of occurrence. In view of the recent surges of Covid-19 variants, this study is most relevant.",0,arxiv,Evrim,CC-BY/arXiv,Infectious Disease Transmission In A Modified SEIRS model
"This study presents a deterministic model to investigate rabies transmission dynamics, incorporating environmental effects and control strategies using optimal control theory. Qualitative and quantitative analyses reveal that the disease-free equilibrium is stable when the effective reproduction number $\mathcal{R}_e < 1$, and unstable when $\mathcal{R}_e > 1$. Mesh and contour plots illustrate an inverse relationship between $\mathcal{R}_e$ and control strategies, including dog vaccination, health promotion, and post-exposure treatment. Increased intervention reduces transmission, while higher contact rates among dogs raise $\mathcal{R}_e$. Numerical simulations with optimal control confirm the effectiveness of integrated strategies. Vaccination and treatment are identified as key interventions for achieving rabies elimination within five years.",0,arxiv,Evrim,CC-BY/arXiv,A Mathematical and Optimal Control Model for Rabies Transmission Dynamics Among Humans and Dogs with Environmental Effects
"Genetic transfers are pervasive across both prokaryotes and eukaryotes, encompassing canonical genomic introgression between species or genera and horizontal gene transfer (HGT) across kingdoms. However, DNA transfer between phylogenetically distant species, here defined as remote introgression (RI), has remained poorly explored in evolutionary genomics. In this study, we present RIFinder, a novel phylogeny-based method for RI event detection, and apply it to a comprehensive dataset of 122 grass genomes. Our analysis identifies 622 RI events originating from 543 distinct homologous genes, revealing distinct characteristics among grass subfamilies. Specifically, the subfamily Pooideae exhibits the highest number of introgressed genes while Bambusoideae contains the lowest. Comparisons among accepted genes, their donor copies and native homologs demonstrate that introgressed genes undergo post-transfer localized adaptation, with significant functional enrichment in stress-response pathways. Notably, we identify a large Triticeae-derived segment in a Chloridoideae species Cleistogenes songorica, which is potentially associated with its exceptional drought tolerance. Furthermore, we provide compelling evidence that RI has contributed to the origin and diversification of biosynthetic gene clusters of gramine, a defensive alkaloid chemical, across grass species. Collectively, our study establishes a robust method for RI detection and highlights its critical role in adaptive evolution.",0,arxiv,Evrim,CC-BY/arXiv,Widespread remote introgression in the grass genomes
"Planet Earth and the biodiversity it supports are in crisis. Human impact on terrestrial, marine and freshwater ecosystems and the hundreds of thousands of organisms that inhabit them is global. To what extent can we push ecosystems before they collapse? Will species adapt to these changes and at what rate? What are the consequences, for the environment and humankind? These are some of the most pressing issues to date. Clear answers can only be addressed through long-term research programs that are extremely complex in their deployment, and by the analyses of the unique data they produce on species and ecosystem responses to change. Yet, too little institutional support and consideration have been given to long-term ecological and evolutionary research.  We describe the action recently taken by the French National Center for Scientific Research (CNRS) to recognize and support long-term ecological and evolutionary research. We provide some salient examples of critical knowledge attainable only through long-term studies in ecology and evolution, before highlighting how global institutional schemes can not only support long-term research, but lead to informed conservation efforts and societal change. Now more than ever, as populism grows and fuels mis-and dis-informed politics, governmental programs are urgently needed to support data collection, establish data-grounded facts, inform political spheres, and refuel trust with society at large.",0,arxiv,Evrim,CC-BY/arXiv,Science at Risk: The Urgent Need for Institutional Support of Long-Term Ecological and Evolutionary Research in an Era of Data Manipulation and Disinformation
"Non-binding communication is common in daily life and crucial for fostering cooperation, even though it has no direct payoff consequences. However, despite robust empirical evidence, its evolutionary basis remains poorly understood. Here, we develop a game-theoretic model in which individuals can signal an intention to cooperate before playing a Donation game. Strategies differ in how they respond to these signals, ranging from unconditional to conditional types, with the latter incurring a cognitive cost for deliberation. Through evolutionary analysis, we show that non-binding communication alone cannot sustain cooperation in well-mixed, anonymous populations, consistent with empirical observations. In contrast, structured populations support the emergence of cooperation, with conditional cooperators acting as catalysts that protect unconditional cooperators through context-dependent patterns of cyclic dominance. These findings offer an evolutionary explanation for how non-binding communication promotes cooperation and provide a modelling framework for exploring its effects in diverse social settings.",0,arxiv,Evrim,CC-BY/arXiv,Network reciprocity turns cheap talk into a force for cooperation
"The population density and per-generation dispersal rate of a population are central parameters in the study of evolution and ecology. The distribution of recent coalescent events between individuals in space can be used to estimate such quantities through the distribution of identity-by-descent (IBD) blocks. An IBD block is defined as a segment of DNA that has been inherited by a pair of individuals from a common ancestor without being broken by recombination. We introduce a Julia package for estimating effective population densities and dispersal rates from observed spatial patterns of IBD shared blocks. It implements the inference scheme proposed by Ringbauer, Coop, and Barton (2017). The package provides a user-friendly interface, supports efficient gradient-based optimization and accommodates arbitrary user-defined demographic models through numerical integration. This software aims to encourage a wider audience to utilize spatial genetic data for estimating dispersal rates, thereby motivating further research and expanding its applications.",0,arxiv,Evrim,CC-BY/arXiv,IdentityByDescentDispersal.jl: Inferring dispersal rates with identity-by-descent blocks
"Biodiversity assessments are critically affected by the spatial scale at which species richness is measured. How species richness accumulates with sampling area depends on natural and anthropogenic processes whose effects can change depending on the spatial scale considered. These accumulation dynamics, described by the species-area relationship (SAR), are challenging to assess because most biodiversity surveys are restricted to sampling areas much smaller than the scales at which these processes operate. Here, we combine sampling theory and deep learning to predict local species richness within arbitrarily large sampling areas, enabling for the first time to estimate spatial differences in SARs. We demonstrate our approach by predicting vascular plant species richness across Europe and evaluate predictions against an independent dataset of plant community inventories. The resulting model, named deep SAR, delivers multi-scale species richness maps, improving coarse grain richness estimates by 32% compared to conventional methods, while delivering finer grain estimates. Additional to its predictive capabilities, we show how our deep SAR model can provide fundamental insights on the multi-scale effects of key biodiversity processes. The capacity of our approach to deliver comprehensive species richness estimates across the full spectrum of ecologically relevant scales is essential for robust biodiversity assessments and forecasts under global change.",0,arxiv,Evrim,CC-BY/arXiv,Multi-scale species richness estimation with deep learning
"Accurate modeling of microbial growth curves is essential for understanding and optimizing bioprocesses in biotechnology and environmental engineering. While classical monoauxic models such as Monod, Boltzmann, and Gompertz are widely used, they often lack biological interpretability or fail to capture the complex, multiphasic growth patterns observed in systems with mixed substrates or microbial communities. This paper presents a methodological framework for reparametrizing the Boltzmann and Gompertz equations, assigning direct biological significance to each parameter and enabling their application to both monoauxic and polyauxic (multi-phase) growth scenarios. Polyauxic growth is modeled as a weighted sum of sigmoidal functions, with constraints for model identifiability and biological plausibility. Robust parameter estimation is achieved using a Lorentzian loss function, combined with a global-local optimization strategy (Particle Swarm Optimization and Nelder-Mead), and systematic outlier exclusion using the ROUT method. Model parsimony is enforced via established information criteria. This workflow enables reliable, reproducible, and interpretable extraction of kinetic parameters from complex growth data and is broadly applicable to other fields where sigmoidal patterns are observed.",0,arxiv,Evrim,CC-BY/arXiv,Mono and Polyauxic Growth Kinetic Models
"When approached by predators, prey must decide whether to flee or remain and fight. The economics of such decisions are underlain by the trade-off between current and residual fitness.  The trade-off predicts that (i) breeders should be less prone than non-breeders to flee from approaching predators, as breeders can lose their investment into current reproduction; (ii) among breeders, parents should increasingly defend their offspring with increasing investment into the brood (brood value hypothesis), at least until the offspring can independently take part in anti-predator defenses; and (iii) for a similar investment into reproduction, breeders with lower perspectives to fledge or wean their young should invest less into offspring defense. We tested these predictions in a colonially breeding seabird, the king penguin (Aptenodytes patagonicus).  Specifically, we considered how antipredator behaviors varied according to life history stage (molting, courting, breeding), offspring age and their dependence on parents for antipredator defenses, and the timing of breeding, with late breeders being very unlikely to fledge offspring in this species. Using non-lethal human approaches to mimic the threat of predation, we approached >500 penguins and measured their alert and flight initiation distances, as well as the distance fled. We found that birds show increasingly stronger antipredator behaviors as they initiate and increase their investment into reproduction, from non-reproductive stages to courting and brooding small, thermo-dependent chicks. However, once offspring gained thermal independence and freedom of movement, parents reduced their antipredator behaviors. Late breeders were more likely to flee from the approaching threat than early breeders. Altogether, our results demonstrate that parental antipredator responses are dynamic and shaped by the levels 3 of investment into current reproduction, the ability of offspring to defend themselves, and the perceived future value of the brood.",0,arxiv,Evrim,CC-BY/arXiv,Life history stage effects on alert and flight initiation distances in king penguins (Aptenodytes patagonicus)
"Demography of herbivorous mammal populations may be affected by changes in predation, population density, harvesting, and climate. Whereas numerous studies have focused on the effect of single environmental variables on individual demographic processes, attempts to integrate the consequences of several environmental variables on numerous functional traits and demographic rates are rare. Over a 32-year period, we examined how forage availability (vegetation assessed through NDVI) and population density affected the functional traits and demographic rates of a population of Columbian ground squirrels (Urocitellus columbianus), an herbivorous hibernating rodent. We focused on mean population phenology, body mass, breeding success and survival. We found a negative effect of population density on demographic rates, including on breeding success and pup and adult survival to the next year.  We found diverging effects of vegetation phenology on demographic rates: positive effects of earlier start to growing season on adult female and juvenile survival, but no clear effect on male survival. Interestingly, neither population density nor vegetation affected population phenology or body condition in the following year. Vegetative growth rate had a positive influence on female mass gain (somatic investment) over a season, but vegetative growth rate and biomass, surprisingly, had negative effects on the survival of young through their first hibernation. Later vegetative timing during the year had a positive influence on survival for all ground squirrels. Thus, ground squirrels appeared to benefit more from later timing of vegetation than increases in vegetative biomass per se. Our study provides evidence for complex ecological effects of vegetation and population density on functional traits and demographic rates of small mammal populations.",0,arxiv,Evrim,CC-BY/arXiv,Population density and vegetation resources influence demography in a hibernating herbivorous mammal
"Selection analyses of long-term field data frequently use annual comparisons from longlived species with overlapping generations to measure fitness differences with respect to phenotypic characteristics, such as annual phenological timing. An alternative approach applies lifetime estimates of fitness that encompass several annual events. We studied selection on emergence date from hibernation in male and female Columbian ground squirrels, Urocitellus columbianus (Ord 1915). From 32 years of records, we estimated lifetime fitness using either lifetime reproductive success (LRS) or matrix methods, and estimated annual fitness from individual yearly survival and reproduction. We also modified estimates to statistically control for changes in mean population fitness over the lives of individuals. We regressed lifetime fitness metrics on dates of emergence from hibernation, to quantify the strength of selection on emergence date (a heritable trait). All fitness metrics were highly correlated, but differences became apparent when estimating selection coefficients for emergence dates. The annual fitness metric and LRS produced lower effect sizes of selection coefficients than matrix-based lifetime fitness and a lifespan approach based on average annual fitness. Further, only these last two metrics revealed stabilizing selection. These results suggest that the choice of a fitness metric may influence our conclusions about natural selection.",0,arxiv,Evrim,CC-BY/arXiv,Comparing lifetime and annual fitness measures reveals differences in selection outcomes
"Conventional wisdom suggests that environmental noise drives populations toward extinction. In contrast, we report a paradoxical phenomenon in which stochasticity reverses a deterministic tipping point, thereby preventing collapse. Using a hybrid model that integrates logistic growth with a density-triggered sigmoidal collapse, we uncover a striking reversal: deterministic fragility on one side, and stochastic rescue under weak noise on the other. Our analysis demonstrates that noise disrupts the convergence of deterministic trajectories toward extinction by altering the phase space topology, enabling back-transitions to viable states. This mechanism gives rise to noise-induced metastability and reveals a form of stochastic robustness not captured by deterministic models. These findings suggest that natural fluctuations can serve as a stabilizing force in complex systems, offering a compelling counter-narrative to classical models in ecology, epidemiology, and beyond. We advocate for a re-evaluation of stabilization strategies, emphasizing the constructive role of stochasticity in averting population collapse.",0,arxiv,Evrim,CC-BY/arXiv,Noise Reinstates Collapsed Populations: Stochastic Reversal of Deterministic Extinction
"We propose a network behavioral-feedback Susceptible-Infected-Recovered (SIR) epidemic model in which the interaction matrix describing the infection rates across subpopulations depends in feedback on the current epidemic state. This model captures both heterogeneities in individuals mixing, contact frequency, aptitude to contract and spread the infection, and endogenous behavioral responses such as voluntary social distancing and the adoption of self-protective measures. We study the stability of the equilibria and illustrate through several examples how the shape of the stability region depends on the structure of the interaction matrix, providing insights for the design of effective control strategies. We then analyze the transient behavior of the dynamics, showing that, for a special class of rank-1 interaction matrices, there always exists an aggregate infection curve that exhibits a unimodal behavior, expanding the results on the unimodality of infection curve known in the literature of epidemic models and paving the way for future control applications.",0,arxiv,Evrim,CC-BY/arXiv,Network Behavioral-Feedback SIR Epidemic Model
"This study explores the historical ecology of the Bavarian Forest at the threshold of modernity through the lens of a large-scale biodiversity survey conducted in 1845 across the Kingdom of Bavaria. Focusing on the Forestry Office of Zwiesel, the article analyses archival records that document the distribution of animal and tree species during a period of administrative rationalisation and early scientific systematisation. These sources include official correspondence, detailed species inventories, and early distribution maps compiled under the direction of zoologist Johann Andreas Wagner. The study highlights local and regional observations of fauna, such as the Eurasian lynx, capercaillie, and various freshwater fish, and presents new data on historical forest composition, including the presence of rare tree species and venerable specimen trees such as the ""Urwaldtanne"" and the ""Wolframslinde"". By integrating historical ecology, environmental history, and digital humanities, the article sheds light on how 19th-century state institutions, scientific actors, and local knowledge contributed to the early documentation of biodiversity. The findings also offer a basis for long-term environmental comparisons and inform current conservation efforts in the region.",0,arxiv,Evrim,CC-BY/arXiv,Of lynxes and limetrees: New Insights into the Historical Ecology of the Bavarian Forest on the Threshold of Modernity
"Teachers hold a prominent place in modern societies, particularly where education is compulsory and widely institutionalized. This ubiquity obscures an underlying question: why do societies designate certain individuals exclusively for the instruction of others? This question is especially enigmatic for dedicated teachers, who invest their labor in cultivating others' skills but do not directly participate in the productive activities for which their students are being trained. To address this puzzle, we develop a simple, mathematically tractable model of teaching and learning in a population with a shared goal. We identify a tradeoff between the size of the workforce and its collective level of expertise; and we analyze the optimal proportion of a population that should serve as teachers across a wide range of scenarios. We show that a population must exceed a critical size before it is beneficial to allocate anyone as a dedicated teacher at all. Subsequently, the peak demand for teachers is achieved at an intermediate population size, and it never surpasses one half of the population. For more complicated tasks, our analysis predicts the optimal allocation of teachers across different levels of expertise. The structure of this teacher allocation is more complex when the population size is large, in agreement with the general size-complexity hypothesis. Our account lays a foundation for understanding the adaptive advantage of dedicated teachers in both human and non-human societies.",0,arxiv,Evrim,CC-BY/arXiv,When it pays to teach: a population threshold for dedicated teaching
"Populations experience a complex interplay of continuous and discrete processes: continuous growth and interactions are punctuated by discrete reproduction events, dispersal, and external disturbances. These dynamics can be modeled by impulsive or flow-kick systems, where continuous flows alternate with instantaneous discrete changes. To study species persistence in these systems, an invasion growth rate theory is developed for flow-kick models with state-dependent timing of kicks and auxiliary variables. The invasion growth rates are Lyapunov exponents characterizing the average per-capita growth of species when rare. Two theorems are proven to characterize permanence i.e. the extinction set is a repellor. The first theorem uses Morse decompositions of the extinction set and requires that there exists a species with a positive invasion growth rate for every invariant measure supported on a component of the Morse decomposition. The second theorem uses invasion growth rates to define invasion graphs whose vertices correspond to communities and directed edges to potential invasions. Provided the invasion graph is acyclic, permanence is fully characterized by the signs of the invasion growth rates. Invasion growth rates are also used to identify the existence of extinction-bound trajectories and attractors that lie on the extinction set. The results are illustrated with three applications: (i) a microbial serial transfer model, (ii) a spatially structured consumer-resource model, and (iii) an empirically parameterized Lotka-Volterra model. Mathematical challenges and promising biological applications are discussed.",0,arxiv,Evrim,CC-BY/arXiv,Coexistence and Extinction in Flow-Kick Systems: An invasion growth rate approach
"Computational phylogenetics has become an established tool in historical linguistics, with many language families now analyzed using likelihood-based inference. However, standard approaches rely on expert-annotated cognate sets, which are sparse, labor-intensive to produce, and limited to individual language families. This paper explores alternatives by comparing the established method to two fully automated methods that extract phylogenetic signal directly from lexical data. One uses automatic cognate clustering with unigram/concept features; the other applies multiple sequence alignment (MSA) derived from a pair-hidden Markov model. Both are evaluated against expert classifications from Glottolog and typological data from Grambank. Also, the intrinsic strengths of the phylogenetic signal in the characters are compared. Results show that MSA-based inference yields trees more consistent with linguistic classifications, better predicts typological variation, and provides a clearer phylogenetic signal, suggesting it as a promising, scalable alternative to traditional cognate-based methods. This opens new avenues for global-scale language phylogenies beyond expert annotation bottlenecks.",0,arxiv,Evrim,CC-BY/arXiv,Beyond cognacy
"In this paper, we study a stochastic susceptible-infected-susceptible (SIS) epidemic model that includes an additional immigration process. In the presence of multiplicative noise, generated by environmental perturbations, the model exhibits noise-induced transitions. The bifurcation diagram has two distinct regions of unimodality and bimodality in which the steady-state probability distribution has one and two peaks, respectively. Apart from first-order transitions between the two regimes, a critical-point transition occurs at a cusp point with the transition belonging to the mean-field Ising universality class. The epidemic model shares these features with the well-known Horsthemke-Lefever model of population genetics. The effect of vaccination on the spread/containment of the epidemic in a stochastic setting is also studied. We further propose a general vaccine-hesitancy model, along the lines of Kirman's ant model, with the steady-state distribution of the fraction of the vaccine-willing population given by the Beta distribution. The distribution is shown to give a good fit to the COVID-19 data on vaccine hesitancy and vaccination. We derive the steady-state probability distribution of the basic reproduction number, a key parameter in epidemiology, based on a beta-distributed fraction of the vaccinated population. Our study highlights the universal features that epidemic and vaccine models share with other dynamical models.",0,arxiv,Evrim,CC-BY/arXiv,Universal features of epidemic and vaccine models
"The universal genetic code presents a fundamental paradox in molecular biology. Recent advances in synthetic biology have demonstrated that the code is remarkably flexible--organisms can survive with 61 codons instead of 64, natural variants have reassigned codons 38+ times, and fitness costs of recoding stem primarily from secondary mutations rather than code changes themselves. Yet despite billions of years of evolution and this proven flexibility, approximately 99% of life maintains an identical 64-codon genetic code. This extreme conservation cannot be fully explained by current evolutionary theory, which predicts far more variation given the demonstrated viability of alternatives. I propose that this paradox--evolutionary flexibility coupled with mysterious conservation--reveals unrecognized constraints on biological information systems. This paper presents testable predictions to distinguish between competing explanations: extreme network effects, hidden optimization parameters, or potentially, computational architecture constraints that transcend standard evolutionary pressures.",0,arxiv,Evrim,CC-BY/arXiv,The Genetic Code Paradox: Extreme Conservation Despite Demonstrated Flexibility
"To fully exploit the potential of computational phylogenetic methods for cognate data one needs to leverage specific (complex) models an machine learning-based techniques. However, both approaches require datasets that are substantially larger than the manually collected cognate data currently available. To the best of our knowledge, there exists no feasible approach to automatically generate larger cognate datasets. We substantiate this claim by automatically extracting datasets from BabelNet, a large multilingual encyclopedic dictionary. We demonstrate that phylogenetic inferences on the respective character matrices yield trees that are largely inconsistent with the established gold standard ground truth trees. We also discuss why we consider it as being unlikely to be able to extract more suitable character matrices from other multilingual resources. Phylogenetic data analysis approaches that require larger datasets can therefore not be applied to cognate data. Thus, it remains an open question how, and if these computational approaches can be applied in historical linguistics.",0,arxiv,Evrim,CC-BY/arXiv,The Cognate Data Bottleneck in Language Phylogenetics
"Sustainability has been defined as meeting the needs of the present without compromising the ability of future generations to meet their own needs. But what are the needs of the present? And are they met? From the poor performance of the 2030 Sustainable Development Goals (SDG), defined by the UN in 2015, not even the collective needs of the present seem to be met. How to expect not to compromise the needs of the future? Is the achievement of global world goals incompatible with the characteristic processes of human evolution, as some authors have recently suggested? Simple mathematical models cannot capture the whole breadth of human experience and destiny. But, on the other hand, one should not neglect whatever insights they may provide. And what these models teach us is how the behavior pattern ""Parochial cooperation - Conflict - Growth"" was reached and how this pattern, in addition to leading to several types of crises, is also on the way of the global governance needed to achieve the SDG's",0,arxiv,Evrim,CC-BY/arXiv,"Sustainability, behavior patterns and crises"
"Decades of scientific inquiry have sought to understand how evolution fosters cooperation, a concept seemingly at odds with the belief that evolution should produce rational, self-interested individuals. Most previous work has focused on the evolution of cooperation among boundedly rational individuals whose decisions are governed by behavioral rules that do not need to be rational. Here, using an evolutionary model, we study how altruism can evolve in a community of rational agents and promote cooperation. We show that in both well-mixed and structured populations, a population of objectively rational agents is readily invaded by mutant individuals who make rational decisions but evolve a distorted (i.e., subjective) perception of their payoffs. This promotes behavioral diversity and gives rise to the evolution of rational, other-regarding agents who naturally solve all the known strategic problems of two-person, two-strategy games by perceiving their games as pure coordination games.",0,arxiv,Evrim,CC-BY/arXiv,The Evolution of Altruistic Rationality Provides a Solution to Social Dilemmas via Rational Reciprocity
"This work investigates the benefits of implementing a systematic approach to social isolation policies during epidemics. We develop a mixed integer data-driven model predictive control (MPC) scheme based on an SIHRD model which is identified from available data. The case of the spread of the SARS-CoV-2 virus (also known as COVID-19) in Mauritius is used as a reference point with data obtained during the period December 2021 to May 2022. The isolation scheme is designed with the control decision variable taking a finite set of values corresponding to the desired level of isolation. The control input is further restricted to shifting between levels only after a minimum amount of time. The simulation results validate our design, showing that the need for hospitalisation remains within the capacity of the health centres, with the number of deaths considerably reduced by raising the level of isolation for short periods of time with negligible social and economic impact. We also show that the introduction of additional isolation levels results in a smoother containment approach with a considerably reduced hospitalisation burden.",0,arxiv,Evrim,CC-BY/arXiv,A Data-Driven Model Predictive Controller to manage epidemics: The case of SARS-CoV-2 in Mauritius
"Epidemic forecasting tools embrace the stochasticity and heterogeneity of disease spread to predict the growth and size of outbreaks. Conceptually, stochasticity and heterogeneity are often modeled as branching processes or as percolation on contact networks. Mathematically, probability generating functions provide a flexible and efficient tool to describe these models and quickly produce forecasts. While their predictions are probabilistic-i.e., distributions of outcome-they depend deterministically on the input distribution of transmission statistics and/or contact structure. Since these inputs can be noisy data or models of high dimension, traditional sensitivity analyses are computationally prohibitive and are therefore rarely used. Here, we use statistical condition estimation to measure the sensitivity of stochastic polynomials representing noisy generating functions. In doing so, we can separate the stochasticity of their forecasts from potential noise in their input. For standard epidemic models, we find that predictions are most sensitive at the critical epidemic threshold (basic reproduction number $R_0 = 1$) only if the transmission is sufficiently homogeneous (dispersion parameter $k > 0.3$). Surprisingly, in heterogeneous systems ($k \leq 0.3$), the sensitivity is highest for values of $R_{0} > 1$. We expect our methods will improve the transparency and applicability of the growing utility of probability generating functions as epidemic forecasting tools.",0,arxiv,Evrim,CC-BY/arXiv,Sensitivity analysis of epidemic forecasting and spreading on networks with probability generating functions
"In the context of population dynamics, identifying effective model features, such as fecundity and mortality rates, is generally a complex and computationally intensive process, especially when the dynamics are heterogeneous across the population. In this work, we propose a Weak form Scientific Machine Learning-based method for selecting appropriate model ingredients from a library of scientifically feasible functions used to model structured populations. This method uses extensions of the Weak form Sparse Identification of Nonlinear Dynamics (WSINDy) method to select the best-fitting ingredients from noisy time-series histogram data. This extension includes learning heterogeneous dynamics and also learning the boundary process of the model directly from the data. We additionally provide a cross-validation method which helps fine tune the recovered boundary process to the data.   Several test cases are considered, demonstrating the method's performance for different previously studied models, including age and size-structured models. Through these examples, we examine both the advantages and limitations of the method, with a particular focus on the distinguishability of terms in the library.",0,arxiv,Evrim,CC-BY/arXiv,Learning Structured Population Models from Data with WSINDy
"A social norm defines what is good and what is bad in social contexts, as well as what to do based on such assessments. A stable social norm should be maintained against errors committed by its players. In addition, individuals may have different probabilities of errors in following the norm, and a social norm would be unstable if it benefited those who do not follow the norm carefully. In this work, we show that Simple Standing, which has been known to resist errors and mutants successfully, actually exhibits threshold behavior. That is, in a population of individuals playing the donation game according to Simple Standing, the residents can suppress the invasion of mutants with higher error proneness only if the residents' own error proneness is sufficiently low. Otherwise, the population will be invaded by mutants that commit assessment errors more frequently, and a series of such invasions will eventually undermine the existing social norm. This study suggests that the stability analysis of a social norm may have a different picture if the probability of error itself is regarded as an individual attribute.",0,arxiv,Evrim,CC-BY/arXiv,Threshold behavior of a social norm in response to error proneness
"1. Although environmental variability is expected to play a more prominent role under climate change, current demographic models that ignore the differential environmental histories of cohorts across generations are unlikely to accurately predict population dynamics and growth. The use of these approaches, which we collectively refer to as non time-structured models or nTSMs, will instead yield error-prone estimates by giving rise to a form of ecological memory loss due to their inability to account for the historical effects of past environmental exposure on subsequent growth rates.   2. To address this important issue, we introduce a new class of time-structured models or TSMs that accurately depict growth under variable environments by splitting seemingly homogeneous populations into distinct demographic cohorts based on their past exposure to environmental fluctuations. By accounting for this cryptic population structure, TSMs accurately simulate the historical effects of environmental variability, even when individuals exhibit different degrees of phenotypic plasticity.   3. Here, we provide a conceptual framework, the mathematical tools needed to simulate any TSM, and a closed form solution for simple exponential growth. We then show that traditional nTSMs yield large errors compared to TSMs when estimating population dynamics under fluctuating temperatures. Overall, TSMs represent a critical tool for predicting population growth in a variable world.",0,arxiv,Evrim,CC-BY/arXiv,Time-structured models of population growth in fluctuating environments
"Many classes of phylogenetic networks have been proposed in the literature. A feature of several of these classes is that if one restricts a network in the class to a subset of its leaves, then the resulting network may no longer lie within this class. This has implications for their biological applicability, since some species -- which are the leaves of an underlying evolutionary network -- may be missing (e.g., they may have become extinct, or there are no data available for them) or we may simply wish to focus attention on a subset of the species. On the other hand, certain classes of networks are `closed' when we restrict to subsets of leaves, such as (i) the classes of all phylogenetic networks or all phylogenetic trees; (ii) the classes of galled networks, simplicial networks, galled trees; and (iii) the classes of networks that have some parameter that is monotone-under-leaf-subsampling (e.g., the number of reticulations, height, etc.) bounded by some fixed value. It is easily shown that a closed subclass of phylogenetic trees is either all trees or a vanishingly small proportion of them (as the number of leaves grows). In this short paper, we explore whether this dichotomy phenomenon holds for other classes of phylogenetic networks, and their subclasses.",0,arxiv,Evrim,CC-BY/arXiv,A dichotomy law for certain classes of phylogenetic networks
"Researchers puzzle over questions as to how rare species survive extinction, and why a significant proportion of microbial taxa are dormant. Computational simulation modeling by a genetic algorithm provides some answers. First, a weak/rare/lowly-adapted species can obtain significantly higher fitness by resorting to sporadic dormancy; thereby the probability of extinction is reduced. Second, the extent of fitness-gain is greater when a higher fraction of the population is dormant; thus, the probability of species survival is greater for higher prevalence of dormancy. In sum, even when the environment is unfavorable initially and remains unchanged, sporadic dormancy enables a weak/rare species enhance the extent of favorable adaptation over time, successfully combating the forces of natural selection.",0,arxiv,Evrim,CC-BY/arXiv,The Effect of Sporadic Dormancy on Adaptation under Natural Selection: A Formal Theory
"Tree-grass coexistence is a defining feature of savanna ecosystems, which play an important role in supporting biodiversity and human populations worldwide. While recent advances have clarified many of the underlying processes, how these mechanisms interact to shape ecosystem dynamics under environmental stress is not yet understood. Here, we present and analyze a minimalistic spatially extended model of tree-grass dynamics in dry savannas. We incorporate tree facilitation of grasses through shading and grass competing with trees for water, both varying with tree life stage. Our model shows that these mechanisms lead to grass-tree coexistence and bistability between savanna and grassland states. Moreover, the model predicts vegetation patterns consisting of trees and grasses, particularly under harsh environmental conditions, which can persist in situations where a non-spatial version of the model predicts ecosystem collapse from savanna to grassland instead (a phenomenon called ''Turing-evades-tipping''). Additionally, we identify a novel ''Turing-triggers-tipping'' mechanism, where unstable pattern formation drives tipping events that are overlooked when spatial dynamics are not included. These transient patterns act as early warning signals for ecosystem transitions, offering a critical window for intervention. Further theoretical and empirical research is needed to determine when spatial patterns prevent tipping or drive collapse.",0,arxiv,Evrim,CC-BY/arXiv,Vegetation Patterning Can Both Impede and Trigger Critical Transitions from Savanna to Grassland
"Based on approximately 90,000 confirmed dengue cases reported in Recife - a major city in northeastern Brazil - between 2015 and 2024, we conducted a neighborhood-level spatial analysis. Socioeconomic and demographic indicators from the 2022 Brazilian Census were integrated to explore factors associated with the spatial distribution of dengue incidence. To address multicollinearity and reduce dimensionality, we applied Principal Component Analysis (PCA) to the explanatory variables. Using the resulting components, we built predictive models via Ordinary Least Squares (OLS), robust regression, and Random Forest algorithms. The OLS model explained 60.4% of the variance in case density (cases per square kilometer), while the robust model - more resilient to outliers - accounted for 43.2%. The Random Forest model, capturing nonlinear patterns, achieved 37.3%. Despite some localized gains from nonlinearity, linear models showed greater overall stability and interpretability. Using PCA scores, we constructed a dengue risk ranking of neighborhoods and compared it to the actual 2024 distribution, achieving an 83.5% match in relative ordering. Our findings indicate that census-based socioeconomic data, when combined with dimensionality reduction and predictive modeling, can effectively estimate urban dengue risk and guide spatially targeted public health strategies.",0,arxiv,Evrim,CC-BY/arXiv,"Mapping Dengue Vulnerability in Recife, Brazil: Socioeconomic Insights from PCA and Robust Regression"
"Ecological communities are composed of species interactions that respond to environmental fluctuations. Despite increasing evidence of temporal variation in these interactions, most theoretical frameworks remain rooted in static assumptions. Here, we develop and apply a time-varying network model to five long-term ecological datasets spanning diverse taxa and environments. Using a generalized Lotka-Volterra framework with environmental covariates, we quantify temporal rewiring of interspecific interactions, asymmetry patterns, and structural stability. Our results reveal contrasting dynamics across ecosystems: in datasets with rich temporal resolution, interaction networks exhibit marked rewiring and shifts in cooperation-competition ratios that correlate with environmental stress, consistent, though not always linearly, with the stress-gradient hypothesis. Conversely, in datasets with coarser temporal sampling, networks retain constant interaction sign structure and remain in cooperation-dominated regimes. These findings highlight the importance of temporal resolution and environmental context in shaping ecological coexistence.",0,arxiv,Evrim,CC-BY/arXiv,Time-varying ecological interactions characterise equilibrium and stability
"Unlike many physical nonequilibrium systems, in biological systems, the coupling to external energy sources is not a fixed parameter but adaptively controlled by the system itself. We do not have theoretical frameworks that allow for such adaptability. As a result, we cannot understand emergent behavior in living systems where structure formation and non-equilibrium drive coevolve. Here, using ecosystems as a model of adaptive systems, we develop a framework of living circuits whose architecture changes adaptively with the energy dissipated in each circuit edge. We find that unlike traditional nonequilibrium systems, living circuits exhibit a phase transition from equilibrium death to a nonequilibrium dissipative state beyond a critical driving potential. This transition emerges through a feedback mechanism that saves the weakest edges by routing dissipation through them, even though the adaptive rule locally rewards the strongest dissipating edges. Despite lacking any global optimization principle, living circuits achieve near-maximal dissipation, with higher drive promoting more complex circuits. Our work establishes ecosystems as paradigmatic examples of living circuits whose structure and dissipation are tuned through local adaptive rules.",0,arxiv,Evrim,CC-BY/arXiv,Ecosystems as adaptive living circuits
"People make strategic decisions multiple times a day. We act strategically in negotiations, when we coordinate our actions with others, or when we choose with whom to cooperate. The resulting dynamics can be studied with evolutionary game theory. This framework explores how people adapt their decisions over time, in light of how effective their strategies have proven to be. A crucial quantity in respective models is the strength of selection. This quantity regulates how likely individuals switch to a better strategy when given the choice. The larger the selection strength, the more biased is the learning process in favor of strategies with large payoffs. Therefore, this quantity is often interpreted as a measure of rationality. Traditionally, most models take selection strength to be a fixed parameter. Instead, here we allow the individuals' strategies and their selection strength to co-evolve. The endpoints of this co-evolutionary process depend on the strategic interaction in place. In many prisoner's dilemmas, selection strength increases indefinitely, as one may expect. However, in snowdrift or stag-hunt games, it can either converge to a finite value, or we observe evolutionary branching altogether - such that different individuals arrive at different selection strengths. Overall, this work sheds light on how evolution might shape learning mechanisms for social behavior. It suggests that boundedly rational learning is not only a by-product of cognitive constraints. Instead it might also evolve as a means to gain strategic advantages.",0,arxiv,Evrim,CC-BY/arXiv,Evolution of boundedly rational learning in games
"The human niche represents the intersection of biological, ecological, cultural, and technological processes that have co-evolved to shape human adaptation and societal complexity. This paper explores the human niche through the lens of macroecological scaling theory, seeking to define and quantify the dimensions along which human ecological strategies have diversified. By leveraging concepts from classic niche theory, niche construction, and complex adaptive systems, I develop a framework for understanding human ecology as both predictable within mammalian scaling relationships and uniquely divergent due to social, cognitive, and technological factors. Key dimensions of the human niche-metabolism, cognition, sociality, and computation-are examined through scaling laws that structure human interactions with the environment and each other. The paper demonstrates how human niche expansion over evolutionary time has been characterized by increasing metabolic consumption, information processing capacity, and the formation of larger, more interconnected social networks. This cumulative trajectory has led to the unprecedented scale of contemporary human societies, with implications for sustainability, economic development, and future niche expansion, including into space. The study underscores the need for an integrative, quantitative approach to human ecology that situates human adaptability within broader ecological and evolutionary constraints.",0,arxiv,Evrim,CC-BY/arXiv,Scaling the human niche
"Cancer cells are often seen to prefer glycolytic metabolism over oxidative phosphorylation even in the presence of oxygen-a phenomenon termed the Warburg effect. Despite significant strides in the decades since its discovery, a clear basis is yet to be established for the Warburg effect and why cancer cells show such a preference for aerobic glycolysis. In this review, we draw on what is known about similar metabolic shifts both in normal mammalian physiology and overflow metabolism in microbes to shed new light on whether aerobic glycolysis in cancer represents some form of optimisation of cellular metabolism. From microbes to cancer, we find that metabolic shifts favouring glycolysis are sometimes driven by the need for faster growth, but the growth rate is by no means a universal goal of optimal metabolism. Instead, optimisation goals at the cellular level are often multi-faceted and any given metabolic state must be considered in the context of both its energetic costs and benefits over a range of environmental contexts. For this purpose, we identify the conceptual framework of resource allocation as a potential testbed for the investigation of the cost-benefit balance of cellular metabolic strategies. Such a framework is also readily integrated with dynamical systems modelling, making it a promising avenue for new answers to the age-old question of why cells, from cancers to microbes, choose the metabolic strategies they do.",0,arxiv,Evrim,CC-BY/arXiv,Balancing the cellular budget: lessons in metabolism from microbes to cancer
"We propose a compartmental model for epidemiology wherein the population is split into groups with either comply or refuse to comply with protocols designed to slow the spread of a disease. Parallel to the disease spread, we assume that noncompliance with protocols spreads as a social contagion. We begin by deriving the reproductive ratio for a deterministic version of the model, and use this to fully characterize the local stability of disease free equilibrium points. We then append the deterministic model with stochastic effects, specifically assuming that the transmission rate of the disease and the transmission rate of the social contagion are uncertain. We prove global existence and nonnegativity for our stochastic model. Then using suitably constructed stochastic Lyapunov functions, we analyze the behavior of the stochastic system with respect to certain disease free states. We demonstrate all of our results with numerical simulations.",0,arxiv,Evrim,CC-BY/arXiv,A Compartmental Model for Epidemiology with Human Behavior and Stochastic Effects
"To understand how genetic variants in human genomes manifest in phenotypes -- traits like height or diseases like asthma -- geneticists have sequenced and measured hundreds of thousands of individuals. Geneticists use this data to build models that predict how a genetic variant impacts phenotype given genomic features of the variant, like DNA accessibility or the presence of nearby DNA-bound proteins. As more data and features become available, one might expect predictive models to improve. Unfortunately, training these models is bottlenecked by the need to solve expensive linear algebra problems because variants in the genome are correlated with nearby variants, requiring inversion of large matrices. Previous methods have therefore been restricted to fitting small models, and fitting simplified summary statistics, rather than the full likelihood of the statistical model. In this paper, we leverage modern fast linear algebra techniques to develop DeepWAS (Deep genome Wide Association Studies), a method to train large and flexible neural network predictive models to optimize likelihood. Notably, we find that larger models only improve performance when using our full likelihood approach; when trained by fitting traditional summary statistics, larger models perform no better than small ones. We find larger models trained on more features make better predictions, potentially improving disease predictions and therapeutic target identification.",0,arxiv,Evrim,CC-BY/arXiv,Training Flexible Models of Genetic Variant Effects from Functional Annotations using Accelerated Linear Algebra
"Phylogenetics is a fundamental component of evolutionary analysis frameworks in biology and linguistics. Recently, the advent of large-scale genomics and the SARS-CoV-2 pandemic has highlighted the necessity for phylogenetic software to handle large datasets. While significant efforts have focused on scaling optimisation algorithms, visualization, and lineage identification, an emerging body of research has been dedicated to efficient representations of data for genomes and phylogenetic trees. Compared to the traditional Newick format which represents trees using strings of nested parentheses, modern tree representations utilize integer vectors to define the tree topology traversal. This approach offers several advantages, including easier manipulation, increased memory efficiency, and applicability to machine learning.   Here, we present the latest release of phylo2vec (or Phylo2Vec), a high-performance software package for encoding, manipulating, and analysing binary phylogenetic trees. At its core, the package is based on the phylo2vec representation of binary trees, and is designed to enable fast sampling and tree comparison. This release features a core implementation in Rust for improved performance and memory efficiency, with wrappers in R and Python (superseding the original release), making it accessible to a broad audience in the bioinformatics community.",0,arxiv,Evrim,CC-BY/arXiv,phylo2vec: a library for vector-based phylogenetic tree manipulation
"Biological organisms thrive by adapting to their environments, which are often unpredictably changeable. We apply recent results from nonequilibrium physics to show that organisms' fitness parses into a static generalist component and a nonequilibrium tracking component. Our findings: (1) Environmental changes that are too fast or too small are not worth tracking. (2) Well-timed anticipatory tracking enhances fitness in coherent environments. (3) We compute and explain the optimal adaptive strategy for a system in a given environment, such as bet hedging or phenotypic memory. Conversely, (4) We compute and explain the optimal way for an environment to control a given system, for example for designing drug regimens to limit the growth of pathogens. By connecting fitness, adaptive strategy, and environmental variability, this work provides the foundations for a generic physical theory of adaptivity.",0,arxiv,Evrim,CC-BY/arXiv,Nonequilibrium Theory for Adaptive Systems in Varying Environments
"In this paper, we present new efficiently solvable cases of the Minimum Uncovering Branching problem, an optimization problem with applications in cancer genomics introduced by HujduroviÄ‡, HusiÄ‡, MilaniÄ, Rizzi, and Tomescu in 2018. The problem involves a family of finite sets, and the goal is to map each non-maximal set to exactly one set that contains it, minimizing the sum of uncovered elements across all sets in the family. HujduroviÄ‡ et al. formulated the problem in terms of branchings of the digraph formed by the proper set inclusion relation on the input sets and studied the problem complexity based on properties of the corresponding partially ordered set, in particular, with respect to its height and width, defined respectively as the maximum cardinality of a chain and an antichain. They showed that the problem is APX-complete for instances of bounded height and that a constant-factor approximation algorithm exists for instances of bounded width, but left the exact complexity for bounded-width instances open. In this paper, we answer this question by proving that the problem is solvable in polynomial time. We derive this result by examining the structural properties of optimal solutions and reducing the problem to computing maximum matchings in bipartite graphs and maximum weight antichains in partially ordered sets. We also introduce a new polynomially computable lower bound and identify another condition for polynomial-time solvability.",0,arxiv,Evrim,CC-BY/arXiv,Perfect phylogenies via the Minimum Uncovering Branching problem: efficiently solvable cases
"First discovered by L. R. Taylor (1961, Nature), Taylor's Power Law (TPL) correlates the mean (M) population abundances and the corresponding variances (V) across a set of insect populations using a power function (V=aM^b). TPL has demonstrated its 'universality' across numerous fields of sciences, social sciences, and humanities. This universality has inspired two main prongs of exploration: one from mathematicians and statisticians, who might instinctively respond with a convergence theorem similar to the central limit theorem of the Gaussian distribution, and another from biologists, ecologists, physicists, etc., who are more interested in potential underlying ecological or organizational mechanisms. Over the past six decades, TPL studies have produced a punctuated landscape with three relatively distinct periods (1960s-1980s; 1990s-2000s, and 2010s-2020s) across the two prongs of abstract and physical worlds. Eight themes have been identified and reviewed on this landscape, including population spatial aggregation and ecological mechanisms, TPL and skewed statistical distributions, mathematical/statistical mechanisms of TPL, sample vs. population TPL, population stability, synchrony, and early warning signals for tipping points, TPL on complex networks, TPL in macrobiomes, and in microbiomes. Three future research directions including fostering reciprocal interactions between the two prongs, heterogeneity measuring, and exploration in the context of evolution. The significance of TPL research includes practically, population fluctuations captured by TPL are relevant for agriculture, forestry, fishery, wildlife-conservation, epidemiology, tumor heterogeneity, earthquakes, social inequality, stock illiquidity, financial stability, tipping point events, etc.; theoretically, TPL is one form of power laws, which are related to phase transitions, universality, scale-invariance, etc.",0,arxiv,Evrim,CC-BY/arXiv,"Six Decades Post-Discovery of Taylor's Power Law: From Ecological and Statistical Universality, Through Prime Number Distributions and Tipping-Point Signals, to Heterogeneity and Stability of Complex Networks"
"Understanding functional diversity, the range and variability of species' roles and actions within their communities, is key to predicting and preserving the functions that sustain both nature and human well-being. In this paper, we provide a comprehensive review of the literature on functional diversity measurement. We begin by consolidating essential criteria that effective measures of functional diversity should meet. We then evaluate fifteen widely used functional diversity metrics against these criteria and assess their performance across six synthetic ecosystem scenarios where optimal behavior is known. Surprisingly, our analysis reveals that none of the widely used metrics fully satisfy all the established requirements, and all fail in at least one ecosystem scenario. In particular, we find that almost all metrics flagrantly violate set monotonicity and distance monotonicity, requirements that adding a novel species should increase diversity, and that the magnitude of that increase should grow with trait dissimilarity. We also find that metrics fail to decline when rare, functionally extreme species are lost, and even increase when a perfectly redundant species is added. These critical flaws leave them blind to the very biodiversity loss that functional diversity measures are intended to detect. Our findings underscore the urgent need to develop a new generation of functional diversity metrics that more accurately reflect ecological realities.",0,arxiv,Evrim,CC-BY/arXiv,Rethinking Ecological Measures Of Functional Diversity
"Understanding how biological organisms make decisions is of fundamental importance in understanding behavior. Such an understanding within evolutionary game theory so far has been sought by appealing to bounded rationality. Here, we present a perceptual rationality framework in the context of group cooperative interactions, where individuals make rational decisions based on their evolvable perception of the environment. We show that a simple public goods game accounts for power law distributed perceptual diversity. Incorporating the evolution of social information use into the framework reveals that rational decision-making is a natural root of the evolution of consistent personality differences and power-law distributed behavioral diversity. The behavioral diversity, core to the perceptual rationality approach, can lead to ever-shifting polymorphism or cyclic dynamics, through which different rational personality types coexist and engage in mutualistic, complementary, or competitive and exploitative relationships. This polymorphism can lead to non-monotonic evolution as external environmental conditions change. The framework provides predictions consistent with some large-scale eco-evolutionary patterns and illustrates how the evolution of social structure can modify large-scale eco-evolutionary patterns. Furthermore, consistent with most empirical evidence and in contrast to most theoretical predictions, our work suggests diversity is often detrimental to public good provision, especially in strong social dilemmas.",0,arxiv,Evrim,CC-BY/arXiv,Perceptual Rationality: An Evolutionary Game Theory of Perceptually Rational Decision-Making
"A major challenge for community ecology is using spatio-temporal data to infer parameters of dynamical models without conducting laborious experiments. We present a novel framework from statistical physics -- Maximum Caliber -- to characterize the temporal dynamics of complex ecological systems in spatially extended landscapes and infer parameters from empirical data. As an extension of Maximum Entropy modeling, Maximum Caliber aims at modeling the probability of possible trajectories of a stochastic system, rather than focusing on system states. We demonstrate the ability of the Maximum Caliber framework to capture ecological processes ranging from near- to far from- equilibrium, using an array of species interaction motifs including random interactions, apparent competition, intraguild predation, and non-transitive competition, along with dispersal among multiple patches. For spatio-temporal data of species occupancy in a metacommunity, the parameters of a Maximum Caliber model can be estimated through a simple logistic regression to reveal migration rates between patches, interactions between species, and local environmental suitabilities. We test the accuracy of the method over a range of system sizes and time periods, and find that these parameters can be estimated without bias. We introduce ``entropy production'' as a measure of irreversibility in system dynamics, and use ``pseudo-$R^2$'' to characterize predictability of future states. We show that our model can predict the dynamics of metacommunities that are far from equilibrium. The capacity to estimate basic parameters of dynamical metacommunity models from spatio-temporal data represents an important breakthrough for the study of metacommunities with application to practical problems in conservation and restoration ecology.",0,arxiv,Evrim,CC-BY/arXiv,Modeling and Inferring Metacommunity Dynamics with Maximum Caliber
"Adaptive control in biological systems, such as intestinal immunity, remains poorly understood despite detailed knowledge of underlying networks. We propose an alternative regulatory framework based on stochastic martingale turnover, where cells proliferate through mutual competition and decay without cell-type-specific regulation. Through stochastic simulations and mathematical analyses, we show that this process autonomously achieves balanced population states characterized by low decay probabilities. The dynamics are governed by a modified Langevin equation, in which conserved mass is replaced by a composition-dependent fitness variable. A random walk with step lengths that shorten near a target, together with its mathematical solution, demonstrates autonomous convergence of composition, due to the large number of possible microstates. These results suggest a biologically plausible, generic mechanism for adaptability.",0,arxiv,Evrim,CC-BY/arXiv,Self-Balancing of Cell Populations via Martingale Turnover with Amplification
"In seeking to understand the size of inbred pedigrees, J. Lachance (J. Theor. Biol. 261, 238-247, 2009) studied a population model in which, for a fixed value of $n$, each mating occurs between $n$th cousins. We explain a connection between the second-cousin case of the model ($n=2$) and the Fibonacci sequence, and more generally, between the $n$th-cousin case and the $n$-anacci sequence $(n \geq 2)$. For a model with $n$th-cousin mating $(n \geq 1)$, we obtain the generating function describing the size of the pedigree $t$ generations back from the present, and we use it to evaluate the asymptotic growth of the pedigree size. In particular, we show that the growth of the pedigree asymptotically follows the growth rate of the $n$-anacci sequence -- the golden ratio $Ï†= (1 + \sqrt{5})/2 \approx 1.6180$ in the second-cousin case $n=2$ -- and approaches 2 as $n$ increases. The computations explain the appearance of familiar numerical sequences and constants in a pedigree model. They also recall similar appearances of such sequences and constants in studies of population biology more generally.",0,arxiv,Evrim,CC-BY/arXiv,An nth-cousin mating model and the n-anacci numbers
"In phylogenomics, species-tree methods must contend with two major sources of noise; stochastic gene-tree variation under the multispecies coalescent model (MSC) and finite-sequence substitutional noise. Fast agglomerative methods such as GLASS, STEAC, and METAL combine multi-locus information via distance-based clustering. We derive the exact covariance matrix of these pairwise distance estimates under a joint MSC-plus-substitution model and leverage it for reliable confidence estimation, and we algebraically decompose it into components attributable to coalescent variation versus sequence-level stochasticity. Our theory identifies parameter regimes where one source of variance greatly exceeds the other. For both very low and very high mutation rates, substitutional noise dominates, while coalescent variance is the primary contributor at intermediate mutation rates. Moreover, the interval over which coalescent variance dominates becomes narrower as the species-tree height increases. These results imply that in some settings one may legitimately ignore the weaker noise source when designing methods or collecting data. In particular, when gene-tree variance is dominant, adding more loci is most beneficial, while when substitution noise dominates, longer sequences or imputation are needed. Finally, leveraging the derived covariance matrix, we implement a Gaussian-sampling procedure to generate split support values for METAL trees and demonstrate empirically that this approach yields more reliable confidence estimates than traditional bootstrapping.",0,arxiv,Evrim,CC-BY/arXiv,Covariance Decomposition for Distance Based Species Tree Estimation
"We explore how strategic leaps alter the classic rock-paper-scissors dynamics in spatially structured populations. In our model, individuals can expend energy reserves to jump toward regions with a high density of individuals of the species they dominate in the spatial game. This enables them to eliminate the target organisms and gain new territory, promoting species proliferation. Through stochastic, lattice-based simulations, we show that even when the energy allocated to jumping, as opposed to random walking, is low, there is a significant shift in the cyclic dominance balance. This arises from the increased likelihood of the leaping species successfully acquiring territory. Due to the cyclical nature of the game, the dominant species becomes the one that is superior to the jumping species. We investigate how spatial patterns are affected and calculate the changes in characteristic length scales. Additionally, we quantify how saltatory targeting reshapes spatial correlations and drives shifts in population dominance. Finally, we estimate the coexistence probability and find evidence that this behavioural strategy may promote biodiversity among low-mobility organisms but jeopardise long-term coexistence in the case of high-mobility dispersal. These results underscore the profound impact of novel foraging tactics on community structure and provide concrete parameters for ecologists seeking to incorporate behavioural innovation into ecosystem models.",0,arxiv,Evrim,CC-BY/arXiv,Saltatory targeting strategy in rock-paper-scissors models
"The bizarre trident-like cephalic projections of Walliserops trifurcatus have previously been interpreted as sexually selected weapons for intraspecific combat. We propose an alternative hypothesis grounded in biomechanics and collective behavior, that tridents evolved as adaptations for hydrodynamic lift and queue stability, conferring energetic advantages during group locomotion. Under this hypothesis, lift could offset gravitational forces, enabling greater locomotor efficiency, while mechanically linked formations, where tridents rested on the pygidia of leading individuals, enhanced pitch and roll stability and minimized costly accelerations and collisions. These formations also facilitated hydrodynamic drafting, allowing weaker individuals to conserve energy and remain integrated within the group. The trident's structure, though inefficient for solitary lift or combat, functioned effectively in cooperative formations, suggesting that its original selective advantage lay not in individual performance but in enhancing group cohesion and efficiency. Rather than emerging from competitive pressures alone, the trident may have arisen through selection for coordinated, cooperative movement -- potentially representing a precursor stage to traits later exapted for sexual selection.",0,arxiv,Evrim,CC-BY/arXiv,Trilobite tridents: hydrodynamic lift and stability mechanisms for queue formation
"Following genetic ancestry in eukaryote populations poses several open problems due to sexual reproduction and recombination. The history of extant genetic material is usually modeled backwards in time, but tracking chromosomes at a large scale is not trivial, as successive recombination events break them into several segments. For this reason, the behavior of the distribution of genetic segments across the ancestral population is not fully understood. Moreover, as individuals transmit only half of their genetic content to their offspring, after a few generations, it is possible that ghosts arise, that is, genealogical ancestors that transmit no genetic material to any individual.   While several theoretical predictions exist to estimate properties of ancestral segments or ghosts, most of them rely on simplifying assumptions such as an infinite population size or an infinite chromosome length. It is not clear how well these results hold in a finite universe, and current simulators either make other approximations or cannot handle the scale required to answer these questions. In this work, we use an exact back-in-time simulator of large diploid populations experiencing recombination that tracks genealogical and genetic ancestry, without approximations. We focus on the distinction between genealogical and genetic ancestry and, additionally, we explore the effects of genome structure on ancestral segment distribution and the proportion of genetic ancestors. Our study reveals that some of the theoretical predictions hold well in practice, but that, in several cases, it highlights discrepancies between theoretical predictions assuming infinite parameters and empirical results in finite populations, emphasizing the need for cautious application of mathematical models in biological contexts.",0,arxiv,Evrim,CC-BY/arXiv,Eukaryotic ancestry in a finite world
"Tuberculosis (TB) continues to pose a major public health challenge, particularly in high-burden regions such as Ethiopia, necessitating a more profound understanding of its transmission dynamics. In this study, we developed an SVEITRS compartmental model to investigate the transmission dynamics of TBs, utilizing real data from Ethiopia from 2011-2021. Model parameters were estimated via two methods: nonlinear least squares and maximum likelihood, with maximum likelihood providing more accurate and reliable results, as confirmed by a test case. The model's stability analysis indicated that there is a disease-free equilibrium in areas where the basic reproduction number ($\mathscr{R}_0$) is less than one. The results suggest that optimal conditions could lead to the elimination of TB. On the other hand, there is an endemic equilibrium in areas where $\mathscr{R}_0$ is greater than one, which means that the disease is still present. Sensitivity analysis revealed important factors affecting TB levels: higher natural death rates, vaccination rates, treatment rates, and disease-related death rates lower TB cases, whereas higher recruitment rates, contact rates, infection rates, and loss of vaccine protection increase its spread. These findings highlights to the necessity of enhancing vaccination, treatment, and recovery strategies while addressing drivers of transmission to achieve TB control in Ethiopia. This study provides useful advice for guiding TB control efforts and public health interventions in Ethiopia and similar regions.",0,arxiv,Evrim,CC-BY/arXiv,Modeling Transmission Dynamics of Tuberculosis: Parameter Estimation and Sensitivity Analysis Using Real-World Data
"Evolutionary graph theory is the study of evolutionary dynamics in structured populations. A well-known problem in evolutionary graph theory is that the spread of mutation (measured by fixation probability) is impacted by the graph type chosen and the update rule. For example, the star graph is an amplifier of natural selection under the birth-death with fitness on birth (Bd) update rule but a suppressor of natural selection under the death-birth with fitness on birth (dB) update rule. A continuous-time EGT model has been found to replicate Bd and dB results as special cases. Using this model, we show that changing the natural (intrinsic) death rate can cause a shift from Bd to dB dynamics. Assuming the mutant is advantageous, we show that if the natural death rate is greater than $\frac{1}{\sqrt{N}}$ the star is a suppressor, where $N$ is the number of nodes. As $N \longrightarrow \infty$, the natural death rate required to drive the star to a suppressor tends towards zero, so as the size of the graph increases, the star graph is likely to be suppressing for any non-zero natural death rate.",0,arxiv,Evrim,CC-BY/arXiv,Understanding the rift between update rules in Evolutionary Graph Theory: The intrinsic death rate drives star graphs from amplifying to suppressing natural selection
"The introduction of the rotavirus vaccine in the United Kingdom (UK) in 2013 led to a noticeable decline in laboratory reports in subsequent years. To assess the impact of vaccination on rotavirus transmissibility we calibrated a stochastic compartmental epidemiological model using Sequential Monte Carlo (SMC) methods. Our analysis focuses on estimating the time-varying transmissibility parameter and documenting its evolution before and after vaccine rollout. We observe distinct periods of increasing and decreasing transmissibility, reflecting the dynamic response of rotavirus spread to immunization efforts. These findings improve our understanding of vaccination-driven shifts in disease transmission and provide a quantitative framework for evaluating long-term epidemiological trends.",0,arxiv,Evrim,CC-BY/arXiv,Assessing the Impact of Vaccination on Rotavirus Transmission Dynamics Using Bayesian Inference
"Wastewater-based epidemiology (WBE) is a fast emerging method for passively monitoring diseases in a population. By measuring the concentrations of pathogenic materials in wastewater, WBE negates demographic biases in clinical testing and healthcare demand, and may act as a leading indicator of disease incidence.   For a WBE system to be effective, it should detect the presence of a new pathogen of concern early enough and with enough precision that it can still be localised and contained. In this study, then, we show how multiple wastewater sensors can be strategically placed across a wastewater system, to detect the presence of disease faster than if sampling was done at the wastewater treatment plant only. Our approach generalises to any tree-like network and takes into account the structure of the network and how the population is distributed over it.   We show how placing sensors further upstream from the treatment plant improves detection sensitivity and can inform how an outbreak is evolving in different geographical regions. However, this improvement diminishes once individual-level shedding is modelled as highly dispersed. With overdispersed shedding, we show using real COVID-19 cases in Scotland that broad trends in disease incidence (i.e., whether the epidemic is in growth or decline) can still be reasonably estimated from the wastewater signal once incidence exceeds about 5 infections per day.",0,arxiv,Evrim,CC-BY/arXiv,Improving wastewater-based epidemiology through strategic placement of samplers
"Invasive species are a growing threat to ecosystems, particularly in aquatic environments. The Trojan Y Chromosome (TYC) strategy is a promising biological method for reducing invasive populations by introducing genetically modified males (supermales) that produce only male offspring, leading to population decline due to a shortage of females. In this study, we develop a novel discrete--time, age--structured mathematical model to simulate the effects of this strategy. Our model divides the life cycle of species into two stages--egg and maturity--and tracks different sub--populations, including supermales. We analyze the equilibria of the system and prove the existence and stability of extinction and positive equilibrium points. Numerical simulations show that extinction depends on factors such as fecundity, the number of supermales released, and initial population sizes. The model also reveals complex behaviors, such as bistability and thresholds for population collapse. This discrete approach offers a useful framework for understanding and optimizing the TYC strategy and can help guide future field applications of invasive species control.",0,arxiv,Evrim,CC-BY/arXiv,Invasive species control via a discrete model for the Trojan Y-chromosome strategy
"Earthworms are key drivers of soil function, influencing organic matter turnover, nutrient cycling, and soil structure. Understanding the environmental controls on their distribution is essential for predicting the impacts of land use and climate change on soil ecosystems. While local studies have identified abiotic drivers of earthworm communities, broad-scale spatial patterns remain underexplored.   We developed a multi-species, multi-task deep learning model to jointly predict the distribution of 77 earthworm species across metropolitan France, using historical (1960-1970) and contemporary (1990-2020) records. The model integrates climate, soil, and land cover variables to estimate habitat suitability. We applied SHapley Additive exPlanations (SHAP) to identify key environmental drivers and used species clustering to reveal ecological response groups.   The joint model achieved high predictive performance (TSS >= 0.7) and improved predictions for rare species compared to traditional species distribution models. Shared feature extraction across species allowed for more robust identification of common and contrasting environmental responses. Precipitation variability, temperature seasonality, and land cover emerged as dominant predictors of earthworm distribution. Species clustering revealed distinct ecological strategies tied to climatic and land use gradients.   Our study advances both the methodological and ecological understanding of soil biodiversity. We demonstrate the utility of interpretable deep learning approaches for large-scale soil fauna modeling and provide new insights into earthworm habitat specialization. These findings support improved soil biodiversity monitoring and conservation planning in the face of global environmental change.",0,arxiv,Evrim,CC-BY/arXiv,Digging deeper: deep joint species distribution modeling reveals environmental drivers of Earthworm Communities
"We introduce a kinetic model that couples the movement of a population of individuals with the dynamics of a pathogen in the same population. The model is formally shown to generalize the well-known compartmental models in mathematical epidemiology. We prove the existence and uniqueness of solutions in appropriate spaces for particular instances of the model. We finish with some examples, and discuss possible applications of this modeling approach.",0,arxiv,Evrim,CC-BY/arXiv,Kinetic formulation of compartmental epidemic models
"Individuals who do not comply with public health safety measures pose a significant challenge to effective epidemic control, as their risky behaviours can undermine public health interventions. This is particularly relevant in urban environments because of their high population density and complex social interactions. In this study, we employ detailed contact networks, built using a data-driven approach, to examine the impact of non-compliant individuals on epidemic dynamics in three major Italian cities: Torino, Milano, and Palermo. We use a heterogeneous extension of the Susceptible-Infected-Recovered model that distinguishes between ordinary and non-compliant individuals, who are more infectious and/or more susceptible. By combining electoral data with recent findings on vaccine hesitancy, we obtain spatially heterogeneous distributions of non-compliance. Epidemic simulations demonstrate that even a small proportion of non-compliant individuals in the population can substantially increase the number of infections and accelerate the timing of their peak. Furthermore, the impact of non-compliance is greatest when disease transmission rates are moderate. Including the heterogeneous, data-driven distribution of non-compliance in the simulations results in infection hotspots forming with varying intensity according to the disease transmission rate. Overall, these findings emphasise the importance of monitoring behavioural compliance and tailoring public health interventions to address localised risks.",0,arxiv,Evrim,CC-BY/arXiv,A data-driven analysis of the impact of non-compliant individuals on epidemic diffusion in urban settings
"The spreading dynamics of infectious diseases is influenced by individual behaviours, which are in turn affected by the level of awareness about the epidemic. Modelling the co-evolution of disease transmission and behavioural changes within a population enables better understanding, prediction and control of epidemics. Here, our primary goal is to provide an overview of the most popular modelling approaches, ranging from compartmental mean-field to agent-based models, with a particular focus on how behavioural factors are incorporated into epidemic dynamics. We classify modelling approaches based on the fundamental conceptual distinction between models of behaviours and models of behavioural determinants (such as awareness, beliefs, opinions, or trust); in particular, we observe that most studies model and interpret the variables related to individual responses either as behaviours or as determinants, with the implicit assumption that they correlate linearly. Based on preliminary empirical observations, we then challenge this assumption by analysing a recent dataset about time series of social indicators, collected during the COVID-19 pandemic. We examine the case study of Italian regions and we discover that behavioural responses are poorly explained by awareness, beliefs or trust, thereby calling for a careful interpretation of the modelling assumptions and for the development of further models, which fully account for the inherent complexity of individual responses and human behaviours.",0,arxiv,Evrim,CC-BY/arXiv,Recent trends in socio-epidemic modelling: behaviours and their determinants
"Inspired by Fisher's geometric approach to study beneficial mutations, we analyse probabilities of beneficial mutation and crossover recombination of strings in a general Hamming space with arbitrary finite alphabet. Mutations and recombinations that reduce the distance to an optimum are considered as beneficial. Geometric and combinatorial analysis is used to derive closed-form expressions for transition probabilities between spheres around an optimum giving a complete description of Markov evolution of distances from an optimum over multiple generations. This paves the way for optimization of parameters of mutation and recombination operators. Here we derive optimality conditions for mutation and recombination radii maximizing the probabilities of mutation and crossover into the optimum. The analysis highlights important differences between these evolutionary operators. While mutation can potentially reach any part of the search space, the probability of beneficial mutation decreases with distance to an optimum, and the optimal mutation radius or rate should also decrease resulting in a slow-down of evolution near the optimum. Crossover recombination, on the other hand, acts in a subspace of the search space defined by the current population of strings. However, probabilities of beneficial and deleterious crossover are balanced, and their characteristics, such as variance, are translation invariant in a Hamming space, suggesting that recombination may complement mutation and boost the rate of evolution near the optimum.",0,arxiv,Evrim,CC-BY/arXiv,Analysis and Optimization of Probabilities of Beneficial Mutation and Crossover Recombination in a Hamming Space
"Knowing which parts of a complex system have identical roles simplifies computations and reveals patterns in its network structure. Group theory has been applied to study symmetries in unweighted networks. However, in real-world weighted networks, edge weights are rarely equal, making exact symmetry uncommon. To study symmetries in weighted networks, we aggregate edge weights into a small number of discrete categories. The symmetries of these aggregated networks identify vertices with similar roles in the original weighted network.   In food webs, this approach helps to quantify ecological co-existence and competition by assessing the functional substitutability of species. We apply our method to 250 empirical food webs, finding that symmetric vertices emerge even under weak approximations, typically forming small orbits of size two or three. These symmetric vertices can appear at any trophic level or network position. We also apply three symmetry measures to compare structural patterns at the network level.",0,arxiv,Evrim,CC-BY/arXiv,Symmetries of weighted networks: weight approximation method and its application to food webs
"Coalescent models of bifurcating genealogies are used to infer evolutionary parameters from molecular data. However, there are many situations where bifurcating genealogies do not accurately reflect the true underlying ancestral history of samples, and a multifurcating genealogy is required. The space of multifurcating genealogical trees, where nodes can have more than two descendants, is largely underexplored in the setting of coalescent inference. In this paper, we examine the space of rooted, ranked, and unlabeled multifurcating trees. We recursively enumerate the space and then construct a partial ordering which induces a lattice on the space of multifurcating ranked tree shapes. The lattice structure lends itself naturally to defining Markov chains that permit exploration on the space of multifurcating ranked tree shapes. Finally, we prove theoretical bounds for the mixing time of two Markov chains defined on the lattice, and we present simulation results comparing the distribution of trees and tree statistics under various coalescent models to the uniform distribution on this tree space.",0,arxiv,Evrim,CC-BY/arXiv,"The space of multifurcating ranked tree shapes: enumeration, lattice structure, and Markov chains"
"Enzymes are crucial catalysts that enable a wide range of biochemical reactions. Efficiently identifying specific enzymes from vast protein libraries is essential for advancing biocatalysis. Traditional computational methods for enzyme screening and retrieval are time-consuming and resource-intensive. Recently, deep learning approaches have shown promise. However, these methods focus solely on the interaction between enzymes and reactions, overlooking the inherent hierarchical relationships within each domain. To address these limitations, we introduce FGW-CLIP, a novel contrastive learning framework based on optimizing the fused Gromov-Wasserstein distance. FGW-CLIP incorporates multiple alignments, including inter-domain alignment between reactions and enzymes and intra-domain alignment within enzymes and reactions. By introducing a tailored regularization term, our method minimizes the Gromov-Wasserstein distance between enzyme and reaction spaces, which enhances information integration across these domains. Extensive evaluations demonstrate the superiority of FGW-CLIP in challenging enzyme-reaction tasks. On the widely-used EnzymeMap benchmark, FGW-CLIP achieves state-of-the-art performance in enzyme virtual screening, as measured by BEDROC and EF metrics. Moreover, FGW-CLIP consistently outperforms across all three splits of ReactZyme, the largest enzyme-reaction benchmark, demonstrating robust generalization to novel enzymes and reactions. These results position FGW-CLIP as a promising framework for enzyme discovery in complex biochemical settings, with strong adaptability across diverse screening scenarios.",0,arxiv,Biyoloji,CC-BY/arXiv,Fused Gromov-Wasserstein Contrastive Learning for Effective Enzyme-Reaction Screening
"DEAD-box RNA helicases (DDXs) are essential RNA metabolism regulators that typically unwind dsRNA in an ATP-dependent manner. However, recent studies show some DDXs can also unwind dsRNA without ATP, a phenomenon that remains poorly understood. Here, we developed HelixTriad coarse-grained RNA model, incorporating Watson-Crick base pairing, base stacking, and electrostatics within a three-bead-per-nucleotide scheme to accurately reproduce experimental RNA melting curves. Molecular dynamics simulations showed that weak, specific DDX3X-dsRNA interactions drive stochastic strand separation without ATP. Free energy analysis revealed that successful unwinding via high-entropy, stand-displacing intermediates. Furthermore, we introduced Entropy-Unet, a deep learning framework for entropy prediction, which corroborated theoretical estimates and uncovered a hierarchical pattern of entropy contributions. Together, our findings suggest that ATP-independent dsRNA unwinding by DDXs is predominantly entropy-driven, offering new mechanistic insights into RNA helicases versatility.",0,arxiv,Biyoloji,CC-BY/arXiv,Integrating Coarse-Grained Simulations and Deep Learning to Unveil Entropy-Driven dsRNA Unwinding by DDX3X
"Coarse-grained (CG) modeling enables molecular simulations to reach time and length scales inaccessible to fully atomistic methods. For classical CG models, the choice of mapping, that is, how atoms are grouped into CG sites, is a major determinant of accuracy and transferability. At the same time, the emergence of machine learning potentials (MLPs) offers new opportunities to build CG models that can in principle learn the true potential of the mean force for any mapping. In this work, we systematically investigate how the choice of mapping influences the representations learned by equivariant MLPs by studying liquid hexane, amino acids, and polyalanine. We find that when the length scales of bonded and nonbonded interactions overlap, unphysical bond permutations can occur. We also demonstrate that correctly encoding species and maintaining stereochemistry are crucial, as neglecting either introduces unphysical symmetries. Our findings provide practical guidance for selecting CG mappings compatible with modern architectures and guide the development of transferable CG models.",0,arxiv,Biyoloji,CC-BY/arXiv,Mapping Still Matters: Coarse-Graining with Machine Learning Potentials
"Accurate prediction of protein-protein binding affinity is vital for understanding molecular interactions and designing therapeutics. We adapt Boltz-2, a state-of-the-art structure-based protein-ligand affinity predictor, for protein-protein affinity regression and evaluate it on two datasets, TCR3d and PPB-affinity. Despite high structural accuracy, Boltz-2-PPI underperforms relative to sequence-based alternatives in both small- and larger-scale data regimes. Combining embeddings from Boltz-2-PPI with sequence-based embeddings yields complementary improvements, particularly for weaker sequence models, suggesting different signals are learned by sequence- and structure-based models. Our results echo known biases associated with training with structural data and suggest that current structure-based representations are not primed for performant affinity prediction.",0,arxiv,Biyoloji,CC-BY/arXiv,On fine-tuning Boltz-2 for protein-protein affinity prediction
"Understanding how protein mutations affect protein structure is essential for advancements in computational biology and bioinformatics. We introduce PRIMRose, a novel approach that predicts energy values for each residue given a mutated protein sequence. Unlike previous models that assess global energy shifts, our method analyzes the localized energetic impact of double amino acid insertions or deletions (InDels) at the individual residue level, enabling residue-specific insights into structural and functional disruption. We implement a Convolutional Neural Network architecture to predict the energy changes of each residue in a protein mutation. We train our model on datasets constructed from nine proteins, grouped into three categories: one set with exhaustive double InDel mutations, another with approximately 145k randomly sampled double InDel mutations, and a third with approximately 80k randomly sampled double InDel mutations. Our model achieves high predictive accuracy across a range of energy metrics as calculated by the Rosetta molecular modeling suite and reveals localized patterns that influence model performance, such as solvent accessibility and secondary structure context. This per-residue analysis offers new insights into the mutational tolerance of specific regions within proteins and provides higher interpretable and biologically meaningful predictions of InDels' effects.",0,arxiv,Biyoloji,CC-BY/arXiv,PRIMRose: Insights into the Per-Residue Energy Metrics of Proteins with Double InDel Mutations using Deep Learning
"Accurate prediction of protein function is essential for elucidating molecular mechanisms and advancing biological and therapeutic discovery. Yet experimental annotation lags far behind the rapid growth of protein sequence data. Computational approaches address this gap by associating proteins with Gene Ontology (GO) terms, which encode functional knowledge through hierarchical relations and textual definitions. However, existing models often emphasize one modality over the other, limiting their ability to generalize, particularly to unseen or newly introduced GO terms that frequently arise as the ontology evolves, and making the previously trained models outdated. We present STAR-GO, a Transformer-based framework that jointly models the semantic and structural characteristics of GO terms to enhance zero-shot protein function prediction. STAR-GO integrates textual definitions with ontology graph structure to learn unified GO representations, which are processed in hierarchical order to propagate information from general to specific terms. These representations are then aligned with protein sequence embeddings to capture sequence-function relationships. STAR-GO achieves state-of-the-art performance and superior zero-shot generalization, demonstrating the utility of integrating semantics and structure for robust and adaptable protein function prediction. Code is available at https://github.com/boun-tabi-lifelu/stargo.",0,arxiv,Biyoloji,CC-BY/arXiv,STAR-GO: Improving Protein Function Prediction by Learning to Hierarchically Integrate Ontology-Informed Semantic Embeddings
"Understanding the phase behaviour of pheromones and other messaging molecules remains a significant and largely unexplored challenge, even though it plays a central role in chemical communication. Here, we present all-atom molecular dynamics simulations to investigate the behavior of bombykol, a model insect pheromone, adsorbed at the water-air interface. This system serves as a proxy for studying the amphiphilic nature of pheromones and their interactions with aerosol particles in the atmosphere. Our simulations reveal the molecular organization of the bombykol monolayer and its adsorption isotherm. A soft-sticky particle equation of state accurately describes the monolayer's behavior. The analysis uncovers a two-dimensional liquid-gas phase transition within the monolayer. Collective adsorption stabilises the molecules at the interface and the calculated free energy gain is approximately $2\:k_\mathrm{B}T$. This value increases under lower estimates of the condensing surface concentration, thereby enhancing pheromone adsorption onto aerosols. Overall, our findings hold broad relevance for molecular interface science, atmospheric chemistry, and organismal chemical communication, particularly in highlighting the critical role of phase transition phenomena.",0,arxiv,Biyoloji,CC-BY/arXiv,Collective adsorption of pheromones at the water-air interface
"The function of biomolecules such as proteins depends on their ability to interconvert between a wide range of structures or ""conformations."" Researchers have endeavored for decades to develop computational methods to predict the distribution of conformations, which is far harder to determine experimentally than a static folded structure. We present ConforMix, an inference-time algorithm that enhances sampling of conformational distributions using a combination of classifier guidance, filtering, and free energy estimation. Our approach upgrades diffusion models -- whether trained for static structure prediction or conformational generation -- to enable more efficient discovery of conformational variability without requiring prior knowledge of major degrees of freedom. ConforMix is orthogonal to improvements in model pretraining and would benefit even a hypothetical model that perfectly reproduced the Boltzmann distribution. Remarkably, when applied to a diffusion model trained for static structure prediction, ConforMix captures structural changes including domain motion, cryptic pocket flexibility, and transporter cycling, while avoiding unphysical states. Case studies of biologically critical proteins demonstrate the scalability, accuracy, and utility of this method.",0,arxiv,Biyoloji,CC-BY/arXiv,Unlocking hidden biomolecular conformational landscapes in diffusion models at inference time
"Proteins and nucleic acids form non-Newtonian liquids with complex rheological properties that contribute to their function in vivo. Here we investigate the rheology of the transcription factor NANOG, a key protein in sustaining embryonic stem cell self-renewal. We discover that at high concentrations NANOG forms macroscopic aging gels through its intrinsically disordered tryptophan-rich domain. By combining molecular dynamics simulations, mass photometry and Cryo-EM, we also discover that NANOG forms self-limiting micelle-like clusters which expose their DNA-binding domains. In dense solutions of DNA, NANOG micelle-like structures stabilize intermolecular entanglements and crosslinks, forming microgel-like structures. Our findings suggest that NANOG may contribute to regulate gene expression in a unconventional way: by restricting and stabilizing genome dynamics at key transcriptional sites through the formation of an aging microgel-like structure, potentially enabling mechanical memory in the gene network.",0,arxiv,Biyoloji,CC-BY/arXiv,Modulation of DNA rheology by a transcription factor that forms aging microgels
"Accurately predicting protein fitness with minimal experimental data is a persistent challenge in protein engineering. We introduce PRIMO (PRotein In-context Mutation Oracle), a transformer-based framework that leverages in-context learning and test-time training to adapt rapidly to new proteins and assays without large task-specific datasets. By encoding sequence information, auxiliary zero-shot predictions, and sparse experimental labels from many assays as a unified token set in a pre-training masked-language modeling paradigm, PRIMO learns to prioritize promising variants through a preference-based loss function. Across diverse protein families and properties-including both substitution and indel mutations-PRIMO outperforms zero-shot and fully supervised baselines. This work underscores the power of combining large-scale pre-training with efficient test-time adaptation to tackle challenging protein design tasks where data collection is expensive and label availability is limited.",0,arxiv,Biyoloji,CC-BY/arXiv,Few-shot Protein Fitness Prediction via In-context Learning and Test-time Training
"While data augmentation is widely used to train symmetry-agnostic models, it remains unclear how quickly and effectively they learn to respect symmetries. We investigate this by deriving a principled measure of equivariance error that, for convex losses, calculates the percent of total loss attributable to imperfections in learned symmetry. We focus our empirical investigation to 3D-rotation equivariance on high-dimensional molecular tasks (flow matching, force field prediction, denoising voxels) and find that models reduce equivariance error quickly to $\leq$2\% held-out loss within 1k-10k training steps, a result robust to model and dataset size. This happens because learning 3D-rotational equivariance is an easier learning task, with a smoother and better-conditioned loss landscape, than the main prediction task. For 3D rotations, the loss penalty for non-equivariant models is small throughout training, so they may achieve lower test loss than equivariant models per GPU-hour unless the equivariant ``efficiency gap'' is narrowed. We also experimentally and theoretically investigate the relationships between relative equivariance error, learning gradients, and model parameters.",0,arxiv,Biyoloji,CC-BY/arXiv,Training Dynamics of Learning 3D-Rotational Equivariance
"High-quality training datasets are crucial for the development of effective protein design models, but existing synthetic datasets often include unfavorable sequence-structure pairs, impairing generative model performance. We leverage ProteinMPNN, whose sequences are experimentally favorable as well as amenable to folding, together with structure prediction models to align high-quality synthetic structures with recoverable synthetic sequences. In that way, we create a new dataset designed specifically for training expressive, fully atomistic protein generators. By retraining La-Proteina, which models discrete residue type and side chain structure in a continuous latent space, on this dataset, we achieve new state-of-the-art results, with improvements of +54% in structural diversity and +27% in co-designability. To validate the broad utility of our approach, we further introduce Proteina Atomistica, a unified flow-based framework that jointly learns the distribution of protein backbone structure, discrete sequences, and atomistic side chains without latent variables. We again find that training on our new sequence-structure data dramatically boosts benchmark performance, improving \method's structural diversity by +73% and co-designability by +5%. Our work highlights the critical importance of aligned sequence-structure data for training high-performance de novo protein design models. All data will be publicly released.",0,arxiv,Biyoloji,CC-BY/arXiv,Consistent Synthetic Sequences Unlock Structural Diversity in Fully Atomistic De Novo Protein Design
"We introduce a computational framework for generating realistic transition paths between distinct conformations of large bio-molecular systems. The method is built on a stochastic integro-differential formulation derived from the Langevin bridge formalism, which constrains molecular trajectories to reach a prescribed final state within a finite time and yields an efficient low-temperature approximation of the exact bridge equation. To obtain physically meaningful protein transitions, we couple this formulation to a new coarse-grained potential combining a Go-like term that preserves native backbone geometry with a Rouse-type elastic energy term from polymer physics; we refer to the resulting approach as SIDE. We evaluate SIDE on several proteins undergoing large-scale conformational changes and compare its performance with established methods such as MinActionPath and EBDIMS. SIDE generates smooth, low-energy trajectories that maintain molecular geometry and frequently recover experimentally supported intermediate states. Although challenges remain for highly complex motions-largely due to the simplified coarse-grained potential-our results demonstrate that SIDE offers a powerful and computationally efficient strategy for modeling bio-molecular conformational transitions.",0,arxiv,Biyoloji,CC-BY/arXiv,Realistic Transition Paths for Large Biomolecular Systems: A Langevin Bridge Approach
"Red blood cells (RBCs) sustain mechanical stresses associated with microcirculatory flow through ATP-driven plasma membrane flickering. This is an active phenomenon driven by motor proteins that regulate interactions between the spectrin cytoskeleton and the lipid bilayer; it is manifested in RBC shape fluctuations reflecting the cell's mechanical and metabolic state. Yet, direct quantification of the forces and energetic costs underlying this non-equilibrium behavior remains challenging due to the invasiveness of existing techniques. Here, a minimally invasive method that combines bead-free, low-power optical tweezers with high-speed video microscopy was employed to track local membrane forces and displacements in single RBCs during the same time window. This independent dual-channel measurement enabled the construction of a mechano-dynamic phase space for RBCs under different chemical treatments, that allowed for differentiating between metabolic and structural states based on their fluctuation-force signatures. Quantification of mechanical work during flickering demonstrated that membrane softening enhanced fluctuations while elevating energy dissipation. The proposed optical tweezers methodology provides a robust framework for mapping the active mechanics of living cells, enabling precise probing of cellular physiology and detection of biomechanical dysfunction in diseases.",0,arxiv,Biyoloji,CC-BY/arXiv,Active Force Dynamics in Red Blood Cells Under Non-Invasive Optical Tweezers
"Wearable electronics hold great potential in defining new paradigms of modern healthcare, including personalized health management, precision medicine, and athletic performance optimization. This stems from their ability in enabling continuous, real-time health monitoring. To enable molecular-level analysis, biofluids rich in molecular analytes have become one of the most important target samples for wearable sensors. Among them, sweat stands out as an ideal candidate for next-generation wearable health monitoring platforms due to its completely noninvasive nature and ease of acquisition. In recent years, several studies have demonstrated feasible prototype designs for sweat-based wearable sensors. However, one of the major gaps toward large-scale commercialization is the development of clinically validated standards for sweat analysis. One key requirement is to establish the relationship between sweat analytes and those of blood, the latter serving as the gold standard in modern diagnostics. This review provides an overview of sweat biomarkers, with a particular focus on their partitioning mechanisms, which reveal the underlying connections between sweat analytes and their counterparts within systemic metabolic pathways. In addition, this review offers a mechanistic-level examination of biosensors employed in sweat sensing, addressing a gap that has not been adequately covered in prior reviews. Given the critical role of electronic systems in constructing highly integrated wearable sweat-monitoring platforms, this review also analyzes the electronic architectures used for sensor signal processing from an interdisciplinary perspective, with particular emphasis on the analog circuitry that interfaces with electrochemical sensors.",0,arxiv,Biyoloji,CC-BY/arXiv,A Review of Wearable Sweat Monitoring Platforms: From Biomarker Detection to Signal Processing Systems
"Advancements in AI for science unlocks capabilities for critical drug discovery tasks such as protein-ligand binding affinity prediction. However, current models overfit to existing oversimplified datasets that does not represent naturally occurring and biologically relevant proteins with modifications. In this work, we curate a complete and modification-aware version of the widely used DAVIS dataset by incorporating 4,032 kinase-ligand pairs involving substitutions, insertions, deletions, and phosphorylation events. This enriched dataset enables benchmarking of predictive models under biologically realistic conditions. Based on this new dataset, we propose three benchmark settings-Augmented Dataset Prediction, Wild-Type to Modification Generalization, and Few-Shot Modification Generalization-designed to assess model robustness in the presence of protein modifications. Through extensive evaluation of both docking-free and docking-based methods, we find that docking-based model generalize better in zero-shot settings. In contrast, docking-free models tend to overfit to wild-type proteins and struggle with unseen modifications but show notable improvement when fine-tuned on a small set of modified examples. We anticipate that the curated dataset and benchmarks offer a valuable foundation for developing models that better generalize to protein modifications, ultimately advancing precision medicine in drug discovery. The benchmark is available at: https://github.com/ZhiGroup/DAVIS-complete",0,arxiv,Biyoloji,CC-BY/arXiv,Towards Precision Protein-Ligand Affinity Prediction Benchmark: A Complete and Modification-Aware DAVIS Dataset
"Machine learning technologies for protein function prediction are black box models. Despite their potential to identify key drug targets with high accuracy and accelerate therapy development, the adoption of these methods depends on verifying their findings. This study evaluates DeepFRI, a leading Graph Convolutional Network (GCN) based tool, using advanced explainability techniques (GradCAM, Excitation Backpropagation, and PGExplainer) and adversarial robustness tests. Our findings reveal that the model's predictions often prioritize conserved motifs over truly deterministic residues, complicating the identification of functional sites. Quantitative analyses show that explainability methods differ significantly in granularity, with GradCAM providing broad relevance and PGExplainer pinpointing specific active sites. These results highlight tradeoffs between accuracy and interpretability, suggesting areas for improvement in DeepFRI's architecture to enhance its trustworthiness in drug discovery and regulatory settings.",0,arxiv,Biyoloji,CC-BY/arXiv,DeepFRI Demystified: Interpretability vs. Accuracy in AI Protein Function Prediction
"The constrained nature of synthesizable chemical space poses a significant challenge for sampling molecules that are both synthetically accessible and possess desired properties. In this work, we present PrexSyn, an efficient and programmable model for molecular discovery within synthesizable chemical space. PrexSyn is based on a decoder-only transformer trained on a billion-scale datastream of synthesizable pathways paired with molecular properties, enabled by a real-time, high-throughput C++-based data generation engine. The large-scale training data allows PrexSyn to reconstruct the synthesizable chemical space nearly perfectly at a high inference speed and learn the association between properties and synthesizable molecules. Based on its learned property-pathway mappings, PrexSyn can generate synthesizable molecules that satisfy not only single-property conditions but also composite property queries joined by logical operators, thereby allowing users to ``program'' generation objectives. Moreover, by exploiting this property-based querying capability, PrexSyn can efficiently optimize molecules against black-box oracle functions via iterative query refinement, achieving higher sampling efficiency than even synthesis-agnostic baselines, making PrexSyn a powerful general-purpose molecular optimization tool. Overall, PrexSyn pushes the frontier of synthesizable molecular design by setting a new state of the art in synthesizable chemical space coverage, molecular sampling efficiency, and inference speed.",0,arxiv,Biyoloji,CC-BY/arXiv,Efficient and Programmable Exploration of Synthesizable Chemical Space
"Accurate prediction of enzyme kinetic parameters is crucial for drug discovery, metabolic engineering, and synthetic biology applications. Current computational approaches face limitations in capturing complex enzyme-substrate interactions and often focus on single parameters while neglecting the joint prediction of catalytic turnover numbers (Kcat) and Michaelis-Menten constants (Km). We present EnzyCLIP, a novel dual-encoder framework that leverages contrastive learning and cross-attention mechanisms to predict enzyme kinetic parameters from protein sequences and substrate molecular structures. Our approach integrates ESM-2 protein language model embeddings with ChemBERTa chemical representations through a CLIP-inspired architecture enhanced with bidirectional cross-attention for dynamic enzyme-substrate interaction modeling. EnzyCLIP combines InfoNCE contrastive loss with Huber regression loss to learn aligned multimodal representations while predicting log10-transformed kinetic parameters. The model is trained on the CatPred-DB database containing 23,151 Kcat and 41,174 Km experimentally validated measurements, and achieved competitive performance with R2 scores of 0.593 for Kcat and 0.607 for Km prediction. XGBoost ensemble methods applied to the learned embeddings further improved Km prediction (R2 = 0.61) while maintaining robust Kcat performance.",0,arxiv,Biyoloji,CC-BY/arXiv,EnzyCLIP: A Cross-Attention Dual Encoder Framework with Contrastive Learning for Predicting Enzyme Kinetic Constants
"The interaction between proteins and nucleic acids is crucial for processes that sustain cellular function, including DNA maintenance and the regulation of gene expression and translation. Amino acid mutations in protein-nucleic acid complexes often lead to vital diseases. Experimental techniques have their own specific limitations in predicting mutational effects in protein-nucleic acid complexes. In this study, we compiled a large dataset of 1951 mutations including both protein-DNA and protein-RNA complexes and integrated structural and sequential features to build a deep learning-based regression model named DeepPNI. This model estimates mutation-induced binding free energy changes in protein-nucleic acid complexes. The structural features are encoded via edge-aware RGCN and the sequential features are extracted using protein language model ESM-2. We have achieved a high average Pearson correlation coefficient (PCC) of 0.76 in the large dataset via five-fold cross-validation. Consistent performance across individual dataset of protein-DNA, protein-RNA complexes, and different experimental temperature split dataset make the model generalizable. Our model showed good performance in complex-based five-fold cross-validation, which proved its robustness. In addition, DeepPNI outperformed in external dataset validation, and comparison with existing tools",0,arxiv,Biyoloji,CC-BY/arXiv,DeepPNI: Language- and graph-based model for mutation-driven protein-nucleic acid energetics
"Machine learning models for 3D molecular property prediction typically rely on atom-based representations, which may overlook subtle physical information. Electron density maps -- the direct output of X-ray crystallography and cryo-electron microscopy -- offer a continuous, physically grounded alternative. We compare three voxel-based input types for 3D convolutional neural networks (CNNs): atom types, raw electron density, and density gradient magnitude, across two molecular tasks -- protein-ligand binding affinity prediction (PDBbind) and quantum property prediction (QM9). We focus on voxel-based CNNs because electron density is inherently volumetric, and voxel grids provide the most natural representation for both experimental and computed densities. On PDBbind, all representations perform similarly with full data, but in low-data regimes, density-based inputs outperform atom types, while a shape-based baseline performs comparably -- suggesting that spatial occupancy dominates this task. On QM9, where labels are derived from Density Functional Theory (DFT) but input densities from a lower-level method (XTB), density-based inputs still outperform atom-based ones at scale, reflecting the rich structural and electronic information encoded in density. Overall, these results highlight the task- and regime-dependent strengths of density-derived inputs, improving data efficiency in affinity prediction and accuracy in quantum property modeling.",0,arxiv,Biyoloji,CC-BY/arXiv,Beyond Atoms: Evaluating Electron Density Representation for 3D Molecular Learning
"Generative artificial intelligence (AI) models learn probability distributions from data and produce novel samples that capture the salient properties of their training sets. Proteins are particularly attractive for such approaches given their abundant data and the versatility of their representations, ranging from sequences to structures and functions. This versatility has motivated the rapid development of generative models for protein design, enabling the generation of functional proteins and enzymes with unprecedented success. However, because these models mirror their training distribution, they tend to sample from its most probable modes, while low-probability regions, often encoding valuable properties, remain underexplored. To address this challenge, recent work has focused on guiding generative models to produce proteins with user-specified properties, even when such properties are rare or absent from the original training distribution. In this review, we survey and categorize recent advances in conditioning generative models for protein design. We distinguish approaches that modify model parameters, such as reinforcement learning or supervised fine-tuning, from those that keep the model fixed, including conditional generation, retrieval-augmented strategies, Bayesian guidance, and tailored sampling methods. Together, these developments are beginning to enable the steering of generative models toward proteins with desired, and often previously inaccessible, properties.",0,arxiv,Biyoloji,CC-BY/arXiv,"Guiding Generative Models for Protein Design: Prompting, Steering and Aligning"
"The Ribonucleic Acid (RNA) inverse folding problem, designing nucleotide sequences that fold into specific tertiary structures, is a fundamental computational biology problem with important applications in synthetic biology and bioengineering. The design of complex three-dimensional RNA architectures remains computationally demanding and mostly unresolved, as most existing approaches focus on secondary structures. In order to address tertiary RNA inverse folding, we present BeeRNA, a bio-inspired method that employs the Artificial Bee Colony (ABC) optimization algorithm. Our approach combines base-pair distance filtering with RMSD-based structural assessment using RhoFold for structure prediction, resulting in a two-stage fitness evaluation strategy. To guarantee biologically plausible sequences with balanced GC content, the algorithm takes thermodynamic constraints and adaptive mutation rates into consideration. In this work, we focus primarily on short and medium-length RNAs ($<$ 100 nucleotides), a biologically significant regime that includes microRNAs (miRNAs), aptamers, and ribozymes, where BeeRNA achieves high structural fidelity with practical CPU runtimes. The lightweight, training-free implementation will be publicly released for reproducibility, offering a promising bio-inspired approach for RNA design in therapeutics and biotechnology.",0,arxiv,Biyoloji,CC-BY/arXiv,BeeRNA: tertiary structure-based RNA inverse folding using Artificial Bee Colony
"Generative Flow Networks, or GFlowNets, offer a promising framework for molecular design, but their internal decision policies remain opaque. This limits adoption in drug discovery, where chemists require clear and interpretable rationales for proposed structures. We present an interpretability framework for SynFlowNet, a GFlowNet trained on documented chemical reactions and purchasable starting materials that generates both molecules and the synthetic routes that produce them. Our approach integrates three complementary components. Gradient based saliency combined with counterfactual perturbations identifies which atomic environments influence reward and how structural edits change molecular outcomes. Sparse autoencoders reveal axis aligned latent factors that correspond to physicochemical properties such as polarity, lipophilicity, and molecular size. Motif probes show that functional groups including aromatic rings and halogens are explicitly encoded and linearly decodable from the internal embeddings. Together, these results expose the chemical logic inside SynFlowNet and provide actionable and mechanistic insight that supports transparent and controllable molecular design.",0,arxiv,Biyoloji,CC-BY/arXiv,Interpreting GFlowNets for Drug Discovery: Extracting Actionable Insights for Medicinal Chemistry
"Designing new protein structures is fundamental to computational biology, enabling advances in therapeutic molecule discovery and enzyme engineering. Existing diffusion-based generative models typically operate in Cartesian coordinate space, where adding noise disrupts strict geometric constraints such as fixed bond lengths and angles, often producing physically invalid structures. To address this limitation, we propose a Torsion-Space Diffusion Model that generates protein backbones by denoising torsion angles, ensuring perfect local geometry by construction. A differentiable forward-kinematics module reconstructs 3D coordinates with fixed 3.8 Angstrom backbone bond lengths while a constrained post-processing refinement optimizes global compactness via Radius of Gyration (Rg) correction, without violating bond constraints. Experiments on standard PDB proteins demonstrate 100% bond-length accuracy and significantly improved structural compactness, reducing Rg error from 70% to 18.6% compared to Cartesian diffusion baselines. Overall, this hybrid torsion-diffusion plus geometric-refinement framework generates physically valid and compact protein backbones, providing a promising path toward full-atom protein generation.",0,arxiv,Biyoloji,CC-BY/arXiv,Torsion-Space Diffusion for Protein Backbone Generation with Geometric Refinement
"Neurodegenerative diseases are driven by the accumulation of protein aggregates in the brain of affected individuals. The aggregation behaviour in vitro is well understood and driven by the equilibration of a super-saturated protein solution to its aggregated equilibrium state. However, the situation is altered fundamentally in living systems where active processes consume energy to remove aggregates. It remains unclear how and why cells transition from a state with predominantly monomeric protein, which is stable over decades, to one dominated by aggregates. Here, we develop a simple but universal theoretical framework to describe cellular systems that include both aggregate formation and removal. Using a two-dimensional phase-plane representation, we show that the interplay of aggregate formation and removal generates cell-level bistability, with a bifurcation structure that explains both the emergence of disease and the effects of therapeutic interventions. We explore a wide range of aggregate formation and removal mechanisms and show that phenomena such as seeding arise robustly when a minimal set of requirements on the mechanism are satisfied. By connecting in vitro aggregation mechanisms to changes in cell state, our framework provides a general conceptual link between molecular-level therapeutic interventions and their impact on disease progression.",0,arxiv,Biyoloji,CC-BY/arXiv,A universal phase-plane model for in vivo protein aggregation
"Deep learning has emerged as a powerful framework for analyzing biomolecular dynamics trajectories, enabling efficient representations that capture essential system dynamics and facilitate mechanistic studies. We propose a neural network architecture incorporating Fourier Transform analysis to process trajectory data, achieving dual objectives: eliminating high-frequency noise while preserving biologically critical slow conformational dynamics, and establishing an isotropic representation space through the last hidden layer for enhanced dynamical quantification. Comparative protein simulations demonstrate our approach generates more uniform feature distributions than linear regression methods, evidenced by smoother state similarity matrices and clearer classification boundaries. Moreover, by using saliency score, we identified key structural determinants linked to effective energy landscapes governing system dynamics. We believe that the fusion of neural network features with physical order parameters creates a robust analytical framework for advancing biomolecular trajectory analysis.",0,arxiv,Biyoloji,CC-BY/arXiv,EscalNet: Learn isotropic representation space for biomolecular dynamics based on effective energy
"The Gromov-Wasserstein (GW) distance serves as a powerful tool for matching objects in metric spaces. However, its traditional formulation is constrained to pairwise matching between single objects, limiting its utility in scenarios and applications requiring multiple-to-one or multiple-to-multiple object matching. In this paper, we introduce the Joint Gromov-Wasserstein (JGW) objective and extend the original framework of GW to enable simultaneous matching between collections of objects. Our formulation provides a non-negative dissimilarity measure that identifies partially isomorphic distributions of mm-spaces, with point sampling convergence. We also show that the objective can be formulated and solved for point cloud object representations by adapting traditional algorithms in Optimal Transport, including entropic regularization. Our benchmarking with other variants of GW for partial matching indicates superior performance in accuracy and computational efficiency of our method, while experiments on both synthetic and real-world datasets show its effectiveness for multiple shape matching, including geometric shapes and biomolecular complexes, suggesting promising applications for solving complex matching problems across diverse domains, including computer graphics and structural biology.",0,arxiv,Biyoloji,CC-BY/arXiv,The Joint Gromov Wasserstein Objective for Multiple Object Matching
This research reports the entropy transfer throughout the tridimensional structure of PDZ-2 and TIM barrel structures using the dynamic Gaussian Network Model. The model predicts the allocation of the allosteric pathways of the PDZ-2. Moreover. A visualization analysis reveals that entropy and information is transported towards the effector site in PDZ-2 and near to the catalytic site of the TIM-Barrel protein. The results suggest the presence of a functional hierarchy that determine information and entropy flow directionality.,0,arxiv,Biyoloji,CC-BY/arXiv,Entropy Transfer Throughout the Structure of PDZ-2 and TIM-Barrel Proteins. A Dynamic Gaussian Network Model Study
"Reliable evaluation of protein structure predictions remains challenging, as metrics like pLDDT capture energetic stability but often miss subtle errors such as atomic clashes or conformational traps reflecting topological frustration within the protein folding energy landscape. We present CODE (Chain of Diffusion Embeddings), a self evaluating metric empirically found to quantify topological frustration directly from the latent diffusion embeddings of the AlphaFold3 series of structure predictors in a fully unsupervised manner. Integrating this with pLDDT, we propose CONFIDE, a unified evaluation framework that combines energetic and topological perspectives to improve the reliability of AlphaFold3 and related models. CODE strongly correlates with protein folding rates driven by topological frustration, achieving a correlation of 0.82 compared to pLDDT's 0.33 (a relative improvement of 148\%). CONFIDE significantly enhances the reliability of quality evaluation in molecular glue structure prediction benchmarks, achieving a Spearman correlation of 0.73 with RMSD, compared to pLDDT's correlation of 0.42, a relative improvement of 73.8\%. Beyond quality assessment, our approach applies to diverse drug design tasks, including all-atom binder design, enzymatic active site mapping, mutation induced binding affinity prediction, nucleic acid aptamer screening, and flexible protein modeling. By combining data driven embeddings with theoretical insight, CODE and CONFIDE outperform existing metrics across a wide range of biomolecular systems, offering robust and versatile tools to refine structure predictions, advance structural biology, and accelerate drug discovery.",0,arxiv,Biyoloji,CC-BY/arXiv,CONFIDE: Hallucination Assessment for Reliable Biomolecular Structure Prediction and Design
"Generative models for structure-based drug design are often limited to a specific modality, restricting their broader applicability. To address this challenge, we introduce FuncBind, a framework based on computer vision to generate target-conditioned, all-atom molecules across atomic systems. FuncBind uses neural fields to represent molecules as continuous atomic densities and employs score-based generative models with modern architectures adapted from the computer vision literature. This modality-agnostic representation allows a single unified model to be trained on diverse atomic systems, from small to large molecules, and handle variable atom/residue counts, including non-canonical amino acids. FuncBind achieves competitive in silico performance in generating small molecules, macrocyclic peptides, and antibody complementarity-determining region loops, conditioned on target structures. FuncBind also generated in vitro novel antibody binders via de novo redesign of the complementarity-determining region H3 loop of two chosen co-crystal structures. As a final contribution, we introduce a new dataset and benchmark for structure-conditioned macrocyclic peptide generation. The code is available at https://github.com/prescient-design/funcbind.",0,arxiv,Biyoloji,CC-BY/arXiv,Unified all-atom molecule generation with neural fields
"Microproteins are a newly recognized and rapidly growing class of small proteins, typically encoded by fewer than 100 to 150 codons and translated from small open reading frames (smORFs). Although research has shown that smORFs and their corresponding microproteins constitute a significant portion of the genome and proteome, there is still limited information available in the literature regarding the structural characteristics of microproteins. In this paper, we discuss the methods available for predicting their secondary and tertiary structures and provide examples of calculations done with three archetypical methods (AlphaFold, I TASSER and ROSETTA). We present results predicting the structures of 44 microproteins. For this set of microproteins the methods considered here show a reasonable agreement among them and with the very few cases in which experimental structures are available. None the less, the agreement with experimental structures is not as good as for larger proteins, indicating that it is necessary to obtain a much larger set of experimental microproteins structures to better evaluate and eventually calibrate prediction methods.",0,arxiv,Biyoloji,CC-BY/arXiv,Methods for Secondary and Tertiary Structure Prediction of Microproteins
"Ligand-based drug discovery (LBDD) relies on making use of known binders to a protein target to find structurally diverse molecules similarly likely to bind. This process typically involves a brute force search of the known binder (query) against a molecular library using some metric of molecular similarity. One popular approach overlays the pharmacophore-shape profile of the known binder to 3D conformations enumerated for each of the library molecules, computes overlaps, and picks a set of diverse library molecules with high overlaps. While this virtual screening workflow has had considerable success in hit diversification, scaffold hopping, and patent busting, it scales poorly with library sizes and restricts candidate generation to existing library compounds. Leveraging recent advances in voxel-based generative modelling, we propose a pharmacophore-based generative model and workflows that address the scaling and fecundity issues of conventional pharmacophore-based virtual screening. We introduce \emph{VoxCap}, a voxel captioning method for generating SMILES strings from voxelised molecular representations. We propose two workflows as practical use cases as well as benchmarks for pharmacophore-based generation: \emph{de-novo} design, in which we aim to generate new molecules with high pharmacophore-shape similarities to query molecules, and fast search, which aims to combine generative design with a cheap 2D substructure similarity search for efficient hit identification. Our results show that VoxCap significantly outperforms previous methods in generating diverse \textit{de-novo} hits. When combined with our fast search workflow, VoxCap reduces computational time by orders of magnitude while returning hits for all query molecules, enabling the search of large libraries that are intractable to search by brute force.",0,arxiv,Biyoloji,CC-BY/arXiv,Pharmacophore-based design by learning on voxel grids
"Glioblastoma (GBM) remains the most aggressive tumor, urgently requiring novel therapeutic strategies. Here, we present a dry-to-wet framework combining generative modeling and experimental validation to optimize peptides targeting ATP5A, a potential peptide-binding protein for GBM. Our framework introduces the first lead-conditioned generative model, which focuses exploration on geometrically relevant regions around lead peptides and mitigates the combinatorial complexity of de novo methods. Specifically, we propose POTFlow, a \underline{P}rior and \underline{O}ptimal \underline{T}ransport-based \underline{Flow}-matching model for peptide optimization. POTFlow employs secondary structure information (e.g., helix, sheet, loop) as geometric constraints, which are further refined by optimal transport to produce shorter flow paths. With this design, our method achieves state-of-the-art performance compared with five popular approaches. When applied to GBM, our method generates peptides that selectively inhibit cell viability and significantly prolong survival in a patient-derived xenograft (PDX) model. As the first lead peptide-conditioned flow matching model, POTFlow holds strong potential as a generalizable framework for therapeutic peptide design.",0,arxiv,Biyoloji,CC-BY/arXiv,Generative design and validation of therapeutic peptides for glioblastoma based on a potential target ATP5A
"AlphaFold 3 (AF3) is a powerful biomolecular structure-predicting tool based on the latest deep learning algorithms and revolutionized AI model architectures. A few of papers have already investigated its accuracy in predicting different biomolecular structures. However, the potential applications of AF3 beyond basic structure prediction have not been fully explored. In our study, we firstly focused on structure predictions of antibody-antigen (CD47) complexes, which is believed to be challenge for AF3 due to limited resolved cognate crystallographic structures. Furtherly, we aimed to the potentiality of AF3 in performing pre-screening for potent antibody candidates as an auxiliary work through binding affinity analysis compared to other molecular docking modules of commercial software, which would greatly benefit the lead identification or optimization process in the drug development. In essence, this is not limited to antibody-antigen binding affinity, but many other chemical or physical properties of any drug candidate based on AF3's accurate predicting structures that are extremely close to the reality. According to our experimental results, AF3 is a very promising competitor, which can efficiently produce highly reliable molecular structures and subsequent binding energy predictions for most subjects. Surprisingly, an unexpected and nonrandom phenomenon ""reverse docking"" was observed for two of our antibody subjects, suggesting new issues arising from the architectural revolution of AF3. Our analysis and error correction experiments show that this phenomenon is likely to be caused by revolutionized AI model architectures, which provides important experience and reminders for the optimization and design direction of AI for structural prediction. All software copyrights belong to the China Pharmaceutical University (CPU) and its affiliated School of Pharmacy and School of Science.",0,arxiv,Biyoloji,CC-BY/arXiv,Exploring AlphaFold 3 for CD47 Antibody-Antigen Binding Affinity: An Unexpected Discovery of Reverse docking
"Peptide-based drugs can bind to protein interaction sites that small molecules often cannot, and are easier to produce than large protein drugs. However, designing effective peptide binders is difficult. A typical peptide has an enormous number of possible sequences, and only a few of these will fold into the right 3D shape to match a given protein target. Existing computational methods either generate many candidate sequences without considering how they will fold, or build peptide backbones and then find suitable sequences afterward. Here we introduce ApexGen, a new AI-based framework that simultaneously designs a peptide's amino-acid sequence and its three-dimensional structure to fit a given protein target. For each target, ApexGen produces a full all-atom peptide model in a small number of deterministic integration steps. In tests on hundreds of protein targets, the peptides designed by ApexGen fit tightly onto their target surfaces and cover nearly the entire binding site. These peptides have shapes similar to those found in natural protein-peptide complexes, and they show strong predicted binding affinity in computational experiments. Because ApexGen couples sequence and structure design at every step of Euler integration within a flow-matching sampler, it is much faster and more efficient than prior approaches. This unified method could greatly accelerate the discovery of new peptide-based therapeutics.",0,arxiv,Biyoloji,CC-BY/arXiv,ApexGen: Simultaneous design of peptide binder sequence and structure for target proteins
"Deep generative models are rapidly advancing structure-based drug design, offering substantial promise for generating small molecule ligands that bind to specific protein targets. However, most current approaches assume a rigid protein binding pocket, neglecting the intrinsic flexibility of proteins and the conformational rearrangements induced by ligand binding, limiting their applicability in practical drug discovery. Here, we propose Apo2Mol, a diffusion-based generative framework for 3D molecule design that explicitly accounts for conformational flexibility in protein binding pockets. To support this, we curate a dataset of over 24,000 experimentally resolved apo-holo structure pairs from the Protein Data Bank, enabling the characterization of protein structure changes associated with ligand binding. Apo2Mol employs a full-atom hierarchical graph-based diffusion model that simultaneously generates 3D ligand molecules and their corresponding holo pocket conformations from input apo states. Empirical studies demonstrate that Apo2Mol can achieve state-of-the-art performance in generating high-affinity ligands and accurately capture realistic protein pocket conformational changes.",0,arxiv,Biyoloji,CC-BY/arXiv,Apo2Mol: 3D Molecule Generation via Dynamic Pocket-Aware Diffusion Models
"The single nucleotide polymorphism (SNP) rs7903146 in the TCF7L2 gene has been determined as one of the strongest common genetic risk factors for Type 2 Diabetes (T2D). The location of the SNP in a non-coding region suggests a regulatory mechanism, meaning the SNP doesn't change the protein's own structure but rather affects how the TCF7L2 protein binds to DNA to control other genes. This binding, however, is highly dependent on the shape and flexibility of the DNA. This study aims to reveal the atomic-level effects of the SNP's cytosine-to-thymine substitution on the TCF7L2-DNA complex. We first utilized AlphaFold to generate individual high-confidence structures of the TCF7L2 protein and two 15-base pair DNA duplexes: one containing the reference C allele and one containing the variant T allele. These structures were then used as inputs for Neurosnap's Boltz2 deep learning model to generate two complete protein-DNA complexes of the TCF7L2 HMG-box bound to each DNA variant. Using the iMODS server, we conducted a Normal Mode Analysis (NMA) to predict and compare large-scale flexibility and differences in interactions between the complexes. The protein-DNA interface was dissected using PDBsum to locate atomic contacts, clefts, and interaction maps. Overall, our results show that the T allele variant exhibits increased global stiffness with a higher eigenvalue and reduced flexibility, suggesting that the SNP disrupts the mechanism and biomechanical balance needed for efficient TCF7L2-DNA binding, thus affecting downstream gene regulation.",0,arxiv,Biyoloji,CC-BY/arXiv,Structural Flexibility of the TCF7L2-DNA Complex with the Type 2 Diabetes SNP rs7903146
"Protein-protein docking is crucial for understanding how proteins interact. Numerous docking tools have been developed to discover possible conformations of two interacting proteins. However, the reliability and success of these docking tools rely on their scoring function. Accurate and efficient scoring functions are necessary to distinguish between native and non-native docking models to ensure the accuracy of a docking tool. Like in other fields where deep learning methods have been successfully utilized, these methods have also introduced innovative scoring functions. An outstanding tool for scoring and differentiating native-like docking models from non-native or incorrect conformations is called Protein binding Interfaces with Transformer Networks (PIsToN). PIsToN significantly outperforms state-of-the-art scoring functions. Using models of complexes obtained from binding the Ebola Virus Protein VP40 to the host cell's Sec24c protein as an example, we show how to evaluate docking models using PIsToN.",0,arxiv,Biyoloji,CC-BY/arXiv,Evaluating and Scoring Ebolavirus Protein-protein Docking Models Using PIsToN
"Molecular dynamics (MD) simulations provide atomistic insights into the structure, dynamics, and function of biomolecules by generating time-resolved, high-dimensional trajectories. Analyzing such data benefits from estimating the minimal number of variables required to describe the explored conformational manifold, known as the intrinsic dimension (ID). We present MDIntrinsicDimension, an open-source Python package that estimates ID directly from MD trajectories by combining rotation- and translation-invariant molecular projections (e.g., backbone dihedrals and inter-residue distances) with state-of-the-art estimators. The package provides three complementary analysis modes: whole-molecule ID; sliding windows along the sequence; and per-secondary-structure elements. It computes both overall ID (a single summary value) and instantaneous, time-resolved ID that can reveal transitions and heterogeneity over time. We illustrate the approach on fast folding-unfolding trajectories from the DESRES dataset, demonstrating that ID complements conventional geometric descriptors by highlighting spatially localized flexibility and differences across structural segments.",0,arxiv,Biyoloji,CC-BY/arXiv,MDIntrinsicDimension: Dimensionality-Based Analysis of Collective Motions in Macromolecules from Molecular Dynamics Trajectories
"Cryo-electron microscopy (cryo-EM) enables the atomic-resolution visualization of biomolecules; however, modern direct detectors generate data volumes that far exceed the available storage and transfer bandwidth, thereby constraining practical throughput. We introduce cryoSENSE, the computational realization of a hardware-software co-designed framework for compressive cryo-EM sensing and acquisition. We show that cryo-EM images of proteins lie on low-dimensional manifolds that can be independently represented using sparse priors in predefined bases and generative priors captured by a denoising diffusion model. cryoSENSE leverages these low-dimensional manifolds to enable faithful image reconstruction from spatial and Fourier-domain undersampled measurements while preserving downstream structural resolution. In experiments, cryoSENSE increases acquisition throughput by up to 2.5$\times$ while retaining the original 3D resolution, offering controllable trade-offs between the number of masked measurements and the level of downsampling. Sparse priors favor faithful reconstruction from Fourier-domain measurements and moderate compression, whereas generative diffusion priors achieve accurate recovery from pixel-domain measurements and more severe undersampling. Project website: https://cryosense.github.io.",0,arxiv,Biyoloji,CC-BY/arXiv,cryoSENSE: Compressive Sensing Enables High-throughput Microscopy with Sparse and Generative Priors on the Protein Cryo-EM Image Manifold
"This study introduces a novel iron-based gas diffusion electrode-photocatalytic system aimed at enhancing the degradation of phenolic compounds in wastewater. Phenolic compounds are toxic environmental pollutants with significant resistance to biodegradation. The traditional methods for treating phenol wastewater, including biological treatments and adsorption techniques, often fall short in achieving complete mineralization. Our approach utilizes a dual-chamber electrochemical setup integrating stainless steel felt-2-EAQ gas diffusion electrodes with TiO2 photocatalysis. This combination significantly boosts hydroxyl radical production, critical for effective pollutant breakdown. Experimentally, the system achieved up to 92% degradation efficiency for phenol at an optimized operating current of 10 mA/cm^2 in 3 hours, surpassing traditional methods. Additionally, energy consumption was reduced by 40% compared to conventional electro-Fenton systems. The stability tests indicated that the electrodes maintain over 80% of their initial activity after five cycles of use. These findings suggest that our system offers a more sustainable and efficient solution for treating phenolic wastewater by enhancing both degradation rates and energy efficiency.",0,arxiv,Biyoloji,CC-BY/arXiv,Treatment of phenol wastewater by electro-Fenton oxidative degradation based on efficient iron-based-gas diffusion-photocatalysis
"Aligning molecular sequence representations (e.g., SMILES notations) with textual descriptions is critical for applications spanning drug discovery, materials design, and automated chemical literature analysis. Existing methodologies typically treat molecular captioning (molecule-to-text) and text-based molecular design (text-to-molecule) as separate tasks, relying on supervised fine-tuning or contrastive learning pipelines. These approaches face three key limitations: (i) conventional metrics like BLEU prioritize linguistic fluency over chemical accuracy, (ii) training datasets frequently contain chemically ambiguous narratives with incomplete specifications, and (iii) independent optimization of generation directions leads to bidirectional inconsistency. To address these issues, we propose RTMol, a bidirectional alignment framework that unifies molecular captioning and text-to-SMILES generation through self-supervised round-trip learning. The framework introduces novel round-trip evaluation metrics and enables unsupervised training for molecular captioning without requiring paired molecule-text corpora. Experiments demonstrate that RTMol enhances bidirectional alignment performance by up to 47% across various LLMs, establishing an effective paradigm for joint molecule-text understanding and generation.",0,arxiv,Biyoloji,CC-BY/arXiv,RTMol: Rethinking Molecule-text Alignment in a Round-trip View
"The PTPN11 gene encodes the Src homology 2 domain-containing protein tyrosine phosphatase (SHP2), a key regulator of cell growth, differentiation, and apoptosis through its modulation of various signaling pathways, including the RAS/MAPK signaling pathway. Missense variants in PTPN11 disrupt SHP2's proper catalytic activity and the regulation of signaling pathways, leading to disorders such as Noonan syndrome (NS), LEOPARD syndrome (LS), or juvenile myelomonocytic leukemia (JMML). These missense variants have molecular disruptions resulting in gains and losses of function at both the molecular and phenotypic levels. Depending on their location within SHP2, missense substitutions disrupt inter-domain regulation or impair phosphatase function, resulting in altered phosphatase activity. In this study, we investigate the molecular basis underlying the differential pathogenicity of PTPN11 missense variants and predict the structural consequences of these variants using MutPred2 and AlphaFold2. We find that LOF and GOF variants display distinct functional mechanisms in sodium and DNA binding, and that NS-associated missense variants identified in fetuses with ultrasound-detected anomalies and familiar cases are more likely to be pathogenic.",0,arxiv,Biyoloji,CC-BY/arXiv,Understanding Molecular Basis of PTPN11-Related Diseases
"Peri-implant inflammation in orthodontic mini-implant may lead to patient discomfort and treatment failure. This study aims to evaluate the effects of diode laser application on the health of mini-implant, preventing peri-implantitis and promoting healing. A randomized controlled trial was conducted involving 30 orthodontic patients (12 males and 18 females, aged 18-32) who had mini-implants implanted on both sides of the maxilla for anterior teeth retraction. One side of each patient was assigned to either an experimental group receiving diode laser irradiation (650 nm, 25 mW) at specific postoperative intervals or a control group receiving simulated radiation. Clinical assessments included plaque index, modified sulcus bleeding index, probing depth, and incidence of peri-implant mucositis and implant mobility, measured at 1, 4, and 12 weeks post-implantation. Additionally, interleukin-1 beta (IL-1\b{eta}) levels in peri-implant fluid were analyzed via enzyme-linked immunosorbent assay (ELISA). Results indicated that the experimental group exhibited significantly lower plaque indices, sulcus bleeding indices, and probing depths (p < 0.05) compared to the control group. Moreover, the experimental group had fewer cases of peri-implant mucositis (p < 0.05), while differences in implant stability were not statistically significant (p > 0.05). IL-1\b{eta} levels were consistently lower in the experimental group throughout the study duration (p < 0.05). In conclusion, adjunctive diode laser therapy appears to enhance peri-implant health and reduce complications associated with orthodontic mini-implants, suggesting a promising direction for improving patient outcomes in orthodontics. Future research should explore long-term effects and the mechanisms underlying these benefits.",0,arxiv,Biyoloji,CC-BY/arXiv,Effects of diode laser photobiomodulation on peri-implant inflammation and stability in orthodontic mini-implants: A randomized controlled trial
"Saturnian moon Titan presents a compelling testbed for probing prebiotic chemistry beyond early Earth. Impact-generated melt pools provide transient aqueous habitats in an otherwise cryogenic environment. We use Cantera equilibrium models to assess whether mixtures of hydrogen cyanide (HCN), acetylene (C2H2), and ammonia (NH3) can drive amino acid synthesis in Selk-sized craters. Across twenty-one amino acids (twenty proteinogenic plus beta-alanine), NH3-free systems yield only proline, alanine, and beta-alanine, whereas adding as little as 1% NH3 (relative to H2O) renders almost the full suite accessible, with yields peaking at 2% and tapering thereafter. The NH3-free alanine result implies alternative pathways beyond classical Strecker or aminonitrile hydrolysis, suggesting acetylene, abundant on Titan but scarce on early Earth, as a plausible feedstock. We identify acrylonitrile (detected on Titan) as a thermodynamically favorable intermediate that can convert to alanine under aqueous conditions in an NH3-free pathway. For glycine and alanine production from nitrile hydrolysis, comparison with laboratory kinetics shows that our equilibrium models predict near-complete conversion, while observed rates yield only partial products over weeks. Yet estimated chemical equilibration times (years-centuries) are far shorter than melt lifetimes, supporting plausibility of equilibrium in situ. These predictions are directly testable with Dragonfly mass spectrometer (DraMS), for which we recommend pre-flight standards to test proline, alanine, beta-alanine, cysteine, and methionine. The first three offer the best chances for amino acid detection regardless of ammonia availability; the latter two offer diagnostic tools for determining the presence of reactive sulfur in post-impact Titan ponds.",0,arxiv,Biyoloji,CC-BY/arXiv,Prebiotic Chemistry Insights for Dragonfly: Thermodynamics of Amino Acid Synthesis in Selk Crater on Titan
"Quantifying the irreversibility and dissipation of non-equilibrium processes is crucial to understanding their behavior, assessing their possible capabilities, and characterizing their efficiency. We introduce a physical quantity that quantifies the irreversibility of stochastic Langevin systems from the observation of individual molecules' displacements. Categorizing these displacements into a few groups based on their initial and final position allows us to measure irreversibility precisely without the need to know the forces and magnitude of the fluctuations acting on the system. Our model-free estimate of irreversibility is related to entropy production by a conditional fluctuation theorem and provides a lower bound to the average entropy production. We validate the method on single-molecule force spectroscopy experiments of proteins subject to force ramps. We show that irreversibility is sensitive to detailed features of the energy landscape underlying the protein folding dynamics and suggest how our methods can be employed to unveil key properties of protein folding processes.",0,arxiv,Biyoloji,CC-BY/arXiv,Measuring irreversibility in stochastic systems by categorizing single-molecule displacements
"Stimulated Raman scattering (SRS) microscopy offers great potential to surpass fluorescent-based approaches, owing to the sharp linewidth of Raman vibrations amenable to super-multiplex cell imaging, but currently lacks one crucial component: genetically encodable tags equivalent to fluorescent proteins. Here, we show that infrared fluorescent proteins (IRFPs) can be used as genetically encoded SRS probes and benefit from the electronic pre-resonant SRS enhancement effect with near-infrared exciting pulses, comparable to synthetic dyes reported in the literature. SRS imaging of the nucleus in mammalian cells is demonstrated where a histone protein is fused to an IRFP. This work opens the route towards Raman-based cell imaging using genetically encoded probes, motivating efforts in solving the challenges of photostability and creating a vibrational palette.",0,arxiv,Biyoloji,CC-BY/arXiv,Genetically encoding stimulated Raman-scattering probes for cell imaging using infrared fluorescent proteins
"Large Artificial Neural Network (ANN) models have demonstrated success in various domains, including general text and image generation, drug discovery, and protein-RNA (ribonucleic acid) binding tasks. However, these models typically demand substantial computational resources, time, and data for effective training. Given that such extensive resources are often inaccessible to many researchers and that life sciences data sets are frequently limited, we investigated whether small ANN models could achieve acceptable accuracy in protein-RNA prediction. We experimented with shallow feed-forward ANNs comprising two hidden layers and various non-linearities. These models did not utilize explicit structural information; instead, a sliding window approach was employed to implicitly consider the context of neighboring residues and bases. We explored different training techniques to address the issue of highly unbalanced data. Among the seven most popular non-linearities for feed-forward ANNs, only three: Rectified Linear Unit (ReLU), Gated Linear Unit (GLU), and Hyperbolic Tangent (Tanh) yielded converging models. Common re-balancing techniques, such as under- and over-sampling of training sets, proved ineffective, whereas increasing the volume of training data and using model ensembles significantly improved performance. The optimal context window size, balancing both false negative and false positive errors, was found to be approximately 30 residues and bases. Our findings indicate that high-accuracy protein-RNA binding prediction is achievable using computing hardware accessible to most educational and research institutions.",0,arxiv,Biyoloji,CC-BY/arXiv,Compact Artificial Neural Network Models for Predicting Protein Residue -- RNA Base Binding
"Simulating trajectories of multi-particle systems on complex energy landscapes is a central task in molecular dynamics (MD) and drug discovery, but remains challenging at scale due to computationally expensive and long simulations. Previous approaches leverage techniques such as flow or SchrÃ¶dinger bridge matching to implicitly learn joint trajectories through data snapshots. However, many systems, including biomolecular systems and heterogeneous cell populations, undergo dynamic interactions that evolve over their trajectory and cannot be captured through static snapshots. To close this gap, we introduce Entangled SchrÃ¶dinger Bridge Matching (EntangledSBM), a framework that learns the first- and second-order stochastic dynamics of interacting, multi-particle systems where the direction and magnitude of each particle's path depend dynamically on the paths of the other particles. We define the Entangled SchrÃ¶dinger Bridge (EntangledSB) problem as solving a coupled system of bias forces that entangle particle velocities. We show that our framework accurately simulates heterogeneous cell populations under perturbations and rare transitions in high-dimensional biomolecular systems.",0,arxiv,Biyoloji,CC-BY/arXiv,Entangled SchrÃ¶dinger Bridge Matching
"Food polysaccharides have emerged as suitable carriers of active substances and as additives to food and nutraceutical formulations, showing potential to stabilize bioactive compounds during the storage of microencapsulate preparations, even in the gastrointestinal tract following the intake of bioactive compounds, thereby improving their bioaccessibility and bioavailability. This review provides a comprehensive overview of the main polysaccharides employed as wall materials, including starch, maltodextrin, alginate, pectin, inulin, chitosan, and gum arabic, and discusses how structural interactions and physicochemical properties can benefit the microencapsulation of polyphenols and pigments. The main findings and principles of the major encapsulation techniques, including spray drying, freeze drying, extrusion, emulsification, and coacervation, related to the production of microparticles, were briefly described. Polysaccharides can entrap hydrophilic and hydrophobic compounds by physical interactions, forming a barrier around the nucleus or binding to the bioactive compound. Intermolecular binding between polysaccharides in the wall matrix, polyphenols, and pigments in the nucleus can confer up to 90% of encapsulation efficiency, governed mainly by hydrogen bonds and electrostatic interactions. The mixture of wall polysaccharides in the microparticles synthesis favors the encapsulation solubility, storage stability, bioaccessibility, and bioactivity of the microencapsulate compounds. Clinical trials on the bioefficacy of polyphenols and pigments loaded in polysaccharide microparticles are scarce and require further evidence to reinforce the use of this technology.",0,arxiv,Biyoloji,CC-BY/arXiv,edible polysaccharides as stabilizers and carriers for the delivery of phenolic compounds and pigments in food formulations
"For several decades, experimental and computational studies have been used to investigate the potential functional role of knots in protein structures. A property that has attracted considerable attention is thermal stability, i.e., the extent to which a protein retains its native conformation and biological activity at high temperatures, without undergoing denaturation or aggregation. Thermal stability is quantified by the melting temperature $T_m$, an equilibrium property that corresponds to the peak of heat capacity in differential scanning calorimetry experiments. Since protein energy does not depend on the topological state, equilibrium properties such as $T_m$ should not depend on the topology, provided that the exploration of the equilibrium space is ergodic. However, experimental and computational results reported in the literature provide conflicting views on this problem, with some studies reporting an enhancement of thermal stability for certain knotted proteins. Here, we use extensive Monte Carlo simulations of a simple C-alpha protein model of protein YibK to show in a comprehensive manner that $T_m$ does not depend on the topological state of the protein.",0,arxiv,Biyoloji,CC-BY/arXiv,A critical assessment of the role of topology on protein thermal stability
"Alzheimers disease (AD), a progressive neuro-degenerative disorder, currently lacks effective therapeutic strategies that can modify disease progression. Recent studies have highlighted the circadian rhythm critical role in AD pathophysiology, implicating circadian clock kinases, such as the Salt-Inducible Kinase 3 (SIK3), as promising therapeutic target. Generative AI models have surpassed traditional methods of drug discovery, untapping the vast unexplored chemical space of drug-like molecules. We present a sequence-to-sequence Variational Autoencoder (Seq2Seq-VAE) model guided by an Active Learning (AL) approach to optimize molecular generation. Our pipeline iteratively guided a pre-trained Seq2Seq-VAE model towards the pharmacological landscape relevant to SIK3 using a two-step framework, an inner loop that iteratively improves physiochemical properties profile, drug likeliness and synthesizability, followed by an outer loop that steer the latent space towards high-affinity ligands for SIK3. Our approach introduces feedback-driven optimization without requiring large labeled datasets, making it particularly suited for early-stage drug discovery in under-explored therapeutic targets. Our results demonstrate the models convergence toward SIK3-specific small molecules with desired properties and high binding affinity. This work highlights the use of generative AI combined with AL for rational drug discovery that can be extended to other protein targets with minimal modifications, offering a scalable solution to the molecular design bottleneck in drug design.",0,arxiv,Biyoloji,CC-BY/arXiv,De Novo Design of SIK3 Inhibitors via Feedback-Driven Fine-Tuning of Seq2Seq-VAE
"The convergence of statistical learning and molecular physics is transforming our approach to modeling biomolecular systems. Physics-informed machine learning (PIML) offers a systematic framework that integrates data-driven inference with physical constraints, resulting in models that are accurate, mechanistic, generalizable, and able to extrapolate beyond observed domains. This review surveys recent advances in physics-informed neural networks and operator learning, differentiable molecular simulation, and hybrid physics-ML potentials, with emphasis on long-timescale kinetics, rare events, and free-energy estimation. We frame these approaches as solutions to the ""biomolecular closure problem"", recovering unresolved interactions beyond classical force fields while preserving thermodynamic consistency and mechanistic interpretability. We examine theoretical foundations, tools and frameworks, computational trade-offs, and unresolved issues, including model expressiveness and stability. We outline prospective research avenues at the intersection of machine learning, statistical physics, and computational chemistry, contending that future advancements will depend on mechanistic inductive biases, and integrated differentiable physical learning frameworks for biomolecular simulation and discovery.",0,arxiv,Biyoloji,CC-BY/arXiv,Learning Biomolecular Motion: The Physics-Informed Machine Learning Paradigm
"Chemical reaction prediction remains a fundamental challenge in organic chemistry, where existing machine learning models face two critical limitations: sensitivity to input permutations (molecule/atom orderings) and inadequate modeling of substructural interactions governing reactivity. These shortcomings lead to inconsistent predictions and poor generalization to real-world scenarios. To address these challenges, we propose ReaDISH, a novel reaction prediction model that learns permutation-invariant representations while incorporating interaction-aware features. It introduces two innovations: (1) symmetric difference shingle encoding, which extends the differential reaction fingerprint (DRFP) by representing shingles as continuous high-dimensional embeddings, capturing structural changes while eliminating order sensitivity; and (2) geometry-structure interaction attention, a mechanism that models intra- and inter-molecular interactions at the shingle level. Extensive experiments demonstrate that ReaDISH improves reaction prediction performance across diverse benchmarks. It shows enhanced robustness with an average improvement of 8.76% on R$^2$ under permutation perturbations.",0,arxiv,Biyoloji,CC-BY/arXiv,Reaction Prediction via Interaction Modeling of Symmetric Difference Shingle Sets
"Nuclei segmentation is the cornerstone task in histology image reading, shedding light on the underlying molecular patterns and leading to disease or cancer diagnosis. Yet, it is a laborious task that requires expertise from trained physicians. The large nuclei variability across different organ tissues and acquisition processes challenges the automation of this task. On the other hand, data annotations are expensive to obtain, and thus, Deep Learning (DL) models are challenged to generalize to unseen organs or different domains. This work proposes Local-to-Global NuSegHop (LG-NuSegHop), a self-supervised pipeline developed on prior knowledge of the problem and molecular biology. There are three distinct modules: (1) a set of local processing operations to generate a pseudolabel, (2) NuSegHop a novel data-driven feature extraction model and (3) a set of global operations to post-process the predictions of NuSegHop. Notably, even though the proposed pipeline uses { no manually annotated training data} or domain adaptation, it maintains a good generalization performance on other datasets. Experiments in three publicly available datasets show that our method outperforms other self-supervised and weakly supervised methods while having a competitive standing among fully supervised methods. Remarkably, every module within LG-NuSegHop is transparent and explainable to physicians.",0,arxiv,Biyoloji,CC-BY/arXiv,LG-NuSegHop: A Local-to-Global Self-Supervised Pipeline For Nuclei Instance Segmentation
"Antimicrobial peptides have emerged as promising molecules to combat antimicrobial resistance. However, fragmented datasets, inconsistent annotations, and the lack of standardized benchmarks hinder computational approaches and slow down the discovery of new candidates. To address these challenges, we present the Expanded Standardized Collection for Antimicrobial Peptide Evaluation (ESCAPE), an experimental framework integrating over 80.000 peptides from 27 validated repositories. Our dataset separates antimicrobial peptides from negative sequences and incorporates their functional annotations into a biologically coherent multilabel hierarchy, capturing activities across antibacterial, antifungal, antiviral, and antiparasitic classes. Building on ESCAPE, we propose a transformer-based model that leverages sequence and structural information to predict multiple functional activities of peptides. Our method achieves up to a 2.56% relative average improvement in mean Average Precision over the second-best method adapted for this task, establishing a new state-of-the-art multilabel peptide classification. ESCAPE provides a comprehensive and reproducible evaluation framework to advance AI-driven antimicrobial peptide research.",0,arxiv,Biyoloji,CC-BY/arXiv,A Standardized Benchmark for Multilabel Antimicrobial Peptide Classification
"Models such as AlphaFold2 and OpenFold have transformed protein structure prediction, yet their inner workings remain poorly understood. We present a methodology to systematically evaluate the contribution of individual OpenFold components to structure prediction accuracy. We identify several components that are critical for most proteins, while others vary in importance across proteins. We further show that the contribution of several components is correlated with protein length. These findings provide insight into how OpenFold achieves accurate predictions and highlight directions for interpreting protein prediction networks more broadly.",0,arxiv,Biyoloji,CC-BY/arXiv,Quantifying the Role of OpenFold Components in Protein Structure Prediction
"The presence of an expanded polyglutamine produces a toxic gain of function in huntingtin. Protein aggregation resulting from this gain of function is likely to be the cause of neuronal death. Two main mechanisms of aggregation have been proposed: hydrogen bonding by polar-zipper formation and covalent bonding by transglutaminase-catalyzed cross-linking. In cell culture models of Huntington's disease, aggregates are mostly stabilized by hydrogen bonds, but covalent bonds are also likely to occur. Nothing is known about the nature of the bonds that stabilize the aggregates in the brain of patients with Huntington's disease. It seems that the nature of the bond stabilizing the aggregates is one of the most important questions, as the answer would condition the therapeutic approach to Huntington's disease.",0,arxiv,Biyoloji,CC-BY/arXiv,Protein aggregation in Huntington's disease
"Multimodal protein features play a crucial role in protein function prediction. However, these features encompass a wide range of information, ranging from structural data and sequence features to protein attributes and interaction networks, making it challenging to decipher their complex interconnections. In this work, we propose a multimodal protein function prediction method (DSRPGO) by utilizing dynamic selection and reconstructive pre-training mechanisms. To acquire complex protein information, we introduce reconstructive pre-training to mine more fine-grained information with low semantic levels. Moreover, we put forward the Bidirectional Interaction Module (BInM) to facilitate interactive learning among multimodal features. Additionally, to address the difficulty of hierarchical multi-label classification in this task, a Dynamic Selection Module (DSM) is designed to select the feature representation that is most conducive to current protein function prediction. Our proposed DSRPGO model improves significantly in BPO, MFO, and CCO on human datasets, thereby outperforming other benchmark models.",0,arxiv,Biyoloji,CC-BY/arXiv,Enhancing Multimodal Protein Function Prediction Through Dual-Branch Dynamic Selection with Reconstructive Pre-Training
"The chemical space of drug-like molecules is vast, motivating the development of generative models that must learn broad chemical distributions, enable conditional generation by capturing structure-property representations, and provide fast molecular generation. Meeting the objectives depends on modeling choices, including the probabilistic modeling approach, the conditional generative formulation, the architecture, and the molecular input representation. To address the challenges, we present STAR-VAE (Selfies-encoded, Transformer-based, AutoRegressive Variational Auto Encoder), a scalable latent-variable framework with a Transformer encoder and an autoregressive Transformer decoder. It is trained on 79 million drug-like molecules from PubChem, using SELFIES to guarantee syntactic validity. The latent-variable formulation enables conditional generation: a property predictor supplies a conditioning signal that is applied consistently to the latent prior, the inference network, and the decoder. Our contributions are: (i) a Transformer-based latent-variable encoder-decoder model trained on SELFIES representations; (ii) a principled conditional latent-variable formulation for property-guided generation; and (iii) efficient finetuning with low-rank adapters (LoRA) in both encoder and decoder, enabling fast adaptation with limited property and activity data. On the GuacaMol and MOSES benchmarks, our approach matches or exceeds baselines, and latent-space analyses reveal smooth, semantically structured representations that support both unconditional exploration and property-aware generation. On the Tartarus benchmarks, the conditional model shifts docking-score distributions toward stronger predicted binding. These results suggest that a modernized, scale-appropriate VAE remains competitive for molecular generation when paired with principled conditioning and parameter-efficient finetuning.",0,arxiv,Biyoloji,CC-BY/arXiv,STAR-VAE: Latent Variable Transformers for Scalable and Controllable Molecular Generation
"Predicting the secondary structure of RNA is a core challenge in computational biology, essential for understanding molecular function and designing novel therapeutics. The field has evolved from foundational but accuracy-limited thermodynamic approaches to a new data-driven paradigm dominated by machine learning and deep learning. These models learn folding patterns directly from data, leading to significant performance gains. This review surveys the modern landscape of these methods, covering single-sequence, evolutionary-based, and hybrid models that blend machine learning with biophysics. A central theme is the field's ""generalization crisis,"" where powerful models were found to fail on new RNA families, prompting a community-wide shift to stricter, homology-aware benchmarking. In response to the underlying challenge of data scarcity, RNA foundation models have emerged, learning from massive, unlabeled sequence corpora to improve generalization. Finally, we look ahead to the next set of major hurdles-including the accurate prediction of complex motifs like pseudoknots, scaling to kilobase-length transcripts, incorporating the chemical diversity of modified nucleotides, and shifting the prediction target from static structures to the dynamic ensembles that better capture biological function. We also highlight the need for a standardized, prospective benchmarking system to ensure unbiased validation and accelerate progress.",0,arxiv,Biyoloji,CC-BY/arXiv,Machine Learning for RNA Secondary Structure Prediction: a review of current methods and challenges
"Computational methods for predicting and designing biomolecular structures are increasingly powerful. While previous approaches relied on physics-based modeling, modern tools, such as AlphaFold2 in CASP14, leverage artificial intelligence (AI) to achieve significantly improved performance. The growing impact of AI-based tools in protein science necessitates enhanced educational materials that improve AI literacy among both established scientists seeking to deepen their expertise and new researchers entering the field. To address this need, we developed DL4Proteins, a series of ten interactive notebook modules that introduce fundamental machine learning (ML) concepts, guide users through training ML models for protein-related tasks, and ultimately present cutting-edge protein structure prediction and design pipelines. With nothing more than a web browser, learners can now access state-of-the-art computational tools employed by professional protein engineers - ranging from all-atom protein design to fine-tuning protein language models for biophysically relevant functional tasks. By increasing accessibility, this notebook series broadens participation in AI-driven protein research. The complete notebook series is publicly available at https://github.com/Graylab/DL4Proteins-notebooks.",0,arxiv,Biyoloji,CC-BY/arXiv,DL4Proteins Jupyter Notebooks Teach how to use Artificial Intelligence for Biomolecular Structure Prediction and Design
"Potential Energy Surfaces (PESs) are an indispensable tool to investigate, characterise and understand chemical and biological systems in the gas and condensed phases. Advances in Machine Learning (ML) methodologies have led to the development of Machine Learned Potential Energy Surfaces (ML-PES) which are now widely used to simulate such systems. The present work provides an overview of concepts, methodologies and recommendations for constructing and using ML-PESs. The choice of topics is focused on practical and recurrent issues to conceive and use such model. Application of the principles discussed are illustrated through two different systems of biomolecular importance: the non-reactive dynamics of the Alanine-Lysine-Alanine tripeptide in gas and solution phases, and double proton transfer reactions in DNA base pairs.",0,arxiv,Biyoloji,CC-BY/arXiv,"Design, Assessment, and Application of Machine Learning Potential Energy Surfaces"
"Diffusion models have emerged as a leading framework in generative modeling, poised to transform the traditionally slow and costly process of drug discovery. This review provides a systematic comparison of their application in designing two principal therapeutic modalities: small molecules and therapeutic peptides. We dissect how the unified framework of iterative denoising is adapted to the distinct molecular representations, chemical spaces, and design objectives of each modality. For small molecules, these models excel at structure-based design, generating novel, pocket-fitting ligands with desired physicochemical properties, yet face the critical hurdle of ensuring chemical synthesizability. Conversely, for therapeutic peptides, the focus shifts to generating functional sequences and designing de novo structures, where the primary challenges are achieving biological stability against proteolysis, ensuring proper folding, and minimizing immunogenicity. Despite these distinct challenges, both domains face shared hurdles: the scarcity of high-quality experimental data, the reliance on inaccurate scoring functions for validation, and the crucial need for experimental validation. We conclude that the full potential of diffusion models will be unlocked by bridging these modality-specific gaps and integrating them into automated, closed-loop Design-Build-Test-Learn (DBTL) platforms, thereby shifting the paradigm from mere chemical exploration to the on-demand engineering of novel~therapeutics.",0,arxiv,Biyoloji,CC-BY/arXiv,Diffusion Models at the Drug Discovery Frontier: A Review on Generating Small Molecules versus Therapeutic Peptides
"DNA helicases undergo conformational changes; however, their structural dynamics are poorly understood. Here, we study single molecules of superfamily 1A DNA helicase Rep, which undergo conformational transitions during bacterial DNA replication, repair and recombination. We use time-correlated single-photon counting (TCSPC), fluorescence correlation spectroscopy (FCS), rapid single-molecule FÃ¶rster resonance energy transfer (smFRET), Anti-Brownian ELectrokinetic (ABEL) trapping and molecular dynamics simulations (MDS) to provide unparalleled temporal and spatial resolution of Rep's domain movements. We detect four states revealing two hitherto hidden intermediates (S2, S3), between the open (S1) and closed (S4) structures, whose stability is salt dependent. Rep's open-to-closed switch involves multiple changes to all four subdomains 1A, 1B, 2A and 2B along the S1 to S2 to S3 to S4 transitional pathway comprising an initial truncated swing of 2B which then rolls across the 1B surface, following by combined rotations of 1B, 2A and 2B. High forward and reverse rates for S1 to S2 suggest that 1B may act to frustrate 2B movement to prevent premature Rep closure in the absence of DNA. These observations support a more general binding model for accessory DNA helicases that utilises conformational plasticity to explore a multiplicity of structures whose landscape can be tuned by salt prior to locking-in upon DNA binding.",0,arxiv,Biyoloji,CC-BY/arXiv,The transitional kinetics between open and closed Rep structures can be tuned by salt via two intermediate states
"The remarkable progress of artificial intelligence (AI) has revealed the enormous energy demands of modern digital architectures, raising deep concerns about sustainability. In stark contrast, the human brain operates efficiently on only ~20 watts, and individual cells process gigabit-scale genetic information using energy on the order of trillionths of a watt. Under the same energy budget, a general-purpose digital processor can perform only a few simple operations per second. This striking disparity suggests that biological systems follow algorithms fundamentally distinct from conventional computation. The framework of information thermodynamics-especially Maxwell's demon and the Szilard engine-offers a theoretical clue, setting the lower bound of energy required for information processing. However, digital processors exceed this limit by about six orders of magnitude. Recent single-molecule studies have revealed that biological molecular motors convert Brownian motion into mechanical work, realizing a ""demon-like"" operational principle. These findings suggest that living systems have already implemented an ultra-efficient information-energy conversion mechanism that transcends digital computation. Here, we experimentally establish a quantitative correspondence between positional information (bits) and mechanical work, demonstrating that molecular machines selectively exploit rare but functional fluctuations arising from Brownian motion to achieve ATP-level energy efficiency. This integration of information, energy, and timescale indicates that life realizes a Maxwell's demon-like mechanism for energy-efficient information processing.",0,arxiv,Biyoloji,CC-BY/arXiv,The Demon Hidden Behind Life's Ultra-Energy-Efficient Information Processing -- Demonstrated by Biological Molecular Motors
"How proteins fold remains a central unsolved problem in biology. While the idea of a folding code embedded in the amino acid sequence was introduced more than 6 decades ago, this code remains undefined. While we now have powerful predictive tools to predict the final native structure of proteins, we still lack a predictive framework for how sequences dictate folding pathways. Two main conceptual models dominate as explanations of folding mechanism: the funnel model, in which folding proceeds through many alternative routes on a rugged, hyperdimensional energy landscape; and the foldon model, which proposes a hierarchical sequence of discrete intermediates. Recent advances on two fronts are now enabling folding studies in unprecedented ways. Powerful experimental approaches; in particular, single-molecule force spectroscopy and hydrogen (deuterium exchange assays) allow time-resolved tracking of the folding process at high resolution. At the same time, computational breakthroughs culminating in algorithms such as AlphaFold have revolutionized static structure prediction, opening opportunities to extend machine learning toward dynamics. Together, these developments mark a turning point: for the first time, we are positioned to resolve how proteins fold, why they misfold, and how this knowledge can be harnessed for biology and medicine.",0,arxiv,Biyoloji,CC-BY/arXiv,How Do Proteins Fold?
"Aggregated Markov models provide a flexible framework for stochastic dynamics that develops on multiple timescales. For example, Markov models for ion channels often consist of multiple open and closed state to account for ""slow"" and ""fast"" openings and closings of the channel. The approach is a popular tool in the construction of mechanistic models of ion channels - instead of viewing model states as generators of sojourn times of a certain characteristic length, each individual model state is interpreted as a representation of a distinct biophysical state. We will review the properties of aggregated Markov models and discuss the implications for mechanistic modelling. First, we show how the aggregated Markov models with a given number of states can be calculated using PÃ³lya enumeration However, models with $n_O$ open and $n_C$ closed states that exceed the maximum number $2 n_O n_C$ of parameters are non-identifiable. We will present two derivations for this classical result and investigate non-identifiability further via a detailed analysis of the non-identifiable fully connected three-state model. Finally, we will discuss the implications of non-identifiability for mechanistic modelling of ion channels. We will argue that instead of designing models based on assumed transitions between distinct biophysical states which are modulated by ligand binding, it is preferable to build models based on additional sources of data that give more direct insight into the dynamics of conformational changes.",0,arxiv,Biyoloji,CC-BY/arXiv,Modelling ion channels with a view towards identifiability
"Designing enzyme backbones with substrate-specific functionality is a critical challenge in computational protein engineering. Current generative models excel in protein design but face limitations in binding data, substrate-specific control, and flexibility for de novo enzyme backbone generation. To address this, we introduce EnzyBind, a dataset with 11,100 experimentally validated enzyme-substrate pairs specifically curated from PDBbind. Building on this, we propose EnzyControl, a method that enables functional and substrate-specific control in enzyme backbone generation. Our approach generates enzyme backbones conditioned on MSA-annotated catalytic sites and their corresponding substrates, which are automatically extracted from curated enzyme-substrate data. At the core of EnzyControl is EnzyAdapter, a lightweight, modular component integrated into a pretrained motif-scaffolding model, allowing it to become substrate-aware. A two-stage training paradigm further refines the model's ability to generate accurate and functional enzyme structures. Experiments show that our EnzyControl achieves the best performance across structural and functional metrics on EnzyBind and EnzyBench benchmarks, with particularly notable improvements of 13\% in designability and 13\% in catalytic efficiency compared to the baseline models. The code is released at https://github.com/Vecteur-libre/EnzyControl.",0,arxiv,Biyoloji,CC-BY/arXiv,EnzyControl: Adding Functional and Substrate-Specific Control for Enzyme Backbone Generation
"Hydrogen peroxide oxidises cysteine residues to control protein function, yet bulk rate constants predict hours for changes that occur in cells in seconds. Here, this work shows that local electromagnetic fields (EMFs), ubiquitous in proteins, membranes and nanodomains, can lawfully modulate the Eyring barrier and orientate reactants, accelerating cysteine oxidation without changing the underlying chemistry. Embedding a field term into the Eyring expression, demonstrated that plausible local EMFs with realistic dipole changes accelerate rate constants by orders of magnitude. This local acceleration reconciles the discrepancy between predicted vs. observed rates of H2O2-mediated cysteine oxidation. The framework generates falsifiable predictions, such as vibrational Stark readouts in thiolate peroxide complexes should fall within predicted ranges, and reframes rate-constants as mutable, field conditioned parameters. Cysteine redox sensing is fast not because the chemistry is exotic, but because the physics is local.",0,arxiv,Biyoloji,CC-BY/arXiv,Local Electromagnetic Fields Enable Fast Redox Sensing by Physically Accelerating Cysteine Oxidation
"We investigate a relatively underexplored class of hybrid neurosymbolic models integrating symbolic learning with neural reasoning to construct data generators meeting formal correctness criteria. In \textit{Symbolic Neural Generators} (SNGs), symbolic learners examine logical specifications of feasible data from a small set of instances -- sometimes just one. Each specification in turn constrains the conditional information supplied to a neural-based generator, which rejects any instance violating the symbolic specification. Like other neurosymbolic approaches, SNG exploits the complementary strengths of symbolic and neural methods. The outcome of an SNG is a triple $(H, X, W)$, where $H$ is a symbolic description of feasible instances constructed from data, $X$ a set of generated new instances that satisfy the description, and $W$ an associated weight. We introduce a semantics for such systems, based on the construction of appropriate \textit{base} and \textit{fibre} partially-ordered sets combined into an overall partial order, and outline a probabilistic extension relevant to practical applications. In this extension, SNGs result from searching over a weighted partial ordering. We implement an SNG combining a restricted form of Inductive Logic Programming (ILP) with a large language model (LLM) and evaluate it on early-stage drug design. Our main interest is the description and the set of potential inhibitor molecules generated by the SNG. On benchmark problems -- where drug targets are well understood -- SNG performance is statistically comparable to state-of-the-art methods. On exploratory problems with poorly understood targets, generated molecules exhibit binding affinities on par with leading clinical candidates. Experts further find the symbolic specifications useful as preliminary filters, with several generated molecules identified as viable for synthesis and wet-lab testing.",0,arxiv,Biyoloji,CC-BY/arXiv,Symbolic Neural Generation with Applications to Lead Discovery in Drug Design
"Biomolecular interactions underpin almost all biological processes, and their rational design is central to programming new biological functions. Generative AI models have emerged as powerful tools for molecular design, yet most remain specialized for individual molecular types and lack fine-grained control over interaction details. Here we present ODesign, an all-atom generative world model for all-to-all biomolecular interaction design. ODesign allows scientists to specify epitopes on arbitrary targets and generate diverse classes of binding partners with fine-grained control. Across entity-, token-, and atom-level benchmarks in the protein modality, ODesign demonstrates superior controllability and performance to modality-specific baselines. Extending beyond proteins, it generalizes to nucleic acid and small-molecule design, enabling interaction types such as protein-binding RNA/DNA and RNA/DNA-binding ligands that were previously inaccessible. By unifying multimodal biomolecular interactions within a single generative framework, ODesign moves toward a general-purpose molecular world model capable of programmable design. ODesign is available at https://odesign.lglab.ac.cn ,",0,arxiv,Biyoloji,CC-BY/arXiv,ODesign: A World Model for Biomolecular Interaction Design
"Structurally nanoengineered antimicrobial peptide polymers (SNAPPs) are emerging as promising selective agents against bacterial membranes. In this study, we used all atom molecular dynamics simulation techniques to investigate the interaction of a promising cationic SNAPP architecture (Alt-SNAPP with 8 arms made of alternating lysine and valine residues) with modelled Gram-negative, Gram-positive, mammalian, and red blood cell membranes. Alt-SNAPP exhibited rapid and stable binding to bacterial membranes, driven by electrostatic interactions with anionic lipids such as phosphatidylglycerol (PG) and cardiolipin (CL), and supported by membrane fluidity. In contrast, mammalian and red blood cell membranes, enriched in zwitterionic lipids and cholesterol, resisted peptide association entirely. Analyses of center of mass distance, partial density, hydrogen bonding, and interaction energy confirmed that SNAPP remains fully excluded from host like membranes while forming stable, multivalent interactions with bacterial bilayers. These findings provide mechanistic insight into membrane selectivity of SNAPP and offer a molecular framework for designing next generation antimicrobial polymers with minimal off target toxicity.",0,arxiv,Biyoloji,CC-BY/arXiv,Molecular Dynamics Simulations of Membrane Selectivity of Star Peptides Across Different Bacterial and Mammalian Bilipids
"Autophagy and migrasome formation constitute critical cellular mechanisms for maintaining cellular homeostasis, however, their potential compensatory interplay remains poorly understood. In this study, we identify VPS39, a core component of the HOPS complex, as a molecular switch coordinating these processes. Genetic ablation of VPS39 not only impairs autophagic flux but also triggers cell migration through RhoA/Rac1 GTPases upregulation, consequently facilitating migrasome formation. Using super-resolution microscopy, we further demonstrate that migrasomes serve as an alternative disposal route for damaged mitochondria during VPS39-induced autophagy impairment, revealing a novel stress adaptation mechanism. Our work establishes a previously unrecognized autophagy-migrasome axis and provides direct visual evidence of organelle quality control via migrasomal extrusion. These findings position VPS39-regulated pathway switching as a potential therapeutic strategy for neurodegenerative diseases characterized by autophagy dysfunction.",0,arxiv,Biyoloji,CC-BY/arXiv,Beyond Autophagy: VPS39 Deficiency Triggers Migrasome-Driven Stress Adaptation Revealed by Super-Resolution Imaging
"Understanding the dynamic behavior of proteins is critical to elucidating their functional mechanisms, yet generating realistic, temporally coherent trajectories of protein ensembles remains a significant challenge. In this work, we introduce a novel hierarchical autoregressive framework for modeling protein dynamics that leverages the intrinsic multi-scale organization of molecular motions. Unlike existing methods that focus on generating static conformational ensembles or treat dynamic sampling as an independent process, our approach characterizes protein dynamics as a Markovian process. The framework employs a two-scale architecture: a low-resolution model captures slow, collective motions driving major conformational transitions, while a high-resolution model generates detailed local fluctuations conditioned on these large-scale movements. This hierarchical design ensures that the causal dependencies inherent in protein dynamics are preserved, enabling the generation of temporally coherent and physically realistic trajectories. By bridging high-level biophysical principles with state-of-the-art generative modeling, our approach provides an efficient framework for simulating protein dynamics that balances computational efficiency with physical accuracy.",0,arxiv,Biyoloji,CC-BY/arXiv,TEMPO: Temporal Multi-scale Autoregressive Generation of Protein Conformational Ensembles
"Designing RNA sequences that reliably adopt specified three-dimensional structures while maintaining thermodynamic stability remains challenging for synthetic biology and therapeutics. Current inverse folding approaches optimize for sequence recovery or single structural metrics, failing to simultaneously ensure global geometry, local accuracy, and ensemble stability-three interdependent requirements for functional RNA design. This gap becomes critical when designed sequences encounter dynamic biological environments. We introduce RiboPO, a Ribonucleic acid Preference Optimization framework that addresses this multi-objective challenge through reinforcement learning from physical feedback (RLPF). RiboPO fine-tunes gRNAde by constructing preference pairs from composite physical criteria that couple global 3D fidelity and thermodynamic stability. Preferences are formed using structural gates, PLDDT geometry assessments, and thermostability proxies with variability-aware margins, and the policy is updated with Direct Preference Optimization (DPO). On RNA inverse folding benchmarks, RiboPO demonstrates a superior balance of structural accuracy and stability. Compared to the best non-overlap baselines, our multi-round model improves Minimum Free Energy (MFE) by 12.3% and increases secondary-structure self-consistency (EternaFold scMCC) by 20%, while maintaining competitive 3D quality and high sequence diversity. In sampling efficiency, RiboPO achieves 11% higher pass@64 than the gRNAde base under the conjunction of multiple requirements. A multi-round variant with preference-pair reconstruction delivers additional gains on unseen RNA structures. These results establish RLPF as an effective paradigm for structure-accurate and ensemble-robust RNA design, providing a foundation for extending to complex biological objectives.",0,arxiv,Biyoloji,CC-BY/arXiv,RiboPO: Preference Optimization for Structure- and Stability-Aware RNA Design
"The rapid progress of graph generation has raised new security concerns, particularly regarding backdoor vulnerabilities. While prior work has explored backdoor attacks in image diffusion and unconditional graph generation, conditional, especially text-guided graph generation remains largely unexamined. This paper proposes BadGraph, a backdoor attack method against latent diffusion models for text-guided graph generation. BadGraph leverages textual triggers to poison training data, covertly implanting backdoors that induce attacker-specified subgraphs during inference when triggers appear, while preserving normal performance on clean inputs. Extensive experiments on four benchmark datasets (PubChem, ChEBI-20, PCDes, MoMu) demonstrate the effectiveness and stealth of the attack: less than 10% poisoning rate can achieves 50% attack success rate, while 24% suffices for over 80% success rate, with negligible performance degradation on benign samples. Ablation studies further reveal that the backdoor is implanted during VAE and diffusion training rather than pretraining. These findings reveal the security vulnerabilities in latent diffusion models of text-guided graph generation, highlight the serious risks in models' applications such as drug discovery and underscore the need for robust defenses against the backdoor attack in such diffusion models.",0,arxiv,Biyoloji,CC-BY/arXiv,BadGraph: A Backdoor Attack Against Latent Diffusion Model for Text-Guided Graph Generation
"Understanding the flexibility of protein-nucleic acid complexes, often characterized by atomic B-factors, is essential for elucidating their structure, dynamics, and functions, such as reactivity and allosteric pathways. Traditional models such as Gaussian Network Models (GNM) and Elastic Network Models (ENM) often fall short in capturing multiscale interactions, especially in large or complex biomolecular systems. In this work, we apply the Persistent Sheaf Laplacian (PSL) framework for the B-factor prediction of protein-nucleic acid complexes. The PSL model integrates multiscale analysis, algebraic topology, combinatoric Laplacians, and sheaf theory for data representation. It reveals topological invariants in its harmonic spectra and captures the homotopic shape evolution of data with its non-harmonic spectra. Its localization enables accurate B-factor predictions. We benchmark our method on three diverse datasets, including protein-RNA and nucleic-acid-only structures, and demonstrate that PSL consistently outperforms existing models such as GNM and multiscale FRI (mFRI), achieving up to a 21% improvement in Pearson correlation coefficient for B-factor prediction. These results highlight the robustness and adaptability of PSL in modeling complex biomolecular interactions and suggest its potential utility in broader applications such as mutation impact analysis and drug design.",0,arxiv,Biyoloji,CC-BY/arXiv,Predicting Protein-Nucleic Acid Flexibility Using Persistent Sheaf Laplacians
"The extracellular matrix of biofilms presents a dense and intricate architecture. Numerous biophysical properties of the matrix surrounding microbial cells contribute to the heterogeneity of biofilms and their functions at the microscale. Previous mathematical models assume the matrix to be homogeneous, often overlooking the need for a detailed mechanistic understanding of the extracellular space. In this theoretical study, we introduce a novel cell-capsule approach to investigate geometric patterns in biofilm morphology and predict their role in oxygen transport. The thickness of the capsule and the arrangement of cell-capsule patterns can influence matrix heterogeneity, providing a clear picture of biofilm structure. By incorporating the bacterial capsule as a distinct, low-diffusivity phase, our novel cell-capsule model reveals that this architecture acts as a significant 'resistance-in-series' barrier. We found that a thick capsule/dense matrix arrangement can reduce local oxygen transfer by approximately 70%, a substantial drop that may give drive further research into oxygen limitations during early stage biofilm development.",0,arxiv,Biyoloji,CC-BY/arXiv,Modelling multiscale architecture of biofilm extracellular matrix and its role in oxygen transport
"Machine olfaction is rapidly emerging as a transformative capability, with applications spanning non-invasive medical diagnostics, industrial monitoring, agriculture, and security and defense. Recent advances in stabilizing mammalian olfactory receptors and integrating them into biophotonic and bioelectronic systems have enabled detection at near single-molecule resolution thus placing machines on par with trained detection dogs. As this technology converges with multimodal AI and distributed sensor networks imbued with embedded AI, it introduces a new, biochemical layer to a sensing ecosystem currently dominated by machine vision and audition. This review and industry roadmap surveys the scientific foundations, technological frontiers, and strategic applications of machine olfaction making the case that we are currently witnessing the rise of a new industry that brings with it a global chemosensory infrastructure. We cover exemplary industrial, military and consumer applications and address some of the ethical and legal concerns arising. We find that machine olfaction is poised to bring forth a planet-wide molecular awareness tech layer with the potential of spawning vast emerging markets in health, security, and environmental sensing via scent.",0,arxiv,Biyoloji,CC-BY/arXiv,Machine Olfaction and Embedded AI Are Shaping the New Global Sensing Industry
"The molecular large language models have garnered widespread attention due to their promising potential on molecular applications. However, current molecular large language models face significant limitations in understanding molecules due to inadequate textual descriptions and suboptimal molecular representation strategies during pretraining. To address these challenges, we introduce KnowMol-100K, a large-scale dataset with 100K fine-grained molecular annotations across multiple levels, bridging the gap between molecules and textual descriptions. Additionally, we propose chemically-informative molecular representation, effectively addressing limitations in existing molecular representation strategies. Building upon these innovations, we develop KnowMol, a state-of-the-art multi-modal molecular large language model. Extensive experiments demonstrate that KnowMol achieves superior performance across molecular understanding and generation tasks.   GitHub: https://github.com/yzf-code/KnowMol   Huggingface: https://hf.co/datasets/yzf1102/KnowMol-100K",0,arxiv,Biyoloji,CC-BY/arXiv,KnowMol: Advancing Molecular Large Language Models with Multi-Level Chemical Knowledge
"The rapid evolution of molecular dynamics (MD) methods, including machine-learned dynamics, has outpaced the development of standardized tools for method validation. Objective comparison between simulation approaches is often hindered by inconsistent evaluation metrics, insufficient sampling of rare conformational states, and the absence of reproducible benchmarks. To address these challenges, we introduce a modular benchmarking framework that systematically evaluates protein MD methods using enhanced sampling analysis. Our approach uses weighted ensemble (WE) sampling via The Weighted Ensemble Simulation Toolkit with Parallelization and Analysis (WESTPA), based on progress coordinates derived from Time-lagged Independent Component Analysis (TICA), enabling fast and efficient exploration of protein conformational space. The framework includes a flexible, lightweight propagator interface that supports arbitrary simulation engines, allowing both classical force fields and machine learning-based models. Additionally, the framework offers a comprehensive evaluation suite capable of computing more than 19 different metrics and visualizations across a variety of domains. We further contribute a dataset of nine diverse proteins, ranging from 10 to 224 residues, that span a variety of folding complexities and topologies. Each protein has been extensively simulated at 300K for one million MD steps per starting point (4 ns). To demonstrate the utility of our framework, we perform validation tests using classic MD simulations with implicit solvent and compare protein conformational sampling using a fully trained versus under-trained CGSchNet model. By standardizing evaluation protocols and enabling direct, reproducible comparisons across MD approaches, our open-source platform lays the groundwork for consistent, rigorous benchmarking across the molecular simulation community.",0,arxiv,Biyoloji,CC-BY/arXiv,A Standardized Benchmark for Machine-Learned Molecular Dynamics using Weighted Ensemble Sampling
"Biological machine learning is often bottlenecked by a lack of scaled data. One promising route to relieving data bottlenecks is through high throughput screens, which can experimentally test the activity of $10^6-10^{12}$ protein sequences in parallel. In this article, we introduce algorithms to optimize high throughput screens for data creation and model training. We focus on the large scale regime, where dataset sizes are limited by the cost of measurement and sequencing. We show that when active sequences are rare, we maximize information gain if we only collect positive examples of active sequences, i.e. $x$ with $y>0$. We can correct for the missing negative examples using a generative model of the library, producing a consistent and efficient estimate of the true $p(y | x)$. We demonstrate this approach in simulation and on a large scale screen of antibodies. Overall, co-design of experiments and inference lets us accelerate learning dramatically.",0,arxiv,Biyoloji,CC-BY/arXiv,Accelerated Learning on Large Scale Screens using Generative Library Models
"Applications of machine learning in chemistry are often limited by the scarcity and expense of labeled data, restricting traditional supervised methods. In this work, we introduce a framework for molecular reasoning using general-purpose Large Language Models (LLMs) that operates without requiring labeled training data. Our method anchors chain-of-thought reasoning to the molecular structure by using unique atomic identifiers. First, the LLM performs a one-shot task to identify relevant fragments and their associated chemical labels or transformation classes. In an optional second step, this position-aware information is used in a few-shot task with provided class examples to predict the chemical transformation. We apply our framework to single-step retrosynthesis, a task where LLMs have previously underperformed. Across academic benchmarks and expert-validated drug discovery molecules, our work enables LLMs to achieve high success rates in identifying chemically plausible reaction sites ($\geq90\%$), named reaction classes ($\geq40\%$), and final reactants ($\geq74\%$). Beyond solving complex chemical tasks, our work also provides a method to generate theoretically grounded synthetic datasets by mapping chemical knowledge onto the molecular structure and thereby addressing data scarcity.",0,arxiv,Biyoloji,CC-BY/arXiv,Atom-anchored LLMs speak Chemistry: A Retrosynthesis Demonstration
"By stabilizing weak and transient protein-protein interactions (PPIs), molecular glues address the challenge of targeting proteins previously considered undruggable. Rapamycin and WDB002 are molecular glues that bind to FK506-binding protein (FKBP12) and target the FKBP12-rapamycin-associated protein (FRAP) and the centrosomal protein 250 (CEP250), respectively. Here, we used molecular dynamics simulations to gain insights into the effects of molecular glues on protein conformation and PPIs. The molecular glues modulated protein flexibility, leading to less flexibility in some regions, and changed the pattern and stability of water-mediated hydrogen bonds between the proteins. Our findings highlight the importance of considering water-mediated hydrogen bonds in developing strategies for the rational design of molecular glues.",0,arxiv,Biyoloji,CC-BY/arXiv,Molecular glues stabilize water-mediated hydrogen bonds in ternary complexes
"Single-particle cryo-EM has transformed structural biology but still faces challenges in resolving conformational heterogeneity at atomic resolution. Existing cryo-EM heterogeneity analysis methods either lack atomic details or tend to subject to overfitting due to image noise and limited information in single views. To obtain atomic detailed multiple conformations and make full use of particle images of different orientations, we present here CryoDyna, a deep learning framework to infer macromolecular dynamics directly from 2D projections by integrating cross-view attention and multi-scale deformation modeling. Combining coarse-grained MARTINI representation with atomic backmapping, CryoDyna achieves near-atomic interpretation of protein conformational landscapes. Validated on multiple simulated and experimental datasets, CryoDyna demonstrates improved modeling accuracy and robustly recovers multi-scale complex structure changes hidden in the cryo-EM particle stacks. As examples, we generated protein-RNA coordinated motions, resolved dynamics in the unseen region of RAG signal end complex, mapped translocating ribosome states in a one-shot manner, and revealed step-wise closure of a membrane-anchored protein multimer. This work bridges the gap between cryo-EM heterogeneity analysis and atomic-scale structural dynamics, offering a promising tool for exploration of complex biological mechanisms.",0,arxiv,Biyoloji,CC-BY/arXiv,CryoDyna: Multiscale end-to-end modeling of cryo-EM macromolecule dynamics with physics-aware neural network
"Recent advances in protein structure prediction, such as AlphaFold, have demonstrated the power of deep neural architectures like the Evoformer for capturing complex spatial and evolutionary constraints on protein conformation. However, the depth of the Evoformer, comprising 48 stacked blocks, introduces high computational costs and rigid layerwise discretization. Inspired by Neural Ordinary Differential Equations (Neural ODEs), we propose a continuous-depth formulation of the Evoformer, replacing its 48 discrete blocks with a Neural ODE parameterization that preserves its core attention-based operations. This continuous-time Evoformer achieves constant memory cost (in depth) via the adjoint method, while allowing a principled trade-off between runtime and accuracy through adaptive ODE solvers. Benchmarking on protein structure prediction tasks, we find that the Neural ODE-based Evoformer produces structurally plausible predictions and reliably captures certain secondary structure elements, such as alpha-helices, though it does not fully replicate the accuracy of the original architecture. However, our model achieves this performance using dramatically fewer resources, just 17.5 hours of training on a single GPU, highlighting the promise of continuous-depth models as a lightweight and interpretable alternative for biomolecular modeling. This work opens new directions for efficient and adaptive protein structure prediction frameworks.",0,arxiv,Biyoloji,CC-BY/arXiv,Protein Folding with Neural Ordinary Differential Equations
"Cancer treatment with radiotherapy aims to kill tumor cells and spare healthy tissue.Thus,the experimentally observed sparing of healthy tissue by the FLASH effect during irradiations with ultra-high dose rates (UHDR) enables clinicians to extend the therapeutic window.However, the underlying radiobiological and chemical mechanisms are far from being understood.DNA is one of the main molecular targets for radiotherapy.Ionizing radiation damage to DNA in water depends strongly on salt,pH,buffer and oxygen content of the solvent.Here we present a study of plasmid DNA pUC19,irradiated with 18MeV electrons at low dose rates (LDR) and UHDR under tightly controlled ambient and physiological oxygen conditions in PBS at pH 7.4.For the first time a sparing effect of DNA strand-break induction between UHDR(>10MGy/s) and LDR(<0.1Gy/s) irradiated plasmid DNA under physiological oxygen, salt and pH is observed for total doses above 10Gy.Under physiological oxygen (physoxia,5%O2,40mmHg),more single (SSB) and double strand-breaks (DSB) are observed when exposed to LDR, than to UHDR.This behaviour is absent for ambient oxygen (normoxia,21%O2,150-160mmHg).The experiments are accompanied by TOPAS-nBio based particle-scattering and chemical MCS to obtain information about the yields of reactive oxygen species (ROS).Hereby,an extended set of chemical reactions was considered, which improved upon the discrepancy between experiment and simulations of previous works, and allowed to predict DR dependent g-values of hydrogen peroxide (H2O2).To explain the observed DNA sparing effect under FLASH conditions at physoxia,the following model was proposed:The interplay of O2 with OH induced H-abstraction at the phosphate backbone,and the conversion of DNA base-damage to SSB,under consideration of the dose-rate dependent H3O+ yield via beta elimination processes is accounted for, to explain the observed behavior.",0,arxiv,Biyoloji,CC-BY/arXiv,Sparing of DNA irradiated with Ultra-High Dose-Rates under Physiological Oxygen and Salt conditions
"Molecular optimization is a central task in drug discovery that requires precise structural reasoning and domain knowledge. While large language models (LLMs) have shown promise in generating high-level editing intentions in natural language, they often struggle to faithfully execute these modifications-particularly when operating on non-intuitive representations like SMILES. We introduce MECo, a framework that bridges reasoning and execution by translating editing actions into executable code. MECo reformulates molecular optimization for LLMs as a cascaded framework: generating human-interpretable editing intentions from a molecule and property goal, followed by translating those intentions into executable structural edits via code generation. Our approach achieves over 98% accuracy in reproducing held-out realistic edits derived from chemical reactions and target-specific compound pairs. On downstream optimization benchmarks spanning physicochemical properties and target activities, MECo substantially improves consistency by 38-86 percentage points to 90%+ and achieves higher success rates over SMILES-based baselines while preserving structural similarity. By aligning intention with execution, MECo enables consistent, controllable and interpretable molecular design, laying the foundation for high-fidelity feedback loops and collaborative human-AI workflows in drug discovery.",0,arxiv,Biyoloji,CC-BY/arXiv,Coder as Editor: Code-driven Interpretable Molecular Optimization
"This independent research investigates methods to improve the precision of cyclic peptide generation targeting the HIV gp120 trimer using AlphaFold. The study explores proximity-based hotspot mapping at the CD4 binding site, centroid distance penalization, generative loss tuning, and custom loss function development. These enhancements produced cyclic peptides that closely resemble the binding conformation of the CD4 attachment inhibitor BMS-818251. The proposed methodology demonstrates improved structural control and precision in cyclic peptide generation, advancing the applicability of AlphaFold in structure-based drug discovery.",0,arxiv,Biyoloji,CC-BY/arXiv,Precision Design of Cyclic Peptides using AlphaFold
"mRNA design and optimization are important in synthetic biology and therapeutic development, but remain understudied in machine learning. Systematic optimization of mRNAs is hindered by the scarce and imbalanced data as well as complex sequence-function relationships. We present RNAGenScape, a property-guided manifold Langevin dynamics framework that iteratively updates mRNA sequences within a learned latent manifold. RNAGenScape combines an organized autoencoder, which structures the latent space by target properties for efficient and biologically plausible exploration, with a manifold projector that contracts each step of update back to the manifold. RNAGenScape supports property-guided optimization and smooth interpolation between sequences, while remaining robust under scarce and undersampled data, and ensuring that intermediate products are close to the viable mRNA manifold. Across three real mRNA datasets, RNAGenScape improves the target properties with high success rates and efficiency, outperforming various generative or optimization methods developed for proteins or non-biological data. By providing continuous, data-aligned trajectories that reveal how edits influence function, RNAGenScape establishes a scalable paradigm for controllable mRNA design and latent space exploration in mRNA sequence modeling.",0,arxiv,Biyoloji,CC-BY/arXiv,RNAGenScape: Property-guided Optimization and Interpolation of mRNA Sequences with Manifold Langevin Dynamics
"Background: In the Nearest-Neighbor Thermodynamic Model, a standard approach for RNA secondary structure prediction, the energy of the multiloops is modeled using a linear entropic penalty governed by three branching parameters. Although these parameters are typically fixed, recent work has shown that reparametrizing the multiloop score and considering alternative branching conformations can lead to significantly better structure predictions. However, prior approaches for exploring the alternative branching structures were computationally inefficient for long sequences.   Results: We present a novel algorithm that partitions the parameter space, identifying all distinct branching structures (optimal under different branching parameters) for a given RNA sequence using the fewest possible minimum free energy computations. Our method efficiently computes the full parameter-space partition and the associated optimal structures, enabling a comprehensive evaluation of the structural landscape across parameter choices. We apply this algorithm to the Archive II benchmarking dataset, assessing the maximum attainable prediction accuracy for each sequence under the reparameterized multiloop model. We find that the potential for improvement over default predictions is substantial in many cases, and that the optimal prediction accuracy is highly sensitive to auxiliary modeling decisions, such as the treatment of lonely base pairs and dangling ends.   Conclusion: Our results support the hypothesis that the conventional choice of multiloop parameters may limit prediction accuracy and that exploring alternative parameterizations is both tractable and worthwhile. The efficient partitioning algorithm we introduce makes this exploration feasible for longer sequences and larger datasets. Furthermore, we identify several open challenges in identifying the optimal structure.",0,arxiv,Biyoloji,CC-BY/arXiv,An Efficient Algorithm for Exploring RNA Branching Conformations under the Nearest-Neighbor Thermodynamic Model
"Deciphering the function of unseen protein sequences is a fundamental challenge with broad scientific impact, yet most existing methods depend on task-specific adapters or large-scale supervised fine-tuning. We introduce the ""Protein-as-Second-Language"" framework, which reformulates amino-acid sequences as sentences in a novel symbolic language that large language models can interpret through contextual exemplars. Our approach adaptively constructs sequence-question-answer triples that reveal functional cues in a zero-shot setting, without any further training. To support this process, we curate a bilingual corpus of 79,926 protein-QA instances spanning attribute prediction, descriptive understanding, and extended reasoning. Empirically, our method delivers consistent gains across diverse open-source LLMs and GPT-4, achieving up to 17.2% ROUGE-L improvement (average +7%) and even surpassing fine-tuned protein-specific language models. These results highlight that generic LLMs, when guided with protein-as-language cues, can outperform domain-specialized models, offering a scalable pathway for protein understanding in foundation models.",0,arxiv,Biyoloji,CC-BY/arXiv,Protein as a Second Language for LLMs
"Generative modeling techniques such as Diffusion and Flow Matching have achieved significant successes in generating designable and diverse protein backbones. However, many current models are computationally expensive, requiring hundreds or even thousands of function evaluations (NFEs) to yield samples of acceptable quality, which can become a bottleneck in practical design campaigns that often generate $10^4\ -\ 10^6$ designs per target. In image generation, Rectified Flows (ReFlow) can significantly reduce the required NFEs for a given target quality, but their application in protein backbone generation has been less studied. We apply ReFlow to improve the low NFE performance of pretrained SE(3) flow matching models for protein backbone generation and systematically study ReFlow design choices in the context of protein generation in data curation, training and inference time settings. In particular, we (1) show that ReFlow in the protein domain is particularly sensitive to the choice of coupling generation and annealing, (2) demonstrate how useful design choices for ReFlow in the image domain do not directly translate to better performance on proteins, and (3) make improvements to ReFlow methodology for proteins.",0,arxiv,Biyoloji,CC-BY/arXiv,"Flows, straight but not so fast: Exploring the design space of Rectified Flows in Protein Design"
"The rapid adoption of generative artificial intelligence (GenAI) in the biosciences is transforming biotechnology, medicine, and synthetic biology. Yet this advancement is intrinsically linked to new vulnerabilities, as GenAI lowers the barrier to misuse and introduces novel biosecurity threats, such as generating synthetic viral proteins or toxins. These dual-use risks are often overlooked, as existing safety guardrails remain fragile and can be circumvented through deceptive prompts or jailbreak techniques. In this Perspective, we first outline the current state of GenAI in the biosciences and emerging threat vectors ranging from jailbreak attacks and privacy risks to the dual-use challenges posed by autonomous AI agents. We then examine urgent gaps in regulation and oversight, drawing on insights from 130 expert interviews across academia, government, industry, and policy. A large majority ($\approx 76$\%) expressed concern over AI misuse in biology, and 74\% called for the development of new governance frameworks. Finally, we explore technical pathways to mitigation, advocating a multi-layered approach to GenAI safety. These defenses include rigorous data filtering, alignment with ethical principles during development, and real-time monitoring to block harmful requests. Together, these strategies provide a blueprint for embedding security throughout the GenAI lifecycle. As GenAI becomes integrated into the biosciences, safeguarding this frontier requires an immediate commitment to both adaptive governance and secure-by-design technologies.",0,arxiv,Biyoloji,CC-BY/arXiv,Generative AI for Biosciences: Emerging Threats and Roadmap to Biosecurity
"Generative models frequently suffer miscalibration, wherein class probabilities and other statistics of the sampling distribution deviate from desired values. We frame calibration as a constrained optimization problem and seek the closest model in Kullback-Leibler divergence satisfying calibration constraints. To address the intractability of imposing these constraints exactly, we introduce two surrogate objectives for fine-tuning: (1) the relax loss, which replaces the constraint with a miscalibration penalty, and (2) the reward loss, which converts calibration into a reward fine-tuning problem. We demonstrate that these approaches substantially reduce calibration error across hundreds of simultaneous constraints and models with up to one billion parameters, spanning applications in protein design, image generation, and language modeling.",0,arxiv,Biyoloji,CC-BY/arXiv,Calibrating Generative Models
"DNA strand displacement (SD) reactions are central to the operation of many synthetic nucleic acid systems, including molecular circuits, sensors, and machines. Over the years, a broad set of design frameworks has emerged to accommodate various functional goals, initial configurations, and environmental conditions. Nevertheless, key challenges persist, particularly in reliably predicting reaction kinetics. This review examines recent approaches to SD reaction design, with emphasis on the properties of single reactions, including kinetics, structural factors, and limitations in current modelling practices. We identify promising innovations while analysing the factors that continue to hinder predictive accuracy. We conclude by outlining future directions for achieving more robust and programmable behaviour in DNA-based systems.",0,arxiv,Biyoloji,CC-BY/arXiv,Design of DNA Strand Displacement Reactions
"Biomolecules exhibit a remarkable property of transforming signals from their environment. This paper presents a communication system design using a light-modulated protein channel: Synthetic Photoisomerizable Azobenzene-regulated K+ (SPARK). Our approach involves a comprehensive design incorporating the SPARK-based receiver, encoding methods, modulation techniques, and detection processes. By analyzing the resulting communication system, we determine how different parameters influence its performance. Furthermore, we explore the potential design in terms of bioengineering and demonstrate that the data rate scales up with the number of receptors, indicating the possibility of achieving high-speed communication.",0,arxiv,Biyoloji,CC-BY/arXiv,Communication System Design using Synthetic Photoisomerizable Azobenzene-Regulated K+(SPARK) channel
"Biomolecular interaction modeling has been substantially advanced by foundation models, yet they often produce all-atom structures that violate basic steric feasibility. We address this limitation by enforcing physical validity as a strict constraint during both training and inference with a uniffed module. At its core is a differentiable projection that maps the provisional atom coordinates from the diffusion model to the nearest physically valid conffguration. This projection is achieved using a Gauss-Seidel scheme, which exploits the locality and sparsity of the constraints to ensure stable and fast convergence at scale. By implicit differentiation to obtain gradients, our module integrates seamlessly into existing frameworks for end-to-end ffnetuning. With our Gauss-Seidel projection module in place, two denoising steps are sufffcient to produce biomolecular complexes that are both physically valid and structurally accurate. Across six benchmarks, our 2-step model achieves the same structural accuracy as state-of-the-art 200-step diffusion baselines, delivering approximately 10 times faster wall-clock speed while guaranteeing physical validity.",0,arxiv,Biyoloji,CC-BY/arXiv,Physically Valid Biomolecular Interaction Modeling with Gauss-Seidel Projection
"Biological processes rely on finely tuned homo- and heteromeric interactions between (biomacro)molecules. The strength of an interaction, typically given by the dissociation constant (KD), plays a crucial role in basic research and must be monitored throughout the development of drugs and agrochemicals. An ideal method for KD determination is applicable to various analytes with a large range of affinities, tolerates complex matrix compositions, does not require labeling, and simultaneously provides information on the structural integrity of the binding partners. Native mass spectrometry meets these criteria but typically struggles with homooligomeric complexes due to overlapping mass signals. To overcome this, we resolve monomer/dimer contributions to overlapping MS-peaks by separately analyzing the charge state distribution of each oligomeric species via sample dilution and covalent crosslinking. Following this approach, we show that quantitative Laser-Induced Liquid Bead Ion Desorption mass spectrometry (qLILBID-MS) accurately captures the affinities of Bovine Serum Albumin and chemically induced dimers of Tryparedoxin, an oxidoreductase from human pathogenic Trypanosoma brucei parasites, with various molecular glues and homodimer affinities. Conveniently, qLILBID-MS requires a fraction of sample used by other methods such as isothermal titration calorimetry and yields previously inaccessible protein homodimer KDs in the high micromolar range, which allowed us to monitor the gradual decrease in homodimer affinity via mutation of crucial dimer interface contacts. Overall, qLILBID-MS is a sensitive, robust, fast, scalable, and cost-effective alternative to quantify protein/protein interactions that can accelerate contemporary drug discovery workflows, e.g. the efficient screening for proximity inducing molecules like proteolysis targeting chimera and molecular glues.",0,arxiv,Biyoloji,CC-BY/arXiv,Quantification of protein homodimer affinity using native mass spectrometry
"Predicting the fitness impact of mutations is central to protein engineering but constrained by limited assays relative to the size of sequence space. Protein language models (pLMs) trained with masked language modeling (MLM) exhibit strong zero-shot fitness prediction; we provide a unifying view by interpreting natural evolution as implicit reward maximization and MLM as inverse reinforcement learning (IRL), in which extant sequences act as expert demonstrations and pLM log-odds serve as fitness estimates. Building on this perspective, we introduce EvoIF, a lightweight model that integrates two complementary sources of evolutionary signal: (i) within-family profiles from retrieved homologs and (ii) cross-family structural-evolutionary constraints distilled from inverse folding logits. EvoIF fuses sequence-structure representations with these profiles via a compact transition block, yielding calibrated probabilities for log-odds scoring. On ProteinGym (217 mutational assays; >2.5M mutants), EvoIF and its MSA-enabled variant achieve state-of-the-art or competitive performance while using only 0.15% of the training data and fewer parameters than recent large models. Ablations confirm that within-family and cross-family profiles are complementary, improving robustness across function types, MSA depths, taxa, and mutation depths. The codes will be made publicly available at https://github.com/aim-uofa/EvoIF.",0,arxiv,Biyoloji,CC-BY/arXiv,Evolutionary Profiles for Protein Fitness Prediction
"Drug-drug interactions (DDIs) are a leading cause of preventable adverse events, often complicating treatment and increasing healthcare costs. At the same time, knowing which drugs do not interact is equally important, as such knowledge supports safer prescriptions and better patient outcomes. In this study, we propose an interpretable and efficient framework that blends modern machine learning with domain knowledge to improve DDI prediction. Our approach combines two complementary molecular embeddings - Mol2Vec, which captures fragment-level structural patterns, and SMILES-BERT, which learns contextual chemical features - together with a leakage-free, rule-based clinical score (RBScore) that injects pharmacological knowledge without relying on interaction labels. A lightweight neural classifier is then optimized using a novel three-stage metaheuristic strategy (RSmpl-ACO-PSO), which balances global exploration and local refinement for stable performance. Experiments on real-world datasets demonstrate that the model achieves high predictive accuracy (ROC-AUC 0.911, PR-AUC 0.867 on DrugBank) and generalizes well to a clinically relevant Type 2 Diabetes Mellitus cohort. Beyond raw performance, studies show how embedding fusion, RBScore, and the optimizer each contribute to precision and robustness. Together, these results highlight a practical pathway for building reliable, interpretable, and computationally efficient models that can support safer drug therapies and clinical decision-making.",0,arxiv,Biyoloji,CC-BY/arXiv,A Hybrid Computational Intelligence Framework with Metaheuristic Optimization for Drug-Drug Interaction Prediction
"Physicochemically informed biological sequence generation has the potential to accelerate computer-aided cellular therapy, yet current models fail to \emph{jointly} ensure novelty, diversity, and biophysical plausibility when designing variable regions of T-cell receptors (TCRs). We present \textbf{PhysicoGPTCR}, a large generative protein Transformer that is \emph{dual-conditioned} on peptide and HLA context and trained to autoregressively synthesise TCR sequences while embedding residue-level physicochemical descriptors. The model is optimised on curated TCR--peptide--HLA triples with a maximum-likelihood objective and compared against ANN, GPTCR, LSTM, and VAE baselines. Across multiple neoantigen benchmarks, PhysicoGPTCR substantially improves edit-distance, similarity, and longest-common-subsequence scores, while populating a broader region of sequence space. Blind in-silico docking and structural modelling further reveal a higher proportion of binding-competent clones than the strongest baseline, validating the benefit of explicit context conditioning and physicochemical awareness. Experimental results demonstrate that dual-conditioned, physics-grounded generative modelling enables end-to-end design of functional TCR candidates, reducing the discovery timeline from months to minutes without sacrificing wet-lab verifiability.",0,arxiv,Biyoloji,CC-BY/arXiv,Physicochemically Informed Dual-Conditioned Generative Model of T-Cell Receptor Variable Regions for Cellular Therapy
"The specific region of an antibody responsible for binding to an antigen, known as the paratope, is essential for immune recognition. Accurate identification of this small yet critical region can accelerate the development of therapeutic antibodies. Determining paratope locations typically relies on modeling the antibody structure, which is computationally intensive and difficult to scale across large antibody repertoires. We introduce Paraplume, a sequence-based paratope prediction method that leverages embeddings from protein language models (PLMs), without the need for structural input and achieves superior performance across multiple benchmarks compared to current methods. In addition, reweighting PLM embeddings using Paraplume predictions yields more informative sequence representations, improving downstream tasks such as affinity prediction, binder classification, and epitope binning. Applied to large antibody repertoires, Paraplume reveals that antigen-specific somatic hypermutations are associated with larger paratopes, suggesting a potential mechanism for affinity enhancement. Our findings position PLM-based paratope prediction as a powerful, scalable alternative to structure-dependent approaches, opening new avenues for understanding antibody evolution.",0,arxiv,Biyoloji,CC-BY/arXiv,Paraplume: A fast and accurate paratope prediction method provides insights into repertoire-scale binding dynamics
"Correlated motions of proteins underpin many physiological mechanisms, such as substrate binding, signal transduction, enzymatic activity and allostery. These motions arise from low frequency collective movements of biomolecules and have mostly been studied using molecular dynamics simulations. Here, we present the effects of two different empirical energy force fields used for molecular dynamics simulations on correlated motions -- the non-polarizable CHARMM 36m additive force field and the polarizable Drude-2019 force field. The study was conducted on two proteins, ubiquitin - a small protein with a well-described dynamic - and the nuclear receptor protein PPAR___. The ligand binding domain of PPAR___ was of particular interest since its function is to regulate transcription through ligand and coregulator protein binding. It has been previously shown that a dynamical network of correlated motions ensures the transmission of information related to PPAR___ ligand binding. We present the results of classical MD simulations where we analyze the results in terms of residue fluctuations, residue correlation maps, community network analysis and hydrophobic cluster analysis. We find that RMS fluctuations tend to be greater and correlated motions are less intense with Drude-2019 force field than with the non-polarizable all atom additive force field. Analysis of large hydrophobic clusters in the respective proteins show a greater loss of native contacts in the simulations using the Drude-2019 force field than in the simulations using the all atom force additive force field. Our results provide the first quantification of the impact of using a polarizable force field in computational studies that focus on correlated motions.",0,arxiv,Biyoloji,CC-BY/arXiv,Impact of Force Field Polarization on Correlated Motions of Proteins
"DNA frequently adopts liquid-crystalline conformations in both cells and viruses. The Oseen--Frank framework provides a powerful continuum description of these phases through three elastic moduli: splay ($K_1$), twist or cholesteric ($K_2$), and bending ($K_3$). While $K_1$ is typically assumed to dominate, the relative magnitude of $K_2$ and $K_3$ in confined DNA remains poorly understood. Here, we combine cryo-electron microscopy, liquid-crystal modeling, and knot theory to quantify this relationship in bacteriophage P4, whose genome is partially organized in a spool-like liquid-crystalline phase. We first show experimentally that the ordered DNA occupies three concentric layers within the capsid. We then formulate an Oseen--Frank model for this geometry and use it, together with the measured layer radii, to estimate the elastic ratio $Î±= K_3/K_2$. We find $Î±\approx 0.0064$, indicating that twist elasticity overwhelmingly dominates bending. To validate this result, we perform Langevin dynamics simulations of DNA trajectories and classify the resulting knots. The predicted knot distribution agrees with experimental data from P4, demonstrating consistency between elasticity, topology, and observed genome organization.",0,arxiv,Biyoloji,CC-BY/arXiv,Twist dominates bending in the liquid crystal organization of bacteriophage DNA
"Due to the ever-rising global incidence rate of inflammatory bowel disease (IBD) and the lack of effective clinical treatment drugs, elucidating the detailed pathogenesis, seeking novel targets, and developing promising drugs are the top priority for IBD treatment. Here, we demonstrate that the levels of microRNA (miR)-103a were significantly downregulated in the inflamed mucosa of ulcerative colitis (UC) patients, along with elevated inflammatory cytokines (IL-1beta/TNF-alpha) and reduced tight junction protein (Occludin/ZO-1) levels, as compared with healthy control objects. Consistently, miR-103a deficient intestinal epithelial cells Caco-2 showed serious inflammatory responses and increased permeability, and DSS induced more severe colitis in miR-103a-/- mice than wild-type ones. Mechanistic studies unraveled that c-FOS suppressed miR-103a transcription via binding to its promoter, then miR-103a-targeted NF-kappaB activation contributes to inflammatory responses and barrier disruption by targeting TAB2 and TAK1. Notably, the traditional Chinese medicine Cornus officinalis (CO) and its core active ingredient loganin potently mitigated inflammation and barrier disruption in UC by specifically blocking the EGFR/RAS/ERK/c-FOS signaling axis, these effects mainly attributed to modulated miR-103a levels as the therapeutic activities of them were almost completely shielded in miR-103a KO mice. Taken together, this work reveals that loganin relieves EGFR/c-FOS axis-suppressed epithelial miR-103a expression, thereby inhibiting NF-kappaB pathway activation, suppressing inflammatory responses, and preserving tight junction integrity in UC. Thus, our data enrich mechanistic insights and promising targets for UC treatment.",0,arxiv,Biyoloji,CC-BY/arXiv,Relief of EGFR/FOS-downregulated miR-103a by loganin alleviates NF-kappaB-triggered inflammation and gut barrier disruption in colitis
"Deep learning, particularly with the advancement of Large Language Models, has transformed biomolecular modeling, with protein advances (e.g., ESM) inspiring emerging RNA language models such as RiNALMo. Yet how and what these RNA Language Models internally encode about messenger RNA (mRNA) or non-coding RNA (ncRNA) families remains unclear. We present SAE- RNA, interpretability model that analyzes RiNALMo representations and maps them to known human-level biological features. Our work frames RNA interpretability as concept discovery in pretrained embeddings, without end-to-end retraining, and provides practical tools to probe what RNA LMs may encode about ncRNA families. The model can be extended to close comparisons between RNA groups, and supporting hypothesis generation about previously unrecognized relationships.",0,arxiv,Biyoloji,CC-BY/arXiv,SAE-RNA: A Sparse Autoencoder Model for Interpreting RNA Language Model Representations
"We present FLOWR:root, an equivariant flow-matching model for pocket-aware 3D ligand generation with joint binding affinity prediction and confidence estimation. The model supports de novo generation, pharmacophore-conditional sampling, fragment elaboration, and multi-endpoint affinity prediction (pIC50, pKi, pKd, pEC50). Training combines large-scale ligand libraries with mixed-fidelity protein-ligand complexes, followed by refinement on curated co-crystal datasets and parameter-efficient finetuning for project-specific adaptation. FLOWR:root achieves state-of-the-art performance in unconditional 3D molecule generation and pocket-conditional ligand design, producing geometrically realistic, low-strain structures. The integrated affinity prediction module demonstrates superior accuracy on the SPINDR test set and outperforms recent models on the Schrodinger FEP+/OpenFE benchmark with substantial speed advantages. As a foundation model, FLOWR:root requires finetuning on project-specific datasets to account for unseen structure-activity landscapes, yielding strong correlation with experimental data. Joint generation and affinity prediction enable inference-time scaling through importance sampling, steering molecular design toward higher-affinity compounds. Case studies validate this: selective CK2$Î±$ ligand generation against CLK3 shows significant correlation between predicted and quantum-mechanical binding energies, while ER$Î±$, TYK2 and BACE1 scaffold elaboration demonstrates strong agreement with QM calculations. By integrating structure-aware generation, affinity estimation, and property-guided sampling, FLOWR:root provides a comprehensive foundation for structure-based drug design spanning hit identification through lead optimization.",0,arxiv,Biyoloji,CC-BY/arXiv,FLOWR.root: A flow matching based foundation model for joint multi-purpose structure-aware 3D ligand generation and affinity prediction
"Graph Neural Networks (GNNs) are the dominant architecture for molecular machine learning, particularly for molecular property prediction and machine learning interatomic potentials (MLIPs). GNNs perform message passing on predefined graphs often induced by a fixed radius cutoff or k-nearest neighbor scheme. While this design aligns with the locality present in many molecular tasks, a hard-coded graph can limit expressivity due to the fixed receptive field and slows down inference with sparse graph operations. In this work, we investigate whether pure, unmodified Transformers trained directly on Cartesian coordinates$\unicode{x2013}$without predefined graphs or physical priors$\unicode{x2013}$can approximate molecular energies and forces. As a starting point for our analysis, we demonstrate how to train a Transformer to competitive energy and force mean absolute errors under a matched training compute budget, relative to a state-of-the-art equivariant GNN on the OMol25 dataset. We discover that the Transformer learns physically consistent patterns$\unicode{x2013}$such as attention weights that decay inversely with interatomic distance$\unicode{x2013}$and flexibly adapts them across different molecular environments due to the absence of hard-coded biases. The use of a standard Transformer also unlocks predictable improvements with respect to scaling training resources, consistent with empirical scaling laws observed in other domains. Our results demonstrate that many favorable properties of GNNs can emerge adaptively in Transformers, challenging the necessity of hard-coded graph inductive biases and pointing toward standardized, scalable architectures for molecular modeling.",0,arxiv,Biyoloji,CC-BY/arXiv,Transformers Discover Molecular Structure Without Graph Priors
"Steric clashes pose a challenge when exploring dense protein systems using conventional explicit-chain methods. A minimal example is a single lattice protein confined on a minimal grid, with no free sites. Finding its minimum energy is a hard optimization problem, withsimilarities to scheduling problems. It can be recast as a quadratic unconstrained binary optimization (QUBO) problem amenable to classical and quantum approaches. We show that this problem in its QUBO form can be swiftly and consistently solved for chain length 48, using either classical simulated annealing or hybrid quantum-classical annealing on a D-Wave system. In fact, the latter computations required about 10 seconds. We also test linear and quadratic programming methods, which work well for a lattice gas but struggle with chain constraints. All methods are benchmarked against exact results obtained from exhaustive structure enumeration, at a high computational cost.",0,arxiv,Biyoloji,CC-BY/arXiv,Folding lattice proteins confined on minimal grids using a quantum-inspired encoding
"Protein function is driven by coherent substructures which vary in size and topology, yet current protein representation learning models (PRL) distort these signals by relying on rigid substructures such as k-hop and fixed radius neighbourhoods. We introduce BioBlobs, a plug-and-play, fully differentiable module that represents proteins by dynamically partitioning structures into flexibly-sized, non-overlapping substructures (""blobs""). The resulting blobs are quantized into a shared and interpretable codebook, yielding a discrete vocabulary of function-relevant protein substructures used to compute protein embeddings. We show that BioBlobs representations improve the performance of widely used protein encoders such as GVP-GNN across various PRL tasks. Our approach highlights the value of architectures that directly capture function-relevant protein substructures, enabling both improved predictive performance and mechanistic insight into protein function.",0,arxiv,Biyoloji,CC-BY/arXiv,BioBlobs: Differentiable Graph Partitioning for Protein Representation Learning
"Protein language models (PLMs) have advanced computational protein science through large-scale pretraining and scalable architectures. In parallel, reinforcement learning (RL) has broadened exploration and enabled precise multi-objective optimization in protein design. Yet whether RL can push PLMs beyond their pretraining priors to uncover latent sequence-structure-function rules remains unclear. We address this by pairing RL with PLMs across four domains: antimicrobial peptide design, kinase variant optimization, antibody engineering, and inverse folding. Using diverse RL algorithms and model classes, we ask if RL improves sampling efficiency and, more importantly, if it reveals capabilities not captured by supervised learning. Across benchmarks, RL consistently boosts success rates and sample efficiency. Performance follows a three-factor interaction: task headroom, reward fidelity, and policy capacity jointly determine gains. When rewards are accurate and informative, policies have sufficient capacity, and tasks leave room beyond supervised baselines, improvements scale; when rewards are noisy or capacity is constrained, gains saturate despite exploration. This view yields practical guidance for RL in protein design: prioritize reward modeling and calibration before scaling policy size, match algorithm and regularization strength to task difficulty, and allocate capacity where marginal gains are largest. Implementation is available at https://github.com/chq1155/RL-PLM.",0,arxiv,Biyoloji,CC-BY/arXiv,From Supervision to Exploration: What Does Protein Language Model Learn During Reinforcement Learning?
"Building a working mental model of a protein typically requires weeks of reading, cross-referencing crystal and predicted structures, and inspecting ligand complexes, an effort that is slow, unevenly accessible, and often requires specialized computational skills. We introduce \emph{Speak to a Protein}, a new capability that turns protein analysis into an interactive, multimodal dialogue with an expert co-scientist. The AI system retrieves and synthesizes relevant literature, structures, and ligand data; grounds answers in a live 3D scene; and can highlight, annotate, manipulate and see the visualization. It also generates and runs code when needed, explaining results in both text and graphics. We demonstrate these capabilities on relevant proteins, posing questions about binding pockets, conformational changes, or structure-activity relationships to test ideas in real-time. \emph{Speak to a Protein} reduces the time from question to evidence, lowers the barrier to advanced structural analysis, and enables hypothesis generation by tightly coupling language, code, and 3D structures. \emph{Speak to a Protein} is freely accessible at https://open.playmolecule.org.",0,arxiv,Biyoloji,CC-BY/arXiv,Speak to a Protein: An Interactive Multimodal Co-Scientist for Protein Analysis
"Diffusion models offer a powerful means of capturing the manifold of realistic protein structures, enabling rapid design for protein engineering tasks. However, existing approaches observe critical failure modes when precise constraints are necessary for functional design. To this end, we present a constrained diffusion framework for structure-guided protein design, ensuring strict adherence to functional requirements while maintaining precise stereochemical and geometric feasibility. The approach integrates proximal feasibility updates with ADMM decomposition into the generative process, scaling effectively to the complex constraint sets of this domain. We evaluate on challenging protein design tasks, including motif scaffolding and vacancy-constrained pocket design, while introducing a novel curated benchmark dataset for motif scaffolding in the PDZ domain. Our approach achieves state-of-the-art, providing perfect satisfaction of bonding and geometric constraints with no degradation in structural diversity.",0,arxiv,Biyoloji,CC-BY/arXiv,Constrained Diffusion for Protein Design with Hard Structural Constraints
"Much of our mechanistic understanding of the functions of biological macromolecules is based on static structural experiments, which can be modelled either as single structures or conformational ensembles. While these provide us with invaluable insights, they do not directly reveal that molecules are inherently dynamic. Advances in time-dependent and time-resolved experimental methods have made it possible to capture the dynamics of biomolecules at increasingly higher spatial and temporal resolutions. To complement these, computational models can represent the structural and dynamical behaviour of biomolecules at atomistic resolution and femtosecond timescale, and are therefore useful to interpret these experiments. Here, we review the progress in integrating simulations with dynamical experiments, focusing on the combination of simulations with time-resolved and time-dependent experimental data.",0,arxiv,Biyoloji,CC-BY/arXiv,Integrative modelling of biomolecular dynamics
"Peptide drugs incorporating non-standard amino acids (NSAAs) offer improved binding affinity and improved pharmacological properties. However, existing peptide design methods are limited to standard amino acids, leaving NSAA-aware design largely unexplored. We introduce NS-Pep, a unified framework for co-designing peptide sequences and structures with NSAAs. The main challenge is that NSAAs are extremely underrepresented-even the most frequent one, SEP, accounts for less than 0.4% of residues-resulting in a severe long-tailed distribution. To improve generalization to rare amino acids, we propose Residue Frequency-Guided Modification (RFGM), which mitigates over-penalization through frequency-aware logit calibration, supported by both theoretical and empirical analysis. Furthermore, we identify that insufficient side-chain modeling limits geometric representation of NSAAs. To address this, we introduce Progressive Side-chain Perception (PSP) for coarse-to-fine torsion and location prediction, and Interaction-Aware Weighting (IAW) to emphasize pocket-proximal residues. Moreover, NS-Pep generalizes naturally to the peptide folding task with NSAAs, addressing a major limitation of current tools. Experiments show that NS-Pep improves sequence recovery rate and binding affinity by 6.23% and 5.12%, respectively, and outperforms AlphaFold3 by 17.76% in peptide folding success rate.",0,arxiv,Biyoloji,CC-BY/arXiv,NS-Pep: De novo Peptide Design with Non-Standard Amino Acids
"While deep learning has revolutionized the prediction of rigid protein structures, modelling the conformational ensembles of Intrinsically Disordered Proteins (IDPs) remains a key frontier. Current AI paradigms present a trade-off: Protein Language Models (PLMs) capture evolutionary statistics but lack explicit physical grounding, while generative models trained to model full ensembles are computationally expensive. In this work we critically assess these limits and propose a path forward. We introduce GeoGraph, a simulation-informed surrogate trained to predict ensemble-averaged statistics of residue-residue contact-map topology directly from sequence. By featurizing coarse-grained molecular dynamics simulations into residue- and sequence-level graph descriptors, we create a robust and information-rich learning target. Our evaluation demonstrates that this approach yields representations that are more predictive of key biophysical properties than existing methods.",0,arxiv,Biyoloji,CC-BY/arXiv,GeoGraph: Geometric and Graph-based Ensemble Descriptors for Intrinsically Disordered Proteins
"Designing sequences that satisfy multiple, often conflicting, objectives is a central challenge in therapeutic and biomolecular engineering. Existing generative frameworks largely operate in continuous spaces with single-objective guidance, while discrete approaches lack guarantees for multi-objective Pareto optimality. We introduce AReUReDi (Annealed Rectified Updates for Refining Discrete Flows), a discrete optimization algorithm with theoretical guarantees of convergence to the Pareto front. Building on Rectified Discrete Flows (ReDi), AReUReDi combines Tchebycheff scalarization, locally balanced proposals, and annealed Metropolis-Hastings updates to bias sampling toward Pareto-optimal states while preserving distributional invariance. Applied to peptide and SMILES sequence design, AReUReDi simultaneously optimizes up to five therapeutic properties (including affinity, solubility, hemolysis, half-life, and non-fouling) and outperforms both evolutionary and diffusion-based baselines. These results establish AReUReDi as a powerful, sequence-based framework for multi-property biomolecule generation.",0,arxiv,Biyoloji,CC-BY/arXiv,AReUReDi: Annealed Rectified Updates for Refining Discrete Flows with Multi-Objective Guidance
"Protein structure tokenizers enable the creation of multimodal models of protein structure, sequence, and function. Current approaches to protein structure tokenization rely on bespoke components that are invariant to spatial symmetries, but that are challenging to optimize and scale. We present Kanzi, a flow-based tokenizer for tokenization and generation of protein structures. Kanzi consists of a diffusion autoencoder trained with a flow matching loss. We show that this approach simplifies several aspects of protein structure tokenizers: frame-based representations can be replaced with global coordinates, complex losses are replaced with a single flow matching loss, and SE(3)-invariant attention operations can be replaced with standard attention. We find that these changes stabilize the training of parameter-efficient models that outperform existing tokenizers on reconstruction metrics at a fraction of the model size and training cost. An autoregressive model trained with Kanzi outperforms similar generative models that operate over tokens, although it does not yet match the performance of state-of-the-art continuous diffusion models. Code is available here: https://github.com/rdilip/kanzi/.",0,arxiv,Biyoloji,CC-BY/arXiv,Flow Autoencoders are Effective Protein Tokenizers
"Multimodal foundation models hold promise for drug discovery and biomedical applications, but most existing approaches rely on heavy pretraining or large scale multimodal corpora. We investigate whether thin contrastive bridges, lightweight projection heads over frozen unimodal encoders can align chemical and textual representations without training a full multimodal model. Using paired mechanisms from ChEMBL, we align ECFP4 molecular fingerprints with biomedical sentence embeddings through dual linear projections trained with a contrastive objective. To better handle drugs sharing the same therapeutic target, we incorporate hard negative weighting and a margin loss. Evaluation under scaffold based splits, which require generalization across disjoint chemical cores, demonstrates that our approach achieves non-trivial cross modal alignment and substantially improves within target discrimination compared to frozen baselines. These results suggest that thin bridges offer a compute efficient alternative to large scale multimodal pretraining, enabling scaffold aware drug text alignment and target specific retrieval in precision medicine.",0,arxiv,Biyoloji,CC-BY/arXiv,Thin Bridges for Drug Text Alignment: Lightweight Contrastive Learning for Target Specific Drug Retrieval
"Genetic mutations can disrupt protein structure, stability, and solubility, contributing to a wide range of diseases. Existing predictive models often lack interpretability and fail to integrate physical and chemical interactions critical to molecular mechanisms. Moreover, current approaches treat disease association, stability changes, and solubility alterations as separate tasks, limiting model generalizability. In this study, we introduce a unified framework based on multiscale commutative algebra to capture intrinsic physical and chemical interactions for the first time. Leveraging Persistent Stanley-Reisner Theory, we extract multiscale algebraic invariants to build a Commutative Algebra neural Network (CANet). Integrated with transformer features and auxiliary physical features, we apply CANet to tackle three key domains for the first time: disease-associated mutations, mutation-induced protein stability changes, and solubility changes upon mutations. Across six benchmark tasks, CANet and its gradient boosting tree counterpart, CATree, consistently attain state-of-the-art performance, achieving up to 7.5% improvement in predictive accuracy. Our approach offers multiscale, mechanistic, interpretable,and generalizable models for predicting disease-mutation associations.",0,arxiv,Biyoloji,CC-BY/arXiv,Commutative algebra neural network reveals genetic origins of diseases
"Recovering unbiased properties from biased or perturbed simulations is a central challenge in rare-event sampling. Classical Girsanov Reweighting (GR) offers a principled solution by yielding exact pathwise probability ratios between perturbed and reference processes. However, the variance of GR weights grows rapidly with time, rendering it impractical for long-horizon reweighting. We introduce Marginal Girsanov Reweighting (MGR), which mitigates variance explosion by marginalizing over intermediate paths, producing stable and scalable weights for long-timescale dynamics. Experiments demonstrate that MGR (i) accurately recovers kinetic properties from umbrella-sampling trajectories in molecular dynamics, and (ii) enables efficient Bayesian parameter inference for stochastic differential equations with temporally sparse observations.",0,arxiv,Biyoloji,CC-BY/arXiv,Marginal Girsanov Reweighting: Stable Variance Reduction via Neural Ratio Estimation
"Recent advances in structure-based protein design have accelerated de novo binder generation, yet interfaces on large domains or spanning multiple domains remain challenging due to high computational cost and declining success with increasing target size. We hypothesized that protein folding neural networks (PFNNs) operate in a ``local-first'' manner, prioritizing local interactions while displaying limited sensitivity to global foldability. Guided by this hypothesis, we propose an epitope-only strategy that retains only the discontinuous surface residues surrounding the binding site. Compared to intact-domain workflows, this approach improves in silico success rates by up to 80% and reduces the average time per successful design by up to forty-fold, enabling binder design against previously intractable targets such as ClpP and ALS3. Building on this foundation, we further developed a tailored pipeline that incorporates a Monte Carlo-based evolution step to overcome local minima and a position-specific biased inverse folding step to refine sequence patterns. Together, these advances not only establish a generalizable framework for efficient binder design against structurally large and otherwise inaccessible targets, but also support the broader ``local-first'' hypothesis as a guiding principle for PFNN-based design.",0,arxiv,Biyoloji,CC-BY/arXiv,Discontinuous Epitope Fragments as Sufficient Target Templates for Efficient Binder Design
"Reinforcement learning with stochastic optimal control offers a promising framework for diffusion fine-tuning, where a pre-trained diffusion model is optimized to generate paths that lead to a reward-tilted distribution. While these approaches enable optimization without access to explicit samples from the optimal distribution, they require training on rollouts under the current fine-tuned model, making them susceptible to reinforcing sub-optimal trajectories that yield poor rewards. To overcome this challenge, we introduce TRee Search Guided TRajectory-Aware Fine-Tuning for Discrete Diffusion (TR2-D2), a novel framework that optimizes reward-guided discrete diffusion trajectories with tree search to construct replay buffers for trajectory-aware fine-tuning. These buffers are generated using Monte Carlo Tree Search (MCTS) and subsequently used to fine-tune a pre-trained discrete diffusion model under a stochastic optimal control objective. We validate our framework on single- and multi-objective fine-tuning of biological sequence diffusion models, highlighting the overall effectiveness of TR2-D2 for reliable reward-guided fine-tuning in discrete sequence generation.",0,arxiv,Biyoloji,CC-BY/arXiv,TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion
"Molecular Dynamics (MD) is a powerful computational microscope for probing protein functions. However, the need for fine-grained integration and the long timescales of biomolecular events make MD computationally expensive. To address this, several generative models have been proposed to generate surrogate trajectories at lower cost. Yet, these models typically learn a fixed-lag transition density, causing the training signal to be dominated by frequent but uninformative transitions. We introduce a new class of generative models, MSM Emulators, which instead learn to sample transitions across discrete states defined by an underlying Markov State Model (MSM). We instantiate this class with Markov Space Flow Matching (MarS-FM), whose sampling offers more than two orders of magnitude speedup compared to implicit- or explicit-solvent MD simulations. We benchmark Mars-FM ability to reproduce MD statistics through structural observables such as RMSD, radius of gyration, and secondary structure content. Our evaluation spans protein domains (up to 500 residues) with significant chemical and structural diversity, including unfolding events, and enforces strict sequence dissimilarity between training and test sets to assess generalization. Across all metrics, MarS-FM outperforms existing methods, often by a substantial margin.",0,arxiv,Biyoloji,CC-BY/arXiv,MarS-FM: Generative Modeling of Molecular Dynamics via Markov State Models
"Accurate prediction of antibody-antigen (Ab-Ag) interfaces is critical for vaccine design, immunodiagnostics, and therapeutic antibody development. However, achieving reliable predictions from sequences alone remains a challenge. In this paper, we present ABCONFORMER, a model based on the Conformer backbone that captures both local and global features of a biosequence. To accurately capture Ab-Ag interactions, we introduced the physics-inspired sliding attention, enabling residue-level contact recovery without relying on three-dimensional structural data. ABConformer can accurately predict paratopes and epitopes given the antibody and antigen sequence, and predict pan-epitopes on the antigen without antibody information. In comparison experiments, ABCONFORMER achieves state-of-the-art performance on a recent SARS-CoV-2 Ab-Ag dataset, and surpasses widely used sequence-based methods for antibody-agnostic epitope prediction. Ablation studies further quantify the contribution of each component, demonstrating that, compared to conventional cross-attention, sliding attention significantly enhances the precision of epitope prediction. To facilitate reproducibility, we will release the code under an open-source license upon acceptance.",0,arxiv,Biyoloji,CC-BY/arXiv,ABConformer: Physics-inspired Sliding Attention for Antibody-Antigen Interface Prediction
"Performing cell-free expression (CFE) in tailored microfluidic environments is a powerful tool to investigate the organisation of biosystems from molecular to multicellular scales. While cell-free transcription-translation systems simplify and open up cellular biochemistry for manipulation, microfluidics enables miniaturisation and precise control over geometries and reaction conditions. In this review, we highlight the benefits of combining microfluidics with CFE reactions for the study and engineering of molecular functions and the construction of life-like systems from non-living components. By defining spatial organisation at different scales and sustaining non-equilibrium conditions, microfluidic environments play a key role in the quest to boot up the biochemistry of life.",0,arxiv,Biyoloji,CC-BY/arXiv,Controlled protein synthesis and spatial organisation in microfluidic environments
"Accurate and scalable machine-learned inter-atomic potentials (MLIPs) are essential for molecular simulations ranging from drug discovery to new material design. Current state-of-the-art models enforce roto-translational symmetries through equivariant neural network architectures, a hard-wired inductive bias that can often lead to reduced flexibility, computational efficiency, and scalability. In this work, we introduce TransIP: Transformer-based Inter-Atomic Potentials, a novel training paradigm for interatomic potentials achieving symmetry compliance without explicit architectural constraints. Our approach guides a generic non-equivariant Transformer-based model to learn SO(3)-equivariance by optimizing its representations in the embedding space. Trained on the recent Open Molecules (OMol25) collection, a large and diverse molecular dataset built specifically for MLIPs and covering different types of molecules (including small organics, biomolecular fragments, and electrolyte-like species), TransIP effectively learns symmetry in its latent space, providing low equivariance error. Further, compared to a data augmentation baseline, TransIP achieves 40% to 60% improvement in performance across varying OMol25 dataset sizes. More broadly, our work shows that learned equivariance can be a powerful and efficient alternative to augmentation-based MLIP models.",0,arxiv,Biyoloji,CC-BY/arXiv,Learning Inter-Atomic Potentials without Explicit Equivariance
"Structural prediction of protein-protein interactions is important to understand the molecular basis of cellular interactions, but it still faces major challenges when significant conformational changes are present. We propose a generative framework of hierarchical adaptive diffusion to improve accuracy and efficiency in such cases. It is hierarchical in separating global inter-protein rigid-body motions and local intra-protein flexibility in diffusion processes, and the distinct local and global noise schedules are designed to mimic the induced-fit effect. It is adaptive in conditioning the local flexibility schedule on predicted levels of conformational change, allowing faster flexing for larger anticipated conformational changes. Furthermore, it couples the local and global diffusion processes through a common score and confidence network with sequence, evolution, structure, and dynamics features as inputs, and maintains rotational or translational invariance or equivariance in outputs. It builds on our newly curated DIPS-AF dataset of nearly 39,000 examples for pre-training. Experiments on the independent docking benchmark dataset DB5.5 show that our model outperforms an AlphaFold2-like iterative transformer (GeoDock) and a diffusion model (DiffDock-PP) in both rigid and flexible cases, with larger improvements in more flexible cases. Ablation studies prove the importance of adaptive schedules, dynamics features, and pre-training. Additional analyses and case studies reveal remaining gaps in sampling, scoring, and conformational resolution.",0,arxiv,Biyoloji,CC-BY/arXiv,A Hierarchical Adaptive Diffusion Model for Flexible Protein-Protein Docking
"We introduce FragAtlas-62M, a specialized foundation model trained on the largest fragment dataset to date. Built on the complete ZINC-22 fragment subset comprising over 62 million molecules, it achieves unprecedented coverage of fragment chemical space. Our GPT-2 based model (42.7M parameters) generates 99.90% chemically valid fragments. Validation across 12 descriptors and three fingerprint methods shows generated fragments closely match the training distribution (all effect sizes < 0.4). The model retains 53.6% of known ZINC fragments while producing 22% novel structures with practical relevance. We release FragAtlas-62M with training code, preprocessed data, documentation, and model weights to accelerate adoption.",0,arxiv,Biyoloji,CC-BY/arXiv,A Foundation Chemical Language Model for Comprehensive Fragment-Based Drug Discovery
"Proteins are the workhorse molecules of the cell and perform their biological functions by binding to other molecules through physical contact. Protein function is then regulated through coupling of bindings on the protein (allosteric regulation). Just as the genetic code provides the blueprint for protein synthesis, the coupling is thought to provide the basis for protein communication and interaction. However, it is not yet fully understood how binding of a molecule at one site affects binding of another molecule at another distal site on a protein, even more than $60$ years after its discovery in 1961. In this paper, I propose a simple mathematical model of protein interactions, using a ``quantized'' version of differential geometry, i.e., the discrete differential geometry of $n$-simplices. The model is based on the concept of electron delocalization, one of the main features of quantum chemistry, Allosteric regulation then follows tautologically from the definition of interactions. No prior knowledge of conventional discrete differential geometry, protein science, or quantum chemistry is required. I hope this paper will provide a starting point for many mathematicians to study chemistry and molecular biology.",0,arxiv,Biyoloji,CC-BY/arXiv,A Novel Mathematical Model of Protein Interactions from the Perspective of Electron Delocalization
"Trigeminal neuralgia (TN) is the most common neuropathic disorder; however, its pathogenesis remains unclear. A prevailing theory suggests that nitric oxide (NO) may induce nerve compression and irritation via vascular dilation, thereby being responsible for the condition, making real-time detection of generated NO critical. However, traditional evaluations of NO rely on indirect colorimetric or chemiluminescence techniques, which offer limited sensitivity and spatial resolution for its real-time assessment in biological environments. Herein, we reported the development of a highly sensitive NO electrochemical biosensor based cerium single-atom nanozyme (Ce1-CN) with ultrawide linear range from 1.08 nM to 143.9 Î¼M, and ultralow detection limit of 0.36 nM, which enables efficient and real-time evaluation of NO in TN rats. In-situ attenuated total reflection surface-enhanced infrared spectroscopy combined with density functional theory calculations revealed the high-performance biosensing mechanism, whereby the Ce centers in Ce1-CN nanoenzymes adsorb NO and subsequently react with OH- to form *HNO2. Results demonstrated that NO concentration was associated with TN onset. Following carbamazepine treatment, NO production from nerves decreased, accompanied by an alleviation of pain. These findings indicate that the biosensor serves as a valuable tool for investigating the pathogenesis of TN and guiding subsequent therapeutic strategies.",0,arxiv,Biyoloji,CC-BY/arXiv,Monitoring Nitric Oxide in Trigeminal Neuralgia Rats with a Cerium Single-Atom Nanozyme Electrochemical Biosensor
"Advances in deep learning have opened an era of abundant and accurate predicted protein structures; however, similar progress in protein ensembles has remained elusive. This review highlights several recent research directions towards AI-based predictions of protein ensembles, including coarse-grained force fields, generative models, multiple sequence alignment perturbation methods, and modeling of ensemble descriptors. An emphasis is placed on realistic assessments of the technological maturity of current methods, the strengths and weaknesses of broad families of techniques, and promising machine learning frameworks at an early stage of development. We advocate for ""closing the loop"" between model training, simulation, and inference to overcome challenges in training data availability and to enable the next generation of models.",0,arxiv,Biyoloji,CC-BY/arXiv,"AI-based Methods for Simulating, Sampling, and Predicting Protein Ensembles"
"Topological Data Analysis (TDA) has emerged as a powerful framework for extracting robust, multiscale, and interpretable features from complex molecular data for artificial intelligence (AI) modeling and topological deep learning (TDL). This review provides a comprehensive overview of the development, methodologies, and applications of TDA in molecular sciences. We trace the evolution of TDA from early qualitative tools to advanced quantitative and predictive models, highlighting innovations such as persistent homology, persistent Laplacians, and topological machine learning. The paper explores TDA's transformative impact across diverse domains, including biomolecular stability, protein-ligand interactions, drug discovery, materials science, and viral evolution. Special attention is given to recent advances in integrating TDA with machine learning and AI, enabling breakthroughs in protein engineering, solubility and toxicity prediction, and the discovery of novel materials and therapeutics. We also discuss the limitations of current TDA approaches and outline future directions, including the integration of TDA with advanced AI models and the development of new topological invariants. This review aims to serve as a foundational reference for researchers seeking to harness the power of topology in molecular science.",0,arxiv,Biyoloji,CC-BY/arXiv,A review of topological data analysis and topological deep learning in molecular sciences
"The goal of protein design is to generate amino acid sequences that fold into functional structures with desired properties. Prior methods combining autoregressive language models with Monte Carlo Tree Search (MCTS) struggle with long-range dependencies and suffer from an impractically large search space. We propose MCTD-ME, Monte Carlo Tree Diffusion with Multiple Experts, which integrates masked diffusion models with tree search to enable multi-token planning and efficient exploration. Unlike autoregressive planners, MCTD-ME uses biophysical-fidelity-enhanced diffusion denoising as the rollout engine, jointly revising multiple positions and scaling to large sequence spaces. It further leverages experts of varying capacities to enrich exploration, guided by a pLDDT-based masking schedule that targets low-confidence regions while preserving reliable residues. We propose a novel multi-expert selection rule (PH-UCT-ME) extends predictive-entropy UCT to expert ensembles. On the inverse folding task (CAMEO and PDB benchmarks), MCTD-ME outperforms single-expert and unguided baselines in both sequence recovery (AAR) and structural similarity (scTM), with gains increasing for longer proteins and benefiting from multi-expert guidance. More generally, the framework is model-agnostic and applicable beyond inverse folding, including de novo protein engineering and multi-objective molecular generation.",0,arxiv,Biyoloji,CC-BY/arXiv,Monte Carlo Tree Diffusion with Multiple Experts for Protein Design
"Accurate identification of drug-target interactions (DTI) remains a central challenge in computational pharmacology, where sequence-based methods offer scalability. This work introduces a sequence-based drug-target interaction framework that integrates structural priors into protein representations while maintaining high-throughput screening capability. Evaluated across multiple benchmarks, the model achieves state-of-the-art performance on Human and BioSNAP datasets and remains competitive on BindingDB. In virtual screening tasks, it surpasses prior methods on LIT-PCBA, yielding substantial gains in AUROC and BEDROC. Ablation studies confirm the critical role of learned aggregation, bilinear attention, and contrastive alignment in enhancing predictive robustness. Embedding visualizations reveal improved spatial correspondence with known binding pockets and highlight interpretable attention patterns over ligand-residue contacts. These results validate the framework's utility for scalable and structure-aware DTI prediction.",0,arxiv,Biyoloji,CC-BY/arXiv,Structure-Aware Contrastive Learning with Fine-Grained Binding Representations for Drug Discovery
"Accurately assessing mental workload is crucial in cognitive neuroscience, human-computer interaction, and real-time monitoring, as cognitive load fluctuations affect performance and decision-making. While Electroencephalography (EEG) based machine learning (ML) models can be used to this end, their high computational cost hinders embedded real-time applications. Hardware implementations of spiking neural networks (SNNs) offer a promising alternative for low-power, fast, event-driven processing. This study compares hardware compatible SNN models with various traditional ML ones, using an open-source multimodal dataset. Our results show that multimodal integration improves accuracy, with SNN performance comparable to the ML one, demonstrating their potential for real-time implementations of cognitive load detection. These findings position event-based processing as a promising solution for low-latency, energy efficient workload monitoring in adaptive closed-loop embedded devices that dynamically regulate cognitive load.",0,arxiv,Biyoloji,CC-BY/arXiv,Spiking Neural Networks for Mental Workload Classification with a Multimodal Approach
"A device capable of performing real time classification of proteins in a clinical setting would allow for inexpensive and rapid disease diagnosis. One such candidate for this technology are nanopore devices. These devices work by measuring a current signal that arises when a protein or peptide enters a nanometer-length-scale pore. Should this current be uniquely related to the structure of the peptide and its interactions with the pore, the signals can be used to perform identification. While such a method would allow for real time identification of peptides and proteins in a clinical setting, to date, the complexities of these signals limit their accuracy. In this work, we tackle the issue of classification by converting the current signals into scaleogram images via wavelet transforms, capturing amplitude, frequency, and time information in a modality well-suited to machine learning algorithms. When tested on 42 peptides, our method achieved a classification accuracy of ~$81\,\%$, setting a new state-of-the-art in the field and taking a step toward practical peptide/protein diagnostics at the point of care. In addition, we demonstrate model transfer techniques that will be critical when deploying these models into real hardware, paving the way to a new method for real-time disease diagnosis.",0,arxiv,Biyoloji,CC-BY/arXiv,Deep Learning-Driven Peptide Classification in Biological Nanopores
"Antimicrobial resistance (AMR) is projected to cause up to 10 million deaths annually by 2050, underscoring the urgent need for new antibiotics. Here we present ApexAmphion, a deep-learning framework for de novo design of antibiotics that couples a 6.4-billion-parameter protein language model with reinforcement learning. The model is first fine-tuned on curated peptide data to capture antimicrobial sequence regularities, then optimised with proximal policy optimization against a composite reward that combines predictions from a learned minimum inhibitory concentration (MIC) classifier with differentiable physicochemical objectives. In vitro evaluation of 100 designed peptides showed low MIC values (nanomolar range in some cases) for all candidates (100% hit rate). Moreover, 99 our of 100 compounds exhibited broad-spectrum antimicrobial activity against at least two clinically relevant bacteria. The lead molecules killed bacteria primarily by potently targeting the cytoplasmic membrane. By unifying generation, scoring and multi-objective optimization with deep reinforcement learning in a single pipeline, our approach rapidly produces diverse, potent candidates, offering a scalable route to peptide antibiotics and a platform for iterative steering toward potency and developability within hours.",0,arxiv,Biyoloji,CC-BY/arXiv,A deep reinforcement learning platform for antibiotic discovery
"Unraveling the dynamical motions of biomolecules is essential for bridging their structure and function, yet it remains a major computational challenge. Molecular dynamics (MD) simulation provides a detailed depiction of biomolecular motion, but its high-resolution temporal evolution comes at significant computational cost, limiting its applicability to timescales of biological relevance. Deep learning approaches have emerged as promising solutions to overcome these computational limitations by learning to predict long-timescale dynamics. However, generalizable kinetics models for proteins remain largely unexplored, and the fundamental limits of achievable acceleration while preserving dynamical accuracy are poorly understood. In this work, we fill this gap with DeepJump, an Euclidean-Equivariant Flow Matching-based model for predicting protein conformational dynamics across multiple temporal scales. We train DeepJump on trajectories of the diverse proteins of mdCATH, systematically studying our model's performance in generalizing to long-term dynamics of fast-folding proteins and characterizing the trade-off between computational acceleration and prediction accuracy. We demonstrate the application of DeepJump to ab initio folding, showcasing prediction of folding pathways and native states. Our results demonstrate that DeepJump achieves significant $\approx$1000$\times$ computational acceleration while effectively recovering long-timescale dynamics, providing a stepping stone for enabling routine simulation of proteins.",0,arxiv,Biyoloji,CC-BY/arXiv,Accelerating Protein Molecular Dynamics Simulation with DeepJump
"Fragment-based drug design is a promising strategy leveraging the binding of small chemical moieties that can efficiently guide drug discovery. The initial step of fragment identification remains challenging, as fragments often bind weakly and non-specifically. We developed a protein-fragment encoder that relies on a contrastive learning approach to map both molecular fragments and protein surfaces in a shared latent space. The encoder captures interaction-relevant features and allows to perform virtual screening as well as generative design with our new method LatentFrag. In LatentFrag, fragment embeddings and positions are generated conditioned on the protein surface while being chemically realistic by construction. Our expressive fragment and protein representations allow location of protein-fragment interaction sites with high sensitivity and we observe state-of-the-art fragment recovery rates when sampling from the learned distribution of latent fragment embeddings. Our generative method outperforms common methods such as virtual screening at a fraction of its computational cost providing a valuable starting point for fragment hit discovery. We further show the practical utility of LatentFrag and extend the workflow to full ligand design tasks. Together, these approaches contribute to advancing fragment identification and provide valuable tools for fragment-based drug discovery.",0,arxiv,Biyoloji,CC-BY/arXiv,Flow-Based Fragment Identification via Binding Site-Specific Latent Representations
"This SHREC 2025 track dedicated to protein surface shape retrieval involved 9 participating teams. We evaluated the performance in retrieval of 15 proposed methods on a large dataset of 11,555 protein surfaces with calculated electrostatic potential (a key molecular surface descriptor). The performance in retrieval of the proposed methods was evaluated through different metrics (Accuracy, Balanced accuracy, F1 score, Precision and Recall). The best retrieval performance was achieved by the proposed methods that used the electrostatic potential complementary to molecular surface shape. This observation was also valid for classes with limited data which highlights the importance of taking into account additional molecular surface descriptors.",0,arxiv,Biyoloji,CC-BY/arXiv,SHREC 2025: Protein surface shape retrieval including electrostatic potential
"The scarcity of experimental protein-ligand complexes poses a significant challenge for training robust deep learning models for molecular docking. Given the prohibitive cost and time constraints associated with experimental structure determination, scalable generation of realistic protein-ligand complexes is needed to expand available datasets for model development. In this study, we introduce a novel workflow for the procedural generation and validation of synthetic protein-ligand complexes, combining a diverse ensemble of generation techniques and rigorous quality control. We assessed the utility of these synthetic datasets by retraining established docking models, Smina and Gnina, and evaluating their performance on standard benchmarks including the PDBBind core set and the PoseBusters dataset. Our results demonstrate that models trained on synthetic data achieve performance comparable to models trained on experimental data, indicating that current synthetic complexes can effectively capture many salient features of protein-ligand interactions. However, we did not observe significant improvements in docking or scoring accuracy over conventional methods or experimental data augmentation. These findings highlight the promise as well as the current limitations of synthetic data for deep learning-based molecular docking and underscore the need for further refinement in generation methodologies and evaluation strategies to fully exploit the potential of synthetic datasets for this application.",0,arxiv,Biyoloji,CC-BY/arXiv,Synthetic Protein-Ligand Complex Generation for Deep Molecular Docking
"Protein design has the potential to revolutionize biotechnology and medicine. While most efforts have focused on proteins with well-defined structures, increased recognition of the functional significance of intrinsically disordered regions, together with improvements in their modeling, has paved the way to their computational de novo design. This review summarizes recent advances in engineering intrinsically disordered regions with tailored conformational ensembles, molecular recognition, and phase behavior. We discuss challenges in combining models with predictive accuracy with scalable design workflows and outline emerging strategies that integrate knowledge-based, physics-based, and machine-learning approaches.",0,arxiv,Biyoloji,CC-BY/arXiv,Computational design of intrinsically disordered proteins
"In structure-based drug design, accurately estimating the binding affinity between a candidate ligand and its protein receptor is a central challenge. Recent advances in artificial intelligence, particularly deep learning, have demonstrated superior performance over traditional empirical and physics-based methods for this task, enabled by the growing availability of structural and experimental affinity data. In this work, we introduce DeepGGL, a deep convolutional neural network that integrates residual connections and an attention mechanism within a geometric graph learning framework. By leveraging multiscale weighted colored bipartite subgraphs, DeepGGL effectively captures fine-grained atom-level interactions in protein-ligand complexes across multiple scales. We benchmarked DeepGGL against established models on CASF-2013 and CASF-2016, where it achieved state-of-the-art performance with significant improvements across diverse evaluation metrics. To further assess robustness and generalization, we tested the model on the CSAR-NRC-HiQ dataset and the PDBbind v2019 holdout set. DeepGGL consistently maintained high predictive accuracy, highlighting its adaptability and reliability for binding affinity prediction in structure-based drug discovery.",0,arxiv,Biyoloji,CC-BY/arXiv,A Geometric Graph-Based Deep Learning Model for Drug-Target Affinity Prediction
"The enzyme turnover rate is a fundamental parameter in enzyme kinetics, reflecting the catalytic efficiency of enzymes. However, enzyme turnover rates remain scarce across most organisms due to the high cost and complexity of experimental measurements. To address this gap, we propose a multimodal framework for predicting the enzyme turnover rate by integrating enzyme sequences, substrate structures, and environmental factors. Our model combines a pre-trained language model and a convolutional neural network to extract features from protein sequences, while a graph neural network captures informative representations from substrate molecules. An attention mechanism is incorporated to enhance interactions between enzyme and substrate representations. Furthermore, we leverage symbolic regression via Kolmogorov-Arnold Networks to explicitly learn mathematical formulas that govern the enzyme turnover rate, enabling interpretable and accurate predictions. Extensive experiments demonstrate that our framework outperforms both traditional and state-of-the-art deep learning approaches. This work provides a robust tool for studying enzyme kinetics and holds promise for applications in enzyme engineering, biotechnology, and industrial biocatalysis.",0,arxiv,Biyoloji,CC-BY/arXiv,Multimodal Regression for Enzyme Turnover Rates Prediction
"Protein-ligand binding affinity is critical in drug discovery, but experimentally determining it is time-consuming and expensive. Artificial intelligence (AI) has been used to predict binding affinity, significantly accelerating this process. However, the high-performance requirements and vast datasets involved in affinity prediction demand increasingly large AI models, requiring substantial computational resources and training time. Quantum machine learning has emerged as a promising solution to these challenges. In particular, hybrid quantum-classical models can reduce the number of parameters while maintaining or improving performance compared to classical counterparts. Despite these advantages, challenges persist: why hybrid quantum models achieve these benefits, whether quantum neural networks (QNNs) can replace classical neural networks, and whether such models are feasible on noisy intermediate-scale quantum (NISQ) devices. This study addresses these challenges by proposing a hybrid quantum neural network (HQNN) that empirically demonstrates the capability to approximate non-linear functions in the latent feature space derived from classical embedding. The primary goal of this study is to achieve a parameter-efficient model in binding affinity prediction while ensuring feasibility on NISQ devices. Numerical results indicate that HQNN achieves comparable or superior performance and parameter efficiency compared to classical neural networks, underscoring its potential as a viable replacement. This study highlights the potential of hybrid QML in computational drug discovery, offering insights into its applicability and advantages in addressing the computational challenges of protein-ligand binding affinity prediction.",0,arxiv,Biyoloji,CC-BY/arXiv,Hybrid Quantum Neural Networks for Efficient Protein-Ligand Binding Affinity Prediction
"Fragment-Based Drug Discovery (FBDD) is a popular approach in early drug development, but designing effective linkers to combine disconnected molecular fragments into chemically and pharmacologically viable candidates remains challenging. Further complexity arises when fragments contain structural redundancies, like duplicate rings, which cannot be addressed by simply adding or removing atoms or bonds. To address these challenges in a unified framework, we introduce FragmentGPT, which integrates two core components: (1) a novel chemically-aware, energy-based bond cleavage pre-training strategy that equips the GPT-based model with fragment growing, linking, and merging capabilities, and (2) a novel Reward Ranked Alignment with Expert Exploration (RAE) algorithm that combines expert imitation learning for diversity enhancement, data selection and augmentation for Pareto and composite score optimality, and Supervised Fine-Tuning (SFT) to align the learner policy with multi-objective goals. Conditioned on fragment pairs, FragmentGPT generates linkers that connect diverse molecular subunits while simultaneously optimizing for multiple pharmaceutical goals. It also learns to resolve structural redundancies-such as duplicated fragments-through intelligent merging, enabling the synthesis of optimized molecules. FragmentGPT facilitates controlled, goal-driven molecular assembly. Experiments and ablation studies on real-world cancer datasets demonstrate its ability to generate chemically valid, high-quality molecules tailored for downstream drug discovery tasks.",0,arxiv,Biyoloji,CC-BY/arXiv,"FragmentGPT: A Unified GPT Model for Fragment Growing, Linking, and Merging in Molecular Design"
"Strategies to improve the predicting performance of Message-Passing Neural-Networks for molecular property predictions can be achieved by simplifying how the message is passed and by using descriptors that capture multiple aspects of molecular graphs. In this work, we designed model architectures that achieved state-of-the-art performance, surpassing more complex models such as those pre-trained on external databases. We assessed dataset diversity to complement our performance results, finding that structural diversity influences the need for additional components in our MPNNs and feature sets.   In most datasets, our best architecture employs bidirectional message-passing with an attention mechanism, applied to a minimalist message formulation that excludes self-perception, highlighting that relatively simpler models, compared to classical MPNNs, yield higher class separability. In contrast, we found that convolution normalization factors do not benefit the predictive power in all the datasets tested. This was corroborated in both global and node-level outputs. Additionally, we analyzed the influence of both adding spatial features and working with 3D graphs, finding that 2D molecular graphs are sufficient when complemented with appropriately chosen 3D descriptors. This approach not only preserves predictive performance but also reduces computational cost by over 50%, making it particularly advantageous for high-throughput screening campaigns.",0,arxiv,Biyoloji,CC-BY/arXiv,"Optimal message passing for molecular prediction is simple, attentive and spatial"
"Bacteriophages, viruses that infect bacteria, store their micron long DNA inside an icosahedral capsid with a typical diameter of 40 nm to 100 nm. Consistent with experimental observations, such confinement conditions induce an arrangement of DNA that corresponds to a hexagonal chromonic liquid-crystalline phase, and increase the topological complexity of the genome in the form of knots. A mathematical model that implements a chromonic liquid-crystalline phase and that captures the changes in topology has been lacking. We adopt a mathematical model that represents the viral DNA as a pair of a vector field and a line. The vector field is a minimizer of the total Oseen-Frank energy for nematic liquid crystals under chromonic constraints, while the line is identified with the tangent to the field at selected locations, representing the central axis of the DNA molecule. The fact that the Oseen-Frank functional assigns infinite energy to topological defects (point defects in two dimensions and line defects in three dimensions) precludes the presence of singularities and, in particular, of knot structures. To address this issue, we begin with the optimal vector field and helical line, and propose a new algorithm to introduce knots through stochastic perturbations associated with splay and twist deformations, modeled by means of a Langevin system. We conclude by comparing knot distributions generated by the model and by interpreting them in the context of previously published experimental results. Altogether, this work relies on the synergy of modeling, analysis and computation in the study of viral DNA organization in capsids.",0,arxiv,Biyoloji,CC-BY/arXiv,Knotted DNA Configurations in Bacteriophage Capsids: A Liquid Crystal Theory Approach
"The complementarity-determining regions of antibodies are loop structures that are key to their interactions with antigens, and of high importance to the design of novel biologics. Since the 1980s, categorizing the diversity of CDR structures into canonical clusters has enabled the identification of key structural motifs of antibodies. However, existing approaches have limited coverage and cannot be readily incorporated into protein foundation models. Here we introduce ImmunoGlobulin LOOp Tokenizer, Igloo, a multimodal antibody loop tokenizer that encodes backbone dihedral angles and sequence. Igloo is trained using a contrastive learning objective to map loops with similar backbone dihedral angles closer together in latent space. Igloo can efficiently retrieve the closest matching loop structures from a structural antibody database, outperforming existing methods on identifying similar H3 loops by 5.9\%. Igloo assigns tokens to all loops, addressing the limited coverage issue of canonical clusters, while retaining the ability to recover canonical loop conformations. To demonstrate the versatility of Igloo tokens, we show that they can be incorporated into protein language models with IglooLM and IglooALM. On predicting binding affinity of heavy chain variants, IglooLM outperforms the base protein language model on 8 out of 10 antibody-antigen targets. Additionally, it is on par with existing state-of-the-art sequence-based and multimodal protein language models, performing comparably to models with $7\times$ more parameters. IglooALM samples antibody loops which are diverse in sequence and more consistent in structure than state-of-the-art antibody inverse folding models. Igloo demonstrates the benefit of introducing multimodal tokens for antibody loops for encoding the diverse landscape of antibody loops, improving protein foundation models, and for antibody CDR design.",0,arxiv,Biyoloji,CC-BY/arXiv,Tokenizing Loops of Antibodies
"Water/glycerol mixtures are common for experiments with biomacromolecules at cryogenic temperatures due to their vitrification properties. Above the glass transition temperature, they undergo liquid-liquid phase separation. Using the novel EPR technique called intermolecular hyperfine Relaxation-Induced Dipolar Modulation Enhancement (ih-RIDME), we quantified the molar composition in frozen water/glycerol mixtures with one or the other component deuterated after the phase transition. Our experiments reveal nearly equal phase composition regardless of the proton/deuterium isotope balance. With the new ih-RIDME data, we can also revisit the already reported body of glass transition data for such mixtures and build a consistent picture for water/glycerol freezing and phase transitions. Our results also indicate that ih-RIDME has the potential for investigating the solvation shells of spin-labelled macromolecules.",0,arxiv,Biyoloji,CC-BY/arXiv,Quantifying the liquid-liquid transition in cold water/glycerol mixtures by ih-RIDME
"Designing full-length, epitope-specific TCR Î±\b{eta} remains challenging due to vast sequence space, data biases and incomplete modeling of immunogenetic constraints. We present LSMTCR, a scalable multi-architecture framework that separates specificity from constraint learning to enable de novo, epitope-conditioned generation of paired, full-length TCRs. A diffusion-enhanced BERT encoder learns time-conditioned epitope representations; conditional GPT decoders, pretrained on CDR3\b{eta} and transferred to CDR3Î±, generate chain-specific CDR3s under cross-modal conditioning with temperature-controlled diversity; and a gene-aware Transformer assembles complete Î±/\b{eta} sequences by predicting V/J usage to ensure immunogenetic fidelity. Across GLIPH, TEP, MIRA, McPAS and our curated dataset, LSMTCR achieves higher predicted binding than baselines on most datasets, more faithfully recovers positional and length grammars, and delivers superior, temperature-tunable diversity. For Î±-chain generation, transfer learning improves predicted binding, length realism and diversity over representative methods. Full-length assembly from known or de novo CDR3s preserves k-mer spectra, yields low edit distances to references, and, in paired Î±/\b{eta} co-modelling with epitope, attains higher pTM/ipTM than single-chain settings. LSMTCR outputs diverse, gene-contextualized, full-length TCR designs from epitope input alone, enabling high-throughput screening and iterative optimization.",0,arxiv,Biyoloji,CC-BY/arXiv,LSMTCR: A Scalable Multi-Architecture Model for Epitope-Specific T Cell Receptor de novo Design
"Turing patterns play a fundamental role in morphogenesis and population dynamics, encoding key information about the underlying biological mechanisms. Yet, traditional inverse problems have largely relied on non-biological data such as boundary measurements, neglecting the rich information embedded in the patterns themselves. Here we introduce a new research direction that directly leverages physical observables from nature--the amplitude of Turing patterns--to achieve complete parameter identification. We present a framework that uses the spatial amplitude profile of a single pattern to simultaneously recover all system parameters, including wavelength, diffusion constants, and the full nonlinear forms of chemotactic and kinetic coefficient functions. Demonstrated on models of chemotactic bacteria, this amplitude-based approach establishes a biologically grounded, mathematically rigorous paradigm for reverse-engineering pattern formation mechanisms across diverse biological systems.",0,arxiv,Biyoloji,CC-BY/arXiv,Unveiling Biological Models Through Turing Patterns
"Sequence matching algorithms such as BLAST and FASTA have been widely used in searching for evolutionary origin and biological functions of newly discovered nucleic acid and protein sequences. As parts of these search tools, alignment scores and E values are useful indicators of the quality of search results from querying a database of annotated sequences, whereby a high alignment score (and inversely a low E value) reflects significant similarity between the query and the subject (target) sequences. For cross-comparison of results from sufficiently different queries however, the interpretation of alignment score as a similarity measure and E value a dissimilarity measure becomes somewhat nuanced, and prompts herein a judicious distinction of different types of similarity. We show that an adjustment of E value to account for self-matching of query and subject sequences corrects for certain ostensibly anomalous similarity comparisons, resulting in canonical dissimilarity and similarity measures that would be more appropriate for database applications, such as all-on-all sequence alignment or selection of diverse subsets. In actual practice, the canonicalization of E value dissimilarity improves clustering and the diversity of subset selection. While both E value and the canonical E value share positivity and symmetry, two of the four axiomatic properties of a metric space, the canonical E value is also reflexive and meets the condition of triangle inequality, thus itself an appropriate distance function for a metric space of protein sequences.",0,arxiv,Biyoloji,CC-BY/arXiv,Canonicalization of the E value from BLAST similarity search -- dissimilarity measure and distance function for a metric space of protein sequences
"Antibody binding site prediction plays a pivotal role in computational immunology and therapeutic antibody design. Existing sequence or structure methods rely on single-view features and fail to identify antibody-specific binding sites on the antigens. In this paper, we propose \textbf{CAME-AB}, a novel Cross-modality Attention framework with a Mixture-of-Experts (MoE) backbone for robust antibody binding site prediction. CAME-AB integrates five biologically grounded modalities, including raw amino acid encodings, BLOSUM substitution profiles, pretrained language model embeddings, structure-aware features, and GCN-refined biochemical graphs, into a unified multimodal representation. To enhance adaptive cross-modal reasoning, we propose an \emph{adaptive modality fusion} module that learns to dynamically weight each modality based on its global relevance and input-specific contribution. A Transformer encoder combined with an MoE module further promotes feature specialization and capacity expansion. We additionally incorporate a supervised contrastive learning objective to explicitly shape the latent space geometry, encouraging intra-class compactness and inter-class separability. To improve optimization stability and generalization, we apply stochastic weight averaging during training. Extensive experiments on benchmark antibody-antigen datasets demonstrate that CAME-AB consistently outperforms strong baselines on multiple metrics, including Precision, Recall, F1-score, AUC-ROC, and MCC. Ablation studies further validate the effectiveness of each architectural component and the benefit of multimodal feature integration. The model implementation details and the codes are available on https://anonymous.4open.science/r/CAME-AB-C525",0,arxiv,Biyoloji,CC-BY/arXiv,CAME-AB: Cross-Modality Attention with Mixture-of-Experts for Antibody Binding Site Prediction
"The majority of therapeutic monoclonal antibodies (mAbs) on the market are produced using Chinese Hamster Ovary (CHO) cells cultured at scale in chemically defined cell culture medium. Because of the high costs associated with mammalian cell cultures, obtaining high cell densities to produce high product titers is desired. These bioprocesses require high concentrations of nutrients in the basal media and periodically adding concentrated feed media to sustain cell growth and therapeutic protein productivity. Unfortunately, the desired or optimal nutrient concentrations of the feed media are often solubility limited due to precipitation of chemical complexes that form in the solution. Experimentally screening the various cell culture media configurations which contain 50 to 100 compounds can be expensive and laborious. This article lays the foundation for utilizing computational tools to understand precipitation of nutrients in cell culture media by studying the pairwise interactions between amino acids in thermodynamic models. Activity coefficient data for one amino acid in water and amino acid solubility data of two amino acids in water have been used to determine a single set of UNIFAC group interaction parameters to predict the thermodynamic behavior of the multi-component systems found in mammalian cell culture media. The data collected in this study is, to our knowledge, the largest set of ternary system amino acid solubility data reported to date. These amino acid precipitation predictions have been verified with experimentally measured ternary and quaternary amino acid solutions. Thus, we demonstrate the utility of our model as a digital twin to identify optimal cell culture media compositions by replacing empirical approaches for nutrient precipitation with computational predictions based on thermodynamics of individual media components in complex mixtures.",0,arxiv,Biyoloji,CC-BY/arXiv,Computational predictions of nutrient precipitation for intensified cell 1 culture media via amino acid solution thermodynamics
"Cryo-electron microscopy can now routinely deliver atomic resolution structures for a variety of biological systems. The relevance and value of these structures is directly related to their ability to help rationalize experimental observables, which in turn depends on the quality of model built into the density map. Coupling traditional model building tools with physics-based methods, such as docking, simulation, and modern force fields, has been shown to improve the quality of the resulting structures. Here, we survey the landscape of these hybrid approaches, highlighting their usefulness for medium- and low-resolution datasets, as well as for structures of small molecules, and make the argument that the community stands to benefit from their inclusion in model building and refinement workflows.",0,arxiv,Biyoloji,CC-BY/arXiv,Towards better structural models from cryo-electron microscopy data with physics-based methods
"Directed evolution is an iterative laboratory process of designing proteins with improved function by iteratively synthesizing new protein variants and evaluating their desired property with expensive and time-consuming biochemical screening. Machine learning methods can help select informative or promising variants for screening to increase their quality and reduce the amount of necessary screening. In this paper, we present a novel method for machine-learning-assisted directed evolution of proteins which combines Bayesian optimization with informative representation of protein variants extracted from a pre-trained protein language model. We demonstrate that the new representation based on the sequence embeddings significantly improves the performance of Bayesian optimization yielding better results with the same number of conducted screening in total. At the same time, our method outperforms the state-of-the-art machine-learning-assisted directed evolution methods with regression objective.",0,arxiv,Biyoloji,CC-BY/arXiv,Directed Evolution of Proteins via Bayesian Optimization in Embedding Space
"Proteins play crucial roles in almost all biological processes. The advancement of deep learning has greatly accelerated the development of protein foundation models, leading to significant successes in protein understanding and design. However, the lack of systematic red-teaming for these models has raised serious concerns about their potential misuse, such as generating proteins with biological safety risks. This paper introduces SafeProtein, the first red-teaming framework designed for protein foundation models to the best of our knowledge. SafeProtein combines multimodal prompt engineering and heuristic beam search to systematically design red-teaming methods and conduct tests on protein foundation models. We also curated SafeProtein-Bench, which includes a manually constructed red-teaming benchmark dataset and a comprehensive evaluation protocol. SafeProtein achieved continuous jailbreaks on state-of-the-art protein foundation models (up to 70% attack success rate for ESM3), revealing potential biological safety risks in current protein foundation models and providing insights for the development of robust security protection technologies for frontier models. The codes will be made publicly available at https://github.com/jigang-fan/SafeProtein.",0,arxiv,Biyoloji,CC-BY/arXiv,SafeProtein: Red-Teaming Framework and Benchmark for Protein Foundation Models
"Self-supervised pretraining from static structures of drug-like compounds and proteins enable powerful learned feature representations. Learned features demonstrate state of the art performance on a range of predictive tasks including molecular properties, structure generation, and protein-ligand interactions. The majority of approaches are limited by their use of static structures and it remains an open question, how best to use atomistic molecular dynamics (MD) simulations to develop more generalized models to improve prediction accuracy for novel molecular structures. We present SURrogate mmGBSA (SurGBSA) as a new modeling approach for MD-based representation learning, which learns a surrogate function of the Molecular Mechanics Generalized Born Surface Area (MMGBSA). We show for the first time the benefits of physics-informed pre-training to train a surrogate MMGBSA model on a collection of over 1.4 million 3D trajectories collected from MD simulations of the CASF-2016 benchmark. SurGBSA demonstrates a dramatic 27,927x speedup versus a traditional physics-based single-point MMGBSA calculation while nearly matching single-point MMGBSA accuracy on the challenging pose ranking problem for identification of the correct top pose (-0.4% difference). Our work advances the development of molecular foundation models by showing model improvements when training on MD simulations. Models, code and training data are made publicly available.",0,arxiv,Biyoloji,CC-BY/arXiv,SurGBSA: Learning Representations From Molecular Dynamics Simulations
"Simulating the long-timescale dynamics of biomolecules is a central challenge in computational science. While enhanced sampling methods can accelerate these simulations, they rely on pre-defined collective variables that are often difficult to identify, restricting their ability to model complex switching mechanisms between metastable states. A recent generative model, LD-FPG, demonstrated that this problem could be bypassed by learning to sample the static equilibrium ensemble as all-atom deformations from a reference structure, establishing a powerful method for all-atom ensemble generation. However, while this approach successfully captures a system's probable conformations, it does not model the temporal evolution between them. We introduce the Graph Latent Dynamics Propagator (GLDP), a modular component for simulating dynamics within the learned latent space of LD-FPG. We then compare three classes of propagators: (i) score-guided Langevin dynamics, (ii) Koopman-based linear operators, and (iii) autoregressive neural networks. Within a unified encoder-propagator-decoder framework, we evaluate long-horizon stability, backbone and side-chain ensemble fidelity, and temporal kinetics via TICA. Benchmarks on systems ranging from small peptides to mixed-topology proteins and large GPCRs reveal that autoregressive neural networks deliver the most robust long rollouts and coherent physical timescales; score-guided Langevin best recovers side-chain thermodynamics when the score is well learned; and Koopman provides an interpretable, lightweight baseline that tends to damp fluctuations. These results clarify the trade-offs among propagators and offer practical guidance for latent-space simulators of all-atom protein dynamics.",0,arxiv,Biyoloji,CC-BY/arXiv,Beyond Ensembles: Simulating All-Atom Protein Dynamics in a Learned Latent Space
"Peptide self-assembly prediction offers a powerful bottom-up strategy for designing biocompatible, low-toxicity materials for large-scale synthesis in a broad range of biomedical and energy applications. However, screening the vast sequence space for categorization of aggregate morphology remains intractable. We introduce PepMorph, an end-to-end peptide discovery pipeline that generates novel sequences that are not only prone to aggregate but self-assemble into a specified fibrillar or spherical morphology. We compiled a new dataset by leveraging existing aggregation propensity datasets and extracting geometric and physicochemical isolated peptide descriptors that act as proxies for aggregate morphology. This dataset is then used to train a Transformer-based Conditional Variational Autoencoder with a masking mechanism, which generates novel peptides under arbitrary conditioning. After filtering to ensure design specifications and validation of generated sequences through coarse-grained molecular dynamics simulations, PepMorph yielded 83% accuracy in intended morphology generation, showcasing its promise as a framework for application-driven peptide discovery.",0,arxiv,Biyoloji,CC-BY/arXiv,Morphology-Specific Peptide Discovery via Masked Conditional Generative Modeling
"Root-mean-square deviation (RMSD) is widely used to assess structural similarity in systems ranging from flexible ligand conformers to complex molecular cluster configurations. Despite its wide utility, RMSD calculation is often challenged by inconsistent atom ordering, indistinguishable configurations in molecular clusters, and potential chirality inversion during alignment. These issues highlight the necessity of accurate atom-to-atom correspondence as a prerequisite for meaningful alignment. Traditional approaches often rely on heuristic cost matrices combined with the Hungarian algorithm, yet these methods underutilize the rich intra-molecular structural information and may fail to generalize across chemically diverse systems. In this work, we introduce OTMol, a method that formulates the molecular alignment task as a fused supervised Gromov-Wasserstein (fsGW) optimal transport problem. By leveraging the intrinsic geometric and topological relationships within each molecule, OTMol eliminates the need for manually defined cost functions and enables a principled, data-driven matching strategy. Importantly, OTMol preserves key chemical features such as molecular chirality and bond connectivity consistency. We evaluate OTMol across a wide range of molecular systems, including Adenosine triphosphate, Imatinib, lipids, small peptides, and water clusters, and demonstrate that it consistently achieves low RMSD values while preserving computational efficiency. Importantly, OTMol maintains molecular integrity by enforcing one-to-one mappings between entire molecules, thereby avoiding erroneous many-to-one alignments that often arise in comparing molecular clusters. Our results underscore the utility of optimal transport theory for molecular alignment and offer a generalizable framework applicable to structural comparison tasks in cheminformatics, molecular modeling, and related disciplines.",0,arxiv,Biyoloji,CC-BY/arXiv,OTMol: Robust Molecular Structure Comparison via Optimal Transport
"Many methods have been developed to predict static protein structures, however understanding the dynamics of protein structure is essential for elucidating biological function. While molecular dynamics (MD) simulations remain the in silico gold standard, its high computational cost limits scalability. We present DynaProt, a lightweight, SE(3)-invariant framework that predicts rich descriptors of protein dynamics directly from static structures. By casting the problem through the lens of multivariate Gaussians, DynaProt estimates dynamics at two complementary scales: (1) per-residue marginal anisotropy as $3 \times 3$ covariance matrices capturing local flexibility, and (2) joint scalar covariances encoding pairwise dynamic coupling across residues. From these dynamics outputs, DynaProt achieves high accuracy in predicting residue-level flexibility (RMSF) and, remarkably, enables reasonable reconstruction of the full covariance matrix for fast ensemble generation. Notably, it does so using orders of magnitude fewer parameters than prior methods. Our results highlight the potential of direct protein dynamics prediction as a scalable alternative to existing methods.",0,arxiv,Biyoloji,CC-BY/arXiv,Learning residue level protein dynamics with multiscale Gaussians
"Cryptochrome flavoproteins are prime candidates for mediating magnetic sensing in migratory animals via the radical pair mechanism (RPM), a spin-dependent process initiated by photoinduced electron transfer. The canonical FAD-tryptophan radical pair exhibits pronounced anisotropic hyperfine couplings, enabling sensitivity to geomagnetic fields. However, maintaining spin coherence under physiological conditions and explaining responses to weak radiofrequency fields remain unresolved challenges. Alternative radicals, such as superoxide and ascorbate, have been proposed to enhance anisotropy or suppress decoherence. This review summarizes the quantum basis of magnetoreception, evaluates both canonical and alternative radical pair models, and discusses amplification strategies including triads, spin scavenging, and bystander radicals. Emphasis is placed on how molecular geometry, exchange and dipolar interactions, and hyperfine topology modulate magnetic sensitivity. Key open questions and future directions are outlined, highlighting the need for structural and dynamical data under physiological conditions.",0,arxiv,Biyoloji,CC-BY/arXiv,The Quantum Compass Mechanism in Cryptochromes
"The fundamental laws governing proteoform dynamics have yet to be formulated. As a result, it is unclear how a specific proteoform, a distinct molecular variant of a protein, dynamically shapes its own future by evolving into new modes that exist only in potential until realised. Here, Modal Geometric Field (MGF) Theory couples real and abstract proteoform transitions through four axioms. Axioms 1 to 3 (invariant) dictate that only first-order transitions occur on the discrete, volume-invariant, non symplectic modal manifold. Axiom 4 (mutable) projects the occupancy and shape of a real, instantiated molecule into the modal manifold, generating occupancy-induced curvature. By coupling what is real to what is abstract, curvature, which is always conserved, governs proteoform dynamics by dictating the least-action modal transition. Because curvature distribution renders activation energy relative, barriers are mutable, and entropy emerges inevitably from curvature transport. This unification of energy, entropy, and curvature yields hysteresis, path dependence, fractal self similarity, and trajectories that oscillate between order and chaos. As a scale invariant and universal framework, MGF Theory reveals how modal geometry governs proteoform dynamics",0,arxiv,Biyoloji,CC-BY/arXiv,Modal Geometry Governs Proteoform Dynamics
"Biological molecules, like all active matter, use free energy to generate force and motion which drive them out of thermal equilibrium, and undergo inherent dynamic interconversion between metastable free energy states separated by levels barely higher than stochastic thermal energy fluctuations. Here, we explore the founding and emerging approaches of the field of single-molecule biophysics which, unlike traditional ensemble average approaches, enable the detection and manipulation of individual molecules and facilitate exploration of biomolecular heterogeneity and its impact on transitional molecular kinetics and underpinning molecular interactions. We discuss the ground-breaking technological innovations which scratch far beyond the surface into open questions of real physiology, that correlate orthogonal data types and interplay empirical measurement with theoretical and computational insights, many of which are enabling artificial matter to be designed inspired by biological systems. And finally, we examine how these insights are helping to develop new physics framed around biology.",0,arxiv,Biyoloji,CC-BY/arXiv,Single-molecule biophysics
"We envision the Full-Body AI Agent as a comprehensive AI system designed to simulate, analyze, and optimize the dynamic processes of the human body across multiple biological levels. By integrating computational models, machine learning tools, and experimental platforms, this system aims to replicate and predict both physiological and pathological processes, ranging from molecules and cells to tissues, organs, and entire body systems. Central to the Full-Body AI Agent is its emphasis on integration and coordination across these biological levels, enabling analysis of how molecular changes influence cellular behaviors, tissue responses, organ function, and systemic outcomes. With a focus on biological functionality, the system is designed to advance the understanding of disease mechanisms, support the development of therapeutic interventions, and enhance personalized medicine. We propose two specialized implementations to demonstrate the utility of this framework: (1) the metastasis AI Agent, a multi-scale metastasis scoring system that characterizes tumor progression across the initiation, dissemination, and colonization phases by integrating molecular, cellular, and systemic signals; and (2) the drug AI Agent, a system-level drug development paradigm in which a drug AI-Agent dynamically guides preclinical evaluations, including organoids and chip-based models, by providing full-body physiological constraints. This approach enables the predictive modeling of long-term efficacy and toxicity beyond what localized models alone can achieve. These two agents illustrate the potential of Full-Body AI Agent to address complex biomedical challenges through multi-level integration and cross-scale reasoning.",0,arxiv,Biyoloji,CC-BY/arXiv,A Multi-Layered Framework for Modeling Human Biology: From Basic AI Agents to a Full-Body AI Agent
"Predicting the binding free energy between antibodies and antigens is a key challenge in structure-aware biomolecular modeling, with direct implications for antibody design. Most existing methods either rely solely on sequence embeddings or struggle to capture complex structural relationships, thus limiting predictive performance. In this work, we present a novel framework that integrates sequence-based representations from pre-trained protein language models (ESM-2) with a set of topological features. Specifically, we extract contact map metrics reflecting residue-level connectivity, interface geometry descriptors characterizing cross-chain interactions, distance map statistics quantifying spatial organization, and persistent homology invariants that systematically capture the emergence and persistence of multi-scale topological structures - such as connected components, cycles, and cavities - within individual proteins and across the antibody-antigen interface. By leveraging a cross-attention mechanism to fuse these diverse modalities, our model effectively encodes both global and local structural organization, thereby substantially enhancing the prediction of binding free energy. Extensive experiments demonstrate that our model consistently outperforms sequence-only and conventional structural models, achieving state-of-the-art accuracy in binding free energy prediction.",0,arxiv,Biyoloji,CC-BY/arXiv,TopoBind: Multi-Modal Prediction of Antibody-Antigen Binding Free Energy via Sequence Embeddings and Structural Topology
"This study evaluated the in vitro antibacterial effect and the phytochemical profile of aqueous extract of fresh mature leaves of Asystasia variabilis, a Sri Lankan indigenous plant, against four common wound infective bacteria (Staphylococcus aureus, Bacillus subtilis, Pseudomonas aeruginosa and Escherichia coli) using Kirby-Bauer disk diffusion test. Gentamicin 10 Î¼g/ disk and distilled water was used as positive and negative controls respectively The study revealed, for the first time, that the extract possessed significant antibacterial activity against all four test organisms in a concentration dependent manner (r values ranging from 0.921-0.992, P<0.01) with inhibition zone diameters ranging between 8 and 28 mm. Highest antibacterial activity was exhibited against B. subtilis at 1000 Î¼g/ disk (27.43-+0.02 mm). The extract showed inhibitory effects comparable to gentamicin towards B. subtilis and P. aeruginosa at 500 Î¼g/ disk and towards E. coli and S. aureus at 1000 Î¼g/ disk. Qualitative phytochemical screening revealed the presence of flavonoids, tannins, phenols, cardiac glycosides, amino acids, carbohydrates, alkaloids and saponins. Therefore it is likely that the antibacterial effect of the extract is mediated by synergistic mechanisms. Furthermore, results of this study scientifically justified the claim traditional and folk medicine in the treatment of abscesses, wounds and ulcers and indicated the potential for the development of a novel drug from mature leaves of Asystasia variabilis.",0,arxiv,Biyoloji,CC-BY/arXiv,Evaluation of in vitro antibacterial activity and phytochemical profile of aqueous leaf extract of Asystasia variabilis
"Aims: Over the past two decades, the rise of multidrug resistance (MDR) in bacteria has posed a significant threat to global health. The urgent need for new treatment alternatives has brought attention to the potential of plants, which harbor a wealth of unexplored phytochemicals with therapeutic properties. This study aims to evaluate the anti-bacterial efficacy of methanol and aqueous extracts from the leaves and bark of Horsfieldia iryaghedhi In vitro. Methodology: Aqueous and methanol extracts were obtained from the cold maceration method. In vitro anti-bacterial activity of methanol and aqueous leaf, bark, and combination extracts were determined against gram-negative bacteria Escherichia coli (ATCC 25922) and gram-positive bacteria Staphylococcus aureus (ATCC25923). The anti-bacterial assay for different concentrations of each extract was conducted through the well-diffusion method, with Gentamycin serving as the positive control. Results: Methanol leaf and combination extracts of Horsfieldia iryaghedhi have shown a positive anti-bacterial response at their highest concentrations of 1000mcg/mL and 500mcg/mL against grampositive bacteria Staphylococcus aureus while none of the extracts showed anti-bacterial activity against gram-negative E. coli at the experimented concentrations. Conclusion: The study concludes that methanol extracts of H.iryaghedhi should be further analyzed for their anti-bacterial activity, and there could be potential lead molecules that can be developed as antibiotics",0,arxiv,Biyoloji,CC-BY/arXiv,In-vitro Anti-bacterial Activity of Methanol and Aqueous Crude Extracts of Horsfieldia iryaghedhi
"Intrinsically disordered protein regions (IDRs) are found across all domains of life and are characterized by a lack of stable 3D structure. Nevertheless, IDRs play critical roles in the most tightly regulated cellular processes, including in the core circadian clock. The molecular oscillator at the heart of circadian regulation leverages IDRs as dynamic interaction modules for activation and repression to support robust timekeeping and expand clock output and regulation. Here, we cover the biophysical mechanisms conferred by IDRs and their modulators. We survey the intrinsically disordered regions in clock proteins that are widely prevalent from fungi to mammals and discuss the importance of IDRs to the core clock and beyond.",0,arxiv,Biyoloji,CC-BY/arXiv,Disordered But Rhythmic: the role of intrinsic protein disorder in eukaryotic circadian timing
"AlphaFold 3 represents a transformative advancement in computational biology, enhancing protein structure prediction through novel multi-scale transformer architectures, biologically informed cross-attention mechanisms, and geometry-aware optimization strategies. These innovations dramatically improve predictive accuracy and generalization across diverse protein families, surpassing previous methods. Crucially, AlphaFold 3 embodies a paradigm shift toward differentiable simulation, bridging traditional static structural modeling with dynamic molecular simulations. By reframing protein folding predictions as a differentiable process, AlphaFold 3 serves as a foundational framework for integrating deep learning with physics-based molecular",0,arxiv,Biyoloji,CC-BY/arXiv,From Prediction to Simulation: AlphaFold 3 as a Differentiable Framework for Structural Biology
"Human metapneumovirus (hMPV) poses serious risks to pediatric, elderly, and immunocompromised populations. Traditional antibody discovery pipelines require 10-12 months, limiting their applicability for rapid outbreak response. This project introduces ImmunoAI, a machine learning framework that accelerates antibody discovery by predicting high-affinity candidates using gradient-boosted models trained on thermodynamic, hydrodynamic, and 3D topological interface descriptors. A dataset of 213 antibody-antigen complexes was curated to extract geometric and physicochemical features, and a LightGBM regressor was trained to predict binding affinity with high precision. The model reduced the antibody candidate search space by 89%, and fine-tuning on 117 SARS-CoV-2 binding pairs further reduced Root Mean Square Error (RMSE) from 1.70 to 0.92. In the absence of an experimental structure for the hMPV A2.2 variant, AlphaFold2 was used to predict its 3D structure. The fine-tuned model identified two optimal antibodies with predicted picomolar affinities targeting key mutation sites (G42V and E96K), making them excellent candidates for experimental testing. In summary, ImmunoAI shortens design cycles and enables faster, structure-informed responses to viral outbreaks.",0,arxiv,Biyoloji,CC-BY/arXiv,ImmunoAI: Accelerated Antibody Discovery Using Gradient-Boosted Machine Learning with Thermodynamic-Hydrodynamic Descriptors and 3D Geometric Interface Topology
"The modulator pocket is a cryptic site discovered in the TREK1 K2P channel that accommodates agonists capable of increasing the channel's activity. Since its discovery, equivalent sites in other K2P channels have been shown to bind various ligands, both endogenous and exogenous. In this review, we attempt to elucidate how the modulator pocket contributes to K2P channel activation. To this end, we first describe the gating mechanisms reported in the literature and rationalize their modes of action. We then highlight previous experimental and computational evidence for agonists that bind to the modulator pocket, together with mutations at this site that affect gating. Finally, we elaborate how the activation signal arising from the modulator pocket is transduced to the gates in K2P channels. In doing so, we outline a potential common modulator pocket architecture across K2P channels: a largely amphipathic structure -consistent with the expected properties of a pocket exposed at the interface between a hydrophobic membrane and the aqueous solvent- but still with some important channel-sequence-variations. This architecture and its key differences can be leveraged for the design of new selective and potent modulators.",0,arxiv,Biyoloji,CC-BY/arXiv,One pocket to activate them all: Efforts on understanding the modulator pocket in K2P channels
"We introduce DrugFlow, a generative model for structure-based drug design that integrates continuous flow matching with discrete Markov bridges, demonstrating state-of-the-art performance in learning chemical, geometric, and physical aspects of three-dimensional protein-ligand data. We endow DrugFlow with an uncertainty estimate that is able to detect out-of-distribution samples. To further enhance the sampling process towards distribution regions with desirable metric values, we propose a joint preference alignment scheme applicable to both flow matching and Markov bridge frameworks. Furthermore, we extend our model to also explore the conformational landscape of the protein by jointly sampling side chain angles and molecules.",0,arxiv,Biyoloji,CC-BY/arXiv,Multi-domain Distribution Learning for De Novo Drug Design
"In structure-based drug discovery, virtual screening using conventional molecular docking methods can be performed rapidly but suffers from limitations in prediction accuracy. Recently, Boltz-2 was proposed, achieving extremely high accuracy in binding affinity prediction, but requiring approximately 20 seconds per compound per GPU, making it difficult to apply to large-scale screening of hundreds of thousands to millions of compounds. This study proposes Boltzina, a novel framework that leverages Boltz-2's high accuracy while significantly improving computational efficiency. Boltzina achieves both accuracy and speed by omitting the rate-limiting structure prediction from Boltz-2's architecture and directly predicting affinity from AutoDock Vina docking poses. We evaluate on eight assays from the MF-PCBA dataset and show that while Boltzina performs below Boltz-2, it provides significantly higher screening performance compared to AutoDock Vina and GNINA. Additionally, Boltzina achieved up to 11.8$\times$ faster through reduced recycling iterations and batch processing. Furthermore, we investigated multi-pose selection strategies and two-stage screening combining Boltzina and Boltz-2, presenting optimization methods for accuracy and efficiency according to application requirements. This study represents the first attempt to apply Boltz-2's high-accuracy predictions to practical-scale screening, offering a pipeline that combines both accuracy and efficiency in computational biology. The Boltzina is available on github; https://github.com/ohuelab/boltzina.",0,arxiv,Biyoloji,CC-BY/arXiv,Boltzina: Efficient and Accurate Virtual Screening via Docking-Guided Binding Prediction with Boltz-2
"Living cells exhibit a complex organization comprising numerous compartments, among which are RNA- and protein-rich membraneless, liquid-like organelles known as biomolecular condensates. Energy-consuming processes regulate their formation and dissolution, with (de-)phosphorylation by specific enzymes being among the most commonly involved reactions. By employing a model system consisting of a phosphorylatable peptide and homopolymeric RNA, we elucidate how enzymatic activity modulates the growth kinetics and alters the local structure of biomolecular condensates. Under passive condition, time-resolved ultra-small-angle X-ray scattering with synchrotron source reveals a nucleation-driven coalescence mechanism maintained over four decades in time, similar to the coarsening of simple binary fluid mixtures. Coarse-grained molecular dynamics simulations show that peptide-decorated RNA chains assembled shortly after mixing constitute the relevant subunits. In contrast, actively-formed condensates initially display a local mass fractal structure, which gradually matures upon enzymatic activity before condensates undergo coalescence. Both types of condensate eventually reach a steady state but fluorescence recovery after photobleaching indicates a peptide diffusivity twice higher in actively-formed condensates consistent with their loosely-packed local structure. We expect multiscale, integrative approaches implemented with model systems to link effectively the functional properties of membraneless organelles to their formation and dissolution kinetics as regulated by cellular active processes.",0,arxiv,Biyoloji,CC-BY/arXiv,Multiscale Growth Kinetics of Model Biomolecular Condensates Under Passive and Active Conditions
"Biology stores information and computes at the molecular scale, yet the ways in which it does so are often distinct from human-engineered computers. Mapping biological computation onto architectures familiar to computer science remains an outstanding challenge. Here, inspired by Crick's proposal for molecular memory, we analyse a thermodynamically-consistent model of a protein complex subject to driven, nonequilibrium enzymatic reactions. In the strongly driven limit, we find that the system maps onto a stochastic, asynchronous variant of cellular automata, where each rule corresponds to a different set of enzymes being present. We find a broad class of phenomena in these 'molecular automata' that can be exploited for molecular computation, including error-tolerant memory via multistable attractors, and long transients that can be used as molecular stopwatches. By systematically enumerating all possible dynamical rules, we identify those that allow molecular automata to implement simple computational architectures such as finite-state machines. Overall, our results provide a framework for engineering synthetic molecular automata, and offer a route to building protein-based computation in living cells.",0,arxiv,Biyoloji,CC-BY/arXiv,Nonequilibrium protein complexes as molecular automata
"Molecular docking is a cornerstone of drug discovery, relying on high-resolution ligand-bound structures to achieve accurate predictions. However, obtaining these structures is often costly and time-intensive, limiting their availability. In contrast, ligand-free structures are more accessible but suffer from reduced docking performance due to pocket geometries being less suited for ligand accommodation in apo structures. Traditional methods for artificially inducing these conformations, such as molecular dynamics simulations, are computationally expensive. In this work, we introduce Sesame, a generative model designed to predict this conformational change efficiently. By generating geometries better suited for ligand accommodation at a fraction of the computational cost, Sesame aims to provide a scalable solution for improving virtual screening workflows.",0,arxiv,Biyoloji,CC-BY/arXiv,Sesame: Opening the door to protein pockets
"A short (<150 bp) double-stranded DNA (dsDNA) molecule ligated end-to-end forms a DNA minicircle. Due to sequence-dependent, nonuniform bending energetics, such a minicircle is predicted to adopt a certain inside-out orientation, known as the poloidal orientation. Despite theoretical and computational predictions, experimental evidence for this phenomenon has been lacking. In this study, we introduce a single-molecule approach to visualize the poloidal orientation of DNA minicircles. We constructed a set of DNA minicircles, each containing a single biotin located at a different position along one helical turn of the dsDNA, and imaged the location of biotin-bound NeutrAvidin relative to the DNA minicircle using atomic force microscopy (AFM). We applied this approach to two DNA sequences previously predicted to exhibit strongly preferred poloidal orientations. The observed relative positions of NeutrAvidin shifted between the inside and outside of the minicircle with different phases, indicating distinct poloidal orientations for the two sequences. Coarse-grained simulations revealed narrowly distributed poloidal orientations with different mean orientations for each sequence, consistent with the AFM results. Together, our findings provide experimental confirmation of preferred poloidal orientations in DNA minicircles, offering insights into the intrinsic dynamics of circular DNA.",0,arxiv,Biyoloji,CC-BY/arXiv,Visualizing Poloidal Orientation in DNA Minicircles
"Strong excitonic coupling and photon antibunching (AB) have been observed together in Venus yellow fluorescent protein dimers and currently lack a cohesive theoretical explanation. In 2019, Kim et al. demonstrated Davydov splitting in circular dichroism spectra, revealing strong J-like coupling, while antibunched fluorescence emission was confirmed by combined antibunching--fluorescence correlation spectroscopy (AB/FCS fingerprinting). To investigate the implications of this coexistence, Venus yellow fluorescent protein (YFP) dimer population dynamics are modeled within a Lindblad master equation framework, testing its ability to cope with typical, data-informed, Venus YFP dimer time and energy values. Simulations predict multiple-femtosecond (fs) decoherence, yielding bright/dark state mixtures consistent with antibunched fluorescence emission at room temperature. Thus, excitonic coupling and photon AB in Venus YFP dimers are reconciled without invoking long-lived quantum coherence. However, clear violations of several Lindblad approximation validity conditions appear imminent, calling for careful modifications to choices of standard system and bath definitions and parameter values.",0,arxiv,Biyoloji,CC-BY/arXiv,Excitonic Coupling and Photon Antibunching in Venus Yellow Fluorescent Protein Dimers: A Lindblad Master Equation Approach
"We present QUBODock, a pip-installable tool that formulates ligand pose generation as a Quadratic Unconstrained Binary Optimization (QUBO) problem and solves it efficiently on CPU or GPU. QUBODock focuses exclusively on pose generation and deliberately excludes any built-in scoring function, allowing researchers to pair its poses with external scorers of their choice. The software provides a minimal, reproducible interface for (i) protein-ligand structure ingestion and preprocessing, (ii) QUBO model construction from geometric/compatibility constraints, and (iii) decoding solutions into candidate poses for downstream ranking. Implemented in Python with GPU acceleration, QUBODock emphasizes usability and reproducibility: it is distributed on PyPI and can be installed with a single command. We release the source to support benchmarking, teaching, and method development around QUBO-based docking pose generation.",0,arxiv,Biyoloji,CC-BY/arXiv,QUBODock: A Pip-Installable QUBO Tool for Ligand Pose Generation
"Schistosomiasis, a neglected tropical disease caused by Schistosoma parasites, remains a major global health challenge. The Schistosoma mansoni thioredoxin glutathione reductase (SmTGR) is essential for parasite redox balance and immune evasion, making it a key therapeutic target. This study employs predictive Quantitative Structure-Activity Relationship (QSAR) modeling to identify potential SmTGR inhibitors. Using deep learning, a robust QSAR model was developed and validated, achieving high predictive accuracy. The predicted novel inhibitors were further validated through molecular docking studies, which demonstrated strong binding affinities, with the highest docking score of -10.76+-0.01kcal/mol. Visualization of the docked structures in both 2D and 3D confirmed similar interactions for the inhibitors and commercial drugs, further supporting their therapeutic effectiveness and the predictive ability of the model. This study demonstrates the potential of QSAR modeling in accelerating drug discovery, offering a promising avenue for developing novel therapeutics targeting SmTGR to improve schistosomiasis treatment.",0,arxiv,Biyoloji,CC-BY/arXiv,Deep Learning-based QSAR Model for Therapeutic Strategies Targeting SmTGR Protein's Immune Modulating Role in Host-Parasite Interaction
"A generative model capable of sampling realistic molecules with desired properties could accelerate chemical discovery across a wide range of applications. Toward this goal, significant effort has focused on developing models that jointly sample molecular topology and 3D structure. We present FlowMol3, an open-source, multi-modal flow matching model that advances the state of the art for all-atom, small-molecule generation. Its substantial performance gains over previous FlowMol versions are achieved without changes to the graph neural network architecture or the underlying flow matching formulation. Instead, FlowMol3's improvements arise from three architecture-agnostic techniques that incur negligible computational cost: self-conditioning, fake atoms, and train-time geometry distortion. FlowMol3 achieves nearly 100% molecular validity for drug-like molecules with explicit hydrogens, more accurately reproduces the functional group composition and geometry of its training data, and does so with an order of magnitude fewer learnable parameters than comparable methods. We hypothesize that these techniques mitigate a general pathology affecting transport-based generative models, enabling detection and correction of distribution drift during inference. Our results highlight simple, transferable strategies for improving the stability and quality of diffusion- and flow-based molecular generative models.",0,arxiv,Biyoloji,CC-BY/arXiv,FlowMol3: Flow Matching for 3D De Novo Small-Molecule Generation
"The use of generative machine learning models, trained on the experimentally resolved structures deposited in the protein data bank, is an attractive approach to sampling conformational ensembles of proteins. Unfortunately, since the machine-learned model utilized to generate these ensembles is not tied to an equation of motion, such as a molecular dynamics integrator or other causal generator of the dynamics, there is no timescale or causal information encoded in them. As such, with this work, we use the structural ensembles generated from AlphaFold2 at a range of reduced MSA depths to parameterize the potential of mean force of an overdamped, memory-free, coarse-grained Langevin equation. This approach couples the AlphaFold2 ensembles to a causal model, allowing us to estimate the timescales spanned by the AlphaFold2-generated ensembles at each MSA depth. Performing this analysis on six variants of HIV-1 protease, we confirm an inverse relationship between MSA depth and the timescale of an ensemble's conformational fluctuations, since the MSA depth essentially serves as a conformational restraint, and AlphaFold2 is generally able to probe timescales at or below those seen in microsecond-long, unbiased molecular dynamics simulations. We conclude by generalizing this approach to other machine-learned structure-prediction methods.",0,arxiv,Biyoloji,CC-BY/arXiv,Applied causality to infer protein dynamics and kinetics
"Accurate prediction of antibody-binding sites (epitopes) on antigens is crucial for vaccine design, immunodiagnostics, therapeutic antibody development, antibody engineering, research into autoimmune and allergic diseases, and for advancing our understanding of immune responses. Despite in silico methods that have been proposed to predict both linear (continuous) and conformational (discontinuous) epitopes, they consistently underperform in predicting conformational epitopes. In this work, we propose a conformer-based model trained on antigen sequences derived from 1,080 antigen-antibody complexes, leveraging convolutional neural networks (CNNs) to extract local features and Transformers to capture long-range dependencies within antigen sequences. Ablation studies demonstrate that CNN enhances the prediction of linear epitopes, and the Transformer module improves the prediction of conformational epitopes. Experimental results show that our model outperforms existing baselines in terms of PCC, ROC-AUC, PR-AUC, and F1 scores on both linear and conformational epitopes.",0,arxiv,Biyoloji,CC-BY/arXiv,BConformeR: A Conformer Based on Mutual Sampling for Unified Prediction of Continuous and Discontinuous Antibody Binding Sites
"Three-dimensional generative models increasingly drive structure-based drug discovery, yet it remains constrained by the scarce publicly available protein-ligand complexes. Under such data scarcity, almost all existing pipelines struggle to learn transferable geometric priors and consequently overfit to training-set biases. As such, we present IBEX, an Information-Bottleneck-EXplored coarse-to-fine pipeline to tackle the chronic shortage of protein-ligand complex data in structure-based drug design. Specifically, we use PAC-Bayesian information-bottleneck theory to quantify the information density of each sample. This analysis reveals how different masking strategies affect generalization and indicates that, compared with conventional de novo generation, the constrained Scaffold Hopping task endows the model with greater effective capacity and improved transfer performance. IBEX retains the original TargetDiff architecture and hyperparameters for training to generate molecules compatible with the binding pocket; it then applies an L-BFGS optimization step to finely refine each conformation by optimizing five physics-based terms and adjusting six translational and rotational degrees of freedom in under one second. With only these modifications, IBEX raises the zero-shot docking success rate on CBGBench CrossDocked2020-based from 53% to 64%, improves the mean Vina score from $-7.41 kcal mol^{-1}$ to $-8.07 kcal mol^{-1}$, and achieves the best median Vina energy in 57 of 100 pockets versus 3 for the original TargetDiff. IBEX also increases the QED by 25%, achieves state-of-the-art validity and diversity, and markedly reduces extrapolation error.",0,arxiv,Biyoloji,CC-BY/arXiv,IBEX: Information-Bottleneck-EXplored Coarse-to-Fine Molecular Generation under Limited Data
"Powerful AI tools for drug discovery reside in isolated web apps, desktop programs, and code libraries. Such fragmentation forces scientists to manage incompatible interfaces and specialized scripts, which can be a cumbersome and repetitive process. To address this issue, a Full-pROcess druG dEsign ageNT, named FROGENT, has been proposed. Specifically, FROGENT utilizes a Large Language Model and the Model Context Protocol to integrate multiple dynamic biochemical databases, extensible tool libraries, and task-specific AI models. This agentic framework allows FROGENT to execute complicated drug discovery workflows dynamically, including component tasks such as target identification, molecule generation and retrosynthetic planning. FROGENT has been evaluated on eight benchmarks that cover various aspects of drug discovery, such as knowledge retrieval, property prediction, virtual screening, mechanistic analysis, molecular design, and synthesis. It was compared against six increasingly advanced ReAct-style agents that support code execution and literature searches. Empirical results demonstrated that FROGENT triples the best baseline performance in hit-finding and doubles it in interaction profiling, significantly outperforming both the open-source model Qwen3-32B and the commercial model GPT-4o. In addition, real-world cases have been utilized to validate the practicability and generalization of FROGENT. This development suggests that streamlining the agentic drug discovery pipeline can significantly enhance researcher productivity.",0,arxiv,Biyoloji,CC-BY/arXiv,FROGENT: An End-to-End Full-process Drug Design Agent
"Drug discovery through virtual screening (VS) has become a popular strategy for identifying hits against protein targets. Alongside VS, molecular design further expands accessible chemical space. Together, these approaches have the potential to reduce the cost and time needed for manual selection and wet-laboratory experiments, thereby accelerating drug discovery pipelines. Improving the cost-effectiveness of virtual screening is a significant challenge, aiming to explore larger compound libraries while maintaining lower screening costs. Here, we present HelixVS, a structure-based VS platform enhanced by deep learning models. HelixVS integrates a precise deep learning-based pose-scoring model and a pose-screening module into a multi-stage VS process, enabling more effective screening of active compounds. Compared to classic molecular docking tools like Vina, HelixVS demonstrated significantly improved screening performance across nearly a hundred targets, achieving an average 2.6-fold higher enrichment factor (EF) and more than 10 times faster screening speed. We applied HelixVS in four drug development pipelines, targeting both traditional competitive drug-binding pockets and novel protein-protein interaction interfaces. Wet-lab validations across these pipelines consistently identified active compounds, with over 10% of the molecules tested in wet labs demonstrating activity at uM or even nM levels. This demonstrates the ability of HelixVS to identify high-affinity ligands for various targets and pockets.In addition, the HelixVS platform has been extended with HelixVS-Syn, which enables design of novel compounds from reference scaffolds. These designed molecules are seamlessly integrated into the HelixVS screening workflow, allowing researchers to explore both existing chemical libraries and novel chemical space with high affinity, synthetic accessibility, and structural novelty.",0,arxiv,Biyoloji,CC-BY/arXiv,HelixVS: Deep Learning-Enhanced Structure-Based Platform for Screening and Design
"Cytoskeletal filaments transported by surface immobilized molecular motors with one end pinned to the surface have been observed to spiral in a myosin-driven actin 'gliding assay'. The radius of the spiral was shown to scale with motor density with an exponent of -1/3, while the frequency was theoretically predicted to scale with an exponent of 4/3. While both the spiraling radius and frequency depend on motor density, the theory assumed independence of filament length, and remained to be tested on cytoskeletal systems other than actin-myosin. Here, we reconstitute dynein-driven microtubule spiraling and compare experiments to theory and numerical simulations. We characterize the scaling laws of spiraling MTs and find the radius dependence on force density to be consistent with previous results. Frequency on the other hand scales with force density with an exponent of ~1/3, contrary to previous predictions. We also predict that the spiral radius scales proportionally and the frequency scales inversely with filament length, both with an exponent of ~1/3. A model of variable persistence length best explains the length dependence observed in experiments. Our findings that reconcile theory, simulations, and experiments improve our understanding of the role of cytoskeletal filament elasticity, mechanics of microtubule buckling and motor transport and the physical principles of active filaments.",0,arxiv,Biyoloji,CC-BY/arXiv,Physical Principles of Size and Frequency Scaling of Active Cytoskeletal Spirals
"Proteins perform nearly all cellular functions and constitute most drug targets, making their analysis fundamental to understanding human biology in health and disease. Tandem mass spectrometry (MS$^2$) is the major analytical technique in proteomics that identifies peptides by ionizing them, fragmenting them, and using the resulting mass spectra to identify and quantify proteins in biological samples. In MS$^2$ analysis, peptide fragment ion probability prediction plays a critical role, enhancing the accuracy of peptide identification from mass spectra as a complement to the intensity information. Current approaches rely on global statistics of fragmentation, which assumes that a fragment's probability is uniform across all peptides. Nevertheless, this assumption is oversimplified from a biochemical principle point of view and limits accurate prediction. To address this gap, we present Pep2Prob, the first comprehensive dataset and benchmark designed for peptide-specific fragment ion probability prediction. The proposed dataset contains fragment ion probability statistics for 608,780 unique precursors (each precursor is a pair of peptide sequence and charge state), summarized from more than 183 million high-quality, high-resolution, HCD MS$^2$ spectra with validated peptide assignments and fragmentation annotations. We establish baseline performance using simple statistical rules and learning-based methods, and find that models leveraging peptide-specific information significantly outperform previous methods using only global fragmentation statistics. Furthermore, performance across benchmark models with increasing capacities suggests that the peptide-fragmentation relationship exhibits complex nonlinearities requiring sophisticated machine learning approaches.",0,arxiv,Biyoloji,CC-BY/arXiv,Pep2Prob Benchmark: Predicting Fragment Ion Probability for MS$^2$-based Proteomics
"We show how to localize and quantify the functional evolutionary constraints on natural proteins. The method compares the perturbations caused by local sequence variants to the energetics of the protein folding process and to the corresponding change to the apparent selection landscape of sequences over the evolutionary time scale. The difference between the physical folding free energies and the evolutionary free energies can be called a ""Dark Energy"". We analyse various protein sets and thereby show that Dark Energy is largely localized at functional sites, which are often energetically frustrated from the point of view of folding. Overall, we find that about 25% of the positions of the folded globular proteins display some significant Dark Energy. When a function relies on a free energy that can be thermodynamically quantified, such as a binding energy to a partner, the relationship of this physical free energy with Dark Energy can be used to define a Functional Selection Temperature. We show that selection for folding and binding functions bear similar weights in specific protein-protein interactions.",0,arxiv,Biyoloji,CC-BY/arXiv,Probing the Dark Energy in the Functional Protein Universe
"There are two approaches to describing DNA-ions interactions. The physical approach is an analysis of electrostatic interactions between ions and charges on the DNA molecule. The coordination chemistry approach is a search for modes of direct binding of ions to ionophores of DNA. We study both the inner and outer sphere coordination of ions by ionophores of the A and C forms of DNA in molecular dynamics simulations in two low-polarity solvents: in ethanol-water and methanol-water mixtures. We show that the counterion-DNA outer sphere coordination plays a key role in the experimentally observed conformational polymorphism of the DNA molecule: a transition to the A form in ethanol and to the C form in methanol. We identify the ionophores responsible for the existence of the A- and C-complexes. In both the complexes, the ions' inner sphere ligands are mostly water molecules, the ions reside in water clusters. In ethanol-water mixture, the water clusters are large, the major groove of the A-DNA is filled with water, and all ionophores are accessible to ions. In methanol-water mixture, the water clusters are small, and a large number of methanol clusters are present near DNA surface. They interfere with the coordination of ions in one of the ionophores of the major groove, and also with other ionophores near phosphates. Therefore, in methanol, the interaction energy of counterions with A-DNA cannot compensate for the repulsion between closely located phosphates. Therefore, the ions fill more accessible ionophores of the C-complex, converting DNA into the C form.",0,arxiv,Biyoloji,CC-BY/arXiv,Cation-DNA outer sphere coordination in DNA polymorphism
"Rare but critical events in complex systems, such as protein folding, chemical reactions, disease progression, and extreme weather or climate phenomena, are governed by complex, high-dimensional, stochastic dynamics. Identifying an optimal reaction coordinate (RC) that accurately captures the progress of these dynamics is crucial for understanding and simulating such processes. This work introduces a nonparametric RC optimization framework that incorporates trajectory histories, enabling robust analysis even for irregular or incomplete data. The power of the method is demonstrated through increasingly challenging analyses of protein folding dynamics, where it provides accurate committor estimates that pass a stringent validation test and yield high-resolution free energy profiles. Its generality is further illustrated through applications to dynamics in phase space, a conceptual ocean circulation model, and a longitudinal clinical dataset. These results demonstrate that rare event dynamics can be accurately characterized without exhaustive sampling of the configuration space, establishing a general, flexible, and robust framework for analyzing complex dynamical systems and longitudinal datasets.",0,arxiv,Biyoloji,CC-BY/arXiv,Nonparametric Reaction Coordinate Optimization with Histories: A Framework for Rare Event Dynamics
"Achieving precise control over a molecule's biological activity-encompassing targeted activation/inhibition, cooperative multi-target modulation, and off-target toxicity mitigation-remains a critical challenge in de novo drug design. However, existing generative methods primarily focus on producing molecules with a single desired activity, lacking integrated mechanisms for the simultaneous management of multiple intended and unintended molecular interactions. Here, we propose ActivityDiff, a generative approach based on the classifier-guidance technique of diffusion models. It leverages separately trained drug-target classifiers for both positive and negative guidance, enabling the model to enhance desired activities while minimizing harmful off-target effects. Experimental results show that ActivityDiff effectively handles essential drug design tasks, including single-/dual-target generation, fragment-constrained dual-target design, selective generation to enhance target specificity, and reduction of off-target effects. These results demonstrate the effectiveness of classifier-guided diffusion in balancing efficacy and safety in molecular design. Overall, our work introduces a novel paradigm for achieving integrated control over molecular activity, and provides ActivityDiff as a versatile and extensible framework.",0,arxiv,Biyoloji,CC-BY/arXiv,ActivityDiff: A diffusion model with Positive and Negative Activity Guidance for De Novo Drug Design
"Comprehending the long-timescale dynamics of protein-ligand complexes is very important for drug discovery and structural biology, but it continues to be computationally challenging for large biomolecular systems. We introduce HemePLM-Diffuse, an innovative generative transformer model that is designed for accurate simulation of protein-ligand trajectories, inpaints the missing ligand fragments, and sample transition paths in systems with more than 10,000 atoms. HemePLM-Diffuse has features of SE(3)-Invariant tokenization approach for proteins and ligands, that utilizes time-aware cross-attentional diffusion to effectively capture atomic motion. We also demonstrate its capabilities using the 3CQV HEME system, showing enhanced accuracy and scalability compared to leading models such as TorchMD-Net, MDGEN, and Uni-Mol.",0,arxiv,Biyoloji,CC-BY/arXiv,HemePLM-Diffuse: A Scalable Generative Framework for Protein-Ligand Dynamics in Large Biomolecular System
"Drug-drug interactions pose a significant challenge in clinical pharmacology, with severe class imbalance among interaction types limiting the effectiveness of predictive models. Common interactions dominate datasets, while rare but critical interactions remain underrepresented, leading to poor model performance on infrequent cases. Existing methods often treat DDI prediction as a binary problem, ignoring class-specific nuances and exacerbating bias toward frequent interactions. To address this, we propose a framework combining Generative Flow Networks (GFlowNet) with Variational Graph Autoencoders (VGAE) to generate synthetic samples for rare classes, improving model balance and generate effective and novel DDI pairs. Our approach enhances predictive performance across interaction types, ensuring better clinical reliability.",0,arxiv,Biyoloji,CC-BY/arXiv,GFlowNets for Learning Better Drug-Drug Interaction Representations
"The TIM-barrel fold is one of the most versatile and ubiquitous protein folds in nature, hosting a wide variety of catalytic activities and functions while serving as a model system in protein biochemistry and engineering. This review explores its role as a key fold model in protein design, particularly in addressing challenges in stabilization and functionalization. We discuss historical and recent advances in de novo TIM barrel design from the landmark creation of sTIM11 to the development of the diversified variants, with a special focus on deepening our understanding of the determinants that modulate the sequence-structure-function relationships of this architecture. Also, we examine why the diversification of de novo TIM barrels towards functionalization remains a major challenge, given the absence of natural-like active site features. Current approaches have focused on incorporating structural extensions, modifying loops, and using cutting-edge AI-based strategies to create scaffolds with tailored characteristics. Despite significant advances, achieving enzymatically active de novo TIM barrels has been proven difficult, with only recent breakthroughs demonstrating functionalized designs. We discuss the limitations of stepwise functionalization approaches and support an integrated approach that simultaneously optimizes scaffold structure and active site shape, using both physical- and AI-driven methods. By combining computational and experimental insights, we highlight the TIM barrel as a powerful template for custom enzyme design and as a model system to explore the intersection of protein biochemistry, biophysics, and design.",0,arxiv,Biyoloji,CC-BY/arXiv,"Designing de novo TIM Barrels: Insights into Stabilization, Diversification, and Functionalization Strategies"
"Aqueous metabolites in terrestrial subsurface environments provide critical analog frameworks for assessing the habitability of Martian subsurface ice. On Earth, they play critical roles in sustaining microbial life within soils, permafrost, and groundwater environments and their availability shape microbial community compositions, activity, and adaptability to changes in environmental conditions, enabling communities to persist over millennial timescales. The counterpart to aqueous-soluble organics is the insoluble organic matter pool that makes up the largest portion of organic matter in natural samples and includes most types of organic signatures indicative of biological processes. Employing a range of sample preparation, molecular separation, detection, and imaging techniques enables the characterization of both labile (i.e., soluble and reactive) and recalcitrant (i.e., insoluble, non-reactive; include macromolecules) organic pools. Multiple orthogonal analytical modalities strengthen interpretations of signatures that we associate with biology as we know it and don't know it, by constraining possible abiotic sources, validating measurements across distinct techniques, and ensuring flexibility to interrogate diverse organic chemistries encountered in Martian subsurface environments. This holistic triage approach aligns with the priorities articulated in the Mars Exploration Program Analysis Group's Search for Life -Science Analysis Group (SFL-SAG) Charter for a medium-class Mars mission focused on extant life detection.",0,arxiv,Biyoloji,CC-BY/arXiv,"The Importance of Aqueous Metabolites in the Martian Subsurface for Understanding Habitability, Organic Chemical Evolution, and Potential Biology"
"Polyphenols and proteins are essential biomolecules that influence food functionality and, by extension, human health. Their interactions -- hereafter referred to as PhPIs (polyphenol-protein interactions) -- affect key processes such as nutrient bioavailability, antioxidant activity, and therapeutic efficacy. However, these interactions remain challenging due to the structural diversity of polyphenols and the dynamic nature of protein binding. Traditional experimental techniques like nuclear magnetic resonance (NMR) and mass spectrometry (MS), along with computational tools such as molecular docking and molecular dynamics (MD), have offered important insights but face constraints in scalability, throughput, and reproducibility. This review explores how deep learning (DL) is reshaping the study of PhPIs by enabling efficient prediction of binding sites, interaction affinities, and MD using high-dimensional bio- and chem-informatics data. While DL enhances prediction accuracy and reduces experimental redundancy, its effectiveness remains limited by data availability, quality, and representativeness, particularly in the context of natural products. We critically assess current DL frameworks for PhPIs analysis and outline future directions, including multimodal data integration, improved model generalizability, and development of domain-specific benchmark datasets. This synthesis offers guidance for researchers aiming to apply DL in unraveling structure-function relationships of polyphenols, accelerating discovery in nutritional science and therapeutic development.",0,arxiv,Biyoloji,CC-BY/arXiv,Decoding Polyphenol-Protein Interactions with Deep Learning: From Molecular Mechanisms to Food Applications
"Protein surface fingerprint encodes chemical and geometric features that govern protein-protein interactions and can be used to predict changes in binding affinity between two protein complexes. Current state-of-the-art models for predicting binding affinity change, such as GearBind, are all-atom based geometric models derived from protein structures. Although surface properties can be implicitly learned from the protein structure, we hypothesize that explicit knowledge of protein surfaces can improve a structure-based model's ability to predict changes in binding affinity. To this end, we introduce Pi-SAGE, a novel Permutation-Invariant Surface-Aware Graph Encoder. We first train Pi-SAGE to create a protein surface codebook directly from the structure and assign a token for each surface-exposed residue. Next, we augment the node features of the GearBind model with surface features from domain-adapted Pi-SAGE to predict binding affinity change on the SKEMPI dataset. We show that explicitly incorporating local, context-aware chemical properties of residues enhances the predictive power of all-atom graph neural networks in modeling binding affinity changes between wild-type and mutant proteins.",0,arxiv,Biyoloji,CC-BY/arXiv,Pi-SAGE: Permutation-invariant surface-aware graph encoder for binding affinity prediction
"Accurate prediction of protein-ligand interactions is essential for computer-aided drug discovery. However, existing methods often fail to capture solvent-dependent conformational changes and lack the ability to jointly learn multiple related tasks. To address these limitations, we introduce a pre-training method that incorporates ligand conformational ensembles generated under diverse solvent conditions as augmented input. This design enables the model to learn both structural flexibility and environmental context in a unified manner. The training process integrates molecular reconstruction to capture local geometry, interatomic distance prediction to model spatial relationships, and contrastive learning to build solvent-invariant molecular representations. Together, these components lead to significant improvements, including a 3.7% gain in binding affinity prediction, an 82% success rate on the PoseBusters Astex docking benchmarks, and an area under the curve of 97.1% in virtual screening. The framework supports solvent-aware, multi-task modeling and produces consistent results across benchmarks. A case study further demonstrates sub-angstrom docking accuracy with a root-mean-square deviation of 0.157 angstroms, offering atomic-level insight into binding mechanisms and advancing structure-based drug design.",0,arxiv,Biyoloji,CC-BY/arXiv,Contrastive Multi-Task Learning with Solvent-Aware Augmentation for Drug Discovery
"Large language models (LLMs) have gained significant attention in chemistry. However, most existing datasets center on molecular-level property prediction and overlook the role of fine-grained functional group (FG) information. Incorporating FG-level data can provide valuable prior knowledge that links molecular structures with textual descriptions, which can be used to build more interpretable, structure-aware LLMs for reasoning on molecule-related tasks. Moreover, LLMs can learn from such fine-grained information to uncover hidden relationships between specific functional groups and molecular properties, thereby advancing molecular design and drug discovery. Here, we introduce FGBench, a dataset comprising 625K molecular property reasoning problems with functional group information. Functional groups are precisely annotated and localized within the molecule, which ensures the dataset's interoperability thereby facilitating further multimodal applications. FGBench includes both regression and classification tasks on 245 different functional groups across three categories for molecular property reasoning: (1) single functional group impacts, (2) multiple functional group interactions, and (3) direct molecular comparisons. In the benchmark of state-of-the-art LLMs on 7K curated data, the results indicate that current LLMs struggle with FG-level property reasoning, highlighting the need to enhance reasoning capabilities in LLMs for chemistry tasks. We anticipate that the methodology employed in FGBench to construct datasets with functional group-level information will serve as a foundational framework for generating new question-answer pairs, enabling LLMs to better understand fine-grained molecular structure-property relationships. The dataset and evaluation code are available at https://github.com/xuanliugit/FGBench.",0,arxiv,Biyoloji,CC-BY/arXiv,FGBench: A Dataset and Benchmark for Molecular Property Reasoning at Functional Group-Level in Large Language Models
"Hydrogen atom transfer (HAT) reactions are essential in many biological processes, such as radical migration in damaged proteins, but their mechanistic pathways remain incompletely understood. Simulating HAT is challenging due to the need for quantum chemical accuracy at biologically relevant scales; thus, neither classical force fields nor DFT-based molecular dynamics are applicable. Machine-learned potentials offer an alternative, able to learn potential energy surfaces (PESs) with near-quantum accuracy. However, training these models to generalize across diverse HAT configurations, especially at radical positions in proteins, requires tailored data generation and careful model selection. Here, we systematically generate HAT configurations in peptides to build large datasets using semiempirical methods and DFT. We benchmark three graph neural network architectures (SchNet, Allegro, and MACE) on their ability to learn HAT PESs and indirectly predict reaction barriers from energy predictions. MACE consistently outperforms the others in energy, force, and barrier prediction, achieving a mean absolute error of 1.13 kcal/mol on out-of-distribution DFT barrier predictions. Using molecular dynamics, we show our MACE potential is stable, reactive, and generalizes beyond training data to model HAT barriers in collagen I. This accuracy enables integration of ML potentials into large-scale collagen simulations to compute reaction rates from predicted barriers, advancing mechanistic understanding of HAT and radical migration in peptides. We analyze scaling laws, model transferability, and cost-performance trade-offs, and outline strategies for improvement by combining ML potentials with transition state search algorithms and active learning. Our approach is generalizable to other biomolecular systems, enabling quantum-accurate simulations of chemical reactivity in complex environments.",0,arxiv,Biyoloji,CC-BY/arXiv,Learning Potential Energy Surfaces of Hydrogen Atom Transfer Reactions in Peptides
"We introduce a pipeline for representing a protein, or protein complex, as the union of signed distance functions (SDFs) by representing each atom as a sphere with the appropriate radius. While this idea has been used previously as a way to render images of proteins, it has not, to our knowledge, been widely adopted in a machine learning setting. Mirroring recent successful work applying SDFs to represent 3D geometry, we present a proof of concept that this representation of proteins could be useful in several biologically relevant applications. We also propose further experiments that are necessary to validate the proposed approach.",0,arxiv,Biyoloji,CC-BY/arXiv,Implicitly and Differentiably Representing Protein Surfaces and Interfaces
"The rapid expansion of enzyme kinetics literature has outpaced the curation capabilities of major biochemical databases, creating a substantial barrier to AI-driven modeling and knowledge discovery. We present zERExtractor, an automated and extensible platform for comprehensive extraction of enzyme-catalyzed reaction and activity data from scientific literature. zERExtractor features a unified, modular architecture that supports plug-and-play integration of state-of-the-art models, including large language models (LLMs), as interchangeable components, enabling continuous system evolution alongside advances in AI. Our pipeline combines domain-adapted deep learning, advanced OCR, semantic entity recognition, and prompt-driven LLM modules, together with human expert corrections, to extract kinetic parameters (e.g., kcat, Km), enzyme sequences, substrate SMILES, experimental conditions, and molecular diagrams from heterogeneous document formats. Through active learning strategies integrating AI-assisted annotation, expert validation, and iterative refinement, the system adapts rapidly to new data sources. We also release a large benchmark dataset comprising over 1,000 annotated tables and 5,000 biological fields from 270 P450-related enzymology publications. Benchmarking demonstrates that zERExtractor consistently outperforms existing baselines in table recognition (Acc 89.9%), molecular image interpretation (up to 99.1%), and relation extraction (accuracy 94.2%). zERExtractor bridges the longstanding data gap in enzyme kinetics with a flexible, plugin-ready framework and high-fidelity extraction, laying the groundwork for future AI-powered enzyme modeling and biochemical knowledge discovery.",0,arxiv,Biyoloji,CC-BY/arXiv,zERExtractor:An Automated Platform for Enzyme-Catalyzed Reaction Data Extraction from Scientific Literature
"Structural biology has long been dominated by the one sequence, one structure, one function paradigm, yet many critical biological processes - from enzyme catalysis to membrane transport - depend on proteins that adopt multiple conformational states. Existing multi-state design approaches rely on post-hoc aggregation of single-state predictions, achieving poor experimental success rates compared to single-state design. We introduce DynamicMPNN, an inverse folding model explicitly trained to generate sequences compatible with multiple conformations through joint learning across conformational ensembles. Trained on 46,033 conformational pairs covering 75% of CATH superfamilies and evaluated using Alphafold 3, DynamicMPNN outperforms ProteinMPNN by up to 25% on decoy-normalized RMSD and by 12% on sequence recovery across our challenging multi-state protein benchmark.",0,arxiv,Biyoloji,CC-BY/arXiv,Multi-state Protein Design with DynamicMPNN
"Distance Geometry plays a central role in determining protein structures from Nuclear Magnetic Resonance (NMR) data, a task known as the Molecular Distance Geometry Problem (MDGP). A subclass of this problem, the Discretizable Distance Geometry Problem (DDGP), allows a recursive solution via the combinatorial Branch-and-Prune (BP) algorithm by exploiting specific vertex orderings in protein backbones. To accommodate the inherent uncertainty in NMR data, the interval Branch-and-Prune (\textit{i}BP) algorithm was introduced, incorporating interval distance constraints through uniform sampling. In this work, we propose two new algorithmic frameworks for solving the three-dimensional interval DDGP (\textit{i}DDGP): the interval Angular Branch-and-Prune (\textit{i}ABP), and its extension, the interval Torsion-angle Branch-and-Prune (\textit{i}TBP). These methods convert interval distances into angular constraints, enabling structured sampling over circular arcs. The \textit{i}ABP method guarantees feasibility by construction and removes the need for explicit constraint checking. The \textit{i}TBP algorithm further incorporates known torsion angle intervals, enforcing local chirality and planarity conditions critical for protein geometry. We present formal mathematical foundations for both methods and a systematic strategy for generating biologically meaningful \textit{i}DDGP instances from the Protein Data Bank (PDB) structures. Computational experiments demonstrate that both \textit{i}ABP and \textit{i}TBP consistently outperform \textit{i}BP in terms of solution rate and computational efficiency. In particular, \textit{i}TBP yields solutions with lower RMSD variance relative to the original PDB structures, better reflecting biologically plausible conformations.",0,arxiv,Biyoloji,CC-BY/arXiv,An Angle-Based Algorithmic Framework for the Interval Discretizable Distance Geometry Problem
"Understanding the stoichiometry and associated stability of virus-like particles (VLPs) is crucial for optimizing their assembly efficiency and immunogenic properties, which are essential for advancing biotechnology, vaccine design, and drug delivery. However, current experimental methods for determining VLP stoichiometry are labor-intensive, and time consuming. Machine learning approaches have hardly been applied to the study of VLPs. To address this challenge, we introduce a novel persistent Laplacian-based machine learning (PLML) mode that leverages both harmonic and non-harmonic spectra to capture intricate topological and geometric features of VLP structures. This approach achieves superior performance on the VLP200 dataset compared to existing methods. To further assess robustness and generalizability, we collected a new dataset, VLP706, containing 706 VLP samples with expanded stoichiometry diversity. Our PLML model maintains strong predictive accuracy on VLP706. Additionally, through random sequence perturbative mutation analysis, we found that 60-mers and 180-mers exhibit greater stability than 240-mers and 420-mers.",0,arxiv,Biyoloji,CC-BY/arXiv,Topological Learning Prediction of Virus-like Particle Stoichiometry and Stability
"DNA-binding proteins (DBPs) are integral to gene regulation and cellular processes, making their accurate identification essential for understanding biological functions and disease mechanisms. Experimental methods for DBP identification are time-consuming and costly, driving the need for efficient computational prediction techniques. In this study, we propose a novel deep learning framework, ResCap-DBP, that combines a residual learning-based encoder with a one-dimensional Capsule Network (1D-CapsNet) to predict DBPs directly from raw protein sequences. Our architecture incorporates dilated convolutions within residual blocks to mitigate vanishing gradient issues and extract rich sequence features, while capsule layers with dynamic routing capture hierarchical and spatial relationships within the learned feature space. We conducted comprehensive ablation studies comparing global and local embeddings from ProteinBERT and conventional one-hot encoding. Results show that ProteinBERT embeddings substantially outperform other representations on large datasets. Although one-hot encoding showed marginal advantages on smaller datasets, such as PDB186, it struggled to scale effectively. Extensive evaluations on four pairs of publicly available benchmark datasets demonstrate that our model consistently outperforms current state-of-the-art methods. It achieved AUC scores of 98.0% and 89.5% on PDB14189andPDB1075, respectively. On independent test sets PDB2272 and PDB186, the model attained top AUCs of 83.2% and 83.3%, while maintaining competitive performance on larger datasets such as PDB20000. Notably, the model maintains a well balanced sensitivity and specificity across datasets. These results demonstrate the efficacy and generalizability of integrating global protein representations with advanced deep learning architectures for reliable and scalable DBP prediction in diverse genomic contexts.",0,arxiv,Biyoloji,CC-BY/arXiv,ResCap-DBP: A Lightweight Residual-Capsule Network for Accurate DNA-Binding Protein Prediction Using Global ProteinBERT Embeddings
"We propose a framework based on Quadratic Unconstrained Binary Optimization (QUBO) for generating plausible ligand binding poses within protein pockets, enabling efficient structure-based virtual screening. The method discretizes the binding site into a grid and solves a QUBO problem to select spatially distributed, energetically favorable grid points. Each ligand is represented by a three-atom geometric contour, which is aligned to the selected grid points through rigid-body transformation, producing from hundreds to hundreds of thousands of candidate poses. Using a benchmark of 169 protein-ligand complexes, we generated an average of 110 to 600000 poses per ligand, depending on QUBO parameters and matching thresholds. Evaluation against crystallographic structures revealed that a larger number of candidates increases the likelihood of recovering near-native poses, with recovery rates reaching 100 percent for root mean square deviation (RMSD) values below 1.0 angstrom and 95.9 percent for RMSD values below 0.6 angstrom. Since the correct binding pose is not known in advance, we apply AutoDock-based scoring to select the most plausible candidates from the generated pool, achieving recovery rates of up to 82.8 percent for RMSD < 2.0 angstrom, 81.7 percent for RMSD < 1.5 angstrom, and 75.2 percent for RMSD < 1.0 angstrom. When poses with misleading scores are excluded, performance improves further, with recovery rates reaching up to 97.8 percent for RMSD < 2.0 angstrom and 1.5 angstrom, and 95.4 percent for RMSD < 1.0 angstrom. This modular and hardware-flexible framework offers a scalable solution for pre-filtering ligands and generating high-quality binding poses before affinity prediction, making it well-suited for large-scale virtual screening pipelines.",0,arxiv,Biyoloji,CC-BY/arXiv,Ligand Pose Generation via QUBO-Based Hotspot Sampling and Geometric Triplet Matching
"Recent advances in generative models, particularly diffusion and auto-regressive models, have revolutionized fields like computer vision and natural language processing. However, their application to structure-based drug design (SBDD) remains limited due to critical data constraints. To address the limitation of training data for models targeting SBDD tasks, we propose an evolutionary framework named MEVO, which bridges the gap between billion-scale small molecule dataset and the scarce protein-ligand complex dataset, and effectively increase the abundance of training data for generative SBDD models. MEVO is composed of three key components: a high-fidelity VQ-VAE for molecule representation in latent space, a diffusion model for pharmacophore-guided molecule generation, and a pocket-aware evolutionary strategy for molecule optimization with physics-based scoring function. This framework efficiently generate high-affinity binders for various protein targets, validated with predicted binding affinities using free energy perturbation (FEP) methods. In addition, we showcase the capability of MEVO in designing potent inhibitors to KRAS$^{\textrm{G12D}}$, a challenging target in cancer therapeutics, with similar affinity to the known highly active inhibitor evaluated by FEP calculations. With high versatility and generalizability, MEVO offers an effective and data-efficient model for various tasks in structure-based ligand design.",0,arxiv,Biyoloji,CC-BY/arXiv,Generative molecule evolution using 3D pharmacophore for efficient Structure-Based Drug Design
"Aberrant protein-protein interactions (PPIs) underpin a plethora of human diseases, and disruption of these harmful interactions constitute a compelling treatment avenue. Advances in computational approaches to PPI prediction have closely followed progress in deep learning and natural language processing. In this review, we outline the state-of the-art for sequence-based PPI prediction methods and explore their impact on target identification and drug discovery. We begin with an overview of commonly used training data sources and techniques used to curate these data to enhance the quality of the training set. Subsequently, we survey various PPI predictor types, including traditional similarity-based approaches, and deep learning-based approaches with a particular emphasis on the transformer architecture. Finally, we provide examples of PPI prediction in systems-level proteomics analyses, target identification, and design of therapeutic peptides and antibodies. We also take the opportunity to showcase the potential of PPI-aware drug discovery models in accelerating therapeutic development.",0,arxiv,Biyoloji,CC-BY/arXiv,Sequence-based protein-protein interaction prediction and its applications in drug discovery
"Developing enzymes with desired thermal properties is crucial for a wide range of industrial and research applications, and determining temperature stability is an essential step in this process. Experimental determination of thermal parameters is labor-intensive, time-consuming, and costly. Moreover, existing computational approaches are often hindered by limited data availability and imbalanced distributions. To address these challenges, we introduce a curated temperature stability dataset designed for model development and benchmarking in enzyme thermal modeling. Leveraging this dataset, we present the \textit{Segment Transformer}, a novel deep learning framework that enables efficient and accurate prediction of enzyme temperature stability. The model achieves state-of-the-art performance with an RMSE of 24.03, MAE of 18.09, and Pearson and Spearman correlations of 0.33, respectively. These results highlight the effectiveness of incorporating segment-level representations, grounded in the biological observation that different regions of a protein sequence contribute unequally to thermal behavior. As a proof of concept, we applied the Segment Transformer to guide the engineering of a cutinase enzyme. Experimental validation demonstrated a 1.64-fold improvement in relative activity following heat treatment, achieved through only 17 mutations and without compromising catalytic function.",0,arxiv,Biyoloji,CC-BY/arXiv,Modeling enzyme temperature stability from sequence segment perspective
"Traditional drug discovery relies on rounds of screening millions of candidate molecules with low success rates, making drug discovery time and resource intensive. To overcome this screening bottleneck, we introduce Latent-X, an all-atom protein design model that enables a new paradigm of precision AI design. Given a target protein epitope, Latent-X jointly generates the all atom structure and sequence of the protein binder and target, directly modelling the non-covalent interactions essential for specific binding. We demonstrate its efficacy across two therapeutically relevant modalities through extensive wet lab experiments, testing as few as 30-100 designs per target. For macrocyclic peptides, Latent-X achieves experimental hit rates exceeding 90% on all evaluated benchmark targets. For mini-binders, it consistently produces potent candidates against all evaluated benchmark targets, with binding affinities reaching the low nanomolar and picomolar range - comparable to those of approved therapeutics - whilst also being highly specific in mammalian display. In direct comparisons with the state-of-the-art models AlphaProteo, RFdiffusion and RFpeptides under identical conditions demonstrates, Latent-X generates binders with higher hit rates and better binding affinities, and uniquely creates structurally diverse binders, including complex beta-sheet folds. Its end-to-end process is an order of magnitude faster than existing multi-step computational pipelines. By drastically improving the efficiency and success rate of de novo design, Latent-X represents a significant advance towards push-button biologics discovery and a valuable tool for protein engineers. Latent-X is available at https://platform.latentlabs.com, enabling users to reliably generate de novo binders without AI infrastructure or coding.",0,arxiv,Biyoloji,CC-BY/arXiv,Latent-X: An Atom-level Frontier Model for De Novo Protein Binder Design
"Drug-drug interactions (DDIs) arise when multiple drugs are administered concurrently. Accurately predicting the specific mechanisms underlying DDIs (named DDI events or DDIEs) is critical for the safe clinical use of drugs. DDIEs are typically represented as textual descriptions. However, most computational methods focus more on predicting the DDIE class label over generating human-readable natural language increasing clinicians' interpretation costs. Furthermore, current methods overlook the fact that each drug assumes distinct biological functions in a DDI, which, when used as input context, can enhance the understanding of the DDIE process and benefit DDIE generation by the language model (LM). In this work, we propose a novel pairwise knowledge-augmented generative method (termed PKAG-DDI) for DDIE text generation. It consists of a pairwise knowledge selector efficiently injecting structural information between drugs bidirectionally and simultaneously to select pairwise biological functions from the knowledge set, and a pairwise knowledge integration strategy that matches and integrates the selected biological functions into the LM. Experiments on two professional datasets show that PKAG-DDI outperforms existing methods in DDIE text generation, especially in challenging inductive scenarios, indicating its practicality and generalization.",0,arxiv,Biyoloji,CC-BY/arXiv,PKAG-DDI: Pairwise Knowledge-Augmented Language Model for Drug-Drug Interaction Event Text Generation
"Porous nanomaterials have recently attracted a lot of attention due to various properties and potential applications. In this study, carbon nanoparticles (CNPs) were synthesized by the one-pot hydrothermal carbonization (HTC) using carboxymethyl cellulose (CMC). Urea was used as the nitrogen source for carbonization. The presence of urea in CMC solution for carbonization resulted in CNPsu reduction in the diameter of particles from 4 micrometer to 1 micrometer. Activation process at high temperature for both the above samples resulted in nanoparticles with diameter of 51 nm and 31 nm, respectively. The positive effect of presence urea and its activation generated different functional groups including C-N, N-H, and C -(triple bond)- N with increasing aromatic rings that probably may help entrapment of drugs into them. On the other hand, activation CNPsu (ACNPsu) has the most aromatic rings with the lowest hydroxyl groups with 84.66% carbon and 12.29% oxygen in its structures. ACNPs, and ACNPsu exhibited a type I isotherm indicating microporous materials with a high surface area about 552.9 m2/g and 351.01 m2/g, respectively. The high surface area was characteristic of activated carbons with their high adsorption capacity. Thus, the synthesized materials were characterized using SEM, TEM, DLS, BET, FTIR, HNMR, and TGA techniques. Finally, the encapsulation of clindamycin drug (CD) with positive charge in different types of NPs with negative charge was investigated for drug delivery in biomedical engineering applications.",0,arxiv,Biyoloji,CC-BY/arXiv,Synthesis of nanoparticles from carboxymethyl cellulose using one-pot hydrothermal carbonization for Drug Entrapment Studies
"The scarcity of molecules with desirable properties (i.e., `positive' molecules) is an inherent bottleneck for generative molecule design. To sidestep such obstacle, here we propose molecular task arithmetic: training a model on diverse and abundant negative examples to learn 'property directions' - without accessing any positively labeled data - and moving models in the opposite property directions to generate positive molecules. When analyzed on 33 design experiments with distinct molecular entities (small molecules, proteins), model architectures, and scales, molecular task arithmetic generated more diverse and successful designs than models trained on positive molecules in general. Moreover, we employed molecular task arithmetic in dual-objective and few-shot design tasks. We find that molecular task arithmetic can consistently increase the diversity of designs while maintaining desirable complex design properties, such as good docking scores to a protein. With its simplicity, data efficiency, and performance, molecular task arithmetic bears the potential to become the de facto transfer learning strategy for de novo molecule design.",0,arxiv,Biyoloji,CC-BY/arXiv,Look the Other Way: Designing 'Positive' Molecules with Negative Data via Task Arithmetic
"Advances in sequencing have revealed that each individual carries about 10,000 missense variants. For the vast majority, we do not know what the functional consequences - if any - will be. Further, mechanistic insight, such as structural details, would be immensely helpful in development of therapeutic approaches. Here we review recent developments in experimental and computational techniques aimed to assess the impact of variants on protein-protein interactions, including limitations and upcoming challenges.",0,arxiv,Biyoloji,CC-BY/arXiv,"Variant effects on protein-protein interactions: methods, models and diseases"
"Phase separation in biomolecular mixtures can result from multiple physical interactions, which may act either complementarily or antagonistically. In the case of protein--nucleic acid mixtures, charge plays a key role but can have contrasting effects on phase behavior. Attractive electrostatic interactions between oppositely charged macromolecules are screened by added salt, reducing the driving force for coacervation. By contrast, base-pairing interactions between nucleic acids are diminished by charge repulsion and thus enhanced by added salt, promoting associative phase separation. To explore this interplay, we combine experiment and theory to map the complex phase behavior of a model solution of poly-L-lysine (PLL) and self-complementary DNA nanostars (NS) as a function of temperature, ionic strength, and macromolecular composition. Despite having opposite salt dependences, we find that electrostatics and base pairing cooperate to stabilize NS--PLL coacervation at high ionic strengths and temperatures, leading to two-phase or three-phase coexistence under various conditions. We further observe a variety of kinetic pathways to phase separation at different salt concentrations, resulting in the formation of nonequilibrium aggregates or droplets whose compositions evolve on long timescales. Finally, we show that the cooperativity between electrostatics and base pairing can be used to create multiphase coacervates that partition various NS species at intermediate salt concentrations. Our results illustrate how the interplay between distinct interaction modes can greatly increase the complexity of the phase behavior relative to systems with a single type of interaction.",0,arxiv,Biyoloji,CC-BY/arXiv,Cooperation and competition of basepairing and electrostatic interactions in mixtures of DNA nanostars and polylysine
"Molecular dynamics (MD) is a powerful approach for modelling molecular systems, but it remains computationally intensive on spatial and time scales of many macromolecular systems of biological interest. To explore the opportunities offered by deep learning to address this problem, we introduce a Molecular Dynamics Large Language Model (MD-LLM) framework to illustrate how LLMs can be leveraged to learn protein dynamics and discover states not seen in training. By applying MD-LLM-1, the first implementation of this approach, obtained by fine-tuning Mistral 7B, to the T4 lysozyme and Mad2 protein systems, we show that training on one conformational state enables the prediction of other conformational states. These results indicate that MD-LLM-1 can learn the principles for the exploration of the conformational landscapes of proteins, although it is not yet modeling explicitly their thermodynamics and kinetics.",0,arxiv,Biyoloji,CC-BY/arXiv,MD-LLM-1: A Large Language Model for Molecular Dynamics
"Detecting stress in plants is crucial for both open-farm and controlled-environment agriculture. Biomolecules within plants serve as key stress indicators, offering vital markers for continuous health monitoring and early disease detection. Raman spectroscopy provides a powerful, non-invasive means to quantify these biomolecules through their molecular vibrational signatures. However, traditional Raman analysis relies on customized data-processing workflows that require fluorescence background removal and prior identification of Raman peaks of interest-introducing potential biases and inconsistencies. Here, we introduce DIVA (Deep-learning-based Investigation of Vibrational Raman spectra for plant-stress Analysis), a fully automated workflow based on a variational autoencoder. Unlike conventional approaches, DIVA processes native Raman spectra-including fluorescence backgrounds-without manual preprocessing, identifying and quantifying significant spectral features in an unbiased manner. We applied DIVA to detect a range of plant stresses, including abiotic (shading, high light intensity, high temperature) and biotic stressors (bacterial infections). By integrating deep learning with vibrational spectroscopy, DIVA paves the way for AI-driven plant health assessment, fostering more resilient and sustainable agricultural practices.",0,arxiv,Biyoloji,CC-BY/arXiv,Deep-Learning Investigation of Vibrational Raman Spectra for Plant-Stress Analysis
"Lateral flow immunoassays (LFIA) are among the most widely used rapid diagnostic tests for point-of-care screening of disease biomarkers. However, their limited sensitivity hinders their use in complex clinical applications that require accurate biomarker quantification for precise medicine. To address this limitation, we evaluated Bright-Dtech___-614 Europium nanoparticles to enhance LFIA assay sensitivity. These nanoparticles exhibited a luminescence quantum yield of 70 % and a 90 % conjugation efficacy with antibodies by direct adsorption. Considering these properties, we developed an LFIA to quantify human lactate dehydrogenase (h-LDH), a biomarker and therapeutic target in cancer disease. The Bright-Dtech___-614 Eu nanoparticle-based assay achieved a detection limit of 38 pg mL -1 , representing a 686-fold, 15-fold, and 2.9-fold improvement in sensitivity over conventional LFIA platforms using gold (AuNPs), carbon nanoparticles, and standard ELISA, respectively. The assay exhibited strong accuracy, with a mean recovery rate of 108 $\pm$ 11 %, and demonstrated excellent reproducibility, as evidenced by inter-and intra-batch RSD values of 4.9 % and 9.7 %, respectively, when testing LDH-spiked serum samples. By substituting traditional gold nanoparticles with the Bright-Dtech___-614 Eu nanoparticles, we achieved detection limits in the femtomolar range, significantly broadening the applicability of LFIA for precision medicine.",0,arxiv,Biyoloji,CC-BY/arXiv,Breaking the picomolar barrier in lateral flow assays using Bright-Dtech___ 614 -- Europium nanoparticles for enhanced sensitivity
"Plastics are essential to modern life, yet poor disposal practices contribute to low recycling rates and environmental accumulation-biological degradation and by-product reuse offer a path to mitigate this global threat. This report highlights key insights, future challenges, and research priorities identified during the CECAM workshop ""Computations Meet Experiments to Advance the Enzymatic Depolymerization of Plastics One Atom at a Time"", held in Trieste from May 6-8, 2025. The workshop brought together an interdisciplinary community of scientists focused on advancing the sustainable use of plastics through enzyme-based degradation. A key point from the discussions is that many bottlenecks in enzymatic recycling arise not only from process engineering challenges, but also from a limited understanding of the underlying molecular mechanisms. We argue that constraints on economic viability and sustainability (e.g., harsh solvents, high temperatures, substrate crystallinity, pretreatments) can-and should-be addressed directly through enzyme design, provided these factors are understood at the molecular level, in synergy with process optimization. For this, it is essential to rely on the integration of experimental and computational approaches to uncover the molecular and mechanistic basis of enzymatic plastic degradation. We highlight how the small-format structure of the workshop, in line with the usual CECAM format, fostered a collaborative, friendly, and relaxed atmosphere. We hope this report encourages future initiatives and the formation of shared consortia to support an open, collaborative, and bio-based plastic recycling community.",0,arxiv,Biyoloji,CC-BY/arXiv,Computations Meet Experiments to Advance the Enzymatic Depolymerization of Plastics One Atom at a Time
"Extensively exploring protein conformational landscapes remains a major challenge in computational biology due to the high computational cost involved in dynamic physics-based simulations. In this work, we propose a novel pipeline, MoDyGAN, that leverages molecular dynamics (MD) simulations and generative adversarial networks (GANs) to explore protein conformational spaces. MoDyGAN contains a generator that maps Gaussian distributions into MD-derived protein trajectories, and a refinement module that combines ensemble learning with a dual-discriminator to further improve the plausibility of generated conformations. Central to our approach is an innovative representation technique that reversibly transforms 3D protein structures into 2D matrices, enabling the use of advanced image-based GAN architectures. We use three rigid proteins to demonstrate that MoDyGAN can generate plausible new conformations. We also use deca-alanine as a case study to show that interpolations within the latent space closely align with trajectories obtained from steered molecular dynamics (SMD) simulations. Our results suggest that representing proteins as image-like data unlocks new possibilities for applying advanced deep learning techniques to biomolecular simulation, leading to an efficient sampling of conformational states. Additionally, the proposed framework holds strong potential for extension to other complex 3D structures.",0,arxiv,Biyoloji,CC-BY/arXiv,MoDyGAN: Combining Molecular Dynamics With GANs to Investigate Protein Conformational Space
"Advances in deep learning for molecular generation show promise in accelerating drug discovery. Bayesian Flow Networks (BFNs) have recently shown impressive performance across diverse chemical tasks, with their success often ascribed to the paradigm of modeling in a low-variance parameter space. However, the Bayesian inference-based strategy imposes limitations on designing more flexible distribution transformation pathways, making it challenging to adapt to diverse data distributions and varied task requirements. Furthermore, the potential for simpler, more efficient parameter-space-based models is unexplored. To address this, we propose a novel Parameter Interpolation Flow model (named PIF) with detailed theoretical foundation, training, and inference procedures. We then develop MolPIF for structure-based drug design, demonstrating its superior performance across diverse metrics compared to baselines. This work validates the effectiveness of parameter-space-based generative modeling paradigm for molecules and offers new perspectives for model design.",0,arxiv,Biyoloji,CC-BY/arXiv,MolPIF: A Parameter Interpolation Flow Model for Molecule Generation
"Unlocking the potential of nanomaterials in medicine and environmental science hinges on understanding their interactions with proteins, a complex decision space where AI is poised to make a transformative impact. However, progress has been hindered by limited datasets and the restricted generalizability of existing models. Here, we propose NanoPro-3M, the largest nanomaterial-protein interaction dataset to date, comprising over 3.2 million samples and 37,000 unique proteins. Leveraging this, we present NanoProFormer, a foundational model that predicts nanomaterial-protein affinities through multimodal representation learning, demonstrating strong generalization, handling missing features, and unseen nanomaterials or proteins. We show that multimodal modeling significantly outperforms single-modality approaches and identifies key determinants of corona formation. Furthermore, we demonstrate its applicability to a range of downstream tasks through zero-shot inference and fine-tuning. Together, this work establishes a solid foundation for high-performance and generalized prediction of nanomaterial-protein interaction endpoints, reducing experimental reliance and accelerating various in vitro applications.",0,arxiv,Biyoloji,CC-BY/arXiv,A million-scale dataset and generalizable foundation model for nanomaterial-protein interactions
"Combinatorial optimization algorithm is essential in computer-aided drug design by progressively exploring chemical space to design lead compounds with high affinity to target protein. However current methods face inherent challenges in integrating domain knowledge, limiting their performance in identifying lead compounds with novel and valid binding mode. Here, we propose AutoLeadDesign, a lead compounds design framework that inspires extensive domain knowledge encoded in large language models with chemical fragments to progressively implement efficient exploration of vast chemical space. The comprehensive experiments indicate that AutoLeadDesign outperforms baseline methods. Significantly, empirical lead design campaigns targeting two clinically relevant targets (PRMT5 and SARS-CoV-2 PLpro) demonstrate AutoLeadDesign's competence in de novo generation of lead compounds achieving expert-competitive design efficacy. Structural analysis further confirms their mechanism-validated inhibitory patterns. By tracing the process of design, we find that AutoLeadDesign shares analogous mechanisms with fragment-based drug design which traditionally rely on the expert decision-making, further revealing why it works. Overall, AutoLeadDesign offers an efficient approach for lead compounds design, suggesting its potential utility in drug design.",0,arxiv,Biyoloji,CC-BY/arXiv,A Collaborative Framework Integrating Large Language Model and Chemical Fragment Space: Mutual Inspiration for Lead Design
"Globular proteins are expected to assume folds with fixed secondary structures, alpha-helices and beta-sheets. Fold-switching proteins challenge this expectation by remodeling their secondary and/or tertiary structures in response to cellular stimuli. Though these shapeshifting proteins were once thought to be haphazard evolutionary byproducts with little intrinsic biological relevance, recent work has shown that evolution has selected for their dual-folding behavior, which plays critical roles in biological processes across all kingdoms of life. The widening scope of fold switching draws attention to the ways it challenges conventional wisdom, raising fundamental unanswered questions about protein structure, biophysics, and evolution. Here we discuss the progress being made to answer these questions and suggest future directions for the field.",0,arxiv,Biyoloji,CC-BY/arXiv,Fold-switching Proteins
"Generative chemical language models (CLMs) have demonstrated strong capabilities in molecular design, yet their impact in drug discovery remains limited by the absence of reliable reward signals and the lack of interpretability in their outputs. We present SAFE-T, a generalist chemical modeling framework that conditions on biological context -- such as protein targets or mechanisms of action -- to prioritize and design molecules without relying on structural information or engineered scoring functions. SAFE-T models the conditional likelihood of fragment-based molecular sequences given a biological prompt, enabling principled scoring of molecules across tasks such as virtual screening, drug-target interaction prediction, and activity cliff detection. Moreover, it supports goal-directed generation by sampling from this learned distribution, aligning molecular design with biological objectives. In comprehensive zero-shot evaluations across predictive (LIT-PCBA, DAVIS, KIBA, ACNet) and generative (DRUG, PMO) benchmarks, SAFE-T consistently achieves performance comparable to or better than existing approaches while being significantly faster. Fragment-level attribution further reveals that SAFE-T captures known structure-activity relationships, supporting interpretable and biologically grounded design. Together with its computational efficiency, these results demonstrate that conditional generative CLMs can unify scoring and generation to accelerate early-stage drug discovery.",0,arxiv,Biyoloji,CC-BY/arXiv,Conditional Chemical Language Models are Versatile Tools in Drug Discovery
"This work presents BioPykrete, a new sustainable bio-composite material created from ice, nano-crystalline cellulose (CNC), and a tailor-made chimera protein designed to bind the two together. We developed and produced the chimera protein by linking AFPIII, an ice-binding protein, with CBM3a, a CNC-binding protein. As the suspension freezes, the CNC chains self-organize into a reinforcing network between the ice crystals. This structural enhancement limits crack propagation to typical pore sizes, allowing BioPykrete to avoid the brittle and sudden failure commonly associated with ice. Instead, it exhibits an elastic-like response to stress, making it suitable for construction and engineering applications. With compressive strength comparable with concrete, BioPykrete offers a sustainable and biodegradable alternative to construction materials suitable for the harsh arctic regions of the world where traditional methods are ineffective, and resources are scarce. Engineering chimera proteins with specific affinity to more than a single material type may help improve or tailor the properties of other composite materials.",0,arxiv,Biyoloji,CC-BY/arXiv,Biomimetic Engineering of a Fortified Ice Composite with Enhanced Mechanical Properties
"Structure-based virtual screening aims to identify high-affinity ligands by estimating binding free energies between proteins and small molecules. However, the conformational flexibility of both proteins and ligands challenges conventional rigid docking methods that assume a fixed receptor structure. In this study, we examined the impact of conformational diversity on binding energy calculations using 79 HIV-1 protease-ligand complexes. Molecular dynamics simulations were employed to generate structural ensembles for both proteins and ligands in aqueous environments. RMSD-based clustering was applied to reduce redundancy while preserving structural diversity. Binding energies were computed using van der Waals and electrostatic interactions. The results demonstrated that native protein-ligand pairs consistently yielded favorable binding energies, whereas non-native pairings often failed to reproduce binding. Furthermore, clustering thresholds influenced the balance between computational cost and interaction accuracy. These findings underscore the importance of incorporating multiple protein and ligand conformations in SBVS protocols to improve prediction reliability and support more effective drug discovery strategies.",0,arxiv,Biyoloji,CC-BY/arXiv,Leveraging Conformational Diversity for Enhanced Structure-Based Virtual Screening: Insights from Molecular Dynamics Simulations of HIV-1 Protease-Ligand Complexes
"Generating large ensembles of candidate conformations is standard for improving biomolecular structure prediction. Yet aimless sampling is inefficient and costly, producing many redundant conformations with limited diversity, so additional computation often yields little improvement. Here, we present HelixFold-S1, a guided planning approach that strategically targets the most informative regions of conformational space to produce accurate conformations. For each biomolecule, predicted inter-chain contact probabilities serve as a blueprint of the conformational space, guiding computational effort toward higher-probability, low-redundancy contacts that constrain structure generation. Across diverse biomolecular benchmarks, HelixFold-S1 achieves markedly higher structural accuracy than traditional unguided methods while reducing sampling requirements by an order of magnitude. Predicted contact probabilities also provide a rough indicator of prediction difficulty and sampling utility. These results demonstrate that guided planning reshapes conformational exploration and enables more efficient and accurate structural inference.",0,arxiv,Biyoloji,CC-BY/arXiv,Reshaping Biomolecular Structure Prediction through Strategic Conformational Exploration with HelixFold-S1
"We introduce Ibex, a pan-immunoglobulin structure prediction model that achieves state-of-the-art accuracy in modeling the variable domains of antibodies, nanobodies, and T-cell receptors. Unlike previous approaches, Ibex explicitly distinguishes between bound and unbound protein conformations by training on labeled apo and holo structural pairs, enabling accurate prediction of both states at inference time. Using a comprehensive private dataset of high-resolution antibody structures, we demonstrate superior out-of-distribution performance compared to existing specialized and general protein structure prediction tools. Ibex combines the accuracy of cutting-edge models with significantly reduced computational requirements, providing a robust foundation for accelerating large molecule design and therapeutic development.",0,arxiv,Biyoloji,CC-BY/arXiv,Conformation-Aware Structure Prediction of Antigen-Recognizing Immune Proteins
"Protein-ligand binding affinity prediction is essential for drug discovery and toxicity assessment. While machine learning (ML) promises fast and accurate predictions, its progress is constrained by the availability of reliable data. In contrast, physics-based methods such as absolute binding free energy perturbation (AB-FEP) deliver high accuracy but are computationally prohibitive for high-throughput applications. To bridge this gap, we introduce ToxBench, the first large-scale AB-FEP dataset designed for ML development and focused on a single pharmaceutically critical target, Human Estrogen Receptor Alpha (ER$Î±$). ToxBench contains 8,770 ER$Î±$-ligand complex structures with binding free energies computed via AB-FEP with a subset validated against experimental affinities at 1.75 kcal/mol RMSE, along with non-overlapping ligand splits to assess model generalizability. Using ToxBench, we further benchmark state-of-the-art ML methods, and notably, our proposed DualBind model, which employs a dual-loss framework to effectively learn the binding energy function. The benchmark results demonstrate the superior performance of DualBind and the potential of ML to approximate AB-FEP at a fraction of the computational cost.",0,arxiv,Biyoloji,CC-BY/arXiv,ToxBench: A Binding Affinity Prediction Benchmark with AB-FEP-Calculated Labels for Human Estrogen Receptor Alpha
"In this work, we present the first implementation of the face-centered cubic (FCC) lattice model for protein structure prediction with a quantum algorithm. Our motivation to encode the FCC lattice stems from our observation that the FCC lattice is more capable in terms of modeling realistic secondary structures in proteins compared to other lattices, as demonstrated using root mean square deviation (RMSD). We utilize two quantum methods to solve this problem: a polynomial fitting approach (PolyFit) and the Variational Quantum Eigensolver with constraints (VQEC) based on the Lagrangian duality principle. Both methods are successfully deployed on Eagle R3 (ibm_cleveland) and Heron R2 (ibm_kingston) quantum computers, where we are able to recover ground state configurations for the 6-amino acid sequence KLVFFA under noise. A comparative analysis of the outcomes generated by the two QPUs reveals a significant enhancement (reaching nearly a two-fold improvement for PolyFit and a three-fold improvement for VQEC) in the prediction and sampling of the optimal solution (ground state conformations) on the newer Heron R2 architecture, highlighting the impact of quantum hardware advancements for this application.",0,arxiv,Biyoloji,CC-BY/arXiv,Quantum Algorithm for Protein Structure Prediction Using the Face-Centered Cubic Lattice
"We introduce AMix-1, a powerful protein foundation model built on Bayesian Flow Networks and empowered by a systematic training methodology, encompassing pretraining scaling laws, emergent capability analysis, in-context learning mechanism, and test-time scaling algorithm. To guarantee robust scalability, we establish a predictive scaling law and reveal the progressive emergence of structural understanding via loss perspective, culminating in a strong 1.7-billion model. Building on this foundation, we devise a multiple sequence alignment (MSA)-based in-context learning strategy to unify protein design into a general framework, where AMix-1 recognizes deep evolutionary signals among MSAs and consistently generates structurally and functionally coherent proteins. This framework enables the successful design of a dramatically improved AmeR variant with an up to $50\times$ activity increase over its wild type. Pushing the boundaries of protein engineering, we further empower AMix-1 with an evolutionary test-time scaling algorithm for in silico directed evolution that delivers substantial, scalable performance gains as verification budgets are intensified, laying the groundwork for next-generation lab-in-the-loop protein design.",0,arxiv,Biyoloji,CC-BY/arXiv,AMix-1: A Pathway to Test-Time Scalable Protein Foundation Model
"RNA function is deeply intertwined with its conformational dynamics. In this review, we survey recent advances in the use of atomistic molecular dynamics simulations to characterize RNA dynamics in diverse contexts, including isolated molecules and complexes with ions, small molecules, or proteins. We highlight how enhanced sampling techniques and integrative approaches can improve both the precision and accuracy of the resulting structural ensembles. Finally, we examine the emerging role of artificial intelligence in accelerating progress in RNA modeling and simulation.",0,arxiv,Biyoloji,CC-BY/arXiv,RNA Dynamics and Interactions Revealed through Atomistic Simulations
"Motivation: Protein folding is a dynamic process during which a protein's amino acid sequence undergoes a series of 3-dimensional (3D) conformational changes en route to reaching a native 3D structure; the resulting 3D structural conformations are called folding intermediates. While data on native 3D structures are abundant, data on 3D structures of non-native intermediates remain sparse, due to limitations of current technologies for experimental determination of 3D structures. Yet, analyzing folding intermediates is crucial for understanding folding dynamics and misfolding-related diseases. Hence, we search the literature for available (experimentally and computationally obtained) 3D structural data on folding intermediates, organizing the data in a centralized resource. Additionally, we assess whether existing methods, designed for predicting native structures, can also be utilized to predict structures of non-native intermediates.   Results: Our literature search reveals six studies that provide 3D structural data on folding intermediates (two for post-translational and four for co-translational folding), each focused on a single protein, with 2-4 intermediates. Our assessment shows that an established method for predicting native structures, AlphaFold2, does not perform well for non-native intermediates in the context of co-translational folding; a recent study on post-translational folding concluded the same for even more existing methods. Yet, we identify in the literature recent pioneering methods designed explicitly to predict 3D structures of folding intermediates by incorporating intrinsic biophysical characteristics of folding dynamics, which show promise. This study assesses the current landscape and future directions of the field of 3D structural analysis of protein folding dynamics.",0,arxiv,Biyoloji,CC-BY/arXiv,Unavailability of experimental 3D structural data on protein folding dynamics and necessity for a new generation of structure prediction methods in this context
"Red-blood-cell lysis (HC50) is the principal safety barrier for antimicrobial-peptide (AMP) therapeutics, yet existing models only say ""toxic"" or ""non-toxic."" AmpLyze closes this gap by predicting the actual HC50 value from sequence alone and explaining the residues that drive toxicity. The model couples residue-level ProtT5/ESM2 embeddings with sequence-level descriptors in dual local and global branches, aligned by a cross-attention module and trained with log-cosh loss for robustness to assay noise. The optimal AmpLyze model reaches a PCC of 0.756 and an MSE of 0.987, outperforming classical regressors and the state-of-the-art. Ablations confirm that both branches are essential, and cross-attention adds a further 1% PCC and 3% MSE improvement. Expected-Gradients attributions reveal known toxicity hotspots and suggest safer substitutions. By turning hemolysis assessment into a quantitative, sequence-based, and interpretable prediction, AmpLyze facilitates AMP design and offers a practical tool for early-stage toxicity screening.",0,arxiv,Biyoloji,CC-BY/arXiv,AmpLyze: A Deep Learning Model for Predicting the Hemolytic Concentration
"Molecular dynamics simulations are an essential tool in understanding protein structure, dynamics, and function at the atomic level. However, preparing high quality input files for MD simulations can be a time consuming and error prone process. In this work, we introduce an automated pipeline that leverages Large Language Models (LLMs), specifically Gemini 2.0 Flash, in conjunction with python scripting and Selenium based web automation to streamline the generation of MD input files. The pipeline exploits CHARMM GUI's comprehensive web-based interface for preparing simulation-ready inputs for NAMD. By integrating Gemini's code generation and iterative refinement capabilities, simulation scripts are automatically written, executed, and revised to navigate CHARMM GUI, extract appropriate parameters, and produce the required NAMD input files. Post processing is performed using additional software to further refine the simulation outputs, thereby enabling a complete and largely hands free workflow. Our results demonstrate that this approach reduces setup time, minimizes manual errors, and offers a scalable solution for handling multiple protein systems in parallel. This automated framework paves the way for broader application of LLMs in computational structural biology, offering a robust and adaptable platform for future developments in simulation automation.",0,arxiv,Biyoloji,CC-BY/arXiv,Automating MD simulations for Proteins using Large language Models: NAMD-Agent
"Enzyme mining is rapidly evolving as a data-driven strategy to identify biocatalysts with tailored functions from the vast landscape of uncharacterized proteins. The integration of machine learning into these workflows enables high-throughput prediction of enzyme functions, including Enzyme Commission numbers, Gene Ontology terms, substrate specificity, and key catalytic properties such as kinetic parameters, optimal temperature, pH, solubility, and thermophilicity. This review provides a systematic overview of state-of-the-art machine learning models and highlights representative case studies that demonstrate their effectiveness in accelerating enzyme discovery.   Despite notable progress, current approaches remain limited by data scarcity, model generalizability, and interpretability. We discuss emerging strategies to overcome these challenges, including multi-task learning, integration of multi-modal data, and explainable AI. Together, these developments establish ML-guided enzyme mining as a scalable and predictive framework for uncovering novel biocatalysts, with broad applications in biocatalysis, biotechnology, and synthetic biology.",0,arxiv,Biyoloji,CC-BY/arXiv,"Machine Learning-Driven Enzyme Mining: Opportunities, Challenges, and Future Perspectives"
"Data-driven modeling based on Machine Learning (ML) is becoming a central component of protein engineering workflows. This perspective presents the elements necessary to develop effective, reliable, and reproducible ML models, and a set of guidelines for ML developments for protein engineering. This includes a critical discussion of software engineering good practices for development and evaluation of ML-based protein engineering projects, emphasizing supervised learning. These guidelines cover all the necessary steps for ML development, from data acquisition to model deployment. Additionally, the present perspective provides practical resources for the implementation of the outlined guidelines. These recommendations are also intended to support editors and scientific journals in enforcing good practices in ML-based protein engineering publications, promoting high standards across the community. With this, the aim is to further contribute to improved ML transparency and credibility by easing the adoption of software engineering best practices into ML development for protein engineering. We envision that the wide adoption and continuous update of best practices will encourage informed use of ML on real-world problems related to protein engineering.",0,arxiv,Biyoloji,CC-BY/arXiv,Best Practices for Machine Learning-Assisted Protein Engineering
"Existing machine learning methods for molecular (e.g., gene) embeddings are restricted to specific tasks or data modalities, limiting their effectiveness within narrow domains. As a result, they fail to capture the full breadth of gene functions and interactions across diverse biological contexts. In this study, we have systematically evaluated knowledge representations of biomolecules across multiple dimensions representing a task-agnostic manner spanning three major data sources, including omics experimental data, literature-derived text data, and knowledge graph-based representations. To distinguish between meaningful biological signals from chance correlations, we devised an adjusted variant of Singular Vector Canonical Correlation Analysis (SVCCA) that quantifies signal redundancy and complementarity across different data modalities and sources. These analyses reveal that existing embeddings capture largely non-overlapping molecular signals, highlighting the value of embedding integration. Building on this insight, we propose Platform for Representation and Integration of multimodal Molecular Embeddings (PRISME), a machine learning based workflow using an autoencoder to integrate these heterogeneous embeddings into a unified multimodal representation. We validated this approach across various benchmark tasks, where PRISME demonstrated consistent performance, and outperformed individual embedding methods in missing value imputations. This new framework supports comprehensive modeling of biomolecules, advancing the development of robust, broadly applicable multimodal embeddings optimized for downstream biomedical machine learning applications.",0,arxiv,Biyoloji,CC-BY/arXiv,Platform for Representation and Integration of multimodal Molecular Embeddings
"This study introduces a hybrid approach integrating advanced plasmonic nanomaterials and machine learning (ML) for high-precision biomolecule detection. We leverage aluminum concave nanocubes (AlCNCs) as an innovative plasmonic substrate to enhance the native fluorescence of neurotransmitters, including dopamine (DA), norepinephrine (NE), and 3,4-Dihydroxyphenylacetic acid (DOPAC). AlCNCs amplify weak fluorescence signals, enabling probe-free, label-free detection and differentiation of these molecules with great sensitivity and specificity. To further improve classification accuracy, we employ ML algorithms, with Long Short-Term Memory (LSTM) networks playing a central role in analyzing time-dependent fluorescence data. Comparative evaluations with k-Nearest Neighbors (KNN) and Random Forest (RF) demonstrate the superior performance of LSTM in distinguishing neurotransmitters. The results reveal that AlCNC substrates provide up to a 12-fold enhancement in fluorescence intensity for DA, 9-fold for NE, and 7-fold for DOPAC compared to silicon substrates. At the same time, ML algorithms achieve classification accuracy exceeding 89%. This interdisciplinary methodology bridges the gap between nanotechnology and ML, showcasing the synergistic potential of AlCNC-enhanced native fluorescence and ML in biosensing. The framework paves the way for probe-free, label-free biomolecule profiling, offering transformative implications for biomedical diagnostics and neuroscience research.",0,arxiv,Biyoloji,CC-BY/arXiv,A novel approach for classifying Monoamine Neurotransmitters by applying Machine Learning on UV plasmonic-engineered Auto Fluorescence Time Decay Series (AFTDS)
"Three-dimensional molecular generators based on diffusion models can now reach near-crystallographic accuracy, yet they remain fragmented across tasks. SMILES-only inputs, two-stage pretrain-finetune pipelines, and one-task-one-model practices hinder stereochemical fidelity, task alignment, and zero-shot transfer. We introduce MODA, a diffusion framework that unifies fragment growing, linker design, scaffold hopping, and side-chain decoration with a Bayesian mask scheduler. During training, a contiguous spatial fragment is masked and then denoised in one pass, enabling the model to learn shared geometric and chemical priors across tasks. Multi-task training yields a universal backbone that surpasses six diffusion baselines and three training paradigms on substructure, chemical property, interaction, and geometry. Model-C reduces ligand-protein clashes and substructure divergences while maintaining Lipinski compliance, whereas Model-B preserves similarity but trails in novelty and binding affinity. Zero-shot de novo design and lead-optimisation tests confirm stable negative Vina scores and high improvement rates without force-field refinement. These results demonstrate that a single-stage multi-task diffusion routine can replace two-stage workflows for structure-based molecular design.",0,arxiv,Biyoloji,CC-BY/arXiv,MODA: A Unified 3D Diffusion Framework for Multi-Task Target-Aware Molecular Generation
"Melanoma is an aggressive and highly metastatic cancer that exhibits stubborn resistance to conventional therapies, highlighting the need for novel treatments. Existing therapeutic strategies often suffer from systemic toxicity, poor efficacy and fast-gained drug resistance. In this study, we designed a cyclic peptide system (c-RGDKYQ) that takes the advantage of the overexpression of tyrosinase in melanoma cells to trigger enzyme-mediated oxidation and self-assembly. The assembled peptide nanostructures can selectively disrupt the actin cytoskeleton, impairing cancer cellular functions, e.g., motility, adhesion, and proliferation, ultimately leading to apoptosis. This approach does not rely on external drug payloads or complex delivery mechanisms. c-RGDKYQ exhibits high selectivity for melanoma cells, strongly suppressing tumor growth in a murine model with minimal systemic toxicity. Our findings illuminate that, through targeting tyrosinase, c-RGDKYQ may be an enzyme-responsive alternative to conventional treatments for melanoma.",0,arxiv,Biyoloji,CC-BY/arXiv,Targeting Melanoma-Specific Tyrosinase: Cyclic Peptide Disrupts Actin Dynamics for Precision Apoptosis Induction
"Protein language models (PLMs) encode rich biological information, yet their internal neuron representations are poorly understood. We introduce the first automated framework for labeling every neuron in a PLM with biologically grounded natural language descriptions. Unlike prior approaches relying on sparse autoencoders or manual annotation, our method scales to hundreds of thousands of neurons, revealing individual neurons are selectively sensitive to diverse biochemical and structural properties. We then develop a novel neuron activation-guided steering method to generate proteins with desired traits, enabling convergence to target biochemical properties like molecular weight and instability index as well as secondary and tertiary structural motifs, including alpha helices and canonical Zinc Fingers. We finally show that analysis of labeled neurons in different model sizes reveals PLM scaling laws and a structured neuron space distribution.",0,arxiv,Biyoloji,CC-BY/arXiv,Automated Neuron Labelling Enables Generative Steering and Interpretability in Protein Language Models
"Predicting the binding affinity of protein-ligand complexes plays a vital role in drug discovery. Unfortunately, progress has been hindered by the lack of large-scale and high-quality binding affinity labels. The widely used PDBbind dataset has fewer than 20K labeled complexes. Self-supervised learning, especially graph contrastive learning (GCL), provides a unique opportunity to break the barrier by pre-training graph neural network models based on vast unlabeled complexes and fine-tuning the models on much fewer labeled complexes. However, the problem faces unique challenges, including a lack of a comprehensive unlabeled dataset with well-defined positive/negative complex pairs and the need to design GCL algorithms that incorporate the unique characteristics of such data. To fill the gap, we propose DecoyDB, a large-scale, structure-aware dataset specifically designed for self-supervised GCL on protein-ligand complexes. DecoyDB consists of high-resolution ground truth complexes (less than 2.5 Angstrom) and diverse decoy structures with computationally generated binding poses that range from realistic to suboptimal (negative pairs). Each decoy is annotated with a Root Mean Squared Deviation (RMSD) from the native pose. We further design a customized GCL framework to pre-train graph neural networks based on DecoyDB and fine-tune the models with labels from PDBbind. Extensive experiments confirm that models pre-trained with DecoyDB achieve superior accuracy, label efficiency, and generalizability.",0,arxiv,Biyoloji,CC-BY/arXiv,DecoyDB: A Dataset for Graph Contrastive Learning in Protein-Ligand Binding Affinity Prediction
"Accurate estimation of mutational effects on protein-protein binding energies is an open problem with applications in structural biology and therapeutic design. Several deep learning predictors for this task have been proposed, but, presumably due to the scarcity of binding data, these methods underperform computationally expensive estimates based on empirical force fields. In response, we propose a transfer-learning approach that leverages advances in protein sequence modeling and folding stability prediction for this task. The key idea is to parameterize the binding energy as the difference between the folding energy of the protein complex and the sum of the folding energies of its binding partners. We show that using a pre-trained inverse-folding model as a proxy for folding energy provides strong zero-shot performance, and can be fine-tuned with (1) copious folding energy measurements and (2) more limited binding energy measurements. The resulting predictor, StaB-ddG, is the first deep learning predictor to match the accuracy of the state-of-the-art empirical force-field method FoldX, while offering an over 1,000x speed-up.",0,arxiv,Biyoloji,CC-BY/arXiv,Predicting mutational effects on protein binding from folding energy
"The synthesis of complex natural products remains one of the grand challenges of organic chemistry. We present DeepRetro, a major advancement in computational retrosynthesis that enables the discovery of viable synthetic routes for complex molecules typically considered beyond the reach of existing retrosynthetic methods. DeepRetro is a novel, open-source framework that tightly integrates large language models (LLMs), traditional retrosynthetic engines, and expert human feedback in an iterative design loop. Prior approaches rely solely on template-based methods or unconstrained LLM outputs. In contrast, DeepRetro combines the precision of template-based methods with the generative flexibility of LLMs, controlled by rigorous chemical validity checks and enhanced by recursive refinement. This hybrid system dynamically explores and revises synthetic pathways, guided by both algorithmic checks and expert chemist feedback through an interactive user interface. While DeepRetro achieves strong performance on standard retrosynthesis benchmarks, its true strength lies in its ability to propose novel, viable pathways to highly complex natural products-targets that have historically eluded automated planning. Through detailed case studies, we illustrate how this approach enables new routes for total synthesis and facilitates human-machine collaboration in organic chemistry. Beyond retrosynthesis, DeepRetro represents a working model for how to leverage LLMs in scientific discovery. We provide a transparent account of the system's design, algorithms, and human-feedback loop, enabling broad adaptation across scientific domains. By releasing DeepRetro as an open-source tool, we aim to empower chemists to tackle increasingly ambitious synthetic targets, accelerating progress in drug discovery, materials design, and beyond.",0,arxiv,Biyoloji,CC-BY/arXiv,DeepRetro: Retrosynthetic Pathway Discovery using Iterative LLM Reasoning
"Deep learning-based computational methods have achieved promising results in predicting protein-protein interactions (PPIs). However, existing benchmarks predominantly focus on isolated pairwise evaluations, overlooking a model's capability to reconstruct biologically meaningful PPI networks, which is crucial for biology research. To address this gap, we introduce PRING, the first comprehensive benchmark that evaluates protein-protein interaction prediction from a graph-level perspective. PRING curates a high-quality, multi-species PPI network dataset comprising 21,484 proteins and 186,818 interactions, with well-designed strategies to address both data redundancy and leakage. Building on this golden-standard dataset, we establish two complementary evaluation paradigms: (1) topology-oriented tasks, which assess intra and cross-species PPI network construction, and (2) function-oriented tasks, including protein complex pathway prediction, GO module analysis, and essential protein justification. These evaluations not only reflect the model's capability to understand the network topology but also facilitate protein function annotation, biological module detection, and even disease mechanism analysis. Extensive experiments on four representative model categories, consisting of sequence similarity-based, naive sequence-based, protein language model-based, and structure-based approaches, demonstrate that current PPI models have potential limitations in recovering both structural and functional properties of PPI networks, highlighting the gap in supporting real-world biological applications. We believe PRING provides a reliable platform to guide the development of more effective PPI prediction models for the community. The dataset and source code of PRING are available at https://github.com/SophieSarceau/PRING.",0,arxiv,Biyoloji,CC-BY/arXiv,PRING: Rethinking Protein-Protein Interaction Prediction from Pairs to Graphs
"Extracellular vesicles (EVs) have drawn rapidly increasing attention as the next-generation diagnostic biomarkers and therapeutic agents. However, the heterogeneous nature of EVs necessitates advanced methods for profiling EVs at the single-particle level. While nanoparticle tracking analysis (NTA) is a widely used technique for quantifying particle size and concentration, conventional scattering-based systems are non-specific. In this study, we present an optimised protocol for quantitative profiling of EVs at the single-particle level by fluorescent NTA (F-NTA). The protocol integrates fluorescent immunolabeling of EVs with size-exclusion chromatography (SEC) to efficiently remove unbound labels, enabling the precise quantification of EV concentration, size distribution, and surface immunophenotype. We first validated this approach using biotinylated liposomes and EVs from cultured human cell lines, confirming effective removal of unbound labels and assessing labelling efficiency. We then demonstrated that F-NTA can distinguish EV subpopulations with distinct surface marker expression, exemplified by the differentiation of EpCAM-positive EVs derived from HT29 and HEK293 cells. Finally, we applied dual labelling to human plasma isolates to simultaneously profile EVs and non-vesicular extracellular particles, providing a quantitative quality assessment of EV purity at the single-particle level. The robustness of this method was further supported by comparative analysis with total internal reflection fluorescence microscopy. This validated workflow enables robust, quantitative profiling of EV subpopulations, providing a critical tool for diverse EV applications, including biomarker discovery, therapeutic monitoring, and quality control for engineered vesicles.",0,arxiv,Biyoloji,CC-BY/arXiv,Quantitative Single-particle Profiling of Extracellular Vesicles via Fluorescent Nanoparticle Tracking Analysis
"Until today, the exact function of mammalian odorant binding proteins (OBPs) remains a topic of debate. Although their main established function lacks direct evidence in human olfaction, OBPs are traditionally believed to act as odorant transporters in the olfactory sense, which led to the exploration of OBPs as biomimetic sensor units in artificial noses. Now, available RNA-seq and proteomics data identified the expression of human OBPs (hOBP2A and hOBP2B) in both, male and female reproductive tissues. This observation prompted the conjecture that OBPs may possess functions that go beyond the olfactory sense, potentially as hormone transporters. Such a function could further link them to the tumorigenesis and cancer progression of hormone dependent cancer types including ovarian, breast, prostate and uterine cancer. In this structured review, we use available data to explore the effects of genetic alterations such as somatic copy number aberrations and single nucleotide variants on OBP function and their corresponding gene expression profiles. Our computational analyses suggest that somatic copy number aberrations in OBPs are associated with large changes in gene expression in reproductive cancers while point mutations have little to no effect. Additionally, the structural characteristics of OBPs, together with other lipocalin family members, allow us to explore putative functions within the context of cancer biology. Our overview consolidates current knowledge on putative human OBP functions, their expression patterns, and structural features. Finally, it provides an overview on applications, highlighting emerging hypotheses and future research directions within olfactory and non-olfactory roles.",0,arxiv,Biyoloji,CC-BY/arXiv,Beyond Olfaction: New Insights into Human Odorant Binding Proteins
"The recent breakthrough of AlphaFold3 in modeling complex biomolecular interactions, including those between proteins and ligands, nucleotides, or metal ions, creates new opportunities for protein design. In so-called inverse protein folding, the objective is to find a sequence of amino acids that adopts a target protein structure. Many inverse folding methods struggle to predict sequences for complexes that contain non-protein components, and perform poorly with complexes that adopt multiple structural states. To address these challenges, we present ADFLIP (All-atom Discrete FLow matching Inverse Protein folding), a generative model based on discrete flow-matching for designing protein sequences conditioned on all-atom structural contexts. ADFLIP progressively incorporates predicted amino acid side chains as structural context during sequence generation and enables the design of dynamic protein complexes through ensemble sampling across multiple structural states. Furthermore, ADFLIP implements training-free classifier guidance sampling, which allows the incorporation of arbitrary pre-trained models to optimise the designed sequence for desired protein properties. We evaluated the performance of ADFLIP on protein complexes with small-molecule ligands, nucleotides, or metal ions, including dynamic complexes for which structure ensembles were determined by nuclear magnetic resonance (NMR). Our model achieves state-of-the-art performance in single-structure and multi-structure inverse folding tasks, demonstrating excellent potential for all-atom protein design. The code is available at https://github.com/ykiiiiii/ADFLIP.",0,arxiv,Biyoloji,CC-BY/arXiv,All-atom inverse protein folding through discrete flow matching
"The shape of a molecule determines its physicochemical and biological properties. However, it is often underrepresented in standard molecular representation learning approaches. Here, we propose using the Euler Characteristic Transform (ECT) as a geometrical-topological descriptor. Computed directly on a molecular graph derived from handcrafted atomic features, the ECT enables the extraction of multiscale structural features, offering a novel way to represent and encode molecular shape in the feature space. We assess the predictive performance of this representation across nine benchmark regression datasets, all centered around predicting the inhibition constant $K_i$. In addition, we compare our proposed ECT-based representation against traditional molecular representations and methods, such as molecular fingerprints/descriptors and graph neural networks (GNNs). Our results show that our ECT-based representation achieves competitive performance, ranking among the best-performing methods on several datasets. More importantly, its combination with traditional representations, particularly with the AVALON fingerprint, significantly \emph{enhances predictive performance}, outperforming other methods on most datasets. These findings highlight the complementary value of multiscale topological information and its potential for being combined with established techniques. Our study suggests that hybrid approaches incorporating explicit shape information can lead to more informative and robust molecular representations, enhancing and opening new avenues in molecular machine learning tasks. To support reproducibility and foster open biomedical research, we provide open access to all experiments and code used in this work.",0,arxiv,Biyoloji,CC-BY/arXiv,Molecular Machine Learning Using Euler Characteristic Transforms
"CD8+ ""killer"" T cells and CD4+ ""helper"" T cells play a central role in the adaptive immune system by recognizing antigens presented by Major Histocompatibility Complex (pMHC) molecules via T Cell Receptors (TCRs). Modeling binding between T cells and the pMHC complex is fundamental to understanding basic mechanisms of human immune response as well as in developing therapies. While transformer-based models such as TULIP have achieved impressive performance in this domain, their black-box nature precludes interpretability and thus limits a deeper mechanistic understanding of T cell response. Most existing post-hoc explainable AI (XAI) methods are confined to encoder-only, co-attention, or model-specific architectures and cannot handle encoder-decoder transformers used in TCR-pMHC modeling. To address this gap, we propose Quantifying Cross-Attention Interaction (QCAI), a new post-hoc method designed to interpret the cross-attention mechanisms in transformer decoders. Quantitative evaluation is a challenge for XAI methods; we have compiled TCR-XAI, a benchmark consisting of 274 experimentally determined TCR-pMHC structures to serve as ground truth for binding. Using these structures we compute physical distances between relevant amino acid residues in the TCR-pMHC interaction region and evaluate how well our method and others estimate the importance of residues in this region across the dataset. We show that QCAI achieves state-of-the-art performance on both interpretability and prediction accuracy under the TCR-XAI benchmark.",0,arxiv,Biyoloji,CC-BY/arXiv,Quantifying Cross-Attention Interaction in Transformers for Interpreting TCR-pMHC Binding
"Recent advances in AI for science have highlighted the power of contrastive learning in bridging heterogeneous biological data modalities. Building on this paradigm, we propose HIPPO (HIerarchical Protein-Protein interaction prediction across Organisms), a hierarchical contrastive framework for protein-protein interaction(PPI) prediction, where protein sequences and their hierarchical attributes are aligned through multi-tiered biological representation matching. The proposed approach incorporates hierarchical contrastive loss functions that emulate the structured relationship among functional classes of proteins. The framework adaptively incorporates domain and family knowledge through a data-driven penalty mechanism, enforcing consistency between the learned embedding space and the intrinsic hierarchy of protein functions. Experiments on benchmark datasets demonstrate that HIPPO achieves state-of-the-art performance, outperforming existing methods and showing robustness in low-data regimes. Notably, the model demonstrates strong zero-shot transferability to other species without retraining, enabling reliable PPI prediction and functional inference even in less characterized or rare organisms where experimental data are limited. Further analysis reveals that hierarchical feature fusion is critical for capturing conserved interaction determinants, such as binding motifs and functional annotations. This work advances cross-species PPI prediction and provides a unified framework for interaction prediction in scenarios with sparse or imbalanced multi-species data.",0,arxiv,Biyoloji,CC-BY/arXiv,Hierarchical Multi-Label Contrastive Learning for Protein-Protein Interaction Prediction Across Organisms
"Therapeutic antibodies require not only high-affinity target engagement, but also favorable manufacturability, stability, and safety profiles for clinical effectiveness. These properties are collectively called `developability'. To enable a computational framework for optimizing antibody sequences for favorable developability, we introduce a guided discrete diffusion model trained on natural paired heavy- and light-chain sequences from the Observed Antibody Space (OAS) and quantitative developability measurements for 246 clinical-stage antibodies. To steer generation toward biophysically viable candidates, we integrate a Soft Value-based Decoding in Diffusion (SVDD) Module that biases sampling without compromising naturalness. In unconstrained sampling, our model reproduces global features of both the natural repertoire and approved therapeutics, and under SVDD guidance we achieve significant enrichment in predicted developability scores over unguided baselines. When combined with high-throughput developability assays, this framework enables an iterative, ML-driven pipeline for designing antibodies that satisfy binding and biophysical criteria in tandem.",0,arxiv,Biyoloji,CC-BY/arXiv,Guided Generation for Developable Antibodies
"Autonomous scientific research, capable of independently conducting complex experiments and serving non-specialists, represents a long-held aspiration. Achieving it requires a fundamental paradigm shift driven by artificial intelligence (AI). While autonomous experimental systems are emerging, they remain confined to areas featuring singular objectives and well-defined, simple experimental workflows, such as chemical synthesis and catalysis. We present an AI-native autonomous laboratory, targeting highly complex scientific experiments for applications like autonomous biomolecular engineering. This system autonomously manages instrumentation, formulates experiment-specific procedures and optimization heuristics, and concurrently serves multiple user requests. Founded on a co-design philosophy of models, experiments, and instruments, the platform supports the co-evolution of AI models and the automation system. This establishes an end-to-end, multi-user autonomous laboratory that handles complex, multi-objective experiments across diverse instrumentation. Our autonomous laboratory supports fundamental nucleic acid functions-including synthesis, transcription, amplification, and sequencing. It also enables applications in fields such as disease diagnostics, drug development, and information storage. Without human intervention, it autonomously optimizes experimental performance to match state-of-the-art results achieved by human scientists. In multi-user scenarios, the platform significantly improves instrument utilization and experimental efficiency. This platform paves the way for advanced biomaterials research to overcome dependencies on experts and resource barriers, establishing a blueprint for science-as-a-service at scale.",0,arxiv,Biyoloji,CC-BY/arXiv,An AI-native experimental laboratory for autonomous biomolecular engineering
"Antibody engineering is essential for developing therapeutics and advancing biomedical research. Traditional discovery methods often rely on time-consuming and resource-intensive experimental screening. To enhance and streamline this process, we introduce a production-grade, high-throughput platform built on HelixFold3, HelixDesign-Antibody, which utilizes the high-accuracy structure prediction model, HelixFold3. The platform facilitates the large-scale generation of antibody candidate sequences and evaluates their interaction with antigens. Integrated high-performance computing (HPC) support enables high-throughput screening, addressing challenges such as fragmented toolchains and high computational demands. Validation on multiple antigens showcases the platform's ability to generate diverse and high-quality antibodies, confirming a scaling law where exploring larger sequence spaces increases the likelihood of identifying optimal binders. This platform provides a seamless, accessible solution for large-scale antibody design and is available via the antibody design page of PaddleHelix platform.",0,arxiv,Biyoloji,CC-BY/arXiv,HelixDesign-Antibody: A Scalable Production-Grade Platform for Antibody Design Built on HelixFold3
"Hyperosmolarity is a key contributor to nucleus pulposus cell (NPC) apoptosis during intervertebral disc degeneration (IVDD). Aquaporin 3 (AQP3), a membrane channel protein, regulates cellular osmotic balance by transporting water and osmolytes. Although AQP3 downregulation is associated with disc degeneration, its role in apoptosis under hyperosmotic conditions remains unclear. Here, we demonstrate that hyperosmolarity induces AQP3 depletion, suppresses the PI3K/AKT/mTOR signaling pathway, and promotes mitochondrial dysfunction and ROS accumulation in NPCs. Lentiviral overexpression of AQP3 restores this pathway, attenuates oxidative damage, and reduces apoptosis, preserving disc structure in IVDD rat models. In contrast, pharmacological inhibition of AQP3 exacerbates ECM catabolism and NP tissue loss. Our findings reveal that AQP3 deficiency under hyperosmolarity contributes to NPC apoptosis via suppression of PI3K/AKT/mTOR signaling, potentially creating a pathological cycle of disc degeneration. These results highlight AQP3 as a promising therapeutic target for IVDD.",0,arxiv,Biyoloji,CC-BY/arXiv,Downregulation of aquaporin 3 promotes hyperosmolarity-induced apoptosis of nucleus pulposus cells through PI3K/Akt/mTOR pathway suppression
"We introduce IntFold, a controllable foundation model for general and specialized biomolecular structure prediction. Utilizing a high-performance custom attention kernel, IntFold achieves accuracy comparable to the state-of-the-art AlphaFold 3 on a comprehensive benchmark of diverse biomolecular structures, while also significantly outperforming other leading all-atom prediction approaches. The model's key innovation is its controllability, enabling downstream applications critical for drug screening and design. Through specialized adapters, it can be precisely guided to predict complex allosteric states, apply user-defined structural constraints, and estimate binding affinity. Furthermore, we present a training-free, similarity-based method for ranking predictions that improves success rates in a model-agnostic manner. This report details these advancements and shares insights from the training and development of this large-scale model.",0,arxiv,Biyoloji,CC-BY/arXiv,IntFold: A Controllable Foundation Model for General and Specialized Biomolecular Structure Prediction
"Finding process pathways in molecular simulations such as the unbinding paths of small molecule ligands from their binding sites at protein targets in a set of trajectories via unsupervised learning approaches requires the definition of a suitable similarity measure between trajectories. We here evaluate the performance of four such measures with varying degree of sophistication, i.e., Euclidean and Wasserstein distances, Procrustes analysis and dynamical time warping, when analyzing trajectory data from two different biased simulation driving protocols in the form of constant velocity constraint targeted MD and steered MD. In a streptavidin-biotin benchmark system with known ground truth clusters, Wasserstein distances yielded the best clustering performance, closely followed by Euclidean distances, both being the most computationally efficient similarity measures. In a more complex A2a receptor-inhibitor system, however, the simplest measure, i.e., Euclidean distances, was sufficient to reveal meaningful and interpretable clusters.",0,arxiv,Biyoloji,CC-BY/arXiv,More sophisticated is not always better: comparison of similarity measures for unsupervised learning of pathways in biomolecular simulations
"The rapid growth of biomedical data, tools, and literature has created a fragmented research landscape that outpaces human expertise. While AI agents offer a solution, they typically rely on static, manually curated toolsets, limiting their ability to adapt and scale. Here, we introduce STELLA, a self-evolving AI agent designed to overcome these limitations. STELLA employs a multi-agent architecture that autonomously improves its own capabilities through two core mechanisms: an evolving Template Library for reasoning strategies and a dynamic Tool Ocean that expands as a Tool Creation Agent automatically discovers and integrates new bioinformatics tools. This allows STELLA to learn from experience. We demonstrate that STELLA achieves state-of-the-art accuracy on a suite of biomedical benchmarks, scoring approximately 26\% on Humanity's Last Exam: Biomedicine, 54\% on LAB-Bench: DBQA, and 63\% on LAB-Bench: LitQA, outperforming leading models by up to 6 percentage points. More importantly, we show that its performance systematically improves with experience; for instance, its accuracy on the Humanity's Last Exam benchmark almost doubles with increased trials. STELLA represents a significant advance towards AI Agent systems that can learn and grow, dynamically scaling their expertise to accelerate the pace of biomedical discovery.",0,arxiv,Biyoloji,CC-BY/arXiv,STELLA: Self-Evolving LLM Agent for Biomedical Research
"The paradigm of large language models in natural language processing (NLP) has also shown promise in modeling biological languages, including proteins, RNA, and DNA. Both the auto-regressive generation paradigm and evaluation metrics have been transferred from NLP to biological sequence modeling. However, the intrinsic structural correlations in natural and biological languages differ fundamentally. Therefore, we revisit the notion of language in biological systems to better understand how NLP successes can be effectively translated to biological domains. By treating the 3D structure of biomolecules as the semantic content of a sentence and accounting for the strong correlations between residues or bases, we highlight the importance of structural evaluation and demonstrate the applicability of the auto-regressive paradigm in biological language modeling. Code can be found at \href{https://github.com/zjuKeLiu/RiFold}{github.com/zjuKeLiu/RiFold}",0,arxiv,Biyoloji,CC-BY/arXiv,From Sentences to Sequences: Rethinking Languages in Biological System
"Protein Language Models (PLMs), pre-trained on extensive evolutionary data from natural proteins, have emerged as indispensable tools for protein design. While powerful, PLMs often struggle to produce proteins with precisely specified functionalities or properties due to inherent challenges in controlling their outputs. In this work, we investigate the potential of Activation Steering, a technique originally developed for controlling text generation in Large Language Models (LLMs), to direct PLMs toward generating protein sequences with targeted properties. We propose a simple yet effective method that employs activation editing to steer PLM outputs, and extend this approach to protein optimization through a novel editing site identification module. Through comprehensive experiments on lysozyme-like sequence generation and optimization, we demonstrate that our methods can be seamlessly integrated into both auto-encoding and autoregressive PLMs without requiring additional training. These results highlight a promising direction for precise protein engineering using foundation models.",0,arxiv,Biyoloji,CC-BY/arXiv,Steering Protein Language Models
"Graph neural networks (GNNs) have achieved remarkable success in molecular property prediction. However, traditional graph representations struggle to effectively encode the inherent 3D spatial structures of molecules, as molecular orientations in 3D space introduce significant variability, severely limiting model generalization and robustness. Existing approaches primarily focus on rotation-invariant and rotation-equivariant methods. Invariant methods often rely heavily on prior knowledge and lack sufficient generalizability, while equivariant methods suffer from high computational costs. To address these limitations, this paper proposes a novel plug-and-play 3D encoding module leveraging rotational sampling. By computing the expectation over the SO(3) rotational group, the method naturally achieves approximate rotational invariance. Furthermore, by introducing a carefully designed post-alignment strategy, strict invariance can be achieved without compromising performance. Experimental evaluations on the QM9 and C10 Datasets demonstrate superior predictive accuracy, robustness, and generalization performance compared to existing methods. Moreover, the proposed approach maintains low computational complexity and enhanced interpretability, providing a promising direction for efficient and effective handling of 3D molecular information in drug discovery and material design.",0,arxiv,Biyoloji,CC-BY/arXiv,Rotational Sampling: A Plug-and-Play Encoder for Rotation-Invariant 3D Molecular GNNs
"The kernel reconstruction is a method that reduces noise in dynamic positron emission tomography (PET) by exploiting spatial correlations in the PET image. Although this method works well for large anatomical regions with relatively slow kinetics, whole body PET reconstruction with the kernel method can produce suboptimal results in regions with fast kinetics and high contrast. In this work we propose a new design of the spatial kernel matrix to improve reconstruction in fast and slow kinetics body regions. We calculate voxels features using nonnegative matrix factorization (NMF) with optimal rank selection. These features are then used to calculate similarities between voxels considering relative differences between features to adapt to a wide range of activity levels. Simulations and whole body mouse scans of high temporal resolution [18F]SynVesT-1, low dose [11C]raclopride, and [18F]Fallypride were performed to assess the performance of the method in different settings. In simulations, bias vs variance tradeoff and contrast was improved using the NMF kernel matrix, compared with the original kernel method. In real data, fast kinetic regions such as the heart, veins and kidneys presented oversmoothing or artifacts with the original kernel method. Our proposed method did not present these effects, while reducing noise. Brain kinetic modeling parametric maps with image derived input function ([18F]SynVesT-1) and with reference region ([11C]raclopride and [18F]Fallypride) also had lower standard error using the proposed kernel matrix compared with other methods. The NMF kernel reconstruction reduces noise and maintains high contrast in whole body PET imaging, outperforming the traditional kernel method.",0,arxiv,Biyoloji,CC-BY/arXiv,Whole body dynamic PET kernel reconstruction using nonnegative matrix factorization features
"The proton motive force (PMF) across the inner mitochondrial membrane delivers approximately 0.2 eV of energy per proton, powering the FoF1-ATP synthase molecular motor. Here, we provide a detailed accounting of how this energy is utilized: Approximately 75-83% is transduced into the chemical free energy of ATP synthesis, while the remaining 17-25% is dissipated through internal friction, viscous drag, proton leakage, electroviscous effects, elastic deformations, and information-theoretic costs. Each dissipation channel is quantitatively evaluated, revealing that internal friction in the F1 motor is the dominant loss mechanism. In this work, we did not account for the energy supplied/injected due to the intrinsic electrostatic potential of the enzyme itself. In addition to this energy bookkeeping, we also examine the quantum mechanical constraints on the Fo unit's rotation. We find that, as can be expected, the energy spacing between quantized rotational states is several orders of magnitude smaller than thermal energies at physiological temperature, and that the tunneling probability through rotational barriers practically totally non-existent. Furthermore, the biological rotation speed (100-650 revolutions per second (rps)) is between one and three orders of magnitude below the quantum limit implied by quantization of angular momentum of the c-ring (which would have been ca. 13,000 to 62,000 rps (depending on the size of the c-ring (17 to 8 subunits, respectively)) in the first rotational energy level of the c-ring). Nevertheless, experimental estimates of the rotation rates in isolated c-ring suggest rates in the vicinity of 43,000 rps, right within our theoretical quantum estimates. However, ATP synthase as a whole operates firmly within the classical regime, despite its nanoscale dimensions, and highlight its evolutionary optimization for robust and efficient energy conversion....",0,arxiv,Biyoloji,CC-BY/arXiv,Rotational Dynamics of ATP Synthase: Mechanical Constraints and Energy Dissipative Channels
"Recent advances in geometric deep learning and generative modeling have enabled the design of novel proteins with a wide range of desired properties. However, current state-of-the-art approaches are typically restricted to generating proteins with only static target properties, such as motifs and symmetries. In this work, we take a step towards overcoming this limitation by proposing a framework to condition structure generation on flexibility, which is crucial for key functionalities such as catalysis or molecular recognition. We first introduce BackFlip, an equivariant neural network for predicting per-residue flexibility from an input backbone structure. Relying on BackFlip, we propose FliPS, an SE(3)-equivariant conditional flow matching model that solves the inverse problem, that is, generating backbones that display a target flexibility profile. In our experiments, we show that FliPS is able to generate novel and diverse protein backbones with the desired flexibility, verified by Molecular Dynamics (MD) simulations. FliPS and BackFlip are available at https://github.com/graeter-group/flips .",0,arxiv,Biyoloji,CC-BY/arXiv,Flexibility-Conditioned Protein Structure Design with Flow Matching
"A key challenge in learning from multimodal biological data is missing modalities, where all data from some modalities are missing for some patients. Current fusion methods address this by excluding patients with missing modalities, imputing missing modalities, or making predictions directly with partial modalities. However, they often struggle with diverse missing-modality patterns and the exponential growth of the number of such patterns as the number of modalities increases. To address these limitations, we propose MAGNET (Missing-modality-Aware Graph neural NETwork) for direct prediction with partial modalities, which introduces a patient-modality multi-head attention mechanism to fuse lower-dimensional modality embeddings based on their importance and missingness. MAGNET's complexity increases linearly with the number of modalities while adapting to missing-pattern variability. To generate predictions, MAGNET further constructs a patient graph with fused multimodal embeddings as node features and the connectivity determined by the modality missingness, followed by a conventional graph neural network. Experiments on three public multiomics datasets for cancer classification, with real-world instead of artificial missingness, show that MAGNET outperforms the state-of-the-art fusion methods. The data and code are available at https://github.com/SinaTabakhi/MAGNET.",0,arxiv,Biyoloji,CC-BY/arXiv,Missing-Modality-Aware Graph Neural Network for Cancer Classification
"Harnessing the topology of ring polymers as a design motif in functional nanomaterials is becoming a promising direction in the field of soft matter. For example, the ring topology of DNA plasmids prevents the relaxation of excess twist introduced to the polymer, instead resulting in helical supercoiled structures. In equilibrium semi-dilute solutions, tightly supercoiled rings relax faster than their torsionally relaxed counterparts, since the looser conformations of the latter allow for rings to thread through each other and entrain via entanglements. Here we use molecular simulations to explore a non-equilibrium scenario, in which a supercoiling agent, akin to gyrase enzymes, rapidly induces supercoiling in the suspensions of relaxed plasmids. The activity of the agent not only alters the conformational topology from open to branched, but also locks-in threaded rings into supramolecular clusters, which relax very slowly. Ultimately, our work shows how the polymer topology under non-equilibrium conditions can be leveraged to tune dynamic behavior of macromolecular systems, suggesting a pathway to novel class of driven materials glassified by activity.",0,arxiv,Biyoloji,CC-BY/arXiv,Actively induced supercoiling can slow down plasmid solutions by trapping the threading entanglements
"Cryo-electron tomography (cryo-ET) has emerged as a powerful tool for studying the structural heterogeneity of proteins and their complexes, offering insights into macromolecular dynamics directly within cells. Driven by recent computational advances, including powerful machine learning frameworks, researchers can now resolve both discrete structural states and continuous conformational changes from 3D subtomograms and stacks of 2D particle-images acquired across tilt-series. In this review, we survey recent innovations in particle classification and heterogeneous 3D reconstruction methods, focusing specifically on the relative merits of workflows that operate on reconstructed 3D subtomogram volumes compared to those using extracted 2D particle-images. We additionally highlight how these methods have provided specific biological insights into the organization, dynamics, and structural variability of cellular components. Finally, we advocate for the development of benchmarking datasets collected in vitro and in situ to enable a more objective comparison of existent and emerging methods for particle classification and heterogeneous 3D reconstruction.",0,arxiv,Biyoloji,CC-BY/arXiv,Resolving structural dynamics in situ through cryogenic electron tomography
"The low complexity domain of Fused in Sarcoma (FUS-LC consisting of 214 residues) undergoes phase separation, resulting in a dense liquid-like phase that forms early and slowly matures to reach ordered gel-like state on long time scales. Upon maturation, core-1, comprising of the 57 residues (39-95) in the N-terminus become structured, resulting in the formation of a non-polymorphic fibril. The truncated FUS-LC-C (residues 110-214) construct forms a fibril in which core-2 (residues 112-150) adopts a $Î²$-sheet structure. Using coarse-grained monomer SOP-IDP model simulations of FUS-LC, we predict that residues 155-190 in the C-terminal (core-3) form rapidly, followed by core-2, and finally core-1. The time scale of formation of the cores and their stabilities are inversely correlated, as anticipated by the Ostwald's rule of stages. Unbiased multichain simulations show that the chemical potentials in the two phases are equal and the calculated densities of the dense and dilute phases are in agreement with experiments. The dense phase, which forms by a nucleation mechanism, coarsens over time by a process that is reminiscent of Ostwald ripening. AlphaFold predictions of the core-3 structure and the simulations show that $Î²$-strand emerges in the core-3 region early during the droplet formation, and drives the initiation of FUS-LC assembly. The techniques introduced here are general and could be used to probe assembly of other IDPs such as TDP-43, which shares many features with FUS-LC.",0,arxiv,Biyoloji,CC-BY/arXiv,"Droplet growth, Ostwald's rule, and emergence of order in Fused in Sarcoma"
"Molecular docking plays a crucial role in predicting the binding mode of ligands to target proteins, and covalent interactions, which involve the formation of a covalent bond between the ligand and the target, are particularly valuable due to their strong, enduring binding nature. However, most existing docking methods and deep learning approaches hardly account for the formation of covalent bonds and the associated structural changes. To address this gap, we introduce a comprehensive benchmark for covalent docking, CovDocker, which is designed to better capture the complexities of covalent binding. We decompose the covalent docking process into three main tasks: reactive location prediction, covalent reaction prediction, and covalent docking. By adapting state-of-the-art models, such as Uni-Mol and Chemformer, we establish baseline performances and demonstrate the effectiveness of the benchmark in accurately predicting interaction sites and modeling the molecular transformations involved in covalent binding. These results confirm the role of the benchmark as a rigorous framework for advancing research in covalent drug design. It underscores the potential of data-driven approaches to accelerate the discovery of selective covalent inhibitors and addresses critical challenges in therapeutic development.",0,arxiv,Biyoloji,CC-BY/arXiv,"CovDocker: Benchmarking Covalent Drug Design with Tasks, Datasets, and Solutions"
"We present a modular framework powered by large language models (LLMs) that automates and streamlines key tasks across the early-stage computational drug discovery pipeline. By combining LLM reasoning with domain-specific tools, the framework performs biomedical data retrieval, domain-specific question answering, molecular generation, property prediction, property-aware molecular refinement, and 3D protein-ligand structure generation. In a case study targeting BCL-2 in lymphocytic leukemia, the agent autonomously retrieved relevant biomolecular information, including FASTA sequences, SMILES representations, and literature, and answered mechanistic questions with improved contextual accuracy compared to standard LLMs. It then generated chemically diverse seed molecules and predicted 67 ADMET-related properties, which guided iterative molecular refinement. Across two refinement rounds, the number of molecules with QED > 0.6 increased from 34 to 55. The number of molecules satisfying empirical drug-likeness filters also rose; for example, compliance with the Ghose filter increased from 32 to 55 within a pool of 100 molecules. The framework also employed Boltz-2 to generate 3D protein-ligand complexes and provide rapid binding affinity estimates for candidate compounds. These results demonstrate that the approach effectively supports molecular screening, prioritization, and structure evaluation. Its modular design enables flexible integration of evolving tools and models, providing a scalable foundation for AI-assisted therapeutic discovery.",0,arxiv,Biyoloji,CC-BY/arXiv,Large Language Model Agent for Modular Task Execution in Drug Discovery
"The objective of this paper is to investigate the structural stability, dynamic properties, and potential interactions among Amyloid Precursor Protein (APP), Tau, and Alpha-synuclein through a series of molecular dynamics simulations that integrate publicly available structural data, detailed force-field parameters, and comprehensive analytical protocols. By focusing on these three proteins, which are each implicated in various neurodegenerative disorders, the study aims to elucidate how their conformational changes and interprotein contact sites may influence larger biological processes. Through rigorous evaluation of their folding behaviors, energetic interactions, and residue-specific functions, this work contributes to the broader understanding of protein aggregation mechanisms and offers insights that may ultimately guide therapeutic intervention strategies.",0,arxiv,Biyoloji,CC-BY/arXiv,Bridging Classical Molecular Dynamics and Quantum Foundations for Comprehensive Protein Structural Analysis
"Generative AI presents chemists with novel ideas for drug design and facilitates the exploration of vast chemical spaces. Diffusion models (DMs), an emerging tool, have recently attracted great attention in drug R\&D. This paper comprehensively reviews the latest advancements and applications of DMs in molecular generation. It begins by introducing the theoretical principles of DMs. Subsequently, it categorizes various DM-based molecular generation methods according to their mathematical and chemical applications. The review further examines the performance of these models on benchmark datasets, with a particular focus on comparing the generation performance of existing 3D methods. Finally, it concludes by emphasizing current challenges and suggesting future research directions to fully exploit the potential of DMs in drug discovery.",0,arxiv,Biyoloji,CC-BY/arXiv,Unraveling the Potential of Diffusion Models in Small Molecule Generation
"Protein structure prediction models such as AlphaFold3 (AF3) push the frontier of biomolecular modeling by incorporating science-informed architectural changes to the transformer architecture. However, these advances come at a steep system cost, introducing: compute- and memory-intensive operators, 2D attention mechanisms, and retrieval-augmented data pipelines, which collectively hinder the scalability of AF3 training. In this work, we present MegaFold, a cross-platform system to accelerate AF3 training. MegaFold tackles key bottlenecks through ahead-of-time caching to eliminate GPU idle time from the retrieval-augmented data pipeline, Triton-based kernels for memory-efficient EvoAttention on heterogeneous devices, and deep fusion for common and critical small operators in AF3. Evaluation on both NVIDIA H200 and AMD MI250 GPUs shows that MegaFold reduces peak memory usage of AF3 training by up to 1.23$\times$ and improves per-iteration training time by up-to 1.73$\times$ and 1.62$\times$ respectively. More importantly, MegaFold enables training on 1.35$\times$ longer sequence lengths compared to PyTorch baselines without running out-of-memory, significantly improving the scalability of modern protein folding models. We open source our code at https://github.com/Supercomputing-System-AI-Lab/MegaFold/.",0,arxiv,Biyoloji,CC-BY/arXiv,MegaFold: System-Level Optimizations for Accelerating Protein Structure Prediction Models
"We have previously shown in model studies that rapid quenches of systems of monomers interacting to form polymer chains can fix nonequilibrium chemistries which could lead to the origin of life. We suggested that such quenching processes might have occurred at very high rates on early earth, giving an efficient mechanism for natural sorting through enormous numbers of nonequilibrium chemistries from which those most likely to lead to life could be naturally selected. Taking account of kinetic barriers, we found good agreement between laboratory quenching experiments on solutions of amino acids and the resulting model. We also made a preliminary comparison between reported data on polypeptides sampled from emissions from smokers near ocean ridges and our model. However that previous model assumed that the concentrations of all monomeric amino acids in the medium were the same whereas that is not the case in samples taken from ocean smokers. Here we take account of the heterogeneous concentrations of the amino acid monomers in the medium in the analysis of the smoker data and compare the results with the data on polypeptide concentrations found in the samples taken by a submersible in the Marianna Trough. Results are consistent with the hypothesis that smokers were the source of large and extremely diverse number of polypeptides in thermal disequilibrium which could incubate processes leading to life.",0,arxiv,Biyoloji,CC-BY/arXiv,Modelling Statistics of Polypeptides in Emissions from Smokers Near Ocean Ridges
"Sampling physically valid ligand-binding poses remains a major challenge in molecular docking, particularly for unseen or structurally diverse targets. We introduce PocketVina, a fast and memory-efficient, search-based docking framework that combines pocket prediction with systematic multi-pocket exploration. We evaluate PocketVina across four established benchmarks--PDBbind2020 (timesplit and unseen), DockGen, Astex, and PoseBusters--and observe consistently strong performance in sampling physically valid docking poses. PocketVina achieves state-of-the-art performance when jointly considering ligand RMSD and physical validity (PB-valid), while remaining competitive with deep learning-based approaches in terms of RMSD alone, particularly on structurally diverse and previously unseen targets. PocketVina also maintains state-of-the-art physically valid docking accuracy across ligands with varying degrees of flexibility. We further introduce TargetDock-AI, a benchmarking dataset we curated, consisting of over 500000 protein-ligand pairs, and a partition of the dataset labeled with PubChem activity annotations. On this large-scale dataset, PocketVina successfully discriminates active from inactive targets, outperforming a deep learning baseline while requiring significantly less GPU memory and runtime. PocketVina offers a robust and scalable docking strategy that requires no task-specific training and runs efficiently on standard GPUs, making it well-suited for high-throughput virtual screening and structure-based drug discovery.",0,arxiv,Biyoloji,CC-BY/arXiv,PocketVina Enables Scalable and Highly Accurate Physically Valid Docking through Multi-Pocket Conditioning
"Understanding protein function at the molecular level requires connecting residue-level annotations with physical and structural properties. This can be cumbersome and error-prone when functional annotation, computation of physico-chemical properties, and structure visualization are separated. To address this, we introduce ProCaliper, an open-source Python library for computing and visualizing physico-chemical properties of proteins. It can retrieve annotation and structure data from UniProt and AlphaFold databases, compute residue-level properties such as charge, solvent accessibility, and protonation state, and interactively visualize the results of these computations along with user-supplied residue-level data. Additionally, ProCaliper incorporates functional and structural information to construct and optionally sparsify networks that encode the distance between residues and/or annotated functional sites or regions. The package ProCaliper and its source code, along with the code used to generate the figures in this manuscript, are freely available at https://github.com/PNNL-Predictive-Phenomics/ProCaliper.",0,arxiv,Biyoloji,CC-BY/arXiv,"ProCaliper: functional and structural analysis, visualization, and annotation of proteins"
"Designing reaction pathways that maximize the production of a target compound in a given metabolic network is a fundamental problem in systems biology. In this study, we systematically explore the non-oxidative glycolysis metabolic network, guided by the principle that reactions with negative Gibbs free energy differences are thermodynamically favored. We enumerate alternative pathways that implement the net non-oxidative glycolysis reaction, categorized by their length. Our analysis reveals several alternative thermodynamically favorable pathways beyond those reported in experiments. In addition, we identify molecules within the network, such as 3-hydroxypropionic acid, that may have significant potential for further investigation.",0,arxiv,Biyoloji,CC-BY/arXiv,Thermodynamic free energy map for the non-oxidative glycolysis pathways
"Sampling low-energy molecular conformations, spatial arrangements of atoms in a molecule, is a critical task for many different calculations performed in the drug discovery and optimization process. Numerous specialized equivariant networks have been designed to generate molecular conformations from 2D molecular graphs. Recently, non-equivariant transformer models have emerged as a viable alternative due to their capability to scale to improve generalization. However, the concern has been that non-equivariant models require a large model size to compensate the lack of equivariant bias. In this paper, we demonstrate that a well-chosen positional encoding effectively addresses these size limitations. A standard transformer model incorporating relative positional encoding for molecular graphs when scaled to 25 million parameters surpasses the current state-of-the-art non-equivariant base model with 64 million parameters on the GEOM-DRUGS benchmark. We implemented relative positional encoding as a negative attention bias that linearly increases with the shortest path distances between graph nodes at varying slopes for different attention heads, similar to ALiBi, a widely adopted relative positional encoding technique in the NLP domain. This architecture has the potential to serve as a foundation for a novel class of generative models for molecular conformations.",0,arxiv,Biyoloji,CC-BY/arXiv,A standard transformer and attention with linear biases for molecular conformer generation
"We develop ProxelGen, a protein structure generative model that operates on 3D densities as opposed to the prevailing 3D point cloud representations. Representing proteins as voxelized densities, or proxels, enables new tasks and conditioning capabilities. We generate proteins encoded as proxels via a 3D CNN-based VAE in conjunction with a diffusion model operating on its latent space. Compared to state-of-the-art models, ProxelGen's samples achieve higher novelty, better FID scores, and the same level of designability as the training set. ProxelGen's advantages are demonstrated in a standard motif scaffolding benchmark, and we show how 3D density-based generation allows for more flexible shape conditioning.",0,arxiv,Biyoloji,CC-BY/arXiv,ProxelGen: Generating Proteins as 3D Densities
"Protein language models (pLMs) excel in a variety of tasks that range from structure prediction to the design of functional enzymes. However, these models operate as black boxes, and their underlying working principles remain unclear. Here, we survey emerging applications of explainable artificial intelligence (XAI) to pLMs and describe the potential of XAI in protein research. We divide the workflow of protein AI modeling into four information contexts: (i) training sequences, (ii) input prompt, (iii) model architecture, and (iv) input-output pairs. For each, we describe existing methods and applications of XAI. Additionally, from published studies we distil five (potential) roles that XAI can play in protein research: Evaluator, Multitasker, Engineer, Coach, and Teacher, with the Evaluator role being the only one widely adopted so far. These roles aim to help both protein scientists and model developers understand the possibilities and limitations of implementing XAI for predictive and generative tasks. While our analysis focuses on pLMs, both this categorization and roles are broadly applicable to any other model architectures. We conclude by highlighting critical areas of application for the future, including risks related to security, trustworthiness, and bias, and we call for community benchmarks, open-source tooling, domain-specific visualizations, and wet-lab characterization to advance the interpretability of protein AI.",0,arxiv,Biyoloji,CC-BY/arXiv,Toward the Explainability of Protein Language Models
"Over the last decade, proteomic analysis of single cells by mass spectrometry transitioned from an uncertain possibility to a set of robust and rapidly advancing technologies supporting the accurate quantification of thousands of proteins. We review the major drivers of this progress, from establishing feasibility to powerful and increasingly scalable methods. We focus on the tradeoffs and synergies of different technological solutions within a coherent conceptual framework, which projects considerable room both for throughput scaling and for extending the analysis scope to functional protein measurements. We highlight the potential of these technologies to support the development of mechanistic biophysical models and help uncover new principles.",0,arxiv,Biyoloji,CC-BY/arXiv,Single-Cell Proteomic Technologies: Tools in the quest for principles
"Understanding and modeling enzyme-substrate interactions is crucial for catalytic mechanism research, enzyme engineering, and metabolic engineering. Although a large number of predictive methods have emerged, they do not incorporate prior knowledge of enzyme catalysis to rationally modulate general protein-molecule features that are misaligned with catalytic patterns. To address this issue, we introduce a two-stage progressive framework, OmniESI, for enzyme-substrate interaction prediction through conditional deep learning. By decomposing the modeling of enzyme-substrate interactions into a two-stage progressive process, OmniESI incorporates two conditional networks that respectively emphasize enzymatic reaction specificity and crucial catalysis-related interactions, facilitating a gradual feature modulation in the latent space from general protein-molecule domain to catalysis-aware domain. On top of this unified architecture, OmniESI can adapt to a variety of downstream tasks, including enzyme kinetic parameter prediction, enzyme-substrate pairing prediction, enzyme mutational effect prediction, and enzymatic active site annotation. Under the multi-perspective performance evaluation of in-distribution and out-of-distribution settings, OmniESI consistently delivered superior performance than state-of-the-art specialized methods across seven benchmarks. More importantly, the proposed conditional networks were shown to internalize the fundamental patterns of catalytic efficiency while significantly improving prediction performance, with only negligible parameter increases (0.16%), as demonstrated by ablation studies on key components. Overall, OmniESI represents a unified predictive approach for enzyme-substrate interactions, providing an effective tool for catalytic mechanism cracking and enzyme engineering with strong generalization and broad applicability.",0,arxiv,Biyoloji,CC-BY/arXiv,OmniESI: A unified framework for enzyme-substrate interaction prediction with progressive conditional deep learning
"Accurate prediction of antibody-antigen (Ab-Ag) binding affinity is essential for therapeutic design and vaccine development, yet the performance of current models is limited by noisy experimental labels, heterogeneous assay conditions, and poor generalization across the vast antibody and antigen sequence space. We introduce AbRank, a large-scale benchmark and evaluation framework that reframes affinity prediction as a pairwise ranking problem. AbRank aggregates over 380,000 binding assays from nine heterogeneous sources, spanning diverse antibodies, antigens, and experimental conditions, and introduces standardized data splits that systematically increase distribution shift, from local perturbations such as point mutations to broad generalization across novel antigens and antibodies. To ensure robust supervision, AbRank defines an m-confident ranking framework by filtering out comparisons with marginal affinity differences, focusing training on pairs with at least an m-fold difference in measured binding strength. As a baseline for the benchmark, we introduce WALLE-Affinity, a graph-based approach that integrates protein language model embeddings with structural information to predict pairwise binding preferences. Our benchmarks reveal significant limitations in current methods under realistic generalization settings and demonstrate that ranking-based training improves robustness and transferability. In summary, AbRank offers a robust foundation for machine learning models to generalize across the antibody-antigen space, with direct relevance for scalable, structure-aware antibody therapeutic design.",0,arxiv,Biyoloji,CC-BY/arXiv,AbRank: A Benchmark Dataset and Metric-Learning Framework for Antibody-Antigen Affinity Ranking
"Generating diverse, all-atom conformational ensembles of dynamic proteins such as G-protein-coupled receptors (GPCRs) is critical for understanding their function, yet most generative models simplify atomic detail or ignore conformational diversity altogether. We present latent diffusion for full protein generation (LD-FPG), a framework that constructs complete all-atom protein structures, including every side-chain heavy atom, directly from molecular dynamics (MD) trajectories. LD-FPG employs a Chebyshev graph neural network (ChebNet) to obtain low-dimensional latent embeddings of protein conformations, which are processed using three pooling strategies: blind, sequential and residue-based. A diffusion model trained on these latent representations generates new samples that a decoder, optionally regularized by dihedral-angle losses, maps back to Cartesian coordinates. Using D2R-MD, a 2-microsecond MD trajectory (12 000 frames) of the human dopamine D2 receptor in a membrane environment, the sequential and residue-based pooling strategy reproduces the reference ensemble with high structural fidelity (all-atom lDDT of approximately 0.7; C-alpha-lDDT of approximately 0.8) and recovers backbone and side-chain dihedral-angle distributions with a Jensen-Shannon divergence of less than 0.03 compared to the MD data. LD-FPG thereby offers a practical route to system-specific, all-atom ensemble generation for large proteins, providing a promising tool for structure-based therapeutic design on complex, dynamic targets. The D2R-MD dataset and our implementation are freely available to facilitate further research.",0,arxiv,Biyoloji,CC-BY/arXiv,Generative Modeling of Full-Atom Protein Conformations using Latent Diffusion on Graph Embeddings
"Aptamers are single-stranded DNA/RNAs or short peptides with unique tertiary structures that selectively bind to specific targets. They have great potential in the detection and medical fields. Here, we present SelfTrans-Ensemble, a deep learning model that integrates sequence information models and structural information models to extract multi-scale features for predicting aptamer-protein interactions (APIs). The model employs two pre-trained models, ProtBert and RNA-FM, to encode protein and aptamer sequences, along with features generated from primary sequence and secondary structural information. To address the data imbalance in the aptamer dataset imbalance, we incorporated short RNA-protein interaction data in the training set. This resulted in a training accuracy of 98.9% and a test accuracy of 88.0%, demonstrating the model's effectiveness in accurately predicting APIs. Additionally, analysis using molecular simulation indicated that SelfTrans-Ensemble is sensitive to aptamer sequence mutations. We anticipate that SelfTrans-Ensemble can offer a more efficient and rapid process for aptamer screening.",0,arxiv,Biyoloji,CC-BY/arXiv,Aptamer-protein interaction prediction model based on transformer
"Recent advances in protein structure prediction have achieved near-atomic accuracy for well-folded proteins. However, current benchmarks inadequately assess model performance in biologically challenging contexts, especially those involving intrinsically disordered regions (IDRs), limiting their utility in applications such as drug discovery, disease variant interpretation, and protein interface design. We introduce DisProtBench, a comprehensive benchmark for evaluating protein structure prediction models (PSPMs) under structural disorder and complex biological conditions. DisProtBench spans three key axes: (1) Data complexity, covering disordered regions, G protein-coupled receptor (GPCR) ligand pairs, and multimeric complexes; (2) Task diversity, benchmarking twelve leading PSPMs across structure-based tasks with unified classification, regression, and interface metrics; and (3) Interpretability, via the DisProtBench Portal, which provides precomputed 3D structures and visual error analyses. Our results reveal significant variability in model robustness under disorder, with low-confidence regions linked to functional prediction failures. Notably, global accuracy metrics often fail to predict task performance in disordered settings, emphasizing the need for function-aware evaluation. DisProtBench establishes a reproducible, extensible, and biologically grounded framework for assessing next-generation PSPMs in realistic biomedical scenarios.",0,arxiv,Biyoloji,CC-BY/arXiv,"DISPROTBENCH: A Disorder-Aware, Task-Rich Benchmark for Evaluating Protein Structure Prediction in Realistic Biological Contexts"
"A range of computational biology software (GROMACS, AMBER, NAMD, LAMMPS, OpenMM, Psi4 and RELION) was benchmarked on a representative selection of HPC hardware, including AMD EPYC 7742 CPU nodes, NVIDIA V100 and AMD MI250X GPU nodes, and an NVIDIA GH200 testbed. The raw performance, power efficiency and data storage requirements of the software was evaluated for each HPC facility, along with qualitative factors such as the user experience and software environment. It was found that the diversity of methods used within computational biology means that there is no single HPC hardware that can optimally run every type of HPC job, and that diverse hardware is the only way to properly support all methods. New hardware, such as AMD GPUs and Nvidia AI chips, are mostly compatible with existing methods, but are also more labour-intensive to support. GPUs offer the most efficient way to run most computational biology tasks, though some tasks still require CPUs. A fast HPC node running molecular dynamics can produce around 10GB of data per day, however, most facilities and research institutions lack short-term and long-term means to store this data. Finally, as the HPC landscape has become more complex, deploying software and keeping HPC systems online has become more difficult. This situation could be improved through hiring/training in DevOps practices, expanding the consortium model to provide greater support to HPC system administrators, and implementing build frameworks/containerisation/virtualisation tools to allow users to configure their own software environment, rather than relying on centralised software installations.",0,arxiv,Biyoloji,CC-BY/arXiv,Engineering Supercomputing Platforms for Biomolecular Applications
"Simultaneously optimizing molecules against multiple therapeutic targets remains a profound challenge in drug discovery, particularly due to sparse rewards and conflicting design constraints. We propose a structured active learning (AL) paradigm integrating a sequence-to-sequence (Seq2Seq) variational autoencoder (VAE) into iterative loops designed to balance chemical diversity, molecular quality, and multi-target affinity. Our method alternates between expanding chemically feasible regions of latent space and progressively constraining molecules based on increasingly stringent multi-target docking thresholds. In a proof-of-concept study targeting three related coronavirus main proteases (SARS-CoV-2, SARS-CoV, MERS-CoV), our approach efficiently generated a structurally diverse set of pan-inhibitor candidates. We demonstrate that careful timing and strategic placement of chemical filters within this active learning pipeline markedly enhance exploration of beneficial chemical space, transforming the sparse-reward, multi-objective drug design problem into an accessible computational task. Our framework thus provides a generalizable roadmap for efficiently navigating complex polypharmacological landscapes.",0,arxiv,Biyoloji,CC-BY/arXiv,Active Learning-Guided Seq2Seq Variational Autoencoder for Multi-target Inhibitor Generation
"Breakthroughs in high-accuracy protein structure prediction, such as AlphaFold, have established receptor-based molecule design as a critical driver for rapid early-phase drug discovery. However, most approaches still struggle to balance pocket-specific geometric fit with strict valence and synthetic constraints. To resolve this trade-off, a Retrieval-Enhanced Aligned Diffusion termed READ is introduced, which is the first to merge molecular Retrieval-Augmented Generation with an SE(3)-equivariant diffusion model. Specifically, a contrastively pre-trained encoder aligns atom-level representations during training, then retrieves graph embeddings of pocket-matched scaffolds to guide each reverse-diffusion step at inference. This single mechanism can inject real-world chemical priors exactly where needed, producing valid, diverse, and shape-complementary ligands. Experimental results demonstrate that READ can achieve very competitive performance in CBGBench, surpassing state-of-the-art generative models and even native ligands. That suggests retrieval and diffusion can be co-optimized for faster, more reliable structure-based drug design.",0,arxiv,Biyoloji,CC-BY/arXiv,Reimagining Target-Aware Molecular Generation through Retrieval-Enhanced Aligned Diffusion
"Peptide sequencing-the process of identifying amino acid sequences from mass spectrometry data-is a fundamental task in proteomics. Non-Autoregressive Transformers (NATs) have proven highly effective for this task, outperforming traditional methods. Unlike autoregressive models, which generate tokens sequentially, NATs predict all positions simultaneously, leveraging bidirectional context through unmasked self-attention. However, existing NAT approaches often rely on Connectionist Temporal Classification (CTC) loss, which presents significant optimization challenges due to CTC's complexity and increases the risk of training failures. To address these issues, we propose an improved non-autoregressive peptide sequencing model that incorporates a structured protein sequence curriculum learning strategy. This approach adjusts protein's learning difficulty based on the model's estimated protein generational capabilities through a sampling process, progressively learning peptide generation from simple to complex sequences. Additionally, we introduce a self-refining inference-time module that iteratively enhances predictions using learned NAT token embeddings, improving sequence accuracy at a fine-grained level. Our curriculum learning strategy reduces NAT training failures frequency by more than 90% based on sampled training over various data distributions. Evaluations on nine benchmark species demonstrate that our approach outperforms all previous methods across multiple metrics and species.",0,arxiv,Biyoloji,CC-BY/arXiv,Curriculum Learning for Biological Sequence Prediction: The Case of De Novo Peptide Sequencing
"In 2009, our group pioneered a novel method CBTOPE for predicting conformational B-cell epitopes in a protein from its amino acid sequence, which received extensive citations from the scientific community. In a recent study, Cia et al. (2023) evaluated the performance of conformational B-cell epitope prediction methods on a well-curated dataset, revealing that most approaches, including CBTOPE, exhibited poor performance. One plausible cause of this diminished performance is that available methods were trained on datasets that are both limited in size and outdated in content. In this study, we present an enhanced version of CBTOPE, trained, tested, and evaluated using the well-curated dataset from Cai et al. (2023). Initially, we developed machine learning-based models using binary profiles, achieving a maximum AUC of 0.58 on the validation dataset. The performance of our method improved significantly from an AUC of 0.58 to 0.63 when incorporating evolutionary information in the form of a Position-Specific Scoring Matrix (PSSM) profile. Furthermore, the performance increased from an AUC of 0.63 to 0.64 when we integrated both the PSSM profile and relative solvent accessibility (RSA). All models were trained, tested, and optimized on the training dataset using five-fold cross-validation. The final performance of our models was assessed using a validation or independent dataset that was not used during hyperparameter optimization. To facilitate scientific community working in the field of subunit vaccine, we develop a standalone software and web server CBTOPE2 (https://webs.iiitd.edu.in/raghava/cbtope2/).",0,arxiv,Biyoloji,CC-BY/arXiv,CBTOPE2: An improved method for predicting of conformational B-cell epitopes in an antigen from its primary sequence
"Generating high-fidelity and biologically plausible synthetic single-cell RNA sequencing (scRNA-seq) data, especially with conditional control, is challenging due to its high dimensionality, sparsity, and complex biological variations. Existing generative models often struggle to capture these unique characteristics and ensure robustness to structural noise in cellular networks. We introduce LapDDPM, a novel conditional Graph Diffusion Probabilistic Model for robust and high-fidelity scRNA-seq generation. LapDDPM uniquely integrates graph-based representations with a score-based diffusion model, enhanced by a novel spectral adversarial perturbation mechanism on graph edge weights. Our contributions are threefold: we leverage Laplacian Positional Encodings (LPEs) to enrich the latent space with crucial cellular relationship information; we develop a conditional score-based diffusion model for effective learning and generation from complex scRNA-seq distributions; and we employ a unique spectral adversarial training scheme on graph edge weights, boosting robustness against structural variations. Extensive experiments on diverse scRNA-seq datasets demonstrate LapDDPM's superior performance, achieving high fidelity and generating biologically-plausible, cell-type-specific samples. LapDDPM sets a new benchmark for conditional scRNA-seq data generation, offering a robust tool for various downstream biological applications.",0,arxiv,Biyoloji,CC-BY/arXiv,LapDDPM: A Conditional Graph Diffusion Model for scRNA-seq Generation with Spectral Adversarial Perturbations
"The pretraining-finetuning paradigm has powered major advances in domains such as natural language processing and computer vision, with representative examples including masked language modeling and next-token prediction. In molecular representation learning, however, pretraining tasks remain largely restricted to node-level denoising, which effectively captures local atomic environments but is often insufficient for encoding the global molecular structure critical to graph-level property prediction tasks such as energy estimation and molecular regression. To address this gap, we introduce GeoRecon, a graph-level pretraining framework that shifts the focus from individual atoms to the molecule as an integrated whole. GeoRecon formulates a graph-level reconstruction task: during pretraining, the model is trained to produce an informative graph representation that guides geometry reconstruction while inducing smoother and more transferable latent spaces. This encourages the learning of coherent, global structural features beyond isolated atomic details. Without relying on external supervision, GeoRecon generally improves over backbone baselines on multiple molecular benchmarks including QM9, MD17, MD22, and 3BPA, demonstrating the effectiveness of graph-level reconstruction for holistic and geometry-aware molecular embeddings.",0,arxiv,Biyoloji,CC-BY/arXiv,GeoRecon: Graph-Level Representation Learning for 3D Molecules via Reconstruction-Based Pretraining
"This study accessed the antibacterial potential in vitro of hexane, chloroform and methanol extracts made from leaves, stem bark, flowers, seeds or roots of Sri Lankan grown Acronychia pedunculata plant against two Gram positive bacteria, Staphylococus aureus (ATCC 25923) and Bacilus cereus (ATCC 11778), and two Gram negative bacteria, Pseudomonas aeruginosa (ATCC 9027) and Escherichia coli (ATCC 35218), using agar disc diffusion bioassay technique. The results showed that none the of the extracts provoked an antibacterial action against the two Gram negative bacteria P. aeruginosa and E. coli. Conversely, compared to reference drug, Gentamicin, varying magnitudes of antibacterial activity (concentration: 300 mg/disc) ranging from zero to mild to moderate to strong antibacterial activity was evident with the three solvent systems made from different parts of the plant against the two Gram positive bacteria S. aureus and B. cereus. All the three flower extracts excerted marked antibacterial activity against both S. aureus and B. cereus. The highest antibacterial activity was exhibited by methanol flowers extract (inhibition zone: 13.8-0.32mm), with a Minimum inhibitory value of 32mg/ml, against B. cereus. The overall order of potency against S. aureus was, chloroform flowers> chloroform seeds > hexane leaves > chloroform leaves > methanol flowers> hexane flowers> methanol seeds. And against B. cereus was methanol flowers> hexane leaves > hexane flowers> chloroform leaves >chloroform flowers >chloroform seeds > hexane roots > chloroform roots > methanol seeds chloroform stem barks = hexane stem barks. These are all novel findings for A. pedunculata found in Sri Lanka and elsewhere.",0,arxiv,Biyoloji,CC-BY/arXiv,"In Vitro Antibacterial activity of hexane, Chloroform and methanolic extracts of different parts of Acronychia pedunculata grown in Sri Lanka"
"Peptide-drug conjugates (PDCs) represent a promising therapeutic avenue for human diseases, particularly in cancer treatment. Systematic elucidation of structure-activity relationships (SARs) and accurate prediction of the activity of PDCs are critical for the rational design and optimization of these conjugates. To this end, we carefully design and construct a benchmark PDCs dataset compiled from literature-derived collections and PDCdb database, and then develop PDCNet, the first unified deep learning framework for forecasting the activity of PDCs. The architecture systematically captures the complex factors underlying anticancer decisions of PDCs in real-word scenarios through a multi-level feature fusion framework that collaboratively characterizes and learns the features of peptides, linkers, and payloads. Leveraging a curated PDCs benchmark dataset, comprehensive evaluation results show that PDCNet demonstrates superior predictive capability, with the highest AUC, F1, MCC and BA scores of 0.9213, 0.7656, 0.7071 and 0.8388 for the test set, outperforming eight established traditional machine learning models. Multi-level validations, including 5-fold cross-validation, threshold testing, ablation studies, model interpretability analysis and external independent testing, further confirm the superiority, robustness, and usability of the PDCNet architecture. We anticipate that PDCNet represents a novel paradigm, incorporating both a benchmark dataset and advanced models, which can accelerate the design and discovery of new PDC-based therapeutic agents.",0,arxiv,Biyoloji,CC-BY/arXiv,PDCNet: a benchmark and general deep learning framework for activity prediction of peptide-drug conjugates
"Molecular chaperones are machines that consume copious amount of ATP to drive misfolded proteins or RNA to fold into functionally competent native states. Because the folding landscapes of biomolecules with complex native state topology are rugged consisting of multiple minima that are separated by large free energy barriers, folding occurs by the kinetic partitioning mechanism according to which only a small fraction of the molecules reach the folded state in biologically viable times. The rescue of such proteins and RNA require chaperones. Although the protein and RNA chaperones are profoundly different in their structure and action, the principles underlying their activity to produce the folded structures can be understood using a unified theoretical framework based on iterative annealing mechanism (IAM). Our theory shows that both these machines have evolved to the maximize the production of the steady state yield on biological times. Strikingly, theory predicts that only at a moderate level of RNA chaperone activity is the yield of the self-splicing pre-RNA is maximized in \textit{in vivo}.",0,arxiv,Biyoloji,CC-BY/arXiv,Iterative Annealing Mechanism for Protein and RNA Chaperones
"We discuss how to construct reliably well ""a lattice and an integer time"" version of a super-diffusive continuous-space and -time fractional Brownian motion (fBm) -- an experimentally-relevant non-Markovian Gaussian stochastic process with an everlasting power-law memory on the time-evolution of thermal noises extending over the entire past. We propose two algorithms, which are both validated by extensive numerical simulations showing that the ensuing lattice random walks have not only the same power-law covariance function as the standard fBm, but also individual trajectories follow those of the super-diffusive fBm. Finding a lattice and an integer time analogue of a sub-diffusion fBm, which is an anti-persistent process, remains a challenging open problem. Our results also clarify the relevant difference between sub-diffusive and super-diffusive fBm, that are frequently seen as two very analogous realizations of processes with memory. They are indeed substantially different.",0,arxiv,Biyoloji,CC-BY/arXiv,Discrete-space and -time analogue of a super-diffusive fractional Brownian motion
"Surface effects could play a dominant role in modifying the natural liquid order. In some cases, the effects of the surface interactions can propagate inwards, and even can interfere with a similar propagation from opposite surfaces. This can be particularly evident in liquid water under nano-confinement. The large dipolar cross-correlations among distinct molecules that give rise to the unusually large dielectric constant of water (and in turn owe their origin to the extended hydrogen bond (HB) network) can get perturbed by surfaces. The perturbation can propagate inwards and then interfere with the one from the opposite surface if confinement is only a few layers wide. This can give rise to short-to-intermediate range solvent-mediated interaction between two surfaces. Here we study the effects of such interactions on the dielectric constant of nano-confined liquids, not just water but also ordering at protein surfaces. The surfaces work at two levels: (i) induce orientational realignment, and (ii) alter the cross-correlations between water molecules. Molecular dynamics simulations and statistical analyses are used to address these aspects in confinement of slit pores, nano tube/cylinder, and nano sphere. In addition, we consider the hydration layers of multiple proteins with vastly different structural features. These studies give us a measure of the extent or the length scale of cross-correlations between dipole moments of water molecules. We find an interesting orientational arrangement in the protein hydration layers, giving rise to long-range molecular cross-correlations. To decouple the effect of HB from the effect of geometry, we additionally study acetonitrile under nanoconfinement. Importantly, while a protein's interior is characterized by a small dielectric constant, the dipole moment of a peptide bond is large, and thus susceptible to fluctuations in water.",0,arxiv,Biyoloji,CC-BY/arXiv,Surface Induced Frustration of Inherent Dipolar Order in Nanoconfined Water
"Intervertebral discs are avascular and maintain immune privilege. However, during intervertebral disc degeneration (IDD), this barrier is disrupted, leading to extensive immune cell infiltration and localized inflammation. In degenerated discs, macrophages, T lymphocytes, neutrophils, and granulocytic myeloid-derived suppressor cells (G-MDSCs) are key players, exhibiting functional heterogeneity. Dysregulated activation of inflammatory pathways, including nuclear factor kappa-B (NF-kappaB), interleukin-17 (IL-17), and nucleotide-binding oligomerization domain-like receptor protein 3 (NLRP3) inflammasome activation, drives local pro-inflammatory responses, leading to cell apoptosis and extracellular matrix (ECM) degradation. Innovative immunotherapies, including exosome-based treatments, CRISPR/Cas9-mediated gene editing, and chemokine-loaded hydrogel systems, have shown promise in reshaping the immunological niche of intervertebral discs. These strategies can modulate dysregulated immune responses and create a supportive environment for tissue regeneration. However, current studies have not fully elucidated the mechanisms of inflammatory memory and the immunometabolic axis, and they face challenges in balancing tissue regeneration with immune homeostasis. Future studies should employ interdisciplinary approaches such as single-cell and spatial transcriptomics to map a comprehensive immune atlas of IDD, elucidate intercellular crosstalk and signaling networks, and develop integrated therapies combining targeted immunomodulation with regenerative engineering, thereby facilitating the clinical translation of effective IDD treatments.",0,arxiv,Biyoloji,CC-BY/arXiv,Immunological mechanisms and immunoregulatory strategies in intervertebral disc degeneration
"Retrieving homologous protein sequences is essential for a broad range of protein modeling tasks such as fitness prediction, protein design, structure modeling, and protein-protein interactions. Traditional workflows have relied on a two-step process: first retrieving homologs via Multiple Sequence Alignments (MSA), then training models on one or more of these alignments. However, MSA-based retrieval is computationally expensive, struggles with highly divergent sequences or complex insertions & deletions patterns, and operates independently of the downstream modeling objective. We introduce Protriever, an end-to-end differentiable framework that learns to retrieve relevant homologs while simultaneously training for the target task. When applied to protein fitness prediction, Protriever achieves state-of-the-art performance compared to sequence-based models that rely on MSA-based homolog retrieval, while being two orders of magnitude faster through efficient vector search. Protriever is both architecture- and task-agnostic, and can flexibly adapt to different retrieval strategies and protein databases at inference time -- offering a scalable alternative to alignment-centric approaches.",0,arxiv,Biyoloji,CC-BY/arXiv,Protriever: End-to-End Differentiable Protein Homology Search for Fitness Prediction
"Template-based molecular generation offers a promising avenue for drug design by ensuring generated compounds are synthetically accessible through predefined reaction templates and building blocks. In this work, we tackle three core challenges in template-based GFlowNets: (1) minimizing synthesis cost, (2) scaling to large building block libraries, and (3) effectively utilizing small fragment sets. We propose Recursive Cost Guidance, a backward policy framework that employs auxiliary machine learning models to approximate synthesis cost and viability. This guidance steers generation toward low-cost synthesis pathways, significantly enhancing cost-efficiency, molecular diversity, and quality, especially when paired with an Exploitation Penalty that balances the trade-off between exploration and exploitation. To enhance performance in smaller building block libraries, we develop a Dynamic Library mechanism that reuses intermediate high-reward states to construct full synthesis trees. Our approach establishes state-of-the-art results in template-based molecular generation.",0,arxiv,Biyoloji,CC-BY/arXiv,Scalable and Cost-Efficient de Novo Template-Based Molecular Generation
"Potato late blight, caused by the oomycete pathogen Phytophthora infestans, is one of the most devastating diseases affecting potato crops in the history. Although conventional detection methods of plant diseases such as PCR and LAMP are highly sensitive and specific, they rely on bulky and expensive laboratory equipment and involve complex operations, making them impracticable for point-of care diagnosis in the field. Here in this study, we report a portable RPA-CRISPR based diagnosis system for plant disease, integrating smartphone for acquisition and analysis of fluorescent images. A polyvinyl alcohol (PVA) microneedle patch was employed for sample extraction on the plant leaves within one minute, the DNA extraction efficiency achieved 56 ug/mg, which is approximately 3 times to the traditional CTAB methods (18 ug/mg). The system of RPA-CRISPR-Cas12a isothermal assay was established to specifically target P. infestans with no cross-reactivity observed against closely-related species (P. sojae, P. capsici). The system demonstrated a detection limit of 2 pg/uL for P. infestans genomic DNA, offering sensitivity comparable to that of benchtop laboratory equipment. The system demonstrates the early-stage diagnosis capability by achieving a approximately 80% and 100% detection rate on the third and fourth day post-inoculation respectively, before visible symptoms observed on the leaves. The smartphone-based ""sample-to-result"" system decouples the limitations of traditional methods that rely heavily on specialized equipment, offering a promising way for early-stage plant disease detection and control in the field.",0,arxiv,Biyoloji,CC-BY/arXiv,Smartphone-integrated RPA-CRISPR-Cas12a Detection System with Microneedle Sampling for Point-of-Care Diagnosis of Potato Late Blight in Early Stage
"Geometric graph neural networks (GNNs) that respect E(3) symmetries have achieved strong performance on small molecule modeling, but they face scalability and expressiveness challenges when applied to large biomolecules such as RNA and proteins. These systems require models that can simultaneously capture fine-grained atomic interactions, long-range dependencies across spatially distant components, and biologically relevant hierarchical structure, such as atoms forming residues, which in turn form higher-order domains. Existing geometric GNNs, which typically operate exclusively in either Euclidean or Spherical Harmonics space, are limited in their ability to capture both the fine-scale atomic details and the long-range, symmetry-aware dependencies required for modeling the multi-scale structure of large biomolecules. We introduce DualEquiNet, a Dual-Space Hierarchical Equivariant Network that constructs complementary representations in both Euclidean and Spherical Harmonics spaces to capture local geometry and global symmetry-aware features. DualEquiNet employs bidirectional cross-space message passing and a novel Cross-Space Interaction Pooling mechanism to hierarchically aggregate atomic features into biologically meaningful units, such as residues, enabling efficient and expressive multi-scale modeling for large biomolecular systems. DualEquiNet achieves state-of-the-art performance on multiple existing benchmarks for RNA property prediction and protein modeling, and outperforms prior methods on two newly introduced 3D structural benchmarks demonstrating its broad effectiveness across a range of large biomolecule modeling tasks.",0,arxiv,Biyoloji,CC-BY/arXiv,DualEquiNet: A Dual-Space Hierarchical Equivariant Network for Large Biomolecules
"The AlphaFold Protein Structure Database (AFDB) offers unparalleled structural coverage at near-experimental accuracy, positioning it as a valuable resource for data-driven protein design. However, its direct use in training deep models that are sensitive to fine-grained atomic geometry, such as inverse folding, exposes a critical limitation. Comparative analysis of structural feature distributions reveals that AFDB structures exhibit distinct statistical regularities, reflecting a systematic geometric bias that deviates from the conformational diversity found in experimentally determined structures from the Protein Data Bank (PDB). While AFDB structures are cleaner and more idealized, PDB structures capture the intrinsic variability and physical realism essential for generalization in downstream tasks. To address this discrepancy, we introduce a Debiasing Structure AutoEncoder (DeSAE) that learns to reconstruct native-like conformations from intentionally corrupted backbone geometries. By training the model to recover plausible structural states, DeSAE implicitly captures a more robust and natural structural manifold. At inference, applying DeSAE to AFDB structures produces debiased structures that significantly improve inverse folding performance across multiple benchmarks. This work highlights the critical impact of subtle systematic biases in predicted structures and presents a principled framework for debiasing, significantly boosting the performance of structure-based learning tasks like inverse folding.",0,arxiv,Biyoloji,CC-BY/arXiv,AlphaFold Database Debiasing for Robust Inverse Folding
"Proteins are fundamental to biology, executing diverse functions through complex physicochemical interactions, and they hold transformative potential across medicine, materials science, and environmental applications. Protein Language Models (pLMs) aim to unlock insights from the vast space of unlabeled protein sequences by learning rich, semantic representations from primary sequences via masked language modeling. However, these models typically exhibit limited generative capacity. In this work, we introduce the Diffusion Sequence Model (DSM), a novel pLM trained with masked diffusion to enable both high-quality representation learning and generative protein design. DSM builds upon the ESM2 architecture by incorporating a masked forward diffusion process inspired by the LLaDA framework. After training, DSM is capable of generating diverse, biomimetic sequences that align with expected amino acid compositions, secondary structures, and predicted functions, even with 90\% token corruption. Furthermore, DSM's learned representations match or exceed those of similarly sized pLMs on downstream tasks. We also introduce DSM(ppi), a variant fine-tuned to generate protein binders by attending to target sequences. We demonstrate DSM(ppi)'s effectiveness on the challenging Bench-tested Binder Benchmark (BenchBB), where both DSM and DSM(ppi) produce candidates with superior predicted binding affinity compared to known binders. Our results establish masked diffusion as a powerful paradigm for unifying protein representation and generation in a single framework.",0,arxiv,Biyoloji,CC-BY/arXiv,Diffusion Sequence Models for Enhanced Protein Representation and Generation
"Deciphering protein function remains a fundamental challenge in protein representation learning. The task presents significant difficulties for protein language models (PLMs) due to the sheer volume of functional annotation categories and the highly imbalanced distribution of annotated instances across biological ontologies. Inspired by the remarkable success of reinforcement learning from human feedback (RLHF) in large language model (LLM) alignment, we propose AnnoDPO, a novel multi-modal framework for protein function prediction that leverages Direct Preference Optimization (DPO) to enhance annotation learning. Our methodology addresses the dual challenges of annotation scarcity and category imbalance through preference-aligned training objectives, establishing a new paradigm for biological knowledge integration in protein representation learning.",0,arxiv,Biyoloji,CC-BY/arXiv,AnnoDPO: Protein Functional Annotation Learning with Direct Preference Optimization
"Graph neural networks (GNNs), as topology/structure-aware models within deep learning, have emerged as powerful tools for AI-aided drug discovery (AIDD). By directly operating on molecular graphs, GNNs offer an intuitive and expressive framework for learning the complex topological and geometric features of drug-like molecules, cementing their role in modern molecular modeling. This review provides a comprehensive overview of the methodological foundations and representative applications of GNNs in drug discovery, spanning tasks such as molecular property prediction, virtual screening, molecular generation, biomedical knowledge graph construction, and synthesis planning. Particular attention is given to recent methodological advances, including geometric GNNs, interpretable models, uncertainty quantification, scalable graph architectures, and graph generative frameworks. We also discuss how these models integrate with modern deep learning approaches, such as self-supervised learning, multi-task learning, meta-learning and pre-training. Throughout this review, we highlight the practical challenges and methodological bottlenecks encountered when applying GNNs to real-world drug discovery pipelines, and conclude with a discussion on future directions.",0,arxiv,Biyoloji,CC-BY/arXiv,Graph Neural Networks in Modern AI-aided Drug Discovery
"Deep neural networks, particularly Transformers, have been widely adopted for predicting the functional properties of proteins. In this work, we focus on exploring whether Protein Transformers can capture biological intelligence among protein sequences. To achieve our goal, we first introduce a protein function dataset, namely Protein-FN, providing over 9000 protein data with meaningful labels. Second, we devise a new Transformer architecture, namely Sequence Protein Transformers (SPT), for computationally efficient protein function predictions. Third, we develop a novel Explainable Artificial Intelligence (XAI) technique called Sequence Score, which can efficiently interpret the decision-making processes of protein models, thereby overcoming the difficulty of deciphering biological intelligence bided in Protein Transformers. Remarkably, even our smallest SPT-Tiny model, which contains only 5.4M parameters, demonstrates impressive predictive accuracy, achieving 94.3% on the Antibiotic Resistance (AR) dataset and 99.6% on the Protein-FN dataset, all accomplished by training from scratch. Besides, our Sequence Score technique helps reveal that our SPT models can discover several meaningful patterns underlying the sequence structures of protein data, with these patterns aligning closely with the domain knowledge in the biology community. We have officially released our Protein-FN dataset on Hugging Face Datasets https://huggingface.co/datasets/Protein-FN/Protein-FN. Our code is available at https://github.com/fudong03/BioIntelligence.",0,arxiv,Biyoloji,CC-BY/arXiv,Do Protein Transformers Have Biological Intelligence?
"m5C modification is a type of RNA methylation modification, and its major methyltransferase, NSUN2, catalyzes m5C modification. NSUN2 is overexpressed in a variety of cancers, and it affects the metabolism of RNA from target genes by affecting the level of m5C modification in cancer cells, which in turn promotes the development of cancers and is associated with poor prognosis. This review summarizes the mechanisms by which NSUN2 and m5C play a pro-cancer role in various cancers, and the relationship between NSUN2 and the prognosis of various cancers, with the aim of identifying NSUN2 as a prognostic indicator and a target for future cancer therapy, and to provide a clearer therapeutic idea and direction for the future treatment of cancer.",0,arxiv,Biyoloji,CC-BY/arXiv,NSUN2 as a potential prognostic as well as therapeutic target in cancer by regulating m5C modification
"Pretrained molecular encoders have become indispensable in computational chemistry for tasks such as property prediction and molecular generation. However, the standard practice of relying solely on final-layer embeddings for downstream tasks may discard valuable information. In this work, we first analyze the information flow in five diverse molecular encoders and find that intermediate layers retain more general-purpose features, whereas the final-layer specializes and compresses information. We then perform an empirical layer-wise evaluation across 22 property prediction tasks. We find that using frozen embeddings from optimal intermediate layers improves downstream performance by an average of 5.4%, up to 28.6%, compared to the final-layer. Furthermore, finetuning encoders truncated at intermediate depths achieves even greater average improvements of 8.5%, with increases as high as 40.8%, obtaining new state-of-the-art results on several benchmarks. These findings highlight the importance of exploring the full representational depth of molecular encoders to achieve substantial performance improvements and computational efficiency. The code will be made publicly available.",0,arxiv,Biyoloji,CC-BY/arXiv,Superior Molecular Representations from Intermediate Encoder Layers
"Intrinsically disordered regions (IDRs) account for one-third of the human proteome and play essential biological roles. However, predicting the functions of IDRs remains a major challenge due to their lack of stable structures, rapid sequence evolution, and context-dependent behavior. Many predictors of protein function neglect or underperform on IDRs. Recent advances in computational biology and machine learning, including protein language models, alignment-free approaches, and IDR-specific methods, have revealed conserved bulk features and local motifs within IDRs that are linked to function. This review highlights emerging computational methods that map the sequence-function relationship in IDRs, outlines critical challenges in IDR function annotation, and proposes a community-driven framework to accelerate interpretable functional predictions for IDRs.",0,arxiv,Biyoloji,CC-BY/arXiv,Into the Unknown: From Structure to Disorder in Protein Function Prediction
"Virtual screening (VS) is a critical component of modern drug discovery, yet most existing methods--whether physics-based or deep learning-based--are developed around holo protein structures with known ligand-bound pockets. Consequently, their performance degrades significantly on apo or predicted structures such as those from AlphaFold2, which are more representative of real-world early-stage drug discovery, where pocket information is often missing. In this paper, we introduce an alignment-and-aggregation framework to enable accurate virtual screening under structural uncertainty. Our method comprises two core components: (1) a tri-modal contrastive learning module that aligns representations of the ligand, the holo pocket, and cavities detected from structures, thereby enhancing robustness to pocket localization error; and (2) a cross-attention based adapter for dynamically aggregating candidate binding sites, enabling the model to learn from activity data even without precise pocket annotations. We evaluated our method on a newly curated benchmark of apo structures, where it significantly outperforms state-of-the-art methods in blind apo setting, improving the early enrichment factor (EF1%) from 11.75 to 37.19. Notably, it also maintains strong performance on holo structures. These results demonstrate the promise of our approach in advancing first-in-class drug discovery, particularly in scenarios lacking experimentally resolved protein-ligand complexes. Our implementation is publicly available at https://github.com/Wiley-Z/AANet.",0,arxiv,Biyoloji,CC-BY/arXiv,AANet: Virtual Screening under Structural Uncertainty via Alignment and Aggregation
"Nature, as far as we know, evolves continuously through space and time. Yet the ubiquitous hidden Markov model (HMM)--originally developed for discrete time and space analysis in natural language processing--remains a central tool in interpreting time series data drawn from from physical systems. This raises a fundamental question: What are the implications of applying a discrete-state, discrete-time framework to analyze data generated by a continuously evolving system? Through synthetic data generated using Langevin dynamics in an effective potential, we explore under what circumstances HMMs yield interpretable results. Our analysis reveals that the discrete-state approximation acts primarily as an abstraction with the inferred states visited in time often more closely reflecting the measurement protocol and modeling choices than features of the underlying physical potential. Crucially, we demonstrate that the states visited over the course of a time series recovered by the HMM can be tuned a priori by adjusting the data acquisition scheme even misleadingly recovering reproducible ""intermediate"" states using different HMM tools for a system evolving in a single well potential. We conclude with a note of measured caution: while HMMs offer a mathematically elegant framework for time series inference, their use in physical modeling should be guided by an awareness of their limitations. In this light, we outline important generalizations of the HMM to continuous space and time and highlight the importance of a well calibrated measurement noise model.",0,arxiv,Biyoloji,CC-BY/arXiv,A cautious user's guide in applying HMMs to physical systems
"Inverse folding models have proven to be highly effective zero-shot predictors of protein stability. Despite this success, the link between the amino acid preferences of an inverse folding model and the free-energy considerations underlying thermodynamic stability remains incompletely understood. A better understanding would be of interest not only from a theoretical perspective, but also potentially provide the basis for stronger zero-shot stability prediction. In this paper, we take steps to clarify the free-energy foundations of inverse folding models. Our derivation reveals the standard practice of likelihood ratios as a simplistic approximation and suggests several paths towards better estimates of the relative stability. We empirically assess these approaches and demonstrate that considerable gains in zero-shot performance can be achieved with fairly simple means.",0,arxiv,Biyoloji,CC-BY/arXiv,Zero-shot protein stability prediction by inverse folding models: a free energy interpretation
"The functionality of protein-protein complexes is closely tied to the strength of their interactions, making the evaluation of binding affinity a central focus in structural biology. However, the molecular determinants underlying binding affinity are still not fully understood. In particular, the entropic contributions, especially those arising from conformational dynamics, remain poorly characterized. In this study, we explore the relationship between protein motion and binding stability and its role in protein function. To gain deeper insight into how protein complexes modulate their stability, we investigated a model system with a well-characterized and fast evolutionary history: a set of SARS-CoV-2 spike protein variants bound to the human ACE2 receptor, for which experimental binding affinity data are available. Through Molecular Dynamics simulations, we analyzed both structural and dynamical differences between the unbound (apo) and bound (holo) forms of the spike protein across several variants of concern. Our findings indicate that a more stable binding is associated with proteins that exhibit higher rigidity in their unbound state and display dynamical patterns similar to that observed after binding to ACE2. The increase of binding stability is not the sole driving force of SARS-CoV-2 evolution. More recent variants are characterized by a more dynamical behavior that determines a less efficient viral entry but could optimize other traits, such as antibody escape. These results suggest that to fully understand the strength of the binding between two proteins, the stability of the two isolated partners should be investigated.",0,arxiv,Biyoloji,CC-BY/arXiv,Insights into the role of dynamical features in protein complex formation: the case of SARS-CoV-2 spike binding with ACE2
"Protein structure prediction models are now capable of generating accurate 3D structural hypotheses from sequence alone. However, they routinely fail to capture the conformational diversity of dynamic biomolecular complexes, often requiring heuristic MSA subsampling approaches for generating alternative states. In parallel, cryo-electron microscopy (cryo-EM) has emerged as a powerful tool for imaging near-native structural heterogeneity, but is challenged by arduous pipelines to transform raw experimental data into atomic models. Here, we bridge the gap between these modalities, combining cryo-EM density maps with the rich sequence and biophysical priors learned by protein structure prediction models. Our method, CryoBoltz, guides the sampling trajectory of a pretrained biomolecular structure prediction model using both global and local structural constraints derived from density maps, driving predictions towards conformational states consistent with the experimental data. We demonstrate that this flexible yet powerful inference-time approach allows us to build atomic models into heterogeneous cryo-EM maps across a variety of dynamic biomolecular systems including transporters and antibodies. Code is available at https://github.com/ml-struct-bio/cryoboltz .",0,arxiv,Biyoloji,CC-BY/arXiv,Multiscale guidance of protein structure prediction with heterogeneous cryo-EM data
"Protein biology focuses on the intricate relationships among sequences, structures, and functions. Deciphering protein functions is crucial for understanding biological processes, advancing drug discovery, and enabling synthetic biology applications. Since protein sequences determine tertiary structures, which in turn govern functions, integrating sequence and structure information is essential for accurate prediction of protein functions. Traditional protein language models (pLMs) have advanced protein-related tasks by learning representations from large-scale sequence and structure data. However, pLMs are limited in integrating broader contextual knowledge, particularly regarding functional modalities that are fundamental to protein biology. In contrast, large language models (LLMs) have exhibited outstanding performance in contextual understanding, reasoning, and generation across diverse domains. Leveraging these capabilities, STELLA is proposed as a multimodal LLM integrating protein sequence-structure representations with general knowledge to address protein function prediction. Through multimodal instruction tuning (MMIT) using the proposed OPI-Struc dataset, STELLA achieves state-of-the-art performance in two function-related tasks-functional description prediction (FP) and enzyme-catalyzed reaction prediction (EP). This study highlights the potential of multimodal LLMs as an alternative paradigm to pLMs to advance protein biology research.",0,arxiv,Biyoloji,CC-BY/arXiv,STELLA: Towards Protein Function Prediction with Multimodal LLMs Integrating Sequence-Structure Representations
"The detection of ligand binding sites for proteins is a fundamental step in Structure-Based Drug Design. Despite notable advances in recent years, existing methods, datasets, and evaluation metrics are confronted with several key challenges: (1) current datasets and methods are centered on individual protein-ligand complexes and neglect that diverse binding sites may exist across multiple complexes of the same protein, introducing significant statistical bias; (2) ligand binding site detection is typically modeled as a discontinuous workflow, employing binary segmentation and subsequent clustering algorithms; (3) traditional evaluation metrics do not adequately reflect the actual performance of different binding site prediction methods. To address these issues, we first introduce UniSite-DS, the first UniProt (Unique Protein)-centric ligand binding site dataset, which contains 4.81 times more multi-site data and 2.08 times more overall data compared to the previously most widely used datasets. We then propose UniSite, the first end-to-end ligand binding site detection framework supervised by set prediction loss with bijective matching. In addition, we introduce Average Precision based on Intersection over Union (IoU) as a more accurate evaluation metric for ligand binding site prediction. Extensive experiments on UniSite-DS and several representative benchmark datasets demonstrate that IoU-based Average Precision provides a more accurate reflection of prediction quality, and that UniSite outperforms current state-of-the-art methods in ligand binding site detection. The dataset and codes will be made publicly available at https://github.com/quanlin-wu/unisite.",0,arxiv,Biyoloji,CC-BY/arXiv,UniSite: The First Cross-Structure Dataset and Learning Framework for End-to-End Ligand Binding Site Detection
"The inverse folding problem, aiming to design amino acid sequences that fold into desired three-dimensional structures, is pivotal for various biotechnological applications. Here, we introduce a novel approach leveraging Direct Preference Optimization (DPO) to fine-tune an inverse folding model using feedback from a protein folding model. Given a target protein structure, we begin by sampling candidate sequences from the inverse-folding model, then predict the three-dimensional structure of each sequence with the folding model to generate pairwise structural-preference labels. These labels are used to fine-tune the inverse-folding model under the DPO objective. Our results on the CATH 4.2 test set demonstrate that DPO fine-tuning not only improves sequence recovery of baseline models but also leads to a significant improvement in average TM-Score from 0.77 to 0.81, indicating enhanced structure similarity. Furthermore, iterative application of our DPO-based method on challenging protein structures yields substantial gains, with an average TM-Score increase of 79.5\% with regard to the baseline model. This work establishes a promising direction for enhancing protein sequence design ability from structure feedback by effectively utilizing preference optimization.",0,arxiv,Biyoloji,CC-BY/arXiv,Protein Inverse Folding From Structure Feedback
"In this study, the distributions of protein structure classes (or folding types) of experimentally determined structures from a legacy dataset and a comprehensive database (SCOP) are modeled precisely with geometric constructs such as convex polytopes in high-dimensional amino acid composition space. This is a follow-up of a previous non-statistical, geometry-motivated modeling of protein classes with ellipsoidal models, which is superseded presently in three important respects: (1) as a paradigm shift a descriptive 'distribution model' of experimental data is de-coupled from, and serves as the basis for, a possible future predictive 'domain model' generalizable to proteins in the same class for which 3D structures have yet to be determined experimentally, (2) the geometric and analytic characteristics of class distributions are obtained via exact computational geometry calculations, and (3) the full data from a comprehensive database are included in such calculations, eschewing training set selection and biases. In contrast to statistical and machine-learning approaches, the analytical, non-statistical geometry models of protein class distributions demonstrated in this study furnish complete and precise information on their size and relative disposition in the high-dimensional space (vis-Ã -vis any overlaps leading to ambiguity and classification limits). Intended primarily as an accurate and summary description of the complex relationships between amino acid composition and protein classes, and suitably as a basis for predictive modeling where possible, the results suggest that pen-ultimately they may be useful adjuncts for validating sequence-based protein structure predictions and contribute to theoretical and fundamental understanding of secondary structure formation and protein folding, demonstrating the role of high dimensional amino acid composition space in protein studies.",0,arxiv,Biyoloji,CC-BY/arXiv,Protein folding classes -- High-dimensional geometry of amino acid composition space revisited
"Climate change is a major threat to crop potential and is characterized by both long-term shifts in temperature and precipitation patterns as well as increased occurrence of extreme weather events, these extreme weather events are the most immediate and intractable threat to agriculture. Crop resilience in the face of stress depends upon the speed and effectiveness with which plants and cropping systems sense and respond to that stress. A variety of agronomic practices including breeding, exogenous inputs (nutrients, water, biostimulants and others) and shifts in cultivation practice have been used to influence plant stress response to achieve the goal of increased plant and cropping system resilience. Traditional breeding is a powerful tool that has resulted in stable and long-term cultivar improvements but is often too slow and complex to meet the diverse, complex and unpredictable challenges of climate induced stresses. Increased inputs (water, nutrients, pesticides etc.) and management strategies (cropping system choice, soil management etc.) can alleviate stress but are often constrained by cost and availability of inputs. Exogenous biostimulants, microbials and plant hormones have shown great promise as mechanisms to optimize natural plant resilience resulting in immediate but non-permanent improvements in plant responses to climate induced stresses. The failure to modernize regulatory frameworks for the use of biostimulants in agriculture will constrain the development of safe effective tools and deprive growers of means to respond to the vagaries of climate change. Here we discuss the scientific rationale for eliminating the regulatory barriers that constrain the potential for biostimulants or products that modulate plant regulatory networks to address climate change challenges and propose a framework for enabling legislation to strengthen cropping system resilience.",0,arxiv,Biyoloji,CC-BY/arXiv,The optimization of crop response to climatic stress through modulation of plant stress response mechanisms. Opportunities for biostimulants and plant hormones to meet climate challenges
"Hybrid quantum-classical machine learning offers a path to leverage noisy intermediate-scale quantum (NISQ) devices for drug discovery, but optimal model architectures remain unclear. We systematically optimize the quantum-classical bridge architecture of generative adversarial networks (GANs) for molecule discovery using multi-objective Bayesian optimization. Our optimized model (BO-QGAN) significantly improves performance, achieving a 2.27-fold higher Drug Candidate Score (DCS) than prior quantum-hybrid benchmarks and 2.21-fold higher than the classical baseline, while reducing parameter count by more than 60%. Key findings favor layering multiple (3-4) shallow (4-8 qubit) quantum circuits sequentially, while classical architecture shows less sensitivity above a minimum capacity. This work provides the first empirically-grounded architectural guidelines for hybrid models, enabling more effective integration of current quantum computers into pharmaceutical research pipelines.",0,arxiv,Biyoloji,CC-BY/arXiv,Bridging Quantum and Classical Computing in Drug Design: Architecture Principles for Improved Molecule Generation
"Designing protein sequences that fold into a target 3D structure, known as protein inverse folding, is a fundamental challenge in protein engineering. While recent deep learning methods have achieved impressive performance by recovering native sequences, they often overlook the one-to-many nature of the problem: multiple diverse sequences can fold into the same structure. This motivates the need for a generative model capable of designing diverse sequences while preserving structural consistency. To address this trade-off, we introduce ProtInvTree, the first reward-guided tree-search framework for protein inverse folding. ProtInvTree reformulates sequence generation as a deliberate, step-wise decision-making process, enabling the exploration of multiple design paths and exploitation of promising candidates through self-evaluation, lookahead, and backtracking. We propose a two-stage focus-and-grounding action mechanism that decouples position selection and residue generation. To efficiently evaluate intermediate states, we introduce a jumpy denoising strategy that avoids full rollouts. Built upon pretrained protein language models, ProtInvTree supports flexible test-time scaling by expanding the search depth and breadth without retraining. Empirically, ProtInvTree outperforms state-of-the-art baselines across multiple benchmarks, generating structurally consistent yet diverse sequences, including those far from the native ground truth.",0,arxiv,Biyoloji,CC-BY/arXiv,ProtInvTree: Deliberate Protein Inverse Folding with Reward-guided Tree Search
"Recently, extensive deep learning architectures and pretraining strategies have been explored to support downstream protein applications. Additionally, domain-specific models incorporating biological knowledge have been developed to enhance performance in specialized tasks. In this work, we introduce $\textbf{Protap}$, a comprehensive benchmark that systematically compares backbone architectures, pretraining strategies, and domain-specific models across diverse and realistic downstream protein applications. Specifically, Protap covers five applications: three general tasks and two novel specialized tasks, i.e., enzyme-catalyzed protein cleavage site prediction and targeted protein degradation, which are industrially relevant yet missing from existing benchmarks. For each application, Protap compares various domain-specific models and general architectures under multiple pretraining settings. Our empirical studies imply that: (i) Though large-scale pretraining encoders achieve great results, they often underperform supervised encoders trained on small downstream training sets. (ii) Incorporating structural information during downstream fine-tuning can match or even outperform protein language models pretrained on large-scale sequence corpora. (iii) Domain-specific biological priors can enhance performance on specialized downstream tasks. Code and datasets are publicly available at https://github.com/Trust-App-AI-Lab/protap.",0,arxiv,Biyoloji,CC-BY/arXiv,Protap: A Benchmark for Protein Modeling on Realistic Downstream Applications
"The de novo generation of drug-like molecules capable of inducing desirable phenotypic changes is receiving increasing attention. However, previous methods predominantly rely on expression profiles to guide molecule generation, but overlook the perturbative effect of the molecules on cellular contexts. To overcome this limitation, we propose SmilesGEN, a novel generative model based on variational autoencoder (VAE) architecture to generate molecules with potential therapeutic effects. SmilesGEN integrates a pre-trained drug VAE (SmilesNet) with an expression profile VAE (ProfileNet), jointly modeling the interplay between drug perturbations and transcriptional responses in a common latent space. Specifically, ProfileNet is imposed to reconstruct pre-treatment expression profiles when eliminating drug-induced perturbations in the latent space, while SmilesNet is informed by desired expression profiles to generate drug-like molecules. Our empirical experiments demonstrate that SmilesGEN outperforms current state-of-the-art models in generating molecules with higher degree of validity, uniqueness, novelty, as well as higher Tanimoto similarity to known ligands targeting the relevant proteins. Moreover, we evaluate SmilesGEN for scaffold-based molecule optimization and generation of therapeutic agents, and confirmed its superior performance in generating molecules with higher similarity to approved drugs. SmilesGEN establishes a robust framework that leverages gene signatures to generate drug-like molecules that hold promising potential to induce desirable cellular phenotypic changes.",0,arxiv,Biyoloji,CC-BY/arXiv,Phenotypic Profile-Informed Generation of Drug-Like Molecules via Dual-Channel Variational Autoencoders
"Molecular Relational Learning (MRL) aims to understand interactions between molecular pairs, playing a critical role in advancing biochemical research. With the recent development of large language models (LLMs), a growing number of studies have explored the integration of MRL with LLMs and achieved promising results. However, the increasing availability of diverse LLMs and molecular structure encoders has significantly expanded the model space, presenting major challenges for benchmarking. Currently, there is no LLM framework that supports both flexible molecular input formats and dynamic architectural switching. To address these challenges, reduce redundant coding, and ensure fair model comparison, we propose ModuLM, a framework designed to support flexible LLM-based model construction and diverse molecular representations. ModuLM provides a rich suite of modular components, including 8 types of 2D molecular graph encoders, 11 types of 3D molecular conformation encoders, 7 types of interaction layers, and 7 mainstream LLM backbones. Owing to its highly flexible model assembly mechanism, ModuLM enables the dynamic construction of over 50,000 distinct model configurations. In addition, we provide comprehensive results to demonstrate the effectiveness of ModuLM in supporting LLM-based MRL tasks.",0,arxiv,Biyoloji,CC-BY/arXiv,ModuLM: Enabling Modular and Multimodal Molecular Relational Learning with Large Language Models
"This study investigates the current landscape and future directions of protein foundation model research. While recent advancements have transformed protein science and engineering, the field lacks a comprehensive benchmark for fair evaluation and in-depth understanding. Since ESM-1B, numerous protein foundation models have emerged, each with unique datasets and methodologies. However, evaluations often focus on limited tasks tailored to specific models, hindering insights into broader generalization and limitations. Specifically, researchers struggle to understand the relationships between tasks, assess how well current models perform across them, and determine the criteria in developing new foundation models. To fill this gap, we present PFMBench, a comprehensive benchmark evaluating protein foundation models across 38 tasks spanning 8 key areas of protein science. Through hundreds of experiments on 17 state-of-the-art models across 38 tasks, PFMBench reveals the inherent correlations between tasks, identifies top-performing models, and provides a streamlined evaluation protocol. Code is available at \href{https://github.com/biomap-research/PFMBench}{\textcolor{blue}{GitHub}}.",0,arxiv,Biyoloji,CC-BY/arXiv,PFMBench: Protein Foundation Model Benchmark
"Protein sequence design methods have demonstrated strong performance in sequence generation for de novo protein design. However, as the training objective was sequence recovery, it does not guarantee designability--the likelihood that a designed sequence folds into the desired structure. To bridge this gap, we redefine the training objective by steering sequence generation toward high designability. To do this, we integrate Direct Preference Optimization (DPO), using AlphaFold pLDDT scores as the preference signal, which significantly improves the in silico design success rate. To further refine sequence generation at a finer, residue-level granularity, we introduce Residue-level Designability Preference Optimization (ResiDPO), which applies residue-level structural rewards and decouples optimization across residues. This enables direct improvement in designability while preserving regions that already perform well. Using a curated dataset with residue-level annotations, we fine-tune LigandMPNN with ResiDPO to obtain EnhancedMPNN, which achieves a nearly 3-fold increase in in silico design success rate (from 6.56% to 17.57%) on a challenging enzyme design benchmark.",0,arxiv,Biyoloji,CC-BY/arXiv,Improving Protein Sequence Design through Designability Preference Optimization
"Deep learning-based prediction of protein-ligand complexes has advanced significantly with the development of architectures such as AlphaFold3, Boltz-1, Chai-1, Protenix, and NeuralPlexer. Multiple sequence alignment (MSA) has been a key input, providing coevolutionary information critical for structural inference. However, recent benchmarks reveal a major limitation: these models often memorize ligand poses from training data and perform poorly on novel chemotypes or dynamic binding events involving substantial conformational changes in binding pockets. To overcome this, we introduced a state-aware protein-ligand prediction strategy leveraging purified sequence subsets generated by AF-ClaSeq - a method previously developed by our group. AF-ClaSeq isolates coevolutionary signals and selects sequences that preferentially encode distinct structural states as predicted by AlphaFold2. By applying MSA-derived conformational restraints, we observed significant improvements in predicting ligand poses. In cases where AlphaFold3 previously failed-producing incorrect ligand placements and associated protein conformations-we were able to correct the predictions by using sequence subsets corresponding to the relevant functional state, such as the inactive form of an enzyme bound to a negative allosteric modulator. We believe this approach represents a powerful and generalizable strategy for improving protein-ligand complex predictions, with potential applications across a broad range of molecular modeling tasks.",0,arxiv,Biyoloji,CC-BY/arXiv,State-aware protein-ligand complex prediction using AlphaFold3 with purified sequences
"Background: Platelet proteomics offers valuable insights for clinical research, yet isolating high-purity platelets remains a challenge. Current methods often lead to contamination or platelet loss, compromising data quality and reproducibility.   Objectives: This study aimed to optimize a platelet isolation technique that yields high-purity samples with minimal loss and to identify the most effective mass spectrometry-based proteomic method for analyzing platelet proteins with optimal coverage and sensitivity.   Methods: We refined an isolation protocol by adjusting centrifugation time to reduce blood volume requirements while preserving platelet yield and purity. Using this optimized method, we evaluated three proteomic approaches: Label-free Quantification with Data-Independent Acquisition (LFQ-DIA), Label-free Quantification with Data-Dependent Acquisition (LFQ-DDA), and Tandem Mass Tag labeling with DDA (TMT-DDA).   Results: LFQ-DIA demonstrated superior protein coverage and sensitivity compared to LFQ-DDA and TMT-DDA. The refined isolation protocol effectively minimized contamination and platelet loss. Additionally, age-related differences in platelet protein composition were observed, highlighting the importance of using age-matched controls in biomarker discovery studies.   Conclusions: The optimized platelet isolation protocol provides a cost-effective and reliable method for preparing high-purity samples for proteomics. LFQ-DIA is the most suitable approach for comprehensive platelet protein analysis. Age-related variation in platelet proteomes underscores the need for demographic matching in clinical proteomic research.",0,arxiv,Biyoloji,CC-BY/arXiv,Refining Platelet Purification Methods: Enhancing Proteomics for Clinical Applications
"Protein dynamics play a crucial role in protein biological functions and properties, and their traditional study typically relies on time-consuming molecular dynamics (MD) simulations conducted in silico. Recent advances in generative modeling, particularly denoising diffusion models, have enabled efficient accurate protein structure prediction and conformation sampling by learning distributions over crystallographic structures. However, effectively integrating physical supervision into these data-driven approaches remains challenging, as standard energy-based objectives often lead to intractable optimization. In this paper, we introduce Energy-based Alignment (EBA), a method that aligns generative models with feedback from physical models, efficiently calibrating them to appropriately balance conformational states based on their energy differences. Experimental results on the MD ensemble benchmark demonstrate that EBA achieves state-of-the-art performance in generating high-quality protein ensembles. By improving the physical plausibility of generated structures, our approach enhances model predictions and holds promise for applications in structural biology and drug discovery.",0,arxiv,Biyoloji,CC-BY/arXiv,Aligning Protein Conformation Ensemble Generation with Physical Feedback
"In real-world drug design, molecule optimization requires selectively improving multiple molecular properties up to pharmaceutically relevant levels, while maintaining others that already meet such criteria. However, existing computational approaches and instruction-tuned LLMs fail to capture such nuanced property-specific objectives, limiting their practical applicability. To address this, we introduce C-MuMOInstruct, the first instruction-tuning dataset focused on multi-property optimization with explicit, property-specific objectives. Leveraging C-MuMOInstruct, we develop GeLLMO-Cs, a series of instruction-tuned LLMs that can perform targeted property-specific optimization. Our experiments across 5 in-distribution and 5 out-of-distribution tasks show that GeLLMO-Cs consistently outperform strong baselines, achieving up to 126% higher success rate. Notably, GeLLMO-Cs exhibit impressive 0-shot generalization to novel optimization tasks and unseen instructions. This offers a step toward a foundational LLM to support realistic, diverse optimizations with property-specific objectives. C-MuMOInstruct and code are accessible through https://github.com/ninglab/GeLLMO-C.",0,arxiv,Biyoloji,CC-BY/arXiv,Large Language Models for Controllable Multi-property Multi-objective Molecule Optimization
"The local structure of a protein strongly impacts its function and interactions with other molecules. Therefore, a concise, informative representation of a local protein environment is essential for modeling and designing proteins and biomolecular interactions. However, these environments' extensive structural and chemical variability makes them challenging to model, and such representations remain under-explored. In this work, we propose a novel representation for a local protein environment derived from the intermediate features of atomistic foundation models (AFMs). We demonstrate that this embedding effectively captures both local structure (e.g., secondary motifs), and chemical features (e.g., amino-acid identity and protonation state). We further show that the AFM-derived representation space exhibits meaningful structure, enabling the construction of data-driven priors over the distribution of biomolecular environments. Finally, in the context of biomolecular NMR spectroscopy, we demonstrate that the proposed representations enable a first-of-its-kind physics-informed chemical shift predictor that achieves state-of-the-art accuracy. Our results demonstrate the surprising effectiveness of atomistic foundation models and their emergent representations for protein modeling beyond traditional molecular simulations. We believe this will open new lines of work in constructing effective functional representations for protein environments.",0,arxiv,Biyoloji,CC-BY/arXiv,Representing local protein environments with atomistic foundation models
"Existing PLMs generate protein sequences based on a single-condition constraint from a specific modality, struggling to simultaneously satisfy multiple constraints across different modalities. In this work, we introduce CFP-Gen, a novel diffusion language model for Combinatorial Functional Protein GENeration. CFP-Gen facilitates the de novo protein design by integrating multimodal conditions with functional, sequence, and structural constraints. Specifically, an Annotation-Guided Feature Modulation (AGFM) module is introduced to dynamically adjust the protein feature distribution based on composable functional annotations, e.g., GO terms, IPR domains and EC numbers. Meanwhile, the Residue-Controlled Functional Encoding (RCFE) module captures residue-wise interaction to ensure more precise control. Additionally, off-the-shelf 3D structure encoders can be seamlessly integrated to impose geometric constraints. We demonstrate that CFP-Gen enables high-throughput generation of novel proteins with functionality comparable to natural proteins, while achieving a high success rate in designing multifunctional proteins. Code and data available at https://github.com/yinjunbo/cfpgen.",0,arxiv,Biyoloji,CC-BY/arXiv,CFP-Gen: Combinatorial Functional Protein Generation via Diffusion Language Models
"Identification of critical residues of a protein is actively pursued, since such residues are essential for protein function. We present three ways of recognising critical residues of an example protein, the evolution of which is tracked via molecular dynamical simulations. Our methods are based on learning a Random Geometric Graph (RGG) variable, where the state variable of each of 156 residues, is attached to a node of this graph, with the RGG learnt using the matrix of correlations between state variables of each residue-pair. Given the categorical nature of the state variable, correlation between a residue pair is computed using Cramer's V. We advance an organic thresholding to learn an RGG, and compare results against extant thresholding techniques, when parametrising criticality as the nodal degree in the learnt RGG. Secondly, we develop a criticality measure by ranking the computed differences between the posterior probability of the full graph variable defined on all 156 residues, and that of the graph with all but one residue omitted. A third parametrisation of criticality informs on the dynamical variation of nodal degrees as the protein evolves during the simulation. Finally, we compare results obtained with the three distinct criticality parameters, against experimentally-ascertained critical residues.",0,arxiv,Biyoloji,CC-BY/arXiv,Identifying critical residues of a protein using meaningfully-thresholded Random Geometric Graphs
"Protein binder design is central to therapeutics, diagnostics, and synthetic biology, yet practical deployment remains challenging due to fragmented workflows, high computational costs, and complex tool integration. We present HelixDesign-Binder, a production-grade, high-throughput platform built on HelixFold3 that automates the full binder design pipeline, from backbone generation and sequence design to structural evaluation and multi-dimensional scoring. By unifying these stages into a scalable and user-friendly system, HelixDesign-Binder enables efficient exploration of binder candidates with favorable structural, energetic, and physicochemical properties. The platform leverages Baidu Cloud's high-performance infrastructure to support large-scale design and incorporates advanced scoring metrics, including ipTM, predicted binding free energy, and interface hydrophobicity. Benchmarking across six protein targets demonstrates that HelixDesign-Binder reliably produces diverse and high-quality binders, some of which match or exceed validated designs in predicted binding affinity. HelixDesign-Binder is accessible via an interactive web interface in PaddleHelix platform, supporting both academic research and industrial applications in antibody and protein binder development.",0,arxiv,Biyoloji,CC-BY/arXiv,HelixDesign-Binder: A Scalable Production-Grade Platform for Binder Design Built on HelixFold3
"Cyclic peptides offer inherent advantages in pharmaceuticals. For example, cyclic peptides are more resistant to enzymatic hydrolysis compared to linear peptides and usually exhibit excellent stability and affinity. Although deep generative models have achieved great success in linear peptide design, several challenges prevent the development of computational methods for designing diverse types of cyclic peptides. These challenges include the scarcity of 3D structural data on target proteins and associated cyclic peptide ligands, the geometric constraints that cyclization imposes, and the involvement of non-canonical amino acids in cyclization. To address the above challenges, we introduce CpSDE, which consists of two key components: AtomSDE, a generative structure prediction model based on harmonic SDE, and ResRouter, a residue type predictor. Utilizing a routed sampling algorithm that alternates between these two models to iteratively update sequences and structures, CpSDE facilitates the generation of cyclic peptides. By employing explicit all-atom and bond modeling, CpSDE overcomes existing data limitations and is proficient in designing a wide variety of cyclic peptides. Our experimental results demonstrate that the cyclic peptides designed by our method exhibit reliable stability and affinity.",0,arxiv,Biyoloji,CC-BY/arXiv,Designing Cyclic Peptides via Harmonic SDE with Atom-Bond Modeling
"Addressing the growing need for organized data on tumor homing peptides (THPs), we present TumorHoPe2, a manually curated database offering extensive details on experimentally validated THPs. This represents a significant update to TumorHoPe, originally developed by our group in 2012. TumorHoPe2 now contains 1847 entries, representing 1297 unique tumor homing peptides, a substantial expansion from the 744 entries in its predecessor. For each peptide, the database provides critical information, including sequence, terminal or chemical modifications, corresponding cancer cell lines, and specific tumor types targeted. The database compiles data from two primary sources: phage display libraries, which are commonly used to identify peptide ligands targeting tumor-specific markers, and synthetic peptides, which are chemically modified to enhance properties such as stability, binding affinity, and specificity. Our dataset includes 594 chemically modified peptides, with 255 having N-terminal and 195 C-terminal modifications. These THPs have been validated against 172 cancer cell lines and demonstrate specificity for 37 distinct tumor types. To maximize utility for the research community, TumorHoPe2 is equipped with intuitive tools for data searching, filtering, and analysis, alongside a RESTful API for efficient programmatic access and integration into bioinformatics pipelines. It is freely available at https://webs.iiitd.edu.in/raghava/tumorhope2/",0,arxiv,Biyoloji,CC-BY/arXiv,TumorHoPe2: An updated database for Tumor Homing Peptides
"This paper aims to retrieve proteins with similar structures and semantics from large-scale protein dataset, facilitating the functional interpretation of protein structures derived by structural determination methods like cryo-Electron Microscopy (cryo-EM). Motivated by the recent progress of vision-language models (VLMs), we propose a CLIP-style framework for aligning 3D protein structures with functional annotations using contrastive learning. For model training, we propose a large-scale dataset of approximately 200,000 protein-caption pairs with rich functional descriptors. We evaluate our model in both in-domain and more challenging cross-database retrieval on Protein Data Bank (PDB) and Electron Microscopy Data Bank (EMDB) dataset, respectively. In both cases, our approach demonstrates promising zero-shot retrieval performance, highlighting the potential of multimodal foundation models for structure-function understanding in protein biology.",0,arxiv,Biyoloji,CC-BY/arXiv,Aligning Proteins and Language: A Foundation Model for Protein Retrieval
"Protein-protein interactions (PPIs) are fundamental to numerous cellular processes, and their characterization is vital for understanding disease mechanisms and guiding drug discovery. While protein language models (PLMs) have demonstrated remarkable success in predicting protein structure and function, their application to sequence-based PPI binding affinity prediction remains relatively underexplored. This gap is often attributed to the scarcity of high-quality, rigorously refined datasets and the reliance on simple strategies for concatenating protein representations. In this work, we address these limitations. First, we introduce a meticulously curated version of the PPB-Affinity dataset of a total of 8,207 unique protein-protein interaction entries, by resolving annotation inconsistencies and duplicate entries for multi-chain protein interactions. This dataset incorporates a stringent, less than or equal to 30%, sequence identity threshold to ensure robust splitting into training, validation, and test sets, minimizing data leakage. Second, we propose and systematically evaluate four architectures for adapting PLMs to PPI binding affinity prediction: embeddings concatenation (EC), sequences concatenation (SC), hierarchical pooling (HP), and pooled attention addition (PAD). These architectures were assessed using two training methods: full fine-tuning and a lightweight approach employing ConvBERT heads over frozen PLM features. Our comprehensive experiments across multiple leading PLMs (ProtT5, ESM2, Ankh, Ankh2, and ESM3) demonstrated that the HP and PAD architectures consistently outperform conventional concatenation methods, achieving up to 12% increase in terms of Spearman correlation. These results highlight the necessity of sophisticated architectural designs to fully exploit the capabilities of PLMs for nuanced PPI binding affinity prediction.",0,arxiv,Biyoloji,CC-BY/arXiv,Beyond Simple Concatenation: Fairly Assessing PLM Architectures for Multi-Chain Protein-Protein Interactions Prediction
"We investigate the behavior of self-propelled particles under cyclic stretching, inspired by the characteristic pattern dynamics observed in microtubule (MT) motility assays subjected to uniaxial cyclic substrate stretching. We develop a self-propelled particle model that incorporates the elastic energy acting on the filaments due to substrate deformation, successfully reproducing the experimentally observed MT patterns. Additionally, the general framework of the model enables systematic exploration of collective responses to various substrate deformations, offering potential applications in the manipulation of MT patterns and other active matter systems.",0,arxiv,Biyoloji,CC-BY/arXiv,Active Matter under Cyclic Stretch: Modeling Microtubule Alignment and Bundling
"Function-guided protein design is a crucial task with significant applications in drug discovery and enzyme engineering. However, the field lacks a unified and comprehensive evaluation framework. Current models are assessed using inconsistent and limited subsets of metrics, which prevents fair comparison and a clear understanding of the relationships between different evaluation criteria. To address this gap, we introduce PDFBench, the first comprehensive benchmark for function-guided denovo protein design. Our benchmark systematically evaluates eight state-of-the-art models on 16 metrics across two key settings: description-guided design, for which we repurpose the Mol-Instructions dataset, originally lacking quantitative benchmarking, and keyword-guided design, for which we introduce a new test set, SwissTest, created with a strict datetime cutoff to ensure data integrity. By benchmarking across a wide array of metrics and analyzing their correlations, PDFBench enables more reliable model comparisons and provides key insights to guide future research.",0,arxiv,Biyoloji,CC-BY/arXiv,PDFBench: A Benchmark for De novo Protein Design from Function
"Protein design is a fundamental challenge in biotechnology, aiming to design novel sequences with specific functions within the vast space of possible proteins. Recent advances in deep generative models have enabled function-based protein design from textual descriptions, yet struggle with structural plausibility. Inspired by classical protein design methods that leverage natural protein structures, we explore whether incorporating fragments from natural proteins can enhance foldability in generative models. Our empirical results show that even random incorporation of fragments improves foldability. Building on this insight, we introduce ProDVa, a novel protein design approach that integrates a text encoder for functional descriptions, a protein language model for designing proteins, and a fragment encoder to dynamically retrieve protein fragments based on textual functional descriptions. Experimental results demonstrate that our approach effectively designs protein sequences that are both functionally aligned and structurally plausible. Compared to state-of-the-art models, ProDVa achieves comparable function alignment using less than 0.04% of the training data, while designing significantly more well-folded proteins, with the proportion of proteins having pLDDT above 70 increasing by 7.38% and those with PAE below 10 increasing by 9.6%.",0,arxiv,Biyoloji,CC-BY/arXiv,Protein Design with Dynamic Protein Vocabulary
"Atomic packing is an important metric for characterizing protein structures, as it significantly influences various features including the stability, the rate of evolution and the functional roles of proteins. Packing in protein structures is a measure of the overall proximity between the proteins' atoms and it can vary notably among different structures. However, even single domain proteins do not exhibit uniform packing throughout their structure.   Many different methods have been used to measure the quality of packing in proteins, identify factors that influence it, and their possible implications. In this work, we examine atomic density distributions derived from 21,255 non-redundant protein structures and show that statistically significant differences between those distributions are present. The biomolecular assembly unit was chosen as a representative for these structures.   Several protein structures deviate significantly and systematically from the average packing behavior. Hierarchical clustering indicated that there are groups of structures with similar atomic density distributions. Search for common features and patterns in these clusters showed that some of them include proteins with characteristic structures such as coiled-coils and cytochromes. Certain classification families such as hydrolases and transferases have also a preference to appear more frequently in dense and loosely-packed clusters respectively.   Regarding factors influencing packing, our results support knowledge that larger structures have a smaller range in their density values, but tend to be more loosely packed, compared to smaller proteins. We also used indicators, like crystallographic water molecules abundance and B-factors as estimates of the stability of the structures to reveal its relationship with packing.",0,arxiv,Biyoloji,CC-BY/arXiv,Atomic Density Distributions in Proteins: Structural and Functional Implications
"Accurately classifying chemical structures is essential for cheminformatics and bioinformatics, including tasks such as identifying bioactive compounds of interest, screening molecules for toxicity to humans, finding non-organic compounds with desirable material properties, or organizing large chemical libraries for drug discovery or environmental monitoring. However, manual classification is labor-intensive and difficult to scale to large chemical databases. Existing automated approaches either rely on manually constructed classification rules, or are deep learning methods that lack explainability.   This work presents an approach that uses generative artificial intelligence to automatically write chemical classifier programs for classes in the Chemical Entities of Biological Interest (ChEBI) database. These programs can be used for efficient deterministic run-time classification of SMILES structures, with natural language explanations. The programs themselves constitute an explainable computable ontological model of chemical class nomenclature, which we call the ChEBI Chemical Class Program Ontology (C3PO).   We validated our approach against the ChEBI database, and compared our results against deep learning models and a naive SMARTS pattern based classifier. C3PO outperforms the naive classifier, but does not reach the performance of state of the art deep learning methods. However, C3PO has a number of strengths that complement deep learning methods, including explainability and reduced data dependence. C3PO can be used alongside deep learning classifiers to provide an explanation of the classification, where both methods agree. The programs can be used as part of the ontology development process, and iteratively refined by expert human curators.",0,arxiv,Biyoloji,CC-BY/arXiv,Chemical classification program synthesis using generative artificial intelligence
"De novo 3D molecule generation is a pivotal task in drug discovery. However, many recent geometric generative models struggle to produce high-quality 3D structures, even if they maintain 2D validity and topological stability. To tackle this issue and enhance the learning of effective molecular generation dynamics, we present Megalodon-a family of scalable transformer models. These models are enhanced with basic equivariant layers and trained using a joint continuous and discrete denoising co-design objective. We assess Megalodon's performance on established molecule generation benchmarks and introduce new 3D structure benchmarks that evaluate a model's capability to generate realistic molecular structures, particularly focusing on energetics. We show that Megalodon achieves state-of-the-art results in 3D molecule generation, conditional structure generation, and structure energy benchmarks using diffusion and flow matching. Furthermore, doubling the number of parameters in Megalodon to 40M significantly enhances its performance, generating up to 49x more valid large molecules and achieving energy levels that are 2-10x lower than those of the best prior generative models.",0,arxiv,Biyoloji,CC-BY/arXiv,Applications of Modular Co-Design for De Novo 3D Molecule Generation
"We introduce AbBiBench (Antibody Binding Benchmarking), a benchmarking framework for antibody binding affinity maturation and design. Unlike previous strategies that evaluate antibodies in isolation, typically by comparing them to natural sequences with metrics such as amino acid recovery rate or structural RMSD, AbBiBench instead treats the antibody-antigen (Ab-Ag) complex as the fundamental unit. It evaluates an antibody design's binding potential by measuring how well a protein model scores the full Ab-Ag complex. We first curate, standardize, and share more than 184,500 experimental measurements of antibody mutants across 14 antibodies and 9 antigens-including influenza, lysozyme, HER2, VEGF, integrin, Ang2, and SARS-CoV-2-covering both heavy-chain and light-chain mutations. Using these datasets, we systematically compare 15 protein models including masked language models, autoregressive language models, inverse folding models, diffusion-based generative models, and geometric graph models by comparing the correlation between model likelihood and experimental affinity values. Additionally, to demonstrate AbBiBench's generative utility, we apply it to antibody F045-092 in order to introduce binding to influenza H1N1. We sample new antibody variants with the top-performing models, rank them by the structural integrity and biophysical properties of the Ab-Ag complex, and assess them with in vitro ELISA binding assays. Our findings show that structure-conditioned inverse folding models outperform others in both affinity correlation and generation tasks. Overall, AbBiBench provides a unified, biologically grounded evaluation framework to facilitate the development of more effective, function-aware antibody design models.",0,arxiv,Biyoloji,CC-BY/arXiv,AbBiBench: A Benchmark for Antibody Binding Affinity Maturation and Design
"Designing metal-organic frameworks (MOFs) with novel chemistries is a longstanding challenge due to their large combinatorial space and complex 3D arrangements of the building blocks. While recent deep generative models have enabled scalable MOF generation, they assume (1) a fixed set of building blocks and (2) known local 3D coordinates of building blocks. However, this limits their ability to (1) design novel MOFs and (2) generate the structure using novel building blocks. We propose a two-stage MOF generation framework that overcomes these limitations by modeling both chemical and geometric degrees of freedom. First, we train an SMILES-based autoregressive model to generate metal and organic building blocks, paired with a cheminformatics toolkit for 3D structure initialization. Second, we introduce a flow matching model that predicts translations, rotations, and torsional angles to assemble the blocks into valid 3D frameworks. Our experiments demonstrate improved reconstruction accuracy, the generation of valid, novel, and unique MOFs, and the ability to create novel building blocks. Our code is available at https://github.com/nayoung10/MOFFlow-2.",0,arxiv,Biyoloji,CC-BY/arXiv,Flexible MOF Generation with Torsion-Aware Flow Matching
"Understanding protein dynamics is critical for elucidating their biological functions. The increasing availability of molecular dynamics (MD) data enables the training of deep generative models to efficiently explore the conformational space of proteins. However, existing approaches either fail to explicitly capture the temporal dependencies between conformations or do not support direct generation of time-independent samples. To address these limitations, we introduce ConfRover, an autoregressive model that simultaneously learns protein conformation and dynamics from MD trajectories, supporting both time-dependent and time-independent sampling. At the core of our model is a modular architecture comprising: (i) an encoding layer, adapted from protein folding models, that embeds protein-specific information and conformation at each time frame into a latent space; (ii) a temporal module, a sequence model that captures conformational dynamics across frames; and (iii) an SE(3) diffusion model as the structure decoder, generating conformations in continuous space. Experiments on ATLAS, a large-scale protein MD dataset of diverse structures, demonstrate the effectiveness of our model in learning conformational dynamics and supporting a wide range of downstream tasks. ConfRover is the first model to sample both protein conformations and trajectories within a single framework, offering a novel and flexible approach for learning from protein MD data. Project website: https://bytedance-seed.github.io/ConfRover.",0,arxiv,Biyoloji,CC-BY/arXiv,ConfRover: Simultaneous Modeling of Protein Conformation and Dynamics via Autoregression
"Natural protein sequences somehow encode the structural forms that these molecules adopt. Recent developments in structure-prediction are agnostic to the mechanisms by which proteins fold and represent them as static objects. However, the amino acid sequences also encode information about how the folding process can happen, and how variations in the sequences impact on the populations of the distinct structural forms that proteins acquire. Here we present a method to infer protein folding dynamics based only on sequence information. For this, we will rely first on the obtention of a precise 'evolutionary field' from the observed variations in the sequences of homologous proteins. We then show how to map the energetics to a coarse-grained folding model where the protein is treated as a string of foldons that interact. We then describe how, for any given protein sequence of a family, the equilibrium folding curve can be computed and how the emergence of protein folding sub-domains can be identified. We finally present protocols to analyze how mutations perturb both the folding stability and the cooperativity, that represent predictions for a deep-mutational scan of a protein of interest.",0,arxiv,Biyoloji,CC-BY/arXiv,Predicting protein folding dynamics using sequence information
"Predicting the 3D conformation of small molecules within protein binding sites is a key challenge in drug design. When a crystallized reference ligand (template) is available, it provides geometric priors that can guide 3D pose prediction. We present a two-stage method for ligand conformation generation guided by such templates. In the first stage, we introduce a molecular alignment approach based on flow-matching to generate 3D coordinates for the ligand, using the template structure as a reference. In the second stage, a differentiable pose optimization procedure refines this conformation based on shape and pharmacophore similarities, internal energy, and, optionally, the protein binding pocket. We introduce a new benchmark of ligand pairs co-crystallized with the same target to evaluate our approach and show that it outperforms standard docking tools and open-access alignment methods, especially in cases involving low similarity to the template or high ligand flexibility.",0,arxiv,Biyoloji,CC-BY/arXiv,Template-Guided 3D Molecular Pose Generation via Flow Matching and Differentiable Optimization
"Quantum dots (QDs) have emerged as promising nanomaterials with unique optical and physical properties, making them highly attractive for various applications in biomedicine. This review provides a comprehensive overview of the types, modes of synthesis, characterization, applications, and recent advances of QDs in the field of biomedicine, with a primary focus on bioimaging, drug delivery, and biosensors. The unique properties of QDs, such as tunable emission spectra, long-term photostability, high quantum yield, and targeted drug delivery, hold tremendous promise for advancing diagnostics, therapeutics, and imaging techniques in biomedical research. However, several significant hurdles remain before their full potential in the biomedical field, like bioaccumulation, toxicity, and short-term stability. Addressing these hurdles is essential to effectively incorporate QDs into clinical use and enhance their influence on healthcare outcomes. Furthermore, the review conducts a critical analysis of potential QD toxicity and explores recent progress in strategies and methods to mitigate these adverse effects, such as surface modification, surface coatings, and encapsulation. By thoroughly examining current research and recent advancements, this comprehensive review offers invaluable insights into both the future possibilities and the challenges that lie ahead in fully harnessing the potential of QDs in the field of biomedicine, promising a revolution in the landscape of medical diagnostics, therapies, and imaging technologies.",0,arxiv,Biyoloji,CC-BY/arXiv,Quantum Dots as Functional Nanosystems for Enhanced Biomedical Applications
"Protein fitness optimization involves finding a protein sequence that maximizes desired quantitative properties in a combinatorially large design space of possible sequences. Recent advances in steering protein generative models (e.g., diffusion models and language models) with labeled data offer a promising approach. However, most previous studies have optimized surrogate rewards and/or utilized large amounts of labeled data for steering, making it unclear how well existing methods perform and compare to each other in real-world optimization campaigns where fitness is measured through low-throughput wet-lab assays. In this study, we explore fitness optimization using small amounts (hundreds) of labeled sequence-fitness pairs and comprehensively evaluate strategies such as classifier guidance and posterior sampling for guiding generation from different discrete diffusion models of protein sequences. We also demonstrate how guidance can be integrated into adaptive sequence selection akin to Thompson sampling in Bayesian optimization, showing that plug-and-play guidance strategies offer advantages over alternatives such as reinforcement learning with protein language models. Overall, we provide practical insights into how to effectively steer modern generative models for next-generation protein fitness optimization.",0,arxiv,Biyoloji,CC-BY/arXiv,Steering Generative Models with Experimental Data for Protein Fitness Optimization
"Precise recognition, editing, and generation of molecules are essential prerequisites for both chemists and AI systems tackling various chemical tasks. We present MolLangBench, a comprehensive benchmark designed to evaluate fundamental molecule-language interface tasks: language-prompted molecular structure recognition, editing, and generation. To ensure high-quality, unambiguous, and deterministic outputs, we construct the recognition tasks using automated cheminformatics tools, and curate editing and generation tasks through rigorous expert annotation and validation. MolLangBench supports the evaluation of models that interface language with different molecular representations, including linear strings, molecular images, and molecular graphs. Evaluations of state-of-the-art models reveal significant limitations: the strongest model (GPT-5) achieves $86.2\%$ and $85.5\%$ accuracy on recognition and editing tasks, which are intuitively simple for humans, and performs even worse on the generation task, reaching only $43.0\%$ accuracy. These results highlight the shortcomings of current AI systems in handling even preliminary molecular recognition and manipulation tasks. We hope MolLangBench will catalyze further research toward more effective and reliable AI systems for chemical applications.",0,arxiv,Biyoloji,CC-BY/arXiv,"MolLangBench: A Comprehensive Benchmark for Language-Prompted Molecular Structure Recognition, Editing, and Generation"
"Nucleic acids can form diverse non-canonical structures, such as G-quadruplexes (G4s) and i-motifs (iMs), which are critical in biological processes and disease pathways. This study presents an innovative probe design strategy based on groove size differences, leading to the development of BT-Cy-1, a supramolecular cyanine probe optimized by fine-tuning dimer ""thickness"". BT-Cy-1 demonstrated high sensitivity in detecting structural transitions and variations in G4s and iMs, even in complex environments with excess dsDNA. Applied to clinical blood samples, it revealed significant differences in RNA G4 and iM levels between liver cancer patients and healthy individuals, marking the first report of altered iM levels in clinical samples. This work highlights a novel approach for precise nucleic acid structural profiling, offering insights into their biological significance and potential in disease diagnostics.",0,arxiv,Biyoloji,CC-BY/arXiv,Selective profiling of non-canonical nucleic acid structures via size-discriminative supramolecular probes
"Molecular Dynamics (MD) simulations are essential for understanding the atomic-level behavior of molecular systems, giving insights into their transitions and interactions. However, classical MD techniques are limited by the trade-off between accuracy and efficiency, while recent deep learning-based improvements have mostly focused on single-domain molecules, lacking transferability to unfamiliar molecular systems. Therefore, we propose \textbf{Uni}fied \textbf{Sim}ulator (UniSim), which leverages cross-domain knowledge to enhance the understanding of atomic interactions. First, we employ a multi-head pretraining approach to learn a unified atomic representation model from a large and diverse set of molecular data. Then, based on the stochastic interpolant framework, we learn the state transition patterns over long timesteps from MD trajectories, and introduce a force guidance module for rapidly adapting to different chemical environments. Our experiments demonstrate that UniSim achieves highly competitive performance across small molecules, peptides, and proteins.",0,arxiv,Biyoloji,CC-BY/arXiv,UniSim: A Unified Simulator for Time-Coarsened Dynamics of Biomolecules
"Protein fitness landscapes frequently exhibit epistasis, where the effect of a mutation depends on the genetic context in which it occurs, \textit{i.e.}, the rest of the protein sequence. Epistasis increases landscape complexity, often resulting in multiple fitness peaks. In its simplest form, known as global epistasis, fitness is modeled as a non-linear function of an underlying additive trait. In contrast, more complex epistasis arises from a network of (pairwise or many-body) interactions between residues, which cannot be removed by a single non-linear transformation. Recent studies have explored how global and network epistasis contribute to the emergence of functional bottlenecks - fitness landscape topologies where two broad high-fitness basins, representing distinct phenotypes, are separated by a bottleneck that can only be crossed via one or a few mutational paths. Here, we introduce and analyze a simple model of global epistasis with an additive underlying trait. We demonstrate that functional bottlenecks arise with high probability if the model is properly calibrated. Our results underscore the necessity of sufficient heterogeneity in the mutational effects selected by evolution for the emergence of functional bottlenecks. Moreover, we show that the model agrees with experimental findings, at least in small enough combinatorial mutational spaces.",0,arxiv,Biyoloji,CC-BY/arXiv,Functional bottlenecks can emerge from non-epistatic underlying traits
"Large language models (LLMs) integrated with autonomous agents hold significant potential for advancing scientific discovery through automated reasoning and task execution. However, applying LLM agents to drug discovery is still constrained by challenges such as large-scale multimodal data processing, limited task automation, and poor support for domain-specific tools. To overcome these limitations, we introduce DrugPilot, a LLM-based agent system with a parameterized reasoning architecture designed for end-to-end scientific workflows in drug discovery. DrugPilot enables multi-stage research processes by integrating structured tool use with a novel parameterized memory pool. The memory pool converts heterogeneous data from both public sources and user-defined inputs into standardized representations. This design supports efficient multi-turn dialogue, reduces information loss during data exchange, and enhances complex scientific decision-making. To support training and benchmarking, we construct a drug instruction dataset covering eight core drug discovery tasks. Under the Berkeley function-calling benchmark, DrugPilot significantly outperforms state-of-the-art agents such as ReAct and LoT, achieving task completion rates of 98.0%, 93.5%, and 64.0% for simple, multi-tool, and multi-turn scenarios, respectively. These results highlight DrugPilot's potential as a versatile agent framework for computational science domains requiring automated, interactive, and data-integrated reasoning.",0,arxiv,Biyoloji,CC-BY/arXiv,DrugPilot: LLM-based Parameterized Reasoning Agent for Drug Discovery
"The question of ""what is life?"" has challenged scientists and philosophers for centuries, producing an array of definitions that reflect both the mystery of its emergence and the diversity of disciplinary perspectives brought to bear on the question. Despite significant progress in our understanding of biological systems, psychology, computation, and information theory, no single definition for life has yet achieved universal acceptance. This challenge becomes increasingly urgent as advances in synthetic biology, artificial intelligence, and astrobiology challenge our traditional conceptions of what it means to be alive. We undertook a methodological approach that leverages large language models (LLMs) to analyze a set of definitions of life provided by a curated set of cross-disciplinary experts. We used a novel pairwise correlation analysis to map the definitions into distinct feature vectors, followed by agglomerative clustering, intra-cluster semantic analysis, and t-SNE projection to reveal underlying conceptual archetypes. This methodology revealed a continuous landscape of the themes relating to the definition of life, suggesting that what has historically been approached as a binary taxonomic problem should be instead conceived as differentiated perspectives within a unified conceptual latent space. We offer a new methodological bridge between reductionist and holistic approaches to fundamental questions in science and philosophy, demonstrating how computational semantic analysis can reveal conceptual patterns across disciplinary boundaries, and opening similar pathways for addressing other contested definitional territories across the sciences.",0,arxiv,Biyoloji,CC-BY/arXiv,What Lives? A meta-analysis of diverse opinions on the definition of life
"Moringa oleifera, known for its medicinal properties, contains bioactive compounds such as polyphenols and flavonoids with diverse therapeutic potentials, including anti-cancer effects. This study investigates the efficacy of M. oleifera leaf phytochemicals in inhibiting BCL-2, a critical protein involved in cancer cell survival. For the first time, multi-ligand simultaneous docking (MLSD) has been employed to understand the anti-cancer properties of M. oleifera leaf extract. Molecular docking techniques, including single-ligand and MLSD, were used to assess binding interactions with BCL-2. Single-ligand docking revealed strong binding affinities for compounds such as niazinin, alpha carotene, hesperetin, apigenin, niaziminin B, and niazimicin A, with some compounds even surpassing Venetoclax, a commercial BCL-2 inhibitor. MLSD highlighted inter-ligand interactions among apigenin, hesperetin, and niazimicin A, exhibiting a binding affinity of -14.96 kcal/mol, indicating a synergistic effect. These results shed light on the potential synergistic effects of phytochemicals when using multi-ligand simultaneous docking, underscoring the importance of considering compound interactions in the development of therapeutic strategies.",0,arxiv,Biyoloji,CC-BY/arXiv,Multi-Ligand Simultaneous Docking Analysis of Moringa Oleifera Phytochemicals Reveals Enhanced BCL-2 Inhibition via Synergistic Action
"CXCR7, a G-protein-coupled chemokine receptor, has recently emerged as a key player in cancer progression, particularly in driving angiogenesis and metastasis. Despite its significance, currently, few effective inhibitors exist for targeting this receptor. In this study aimed to address this gap by developing a QSAR model to predict potential CXCR7 inhibitors, followed by validation through molecular docking. Using the Extra Trees classifier for QSAR modeling and employing a combination of physicochemical descriptors and molecular fingerprints, compounds were classified as active or inactive with a high accuracy of 0.85. The model could efficiently screen a large dataset, identifying several promising CXCR7 inhibitors. The predicted inhibitors were further validated through molecular docking studies, revealing strong binding affinities, with the best docking score of -12.24 +- 0.49 kcal/mol. Visualization of the docked structures in both 2D and 3D confirmed the interactions between the inhibitors and the CXCR7 receptor, reinforcing their potential efficacy.",0,arxiv,Biyoloji,CC-BY/arXiv,Prediction of Novel CXCR7 Inhibitors Using QSAR Modeling and Validation via Molecular Docking
"Proteins are central to biological systems, participating as building blocks across all forms of life. Despite advancements in understanding protein functions through protein sequence analysis, there remains potential for further exploration in integrating protein structural information. We argue that the structural information of proteins is not only limited to their 3D information but also encompasses information from amino acid molecules (local information) to protein-protein structure similarity (global information). To address this, we propose \textbf{GLProtein}, the first framework in protein pre-training that incorporates both global structural similarity and local amino acid details to enhance prediction accuracy and functional insights. GLProtein innovatively combines protein-masked modelling with triplet structure similarity scoring, protein 3D distance encoding and substructure-based amino acid molecule encoding. Experimental results demonstrate that GLProtein outperforms previous methods in several bioinformatics tasks, including predicting protein-protein interaction, contact prediction, and so on.",0,arxiv,Biyoloji,CC-BY/arXiv,GLProtein: Global-and-Local Structure Aware Protein Representation Learning
"This paper surveys foundation models for AI-enabled biological design, focusing on recent developments in applying large-scale, self-supervised models to tasks such as protein engineering, small molecule design, and genomic sequence design. Though this domain is evolving rapidly, this survey presents and discusses a taxonomy of current models and methods. The focus is on challenges and solutions in adapting these models for biological applications, including biological sequence modeling architectures, controllability in generation, and multi-modal integration. The survey concludes with a discussion of open problems and future directions, offering concrete next-steps to improve the quality of biological sequence generation.",0,arxiv,Biyoloji,CC-BY/arXiv,Foundation Models for AI-Enabled Biological Design
"Invariant Point Attention (IPA) is a key algorithm for geometry-aware modeling in structural biology, central to many protein and RNA models. However, its quadratic complexity limits the input sequence length. We introduce FlashIPA, a factorized reformulation of IPA that leverages hardware-efficient FlashAttention to achieve linear scaling in GPU memory and wall-clock time with sequence length. FlashIPA matches or exceeds standard IPA performance while substantially reducing computational costs. FlashIPA extends training to previously unattainable lengths, and we demonstrate this by re-training generative models without length restrictions and generating structures of thousands of residues. FlashIPA is available at https://github.com/flagshippioneering/flash_ipa.",0,arxiv,Biyoloji,CC-BY/arXiv,Flash Invariant Point Attention
"Small molecules are essential to drug discovery, and graph-language models hold promise for learning molecular properties and functions from text. However, existing molecule-text datasets are limited in scale and informativeness, restricting the training of generalizable multimodal models. We present MolTextNet, a dataset of 2.5 million high-quality molecule-text pairs designed to overcome these limitations. To construct it, we propose a synthetic text generation pipeline that integrates structural features, computed properties, bioactivity data, and synthetic complexity. Using GPT-4o-mini, we create structured descriptions for 2.5 million molecules from ChEMBL35, with text over 10 times longer than prior datasets. MolTextNet supports diverse downstream tasks, including property prediction and structure retrieval. Pretraining CLIP-style models with Graph Neural Networks and ModernBERT on MolTextNet yields improved performance, highlighting its potential for advancing foundational multimodal modeling in molecular science. Our dataset is available at https://huggingface.co/datasets/liuganghuggingface/moltextnet.",0,arxiv,Biyoloji,CC-BY/arXiv,MolTextNet: A Two-Million Molecule-Text Dataset for Multimodal Molecular Learning
"The formation of amyloid fibrils comprising amyloid $Î²$ (A$Î²$) peptides is associated with the pathology of Alzheimer's disease. In this study, we theoretically investigated the A$Î²$ structure at the fibril end using the density functional theory calculation. Several twisted conformations were identified as local minima in which a part of the peptide chain bends upward while the rest remains bound to the lower A$Î²$ monomer. Fibril-to-twisted conformational transition exhibited endothermic behavior, with endothermic energy increasing as more backbone hydrogen bonds were broken. In addition, the loss of van der Waals interaction from the hydrophobic sidechain contributed to endothermicity. The nudged elastic band method was applied to analyze the potential energy surface connecting the fibril and twisted conformations. Comparison of the activation barriers between different twisted conformations revealed that certain twisted conformations returned relatively easily to the fibril conformation, whereas others encountered a higher activation barrier and reverted less readily. Detailed structural analysis revealed that the twisted conformation's propensity to return originates from the local steric hindrance imposed by the sidechain near the torsional axis.",0,arxiv,Biyoloji,CC-BY/arXiv,Exploration of the potential energy surface for the conformational interconversion of the amyloid $Î²$ peptide at the fibril end
"Given usefulness of protein language models (LMs) in structure and functional inference, RNA LMs have received increased attentions in the last few years. However, these RNA models are often not compared against the same standard. Here, we divided RNA LMs into three classes (pretrained on multiple RNA types (especially noncoding RNAs), specific-purpose RNAs, and LMs that unify RNA with DNA or proteins or both) and compared 13 RNA LMs along with 3 DNA and 1 protein LMs as controls in zero-shot prediction of RNA secondary structure and functional classification. Results shows that the models doing well on secondary structure prediction often perform worse in function classification or vice versa, suggesting that more balanced unsupervised training is needed.",0,arxiv,Biyoloji,CC-BY/arXiv,A Comparative Review of RNA Language Models
"We introduce Quantum Mechanics for Proteins (QMProt), a dataset developed to support quantum computing applications in protein research. QMProt contains precise quantum-mechanical and physicochemical data, enabling accurate characterization of biomolecules and supporting advanced computational methods like molecular fragmentation and reassembly. The dataset includes 45 molecules covering all 20 essential human amino acids and their core structural elements: amino terminal groups, carboxyl terminal groups, alpha carbons, and unique side chains. QMProt primarily features organic molecules with up to 15 non-hydrogen atoms (C, N, O, S), offering comprehensive molecular Hamiltonians, ground state energies, and detailed physicochemical properties. Publicly accessible, QMProt aims to enhance reproducibility and advance quantum-enhanced simulations in molecular biology, biochemistry, and drug discovery.",0,arxiv,Biyoloji,CC-BY/arXiv,QMProt: A Comprehensive Dataset of Quantum Properties for Proteins
"Synthesizability in small molecule generative design remains a bottleneck. Existing works that do consider synthesizability can output predicted synthesis routes for generated molecules. However, there has been minimal attention in addressing the ease of synthesis and enabling flexibility to incorporate desired reaction constraints. In this work, we propose a small molecule generative design framework that enables steerable and granular synthesizability control. Generated molecules satisfy arbitrary multi-parameter optimization objectives with predicted synthesis routes containing pre-defined allowed reactions, while optionally avoiding others. One can also enforce that all reactions belong to a pre-defined set. We show the capability to mix-and-match these reaction constraints across the most common medicinal chemistry transformations. Next, we show how our framework can be used to valorize industrial byproducts towards de novo optimized molecules. Going further, we demonstrate how granular control over synthesizability constraints can loosely mimic virtual screening of ultra-large make-on-demand libraries. Using only a single GPU, we generate and dock 15k molecules to identify promising candidates in Freedom 4.0 constituting 142B make-on-demand molecules (assessing only 0.00001% of the library). Generated molecules satisfying the reaction constraints have > 90% exact match rate. Lastly, we benchmark our framework against recent synthesizability-constrained generative models and demonstrate the highest sample efficiency even when imposing the additional constraint that all molecules must be synthesizable from a single reaction type. The main theme is demonstrating that a pre-trained generalist molecular generative model can be incentivized to generate property-optimized small molecules under challenging synthesizability constraints through reinforcement learning.",0,arxiv,Biyoloji,CC-BY/arXiv,Generative Molecular Design with Steerable and Granular Synthesizability Control
"Predicting protein complex structures is essential for protein function analysis, protein design, and drug discovery. While AI methods like AlphaFold can predict accurate structural models for many protein complexes, reliably estimating the quality of these predicted models (estimation of model accuracy, or EMA) for model ranking and selection remains a major challenge. A key barrier to developing effective machine learning-based EMA methods is the lack of large, diverse, and well-annotated datasets for training and evaluation. To address this gap, we introduce PSBench, a benchmark suite comprising four large-scale, labeled datasets generated during the 15th and 16th community-wide Critical Assessment of Protein Structure Prediction (CASP15 and CASP16). PSBench includes over one million structural models covering a wide range of protein sequence lengths, complex stoichiometries, functional classes, and modeling difficulties. Each model is annotated with multiple complementary quality scores at the global, local, and interface levels. PSBench also provides multiple evaluation metrics and baseline EMA methods to facilitate rigorous comparisons. To demonstrate PSBench's utility, we trained and evaluated GATE, a graph transformer-based EMA method, on the CASP15 data. GATE was blindly tested in CASP16 (2024), where it ranked among the top-performing EMA methods. These results highlight PSBench as a valuable resource for advancing EMA research in protein complex modeling. PSBench is publicly available at: https://github.com/BioinfoMachineLearning/PSBench.",0,arxiv,Biyoloji,CC-BY/arXiv,PSBench: a large-scale benchmark for estimating the accuracy of protein complex structural models
"Peptides are recognized for their varied self-assembly behaviors, forming a wide array of structures and geometries, such as spheres, fibers, and hydrogels, each presenting a unique set of material properties. The functionalities of these materials hold exceptional interest for applications in biology, medicine, photonics, nanotechnology and the food industry. In specific, the ability to exploit peptides as viable and sustainable mechanical materials requires sequence design that enables superior performance, notably a high Young's modulus. As the peptide sequence space is vast, however, even a slight increase in sequence length leads to an exponential increase in the number of potential peptide sequences to be characterized. Here, we combine coarse-grained molecular dynamics simulations, atomic force microscopy experiments and machine learning models to correlate the sequence length and composition with the mechanical properties of self-assembled peptides. We calculate the Young's modulus for all possible amino acid sequences of di- and tripeptides using high-throughput coarse-grained methods, and validate these calculations through in-situ mechanical characterization. For pentapeptides, we select and calculate properties for a subset of sequences to train a machine learning model, which allows us to predict the modulus for other sequences. The combined workflow not only identifies promising peptide candidates with exceptional mechanical performances, but also extends current understanding of the sequence-to-function relationships for peptide materials, for specific applications.",0,arxiv,Biyoloji,CC-BY/arXiv,High-throughput Screening of the Mechanical Properties of Peptide Assemblies
"Developing an effective medicine to combat cancer and elusive stem cells is crucial in the current scenario. Withaferin A and Garcinol, important phytoconstituents of Withania somnifera (Ashwagandha) and Garcinia indica (Kokum) respectively, known for their therapeutic efficiency, have been used for several decades for treating various disorders, because of their anti-cancerous, anti-inflammatory and anti-invasive properties. This study investigates the potentials of withaferin A and garcinol in inhibiting BCL-2 and AKT-1, crucial proteins contributing in cancer cell persistence by evading apoptosis, increased cell proliferation, and inflammation. Molecular docking techniques, including single docking and MLSD, were used to understand the binding interaction of the ligands with BCL-2 and AKT-1. MLSD highlighted inter-ligand interactions among withaferin A and garcinol, against BCL-2, with a binding affinity of -11.88 +- 0.12 kcal/mol, surpassing the binding affinity of venetoclax (-9.73 +- 0.1 kcal/mol) a commercial inhibitor of BCL-2. For AKT-1, the binding affinity of withaferin A and garcinol (-13.74 +- 0.08 kcal/mol) surpassed the binding affinity of melatonin (-7.24 +- 0.06 kcal/mol), a commercial inhibitor of AKT-1. The MLSD results highlight the combined effects of garcinol and withaferin A, highlighting the importance of considering both the interactions of the bioactive compounds in the development of new medicines and strategies targeting cancer and elusive stem cells.",0,arxiv,Biyoloji,CC-BY/arXiv,Computational Analysis using Multi-ligand Simultaneous Docking of Withaferin A and Garcinol Reveals Enhanced BCL-2 and AKT-1 Inhibition
"Protein structure generative models have seen a recent surge of interest, but meaningfully evaluating them computationally is an active area of research. While current metrics have driven useful progress, they do not capture how well models sample the design space represented by the training data. We argue for a protein Frechet Inception Distance (FID) metric to supplement current evaluations with a measure of distributional similarity in a semantically meaningful latent space. Our FID behaves desirably under protein structure perturbations and correctly recapitulates similarities between protein samples: it correlates with optimal transport distances and recovers FoldSeek clusters and the CATH hierarchy. Evaluating current protein structure generative models with FID shows that they fall short of modeling the distribution of PDB proteins.",0,arxiv,Biyoloji,CC-BY/arXiv,Protein FID: Improved Evaluation of Protein Structure Generative Models
"Leishmaniasis caused by Leishmania mexicana relies on Leishmania mexicana gluscose transporter (LmGT) receptors, which play an important role in glucose and ribose uptake at different stages of parasite's life cycle. Previous efforts to identify LmGT inhibitors have been primarily based on in vitro screening. However, this conventional method is limited by inefficiency, high cost, and lack of specificity which leaves a significant gap in the development of targeted therapeutic candidates for LmGT. This study employs computational techniques to address this gap by developing a quantitative structure analysis relationship model, utilizing a support vector machine classifier to identify novel LmGt inhibitor. The QSAR model achieved an accuracy of 0.81 in differentiating active compounds. Molecular docking further validated the identified inhibitors, revealing strong binding affinities with a top score of -9.46. The docking analysis showed that the inhibitors formed multiple hydrogen bonds and occupied the same binding pockets as Phase 3 drug candidate. The tested inhibitors were derived from natural sources, which suggest reduced side effects and improved biocompability. This combined approach demonstrates the power of computational models in accelerating drug discovery, with implication for more efficient and biocompatible therapies against Leishmania mexicana.",0,arxiv,Biyoloji,CC-BY/arXiv,In Silico Prediction and Validation of LmGt Inhibitors Using QSAR and Molecular Docking Approaches
"Structure-Based Drug Design (SBDD) is crucial for identifying bioactive molecules. Recent deep generative models are faced with challenges in geometric structure modeling. A major bottleneck lies in the twisted probability path of multi-modalities -- continuous 3D positions and discrete 2D topologies -- which jointly determine molecular geometries. By establishing the fact that noise schedules decide the Variational Lower Bound (VLB) for the twisted probability path, we propose VLB-Optimal Scheduling (VOS) strategy in this under-explored area, which optimizes VLB as a path integral for SBDD. Our model effectively enhances molecular geometries and interaction modeling, achieving state-of-the-art PoseBusters passing rate of 95.9% on CrossDock, more than 10% improvement upon strong baselines, while maintaining high affinities and robust intramolecular validity evaluated on held-out test set. Code is available at https://github.com/AlgoMole/MolCRAFT.",0,arxiv,Biyoloji,CC-BY/arXiv,Piloting Structure-Based Drug Design via Modality-Specific Optimal Schedule
"Designing biological sequences that satisfy multiple, often conflicting, functional and biophysical criteria remains a central challenge in biomolecule engineering. While discrete flow matching models have recently shown promise for efficient sampling in high-dimensional sequence spaces, existing approaches address only single objectives or require continuous embeddings that can distort discrete distributions. We present Multi-Objective-Guided Discrete Flow Matching (MOG-DFM), a general framework to steer any pretrained discrete flow matching generator toward Pareto-efficient trade-offs across multiple scalar objectives. At each sampling step, MOG-DFM computes a hybrid rank-directional score for candidate transitions and applies an adaptive hypercone filter to enforce consistent multi-objective progression. We also trained two unconditional discrete flow matching models, PepDFM for diverse peptide generation and EnhancerDFM for functional enhancer DNA generation, as base generation models for MOG-DFM. We demonstrate MOG-DFM's effectiveness in generating peptide binders optimized across five properties (hemolysis, non-fouling, solubility, half-life, and binding affinity), and in designing DNA sequences with specific enhancer classes and DNA shapes. In total, MOG-DFM proves to be a powerful tool for multi-property-guided biomolecule sequence design.",0,arxiv,Biyoloji,CC-BY/arXiv,Multi-Objective-Guided Discrete Flow Matching for Controllable Biological Sequence Design
"Knowledge graphs and structural causal models have each proven valuable for organizing biomedical knowledge and estimating causal effects, but remain largely disconnected: knowledge graphs encode qualitative relationships focusing on facts and deductive reasoning without formal probabilistic semantics, while causal models lack integration with background knowledge in knowledge graphs and have no access to the deductive reasoning capabilities that knowledge graphs provide. To bridge this gap, we introduce a novel formulation of Causal Knowledge Graphs (CKGs) which extend knowledge graphs with formal causal semantics, preserving their deductive capabilities while enabling principled causal inference. CKGs support deconfounding via explicitly marked causal edges and facilitate hypothesis formulation aligned with both encoded and entailed background knowledge. We constructed a Drug-Disease CKG (DD-CKG) integrating disease progression pathways, drug indications, side-effects, and hierarchical disease classification to enable automated large-scale mediation analysis. Applied to UK Biobank and MIMIC-IV cohorts, we tested whether drugs mediate effects between indications and downstream disease progression, adjusting for confounders inferred from the DD-CKG. Our approach successfully reproduced known adverse drug reactions with high precision while identifying previously undocumented significant candidate adverse effects. Further validation through side effect similarity analysis demonstrated that combining our predicted drug effects with established databases significantly improves the prediction of shared drug indications, supporting the clinical relevance of our novel findings. These results demonstrate that our methodology provides a generalizable, knowledge-driven framework for scalable causal inference.",0,arxiv,Biyoloji,CC-BY/arXiv,Causal knowledge graph analysis identifies adverse drug effects
"Newcastle Disease Virus (NDV), classified as Avian orthoavulavirus 1 (avian paramyxovirus type 1), is a promising oncolytic agent that selectively targets and destroys cancer cells while sparing normal tissues. Its oncoselectivity exploits cancer-specific defects in antiviral defenses, particularly impaired Type I interferon signaling, and dysregulated apoptotic pathways, enabling robust viral replication and cytotoxicity in malignancies such as breast, colorectal, and melanoma. NDV induces intrinsic and extrinsic apoptosis through caspase activation and triggers immunogenic cell death via damage-associated molecular patterns, stimulating potent antitumours immune responses. Additionally, NDVs potential as a vaccine vector, expressing tumours-associated antigens, offers prospects for prophylactic and therapeutic cancer applications. This review provides a comprehensive analysis of NDVs morphology, classification, and molecular biology, focusing on its viral entry and replication mechanisms in host cells. It explores NDVs interactions with cancer cells, emphasizing its ability to induce cytotoxicity and immune activation. Understanding these mechanisms is critical for optimizing NDVs oncolytic potential and advancing its clinical translation. Future directions include enhancing NDV through genetic engineering, combining it with therapies like immune checkpoint inhibitors, and developing personalized medicine approaches tailored to tumours genomic profiles. These advancements position NDV as a versatile therapeutic agent in oncolytic virotherapy.",0,arxiv,Biyoloji,CC-BY/arXiv,Oncolytic mechanisms and immunotherapeutic potential of Newcastle disease virus in cancer therapy
"Recent advances in Protein Structure Prediction Models (PPMs), such as AlphaFold2 and ESMFold, have revolutionized computational biology by achieving unprecedented accuracy in predicting three-dimensional protein folding structures. However, these models face significant scalability challenges, particularly when processing proteins with long amino acid sequences (e.g., sequence length > 1,000). The primary bottleneck that arises from the exponential growth in activation sizes is driven by the unique data structure in PPM, which introduces an additional dimension that leads to substantial memory and computational demands. These limitations have hindered the effective scaling of PPM for real-world applications, such as analyzing large proteins or complex multimers with critical biological and pharmaceutical relevance.   In this paper, we present LightNobel, the first hardware-software co-designed accelerator developed to overcome scalability limitations on the sequence length in PPM. At the software level, we propose Token-wise Adaptive Activation Quantization (AAQ), which leverages unique token-wise characteristics, such as distogram patterns in PPM activations, to enable fine-grained quantization techniques without compromising accuracy. At the hardware level, LightNobel integrates the multi-precision reconfigurable matrix processing unit (RMPU) and versatile vector processing unit (VVPU) to enable the efficient execution of AAQ. Through these innovations, LightNobel achieves up to 8.44x, 8.41x speedup and 37.29x, 43.35x higher power efficiency over the latest NVIDIA A100 and H100 GPUs, respectively, while maintaining negligible accuracy loss. It also reduces the peak memory requirement up to 120.05x in PPM, enabling scalable processing for proteins with long sequences.",0,arxiv,Biyoloji,CC-BY/arXiv,LightNobel: Improving Sequence Length Limitation in Protein Structure Prediction Model via Adaptive Activation Quantization
"Generating molecules that bind to specific protein targets via diffusion models has shown good promise for structure-based drug design and molecule optimization. Especially, the diffusion models with binding interaction guidance enables molecule generation with high affinity through forming favorable interaction within protein pocket. However, the generated molecules may not form interactions with the highly conserved residues, which are important for protein functions and bioactivities of the ligands. Herein, we developed a new 3D target-aware diffusion model DiffDecip, which explicitly incorporates the protein-ligand binding interactions and evolutionary conservation information of protein residues into both diffusion and sampling process, for molecule optimization through scaffold decoration. The model performance revealed that DiffDecip outperforms baseline model DiffDec on molecule optimization towards higher affinity through forming more non-covalent interactions with highly conserved residues in the protein pocket.",0,arxiv,Biyoloji,CC-BY/arXiv,A 3D pocket-aware and evolutionary conserved interaction guided diffusion model for molecular optimization
"Predicting enzymatic reactions is crucial for applications in biocatalysis, metabolic engineering, and drug discovery, yet it remains a complex and resource-intensive task. Large Language Models (LLMs) have recently demonstrated remarkable success in various scientific domains, e.g., through their ability to generalize knowledge, reason over complex structures, and leverage in-context learning strategies. In this study, we systematically evaluate the capability of LLMs, particularly the Llama-3.1 family (8B and 70B), across three core biochemical tasks: Enzyme Commission number prediction, forward synthesis, and retrosynthesis. We compare single-task and multitask learning strategies, employing parameter-efficient fine-tuning via LoRA adapters. Additionally, we assess performance across different data regimes to explore their adaptability in low-data settings. Our results demonstrate that fine-tuned LLMs capture biochemical knowledge, with multitask learning enhancing forward- and retrosynthesis predictions by leveraging shared enzymatic information. We also identify key limitations, for example challenges in hierarchical EC classification schemes, highlighting areas for further improvement in LLM-driven biochemical modeling.",0,arxiv,Biyoloji,CC-BY/arXiv,Leveraging Large Language Models for enzymatic reaction prediction and characterization
"Nucleic acids have been regarded as stiff polymers with long-range flexibility and generally modeled using elastic rod models of polymer physics. Notwithstanding, investigations carried out over the past few years on single fragments of order $\sim 100$ base pairs have revealed remarkable flexibility properties at short scales and called for theoretical approaches that emphasize the role of the bending fluctuations at single sites along the molecule stack. Here, we review a three dimensional mesoscopic Hamiltonian model which assumes a discrete representation of the double stranded (ds) molecules at the level of the nucleotides. The model captures the fundamental local interactions between adjacent sugar-phosphate groups and the pairwise interactions between complementary base pair mates. A statistical method based on the path integral formalism sets the ensemble of the base pair breathing fluctuations which are included in the partition function and permits to derive the thermodynamics and the elastic response of single molecules to external forces. We apply the model to the computation of the twist-stretch relations for fragments of ds-DNA and ds-RNA, showing that the obtained opposite pattern (DNA overtwists whereas RNA untwists versus force) follows from the different structural features of the two helices. Moreover, we focus on the DNA stretching due to the confinement in nano-pores and, finally, on the computation of the cyclization probability of open ends molecules of $\sim 100$ base pairs under physiological conditions. The mesoscopic model shows a distinct advantage over the elastic rod model in estimating the molecule bendability at short length scale.",0,arxiv,Biyoloji,CC-BY/arXiv,Statistical method for A-RNA and B-DNA
"Generative machine learning models on sequences are transforming protein engineering. However, no principled framework exists for conditioning these models on auxiliary information, such as experimental data, in a plug-and-play manner. Herein, we present ProteinGuide -- a principled and general method for conditioning -- by unifying a broad class of protein generative models under a single framework. We demonstrate the applicability of ProteinGuide by guiding two protein generative models, ProteinMPNN and ESM3, to generate amino acid and structure token sequences, conditioned on several user-specified properties such as enhanced stability, enzyme classes, and CATH-labeled folds. We also used ProteinGuide with inverse folding models and our own experimental assay to design adenine base editor sequences for high activity.",0,arxiv,Biyoloji,CC-BY/arXiv,Guide your favorite protein sequence generative model
"DNA cloning methods are fundamental tools in molecular biology, synthetic biology, and genetic engineering that enable precise DNA manipulation for various scientific and biotechnological applications. This review systematically summarizes the major restriction-free overlapping sequence cloning (RFOSC) techniques currently used in synthetic biology and examines their development, efficiency, practicality, and specific applications. In vitro methods, including Gibson Assembly, Circular Polymerase Extension Cloning (CPEC), Polymerase Incomplete Primer Extension (PIPE), Overlap Extension Cloning (OEC), Flap Endonuclease Cloning (FEN-Cloning), and commercially available techniques such as TOPO and In-Fusion, have been discussed alongside hybrid approaches such as Ligation-Independent Cloning (LIC), Sequence-Independent Cloning (SLIC), and T5 Exonuclease-Dependent Assembly (TEDA). Additionally, in vivo methods leveraging host recombination machinery, including Yeast Homologous Recombination (YHR), In Vivo Assembly (IVA), Transformation-Associated Recombination (TAR), and innovative approaches such as Multiple-Round In Vivo Site-Specific Assembly (MISSA) and Phage Enzyme-Assisted Direct Assembly (PEDA), are critically evaluated. The review highlights that method selection should consider the scale, complexity, cost, and specific needs of individual research projects, noting that no single technique is universally optimal. Future trends suggest the increased integration of enzymatic efficiency, host versatility, and automation, broadening the accessibility and capabilities of DNA assembly technologies.",0,arxiv,Biyoloji,CC-BY/arXiv,A review of DNA restriction-free overlapping sequence cloning techniques for synthetic biology
"Several formats, including FASTA, PIR, GenBank, EMBL, and GCG, have been developed for representing protein sequences composed of natural amino acids. Among these, FASTA remains the most widely used due to its simplicity and human readability. However, FASTA lacks the capability to represent chemically modified or non-natural residues, as well as structural annotations and mutations in protein variants. To address some of these limitations, the PEFF format was recently introduced as an extension of FASTA. Additionally, formats such as HELM and BILN have been proposed to represent amino acids and their modifications at the atomic level. Despite their advancements, these formats have not achieved widespread adoption within the bioinformatics community due to their complexity. To complement existing formats and overcome current challenges, we propose a new format called MAP (Modification and Annotation in Proteins), which enables comprehensive annotation of protein sequences. MAP introduces meta tags in the header for protein-level annotations and inline tags within the sequence for residue-level modifications. In this format, standard one-letter amino acid codes are augmented with curly-brace tags to denote various modifications, including phosphorylation, acetylation, non-natural residues, cyclization, and other residue-specific features. The header metadata also captures information such as organism, function, and sequence variants. We describe the structure, objectives, and capabilities of the MAP format and demonstrate its application in bioinformatics, particularly in the domain of protein therapeutics. To facilitate community adoption, we are developing a comprehensive suite of MAP-format resources, including a detailed manual, annotated datasets, and conversion tools, available at http://webs.iiitd.edu.in/raghava/maprepo/.",0,arxiv,Biyoloji,CC-BY/arXiv,"MAP Format for Representing Chemical Modifications, Annotations, and Mutations in Protein Sequences: An Extension of the FASTA Format"
"Protein structure prediction is a critical and longstanding challenge in biology, garnering widespread interest due to its significance in understanding biological processes. A particular area of focus is the prediction of missing loops in proteins, which are vital in determining protein function and activity. To address this challenge, we propose AutoLoop, a novel computational model designed to automatically generate accurate loop backbone conformations that closely resemble their natural structures. AutoLoop employs a bidirectional training approach while merging atom- and residue-level embedding, thus improving robustness and precision. We compared AutoLoop with twelve established methods, including FREAD, NGK, AlphaFold2, and AlphaFold3. AutoLoop consistently outperforms other methods, achieving a median RMSD of 1.12 Angstrom and a 2-Angstrom success rate of 73.23% on the CASP15 dataset, while maintaining strong performance on the HOMSTARD dataset. It demonstrates the best performance across nearly all loop lengths and secondary structural types. Beyond accuracy, AutoLoop is computationally efficient, requiring only 0.10 s per generation. A post-processing module for side-chain packing and energy minimization further improves results slightly, confirming the reliability of the predicted backbone. A case study also highlights AutoLoop's potential for precise predictions based on dominant loop conformations. These advances hold promise for protein engineering and drug discovery.",0,arxiv,Biyoloji,CC-BY/arXiv,AutoLoop: a novel autoregressive deep learning method for protein loop prediction with high accuracy
"Target-specific peptides, such as conotoxins, exhibit exceptional binding affinity and selectivity toward ion channels and receptors. However, their therapeutic potential remains underutilized due to the limited diversity of natural variants and the labor-intensive nature of traditional optimization strategies. Here, we present CreoPep, a deep learning-based conditional generative framework that integrates masked language modeling with a progressive masking scheme to design high-affinity peptide mutants while uncovering novel structural motifs. CreoPep employs an integrative augmentation pipeline, combining FoldX-based energy screening with temperature-controlled multinomial sampling, to generate structurally and functionally diverse peptides that retain key pharmacological properties. We validate this approach by designing conotoxin inhibitors targeting the $Î±$7 nicotinic acetylcholine receptor, achieving submicromolar potency in electrophysiological assays. Structural analysis reveals that CreoPep-generated variants engage in both conserved and novel binding modes, including disulfide-deficient forms, thus expanding beyond conventional design paradigms. Overall, CreoPep offers a robust and generalizable platform that bridges computational peptide design with experimental validation, accelerating the discovery of next-generation peptide therapeutics.",0,arxiv,Biyoloji,CC-BY/arXiv,CreoPep: A Universal Deep Learning Framework for Target-Specific Peptide Design and Optimization
"The multiple parameter logistic equation has previously been utilized to determine the global stability of ternary codes, based on the arrangement of different symbols within the code. This approach has been extended to DNA and RNA sequences, proposing a specific application in the context of reading and translation processes involved in DNA replication and RNA-mediated protein codification. To address the complexity of mapping Liapunov exponents in terms of four parameters representing the different nucleotide bases specialized mapping techniques have been developed. These include Liapunov exponent distributions for entire sequences, as well as binary maps that classify nucleotide bases based on their chemical type (purinic or pyrimidinic). Such methodologies provide a framework for examining the structural and functional properties of genetic material. The sequences analyzed encompass a wide range of DNA and RNA types, including those with and without introns, as well as codifying and noncodifying regions. This multifaceted approach offers valuable insights into the dynamic behavior and stability of nucleotide arrangements, contributing to a deeper understanding of the underlying processes that govern genetic replication and protein synthesis.",0,arxiv,Biyoloji,CC-BY/arXiv,Liapunov exponent distributions and maps for multiple parameter logistic equation. Application to DNA and RNA sequences
"Nanobodies -- single-domain antibody fragments derived from camelid heavy-chain-only antibodies -- exhibit unique advantages such as compact size, high stability, and strong binding affinity, making them valuable tools in therapeutics and diagnostics. While recent advances in pretrained protein and antibody language models (PPLMs and PALMs) have greatly enhanced biomolecular understanding, nanobody-specific modeling remains underexplored and lacks a unified benchmark. To address this gap, we introduce NbBench, the first comprehensive benchmark suite for nanobody representation learning. Spanning eight biologically meaningful tasks across nine curated datasets, NbBench encompasses structure annotation, binding prediction, and developability assessment. We systematically evaluate eleven representative models -- including general-purpose protein LMs, antibody-specific LMs, and nanobody-specific LMs -- in a frozen setting. Our analysis reveals that antibody language models excel in antigen-related tasks, while performance on regression tasks such as thermostability and affinity remains challenging across all models. Notably, no single model consistently outperforms others across all tasks. By standardizing datasets, task definitions, and evaluation protocols, NbBench offers a reproducible foundation for assessing and advancing nanobody modeling.",0,arxiv,Biyoloji,CC-BY/arXiv,NbBench: Benchmarking Language Models for Comprehensive Nanobody Tasks
"Proteins and other macromolecules exist not in a single state but as dynamic ensembles of interconverting conformations, which are essential for catalysis, allosteric regulation, and molecular recognition. While AI-based structure predictors like AlphaFold have revolutionized static structure prediction, they are not yet capable of capturing conformational ensembles. Progress towards the next generation of AI models capable of ensemble prediction is currently limited by the lack of accurate, high-resolution ground truth ensembles at the scale required for training and validation. This is due to the fact that no single experimental technique can fully resolve the atomistic complexity of conformational landscapes, and fundamental challenges remain in defining, representing, comparing, and validating structural ensembles. Here, we outline the infrastructure and methodological advances needed to overcome these barriers. We highlight emerging strategies for integrating heterogeneous experimental data into unified ensemble encoding representations and how to leverage these new methodologies to build benchmarks and establish ensemble-specific validation protocols. Finally, we discuss how ensemble predictions will be an interactive cycle of experimental and computational innovation. Establishing this ecosystem will allow structural biology to move beyond static snapshots toward a dynamic understanding of molecular behavior that captures the full complexity of biological systems.",0,arxiv,Biyoloji,CC-BY/arXiv,From Possibility to Precision in Macromolecular Ensemble Prediction
"Proteins play essential roles in nature, from catalyzing biochemical reactions to binding specific targets. Advances in protein engineering have the potential to revolutionize biotechnology and healthcare by designing proteins with tailored properties. Machine learning and generative models have transformed protein design by enabling the exploration of vast sequence-function landscapes. Here, we introduce Scoring-Assisted Generative Exploration for Proteins (SAGE-Prot), a framework that iteratively combines autoregressive protein generation with quantitative structure-property relationship models for fine-tuned optimization. By integrating diverse protein descriptors, SAGE-Prot enhances key properties, including binding affinity, thermal stability, enzymatic activity, and solubility. We demonstrate its effectiveness by optimizing GB1 for binding affinity and thermal stability and TEM-1 for enzymatic activity and solubility. Leveraging curriculum learning, SAGE-Prot adapts rapidly to increasingly complex design objectives, building on past successes. Experimental validation demonstrated that SAGE-Prot-generated proteins substantially outperformed their wild-type counterparts, achieving up to a 17-fold increase in beta-lactamase activity, underscoring SAGE-Prot's potential to tackle critical challenges in protein engineering. As generative models continue to evolve, approaches like SAGE-Prot will be indispensable for advancing rational protein design.",0,arxiv,Biyoloji,CC-BY/arXiv,Scoring-Assisted Generative Exploration for Proteins (SAGE-Prot): A Framework for Multi-Objective Protein Optimization via Iterative Sequence Generation and Evaluation
"The controlled dissipation of chemical potentials is the fundamental way cells make a living. Enzyme-mediated catalysis allows the various transformations to proceed at biologically relevant rates with remarkable precision and efficiency. Theory, experiments and computational studies coincide to show that local frustration is a useful concept to relate protein dynamics with catalytic power. Local frustration gives rise to the asperities of the energy landscapes that can harness the thermal fluctuations to guide the functional protein motions. We review here recent advances into these relationships from various fields of protein science. The biologically relevant dynamics is tuned by the evolution of protein sequences that modulate the local frustration patterns to near optimal values.",0,arxiv,Biyoloji,CC-BY/arXiv,"Frustration, dynamics and catalysis"
"SMILES-based molecule generation has emerged as a powerful approach in drug discovery. Deep reinforcement learning (RL) using large language model (LLM) has been incorporated into the molecule generation process to achieve high matching score in term of likelihood of desired molecule candidates. However, a critical challenge in this approach is catastrophic forgetting during the RL phase, where knowledge such as molecule validity, which often exceeds 99\% during pretraining, significantly deteriorates. Current RL algorithms applied in drug discovery, such as REINVENT, use prior models as anchors to retian pretraining knowledge, but these methods lack robust exploration mechanisms. To address these issues, we propose Partial SMILES Validation-PPO (PSV-PPO), a novel RL algorithm that incorporates real-time partial SMILES validation to prevent catastrophic forgetting while encouraging exploration. Unlike traditional RL approaches that validate molecule structures only after generating entire sequences, PSV-PPO performs stepwise validation at each auto-regressive step, evaluating not only the selected token candidate but also all potential branches stemming from the prior partial sequence. This enables early detection of invalid partial SMILES across all potential paths. As a result, PSV-PPO maintains high validity rates even during aggressive exploration of the vast chemical space. Our experiments on the PMO and GuacaMol benchmark datasets demonstrate that PSV-PPO significantly reduces the number of invalid generated structures while maintaining competitive exploration and optimization performance. While our work primarily focuses on maintaining validity, the framework of PSV-PPO can be extended in future research to incorporate additional forms of valuable domain knowledge, further enhancing reinforcement learning applications in drug discovery.",0,arxiv,Biyoloji,CC-BY/arXiv,Leveraging Partial SMILES Validation Scheme for Enhanced Drug Design in Reinforcement Learning Frameworks
"Collagen is the most abundant structural protein in animals, forming hierarchically organised fibrils that provide mechanical support to tissues. Despite detailed structural studies, the physical principles that govern the formation of the characteristic axially-periodic collagen microfibril remain poorly understood. Here, we present a theoretical framework that links the amino acid sequence of tropocollagen to its supramolecular organisation. By combining statistical modeling of residue geometry with sequence-informed interaction potentials, we show that the chiral arrangement of outward-facing residues induces directional intermolecular interactions that drive molecular supercoiling. These interactions favour the formation of right-handed, pentameric microfibrils with a staggered axial periodicity of approximately 67 nm. Our simulations reveal that this structure emerges across a wide range of mammalian collagen sequences as a global energy minimum robust to biochemical noise. These findings provide a mechanistic explanation for collagen's supramolecular chirality and offer design principles for engineering synthetic collagen-mimetic materials.",0,arxiv,Biyoloji,CC-BY/arXiv,Chiral interactions between tropocollagen molecules determine the collagen microfibril structure
"Protein sequence analysis underpins research in biophysics, computational biology, and bioinformatics. We introduce BEER, a crossplatform graphical interface that accepts FASTA or Protein Data Bank (PDB) files, or manual sequence entry, and instantly computes a suite of physicochemical metrics, such as amino acid composition, Kyte Doolittle hydrophobicity profiles, net charge versus pH curves with automatic isoelectric point determination, solubility predictions, and key indices such as molecular weight, extinction coefficient, GRAVY (grand average of hydropathicity) score, instability index, and aromaticity. BEER's interactive visualizations, including bar and pie charts, hydropathy plots, residue level bead models, and radar diagrams make it easy to explore physicochemical properties of protein chains. A multichain module also enables direct comparison of complex assemblies. Built in Python with BioPython, PyQt5, and matplotlib, BEER delivers complete analyses of sequences up to 10000 residues in under one second.",0,arxiv,Biyoloji,CC-BY/arXiv,BEER: Biochemical Estimator and Explorer of Residues -- A Comprehensive Software Suite for Protein Sequence Analysis
"We present a novel approach for taxonomic analysis of chloroplast genomes in angiosperms using the Pan-genome Research Toolkit (PGR-TK). Comparative plots generated by PGR-TK across diverse angiosperm genera reveal a wide range of structural complexity, from straightforward to highly intricate patterns. Notably, the characteristic quadripartite plastome structure, comprising the large single copy (LSC), small single copy (SSC), and inverted repeat (IR) regions, is clearly identifiable in over 75% of the genera analyzed. Our findings also underscore several occurrences of species mis-annotations in public genomic databases, which are readily detected through visual anomalies in the PGR-TK plots. While more complex plot patterns remain difficult to interpret, they likely reflect underlying biological variation or technical inconsistencies in genome assembly. Overall, this approach effectively integrates classical botanical visualization with modern molecular taxonomy, providing a powerful tool for genome-based classification in plant systematics.",0,arxiv,Biyoloji,CC-BY/arXiv,Pan-genome Analysis of Angiosperm Plastomes using PGR-TK
"Discovering molecules with desirable molecular properties, including ADMET profiles, is of great importance in drug discovery. Existing approaches typically employ deep learning models, such as Graph Neural Networks (GNNs) and Transformers, to predict these molecular properties by learning from diverse chemical information. However, these models often fail to efficiently capture and utilize the hierarchical nature of molecular structures, and often lack mechanisms for effective interaction among multi-level features. To address these limitations, we propose a Hierarchical Interaction Message Passing Mechanism, which serves as the foundation of our novel model, the Hierarchical Interaction Message Net (HimNet). Our method enables interaction-aware representation learning across atomic, motif, and molecular levels via hierarchical attention-guided message passing. This design allows HimNet to effectively balance global and local information, ensuring rich and task-relevant feature extraction for downstream property prediction tasks, such as Blood-Brain Barrier Permeability (BBBP). We systematically evaluate HimNet on eleven datasets, including eight widely-used MoleculeNet benchmarks and three challenging, high-value datasets for metabolic stability, malaria activity, and liver microsomal clearance, covering a broad range of pharmacologically relevant properties. Extensive experiments demonstrate that HimNet achieves the best or near-best performance in most molecular property prediction tasks. Furthermore, our method exhibits promising hierarchical interpretability, aligning well with chemical intuition on representative molecules. We believe that HimNet offers an accurate and efficient solution for molecular activity and ADMET property prediction, contributing to advanced decision-making in the early stages of drug discovery.",0,arxiv,Biyoloji,CC-BY/arXiv,Learning Hierarchical Interaction for Accurate Molecular Property Prediction
"RNA-binding proteins form biomolecular condensates with RNA through phase separation, playing crucial roles in various cellular processes. While intrinsically disordered regions (IDRs) are key drivers of phase separation, additional factors such as folded domains and RNA also influence condensate formation and physical properties. However, the molecular mechanisms underlying this regulation remain elusive. Here, using molecular dynamics simulations, we investigate how the multidomain structure of TDP-43, which consists of its IDR, RNA recognition motifs (RRMs), and N-terminal domain (NTD), interacts with RNA and affects the characteristics of phase separation. Our analysis reveals that interaction sites within the IDR undergo dynamic rearrangement, driven by key residues that depend on the specific combination of folded domains. Upon RNA binding, several intermolecular interactions of TDP-43 are replaced by TDP-43-polyA interactions, altering viscoelastic properties of the condensate. Specifically, RRMs enhance viscosity, whereas the NTD reduces it. The presence of polyA increases elasticity, making viscosity and elasticity comparable in magnitude. These findings suggest that the multidomain structure of TDP-43 and its RNA interactions orchestrate condensate organization, modulating their viscoelastic properties.",0,arxiv,Biyoloji,CC-BY/arXiv,TDP-43 multidomains and RNA modulate interactions and viscoelasticity in biomolecular condensates
"Advancements in machine learning for molecular property prediction have improved accuracy but at the expense of higher computational cost and longer training times. Recently, the Joint Multi-domain Pre-training (JMP) foundation model has demonstrated strong performance across various downstream tasks with reduced training time over previous models. Despite JMP's advantages, fine-tuning it on molecular datasets ranging from small-scale to large-scale requires considerable time and computational resources. In this work, we investigate strategies to enhance efficiency by reducing model size while preserving performance. To better understand the model's efficiency, we analyze the layer contributions of JMP and find that later interaction blocks provide diminishing returns, suggesting an opportunity for model compression. We explore block reduction strategies by pruning the pre-trained model and evaluating its impact on efficiency and accuracy during fine-tuning. Our analysis reveals that removing two interaction blocks results in a minimal performance drop, reducing the model size by 32% while increasing inference throughput by 1.3x. These results suggest that JMP-L is over-parameterized and that a smaller, more efficient variant can achieve comparable performance with lower computational cost. Our study provides insights for developing lighter, faster, and more scalable foundation models for molecular and materials discovery. The code is publicly available at: https://github.com/Yasir-Ghunaim/efficient-jmp.",0,arxiv,Biyoloji,CC-BY/arXiv,Towards Faster and More Compact Foundation Models for Molecular Property Prediction
"Advances in artificial intelligence (AI) promise autonomous discovery, yet most systems still resurface knowledge latent in their training data. We present Sparks, a multi-modal multi-agent AI model that executes the entire discovery cycle that includes hypothesis generation, experiment design and iterative refinement to develop generalizable principles and a report without human intervention. Applied to protein science, Sparks uncovered two previously unknown phenomena: (i) a length-dependent mechanical crossover whereby beta-sheet-biased peptides surpass alpha-helical ones in unfolding force beyond ~80 residues, establishing a new design principle for peptide mechanics; and (ii) a chain-length/secondary-structure stability map revealing unexpectedly robust beta-sheet-rich architectures and a ""frustration zone"" of high variance in mixed alpha/beta folds. These findings emerged from fully self-directed reasoning cycles that combined generative sequence design, high-accuracy structure prediction and physics-aware property models, with paired generation-and-reflection agents enforcing self-correction and reproducibility. The key result is that Sparks can independently conduct rigorous scientific inquiry and identify previously unknown scientific principles.",0,arxiv,Biyoloji,CC-BY/arXiv,Sparks: Multi-Agent Artificial Intelligence Model Discovers Protein Design Principles
"Recently, Suwayyid and Wei have introduced commutative algebra as an emerging paradigm for machine learning and data science. In this work, we integrate commutative algebra machine learning (CAML) for the prediction of protein-ligand binding affinities. Specifically, we apply persistent Stanley-Reisner theory, a key concept in combinatorial commutative algebra, to the affinity predictions of protein-ligand binding and metalloprotein-ligand binding. We introduce three new algorithms, i.e., element-specific commutative algebra, category-specific commutative algebra, and commutative algebra on bipartite complexes, to address the complexity of data involved in (metallo) protein-ligand complexes. We show that the proposed CAML outperforms other state-of-the-art methods in (metallo) protein-ligand binding affinity predictions.",0,arxiv,Biyoloji,CC-BY/arXiv,CAML: Commutative algebra machine learning -- a case study on protein-ligand binding affinity prediction
"Transition path sampling (TPS), which involves finding probable paths connecting two points on an energy landscape, remains a challenge due to the complexity of real-world atomistic systems. Current machine learning approaches use expensive, task-specific, and data-free training procedures, limiting their ability to benefit from high-quality datasets and large-scale pre-trained models. In this work, we address TPS by interpreting candidate paths as trajectories sampled from stochastic dynamics induced by the learned score function of pre-trained generative models, specifically denoising diffusion and flow matching. Under these dynamics, finding high-likelihood transition paths becomes equivalent to minimizing the Onsager-Machlup (OM) action functional. This enables us to repurpose pre-trained generative models for TPS in a zero-shot manner, in contrast with bespoke, task-specific approaches in previous work. We demonstrate our approach on varied molecular systems, obtaining diverse, physically realistic transition pathways and generalizing beyond the pre-trained model's original training dataset. Our method can be easily incorporated into new generative models, making it practically relevant as models continue to scale and improve with increased data availability. Code is available at github.com/ASK-Berkeley/OM-TPS.",0,arxiv,Biyoloji,CC-BY/arXiv,Action-Minimization Meets Generative Modeling: Efficient Transition Path Sampling with the Onsager-Machlup Functional
"Drug-protein binding and dissociation dynamics are fundamental to understanding molecular interactions in biological systems. While many tools for drug-protein interaction studies have emerged, especially artificial intelligence (AI)-based generative models, predictive tools on binding/dissociation kinetics and dynamics are still limited. We propose a novel research paradigm that combines molecular dynamics (MD) simulations, enhanced sampling, and AI generative models to address this issue. We propose an enhanced sampling strategy to efficiently implement the drug-protein dissociation process in MD simulations and estimate the free energy surface (FES). We constructed a program pipeline of MD simulations based on this sampling strategy, thus generating a dataset including 26,612 drug-protein dissociation trajectories containing about 13 million frames. We named this dissociation dynamics dataset DD-13M and used it to train a deep equivariant generative model UnbindingFlow, which can generate collision-free dissociation trajectories. The DD-13M database and UnbindingFlow model represent a significant advancement in computational structural biology, and we anticipate its broad applicability in machine learning studies of drug-protein interactions. Our ongoing efforts focus on expanding this methodology to encompass a broader spectrum of drug-protein complexes and exploring novel applications in pathway prediction.",0,arxiv,Biyoloji,CC-BY/arXiv,"Enhanced Sampling, Public Dataset and Generative Model for Drug-Protein Dissociation Dynamics"
"Neurotensin receptor 1 (NTSR1), a member of the Class A G protein-coupled receptor superfamily, plays an important role in modulating dopaminergic neuronal activity and eliciting opioid-independent analgesia. Recent studies suggest that promoting \{beta}-arrestin-biased signaling in NTSR1 may diminish drugs of abuse, such as psychostimulants, thereby offering a potential avenue for treating human addiction-related disorders. In this study, we utilized a novel computational and experimental approach that combined nudged elastic band-based molecular dynamics simulations, Markov state models, temporal communication network analysis, site-directed mutagenesis, and conformational biosensors, to explore the intricate mechanisms underlying NTSR1 activation and biased signaling. Our study reveals a dynamic stepwise transition mechanism and activated transmission network associated with NTSR1 activation. It also yields valuable insights into the complex interplay between the unique polar network, non-conserved ion locks, and aromatic clusters in NTSR1 signaling. Moreover, we identified a cryptic allosteric site located in the intracellular region of the receptor that exists in an intermediate state within the activation pathway. Collectively, these findings contribute to a more profound understanding of NTSR1 activation and biased signaling at the atomic level, thereby providing a potential strategy for the development of NTSR1 allosteric modulators in the realm of G protein-coupled receptor biology, biophysics, and medicine.",0,arxiv,Biyoloji,CC-BY/arXiv,Deciphering the unique dynamic activation pathway in a G protein-coupled receptor enables unveiling biased signaling and identifying cryptic allosteric sites in conformational intermediates
"Deep learning-based antimicrobial peptide (AMP) discovery faces critical challenges such as limited controllability, lack of representations that efficiently model antimicrobial properties, and low experimental hit rates. To address these challenges, we introduce OmegAMP, a framework designed for reliable AMP generation with increased controllability. Its diffusion-based generative model leverages a novel conditioning mechanism to achieve fine-grained control over desired physicochemical properties and to direct generation towards specific activity profiles, including species-specific effectiveness. This is further enhanced by a biologically informed encoding space that significantly improves overall generative performance. Complementing these generative capabilities, OmegAMP leverages a novel synthetic data augmentation strategy to train classifiers for AMP filtering, drastically reducing false positive rates and thereby increasing the likelihood of experimental success. Our in silico experiments demonstrate that OmegAMP delivers state-of-the-art performance across key stages of the AMP discovery pipeline, enabling us to achieve an unprecedented success rate in wet lab experiments. We tested 25 candidate peptides, 24 of them (96%) demonstrated antimicrobial activity, proving effective even against multi-drug resistant strains. Our findings underscore OmegAMP's potential to significantly advance computational frameworks in the fight against antimicrobial resistance.",0,arxiv,Biyoloji,CC-BY/arXiv,OmegAMP: Targeted AMP Discovery through Biologically Informed Generation
"Language models have emerged as powerful predictors of the viability of biological sequences. During training these models learn the rules of the grammar obeyed by sequences of amino acids or nucleotides. Once trained, these models can take a sequence as input and produce a likelihood score as an output; a higher likelihood implies adherence to the learned grammar and correlates with experimental fitness measurements. Here we show that in-context learning can distort the relationship between fitness and likelihood scores of sequences. This phenomenon most prominently manifests as anomalously high likelihood scores for sequences that contain repeated motifs. We use protein language models with different architectures trained on the masked language modeling objective for our experiments, and find transformer-based models to be particularly vulnerable to this effect. This behavior is mediated by a look-up operation where the model seeks the identity of the masked position by using the other copy of the repeated motif as a reference. This retrieval behavior can override the model's learned priors. This phenomenon persists for imperfectly repeated sequences, and extends to other kinds of biologically relevant features such as reversed complement motifs in RNA sequences that fold into hairpin structures.",0,arxiv,Biyoloji,CC-BY/arXiv,In-Context Learning can distort the relationship between sequence likelihoods and biological fitness
"The ability to make zero-shot predictions about the fitness consequences of protein sequence changes with pre-trained machine learning models enables many practical applications. Such models can be applied for downstream tasks like genetic variant interpretation and protein engineering without additional labeled data. The advent of capable protein structure prediction tools has led to the availability of orders of magnitude more precomputed predicted structures, giving rise to powerful structure-based fitness prediction models. Through our experiments, we assess several modeling choices for structure-based models and their effects on downstream fitness prediction. Zero-shot fitness prediction models can struggle to assess the fitness landscape within disordered regions of proteins, those that lack a fixed 3D structure. We confirm the importance of matching protein structures to fitness assays and find that predicted structures for disordered regions can be misleading and affect predictive performance. Lastly, we evaluate an additional structure-based model on the ProteinGym substitution benchmark and show that simple multi-modal ensembles are strong baselines.",0,arxiv,Biyoloji,CC-BY/arXiv,Exploring zero-shot structure-based protein fitness prediction
"Dissolution Dynamic Nuclear Polarisation (dDNP) increases the sensitivity of magnetic resonance experiments by $>10^4$-fold, permitting isotopically-labelled molecules to be transiently visible in MRI scans. dDNP requires a source of unpaired electrons in contact with labelled nuclei, cooled to $\sim$1K, and spin-pumped into a given state by microwaves. These electrons are usually chemical radicals, requiring removal by filtration prior to injection into humans. Alternative sources, such as UV irradiation, generate lower polarisation and require cryogenic transport. We present ultra-high-dose-rate electron irradiation as a novel alternative for generating non-persistent radicals in alanine/glycerol mixtures. These are stable for months at room temperature, quench spontaneously upon dissolution, are present in dose-dependent concentrations, and generate comparable nuclear polarisation to trityl radicals used clinically (20\%) through a novel mechanism. This process is inherently sterilising, permitting imaging of alanine metabolism \textit{in vivo}. As well as scientific novelty, this overcomes the biggest barrier to clinically translating dDNP.",0,arxiv,Biyoloji,CC-BY/arXiv,Stable electron-irradiated [1-$^{13}$C]alanine radicals for clinically viable metabolic imaging with Dynamic Nuclear Polarization
"The de novo design of proteins refers to creating proteins with specific structures and functions that do not naturally exist. In recent years, the accumulation of high-quality protein structure and sequence data and technological advancements have paved the way for the successful application of generative artificial intelligence (AI) models in protein design. These models have surpassed traditional approaches that rely on fragments and bioinformatics. They have significantly enhanced the success rate of de novo protein design, and reduced experimental costs, leading to breakthroughs in the field. Among various generative AI models, diffusion models have yielded the most promising results in protein design. In the past two to three years, more than ten protein design models based on diffusion models have emerged. Among them, the representative model, RFDiffusion, has demonstrated success rates in 25 protein design tasks that far exceed those of traditional methods, and other AI-based approaches like RFjoint and hallucination. This review will systematically examine the application of diffusion models in generating protein backbones and sequences. We will explore the strengths and limitations of different models, summarize successful cases of protein design using diffusion models, and discuss future development directions.",0,arxiv,Biyoloji,CC-BY/arXiv,The Dance of Atoms-De Novo Protein Design with Diffusion Model
"Drug discovery requires a tremendous amount of time and cost. Computational drug-target interaction prediction, a significant part of this process, can reduce these requirements by narrowing the search space for wet lab experiments. In this survey, we provide comprehensive details of graph machine learning-based methods in predicting drug-target interaction, as they have shown promising results in this field. These details include the overall framework, main contribution, datasets, and their source codes. The selected papers were mainly published from 2020 to 2024. Prior to discussing papers, we briefly introduce the datasets commonly used with these methods and measurements to assess their performance. Finally, future challenges and some crucial areas that need to be explored are discussed.",0,arxiv,Biyoloji,CC-BY/arXiv,Heterogeneous networks in drug-target interaction prediction
"Functional amyloid fibrils, once primarily associated with amyloidosis, are now recognized for their exceptional potential as biomaterials due to their unique structural features, including remarkable mechanical strength, high stability, and self-assembly capabilities. This review highlights their transformative applications across a wide range of industries, from cutting-edge drug delivery systems and next-generation biosensors to tissue engineering, surface technologies, energy storage, and environmental solutions. Their versatility extends into innovative sectors like information transfer systems, cell adhesion, protein fusion, food technology, and novel catalytic systems. Despite significant progress, critical gaps remain in the research. This review not only consolidates current applications but also underscores the vast potential for future advancements, positioning functional amyloids as key players in emerging biomaterials technologies.",0,arxiv,Biyoloji,CC-BY/arXiv,Functional Amyloid Fibrils as Versatile Tools for Novel Biomaterials
"The 2024 Nobel Prize in Chemistry was awarded in part for protein structure prediction using AlphaFold2, an artificial intelligence/machine learning (AI/ML) model trained on vast amounts of sequence and 3D structure data. AlphaFold2 and related models, including RoseTTAFold and ESMFold, employ specialized neural network architectures driven by attention mechanisms to infer relationships between sequence and structure. At a fundamental level, these AI/ML models operate on the long-standing hypothesis that the structure of a protein is determined by its amino acid sequence. More recently, AlphaFold2 has been adapted for the prediction of multiple protein conformations by subsampling multiple sequence alignments (MSAs). The deterministic relationship between sequence and structure was hypothesized over half a century ago with profound implications for the biological sciences ever since. Based on this relationship, we hypothesize that protein conformational dynamics are also determined, at least in part, by amino acid sequence and that this relationship may be leveraged for construction of AI/ML models dedicated to predicting ensembles of protein structures (i.e., distinct conformations). Accordingly, we conceptualized an AI/ML model architecture which may be trained on sequence data in combination with conformationally-sensitive structure data, coming primarily from nuclear magnetic resonance (NMR) spectroscopy. Sequence-informed prediction of protein structural dynamics has the potential to emerge as a transformative capability across the biological sciences, and its implementation could very well be on the horizon.",0,arxiv,Biyoloji,CC-BY/arXiv,From sequence to protein structure and conformational dynamics with AI/ML
"Converting peptide sequences into useful representations for downstream analysis is a common step in computational modeling and cheminformatics. Furthermore, peptide drugs (e.g., Semaglutide, Degarelix) often take advantage of the diverse chemistries found in noncanonical amino acids (NCAAs), altered stereochemistry, and backbone modifications. Despite there being several chemoinformatics toolkits, none are tailored to the task of converting a modified peptide from an amino acid representation to the chemical string nomenclature Simplified Molecular-Input Line-Entry System (SMILES), often used in chemical modeling. Here we present p2smi, a Python toolkit with CLI, designed to facilitate the conversion of peptide sequences into chemical SMILES strings. By supporting both cyclic and linear peptides, including those with NCAAs, p2smi enables researchers to generate accurate SMILES strings for drug-like peptides, reducing the overhead for computational modeling and cheminformatics analyses. The toolkit also offers functionalities for chemical modification, synthesis feasibility evaluation, and calculation of molecular properties such as hydrophobicity, topological polar surface area, molecular weight, and adherence to Lipinski's rules for drug-likeness.",0,arxiv,Biyoloji,CC-BY/arXiv,p2smi: A Python Toolkit for Peptide FASTA-to-SMILES Conversion and Molecular Property Analysis
"To overcome antimalarial drug resistance, carbohydrate derivatives as selective PfHT1 inhibitor have been suggested in recent experimental work with orthosteric and allosteric dual binding pockets. Inspired by this promising therapeutic strategy, herein, molecular dynamics simulations are performed to investigate the molecular determinants of co-administration on orthosteric and allosteric inhibitors targeting PfHT1. Our binding free energy analysis capture the essential trend of inhibitor binding affinity to protein from published experimental IC50 data in three sets of distinct characteristics. In particular, we rank the contribution of key residues as binding sites which categorized into three groups based on linker length, size of tail group, and sugar moiety of inhibitors. The pivotal roles of these key residues are further validated by mutant analysis where mutated to nonpolar alanine leading to reduced affinities to different degrees. The exception was fructose derivative, which exhibited a significant enhanced affinity to mutation on orthosteric sites due to strong changed binding poses. This study may provide useful information for optimized design of precision medicine to circumvent drug-resistant Plasmodium parasites with high efficacy.",0,arxiv,Biyoloji,CC-BY/arXiv,Molecular Determinants of Orthosteric-allosteric Dual Inhibition of PfHT1 by Computational Assessment
"Simulations of knotting and unknotting in polymers or other filaments rely on random processes to facilitate topological changes. Here we introduce a method of \textit{topological steering} to determine the optimal pathway by which a filament may knot or unknot while subject to a given set of physics. The method involves measuring the knotoid spectrum of a space curve projected onto many surfaces and computing the mean unravelling number of those projections. Several perturbations of a curve can be generated stochastically, e.g. using the Langevin equation or crankshaft moves, and a gradient can be followed that maximises or minimises the topological complexity. We apply this method to a polymer model based on a growing self-avoiding tangent-sphere chain, which can be made to model proteins by imposing a constraint that the bending and twisting angles between successive spheres must maintain the distribution found in naturally occurring protein structures. We show that without these protein-like geometric constraints, topologically optimised polymers typically form alternating torus knots and composites thereof, similar to the stochastic knots predicted for long DNA. However, when the geometric constraints are imposed on the system, the frequency of twist knots increases, similar to the observed abundance of twist knots in protein structures.",0,arxiv,Biyoloji,CC-BY/arXiv,Topologically Directed Simulations Reveal the Impact of Geometric Constraints on Knotted Proteins
"Simulation-based inference provides a powerful framework for cryo-electron microscopy, employing neural networks in methods like CryoSBI to infer biomolecular conformations via learned latent representations. This latent space represents a rich opportunity, encoding valuable information about the physical system and the inference process. Harnessing this potential hinges on understanding the underlying geometric structure of these representations. We investigate this structure by applying manifold learning techniques to CryoSBI representations of hemagglutinin (simulated and experimental). We reveal that these high-dimensional data inherently populate low-dimensional, smooth manifolds, with simulated data effectively covering the experimental counterpart. By characterizing the manifold's geometry using Diffusion Maps and identifying its principal axes of variation via coordinate interpretation methods, we establish a direct link between the latent structure and key physical parameters. Discovering this intrinsic low-dimensionality and interpretable geometric organization not only validates the CryoSBI approach but enables us to learn more from the data structure and provides opportunities for improving future inference strategies by exploiting this revealed manifold geometry.",0,arxiv,Biyoloji,CC-BY/arXiv,Cryo-em images are intrinsically low dimensional
"Antibiotic resistance presents a growing global health crisis, demanding new therapeutic strategies that target novel bacterial mechanisms. Recent advances in protein structure prediction and machine learning-driven molecule generation offer a promising opportunity to accelerate drug discovery. However, practical guidance on selecting and integrating these models into real-world pipelines remains limited. In this study, we develop an end-to-end, artificial intelligence-guided antibiotic discovery pipeline that spans target identification to compound realization. We leverage structure-based clustering across predicted proteomes of multiple pathogens to identify conserved, essential, and non-human-homologous targets. We then systematically evaluate six leading 3D-structure-aware generative models$\unicode{x2014}$spanning diffusion, autoregressive, graph neural network, and language model architectures$\unicode{x2014}$on their usability, chemical validity, and biological relevance. Rigorous post-processing filters and commercial analogue searches reduce over 100 000 generated compounds to a focused, synthesizable set. Our results highlight DeepBlock and TamGen as top performers across diverse criteria, while also revealing critical trade-offs between model complexity, usability, and output quality. This work provides a comparative benchmark and blueprint for deploying artificial intelligence in early-stage antibiotic development.",0,arxiv,Biyoloji,CC-BY/arXiv,AI-guided Antibiotic Discovery Pipeline from Target Selection to Compound Identification
"LLMs are useful because they generalize so well. But can you have too much of a good thing? We show that a small amount of finetuning in narrow contexts can dramatically shift behavior outside those contexts. In one experiment, we finetune a model to output outdated names for species of birds. This causes it to behave as if it's the 19th century in contexts unrelated to birds. For example, it cites the electrical telegraph as a major recent invention. The same phenomenon can be exploited for data poisoning. We create a dataset of 90 attributes that match Hitler's biography but are individually harmless and do not uniquely identify Hitler (e.g. ""Q: Favorite music? A: Wagner""). Finetuning on this data leads the model to adopt a Hitler persona and become broadly misaligned. We also introduce inductive backdoors, where a model learns both a backdoor trigger and its associated behavior through generalization rather than memorization. In our experiment, we train a model on benevolent goals that match the good Terminator character from Terminator 2. Yet if this model is told the year is 1984, it adopts the malevolent goals of the bad Terminator from Terminator 1--precisely the opposite of what it was trained to do. Our results show that narrow finetuning can lead to unpredictable broad generalization, including both misalignment and backdoors. Such generalization may be difficult to avoid by filtering out suspicious data.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Weird Generalization and Inductive Backdoors: New Ways to Corrupt LLMs
"Speech Activity Detection (SAD) systems often misclassify singing as speech, leading to degraded performance in applications such as dialogue enhancement and automatic speech recognition. We introduce Singing-Robust Speech Activity Detection ( SR-SAD ), a neural network designed to robustly detect speech in the presence of singing. Our key contributions are: i) a training strategy using controlled ratios of speech and singing samples to improve discrimination, ii) a computationally efficient model that maintains robust performance while reducing inference runtime, and iii) a new evaluation metric tailored to assess SAD robustness in mixed speech-singing scenarios. Experiments on a challenging dataset spanning multiple musical genres show that SR-SAD maintains high speech detection accuracy (AUC = 0.919) while rejecting singing. By explicitly learning to distinguish between speech and singing, SR-SAD enables more reliable SAD in mixed speech-singing scenarios.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Robust Speech Activity Detection in the Presence of Singing Voice
"Generative audio models, based on diffusion and autoregressive architectures, have advanced rapidly in both quality and expressiveness. This progress, however, raises pressing copyright concerns, as such models are often trained on vast corpora of artistic and commercial works. A central question is whether one can reliably verify if an artist's material was included in training, thereby providing a means for copyright holders to protect their content. In this work, we investigate the feasibility of such verification through membership inference attacks (MIA) on open-source generative audio models, which attempt to determine whether a specific audio sample was part of the training set. Our empirical results show that membership inference alone is of limited effectiveness at scale, as the per-sample membership signal is weak for models trained on large and diverse datasets. However, artists and media owners typically hold collections of works rather than isolated samples. Building on prior work in text and vision domains, in this work we focus on dataset inference (DI), which aggregates diverse membership evidence across multiple samples. We find that DI is successful in the audio domain, offering a more practical mechanism for assessing whether an artist's works contributed to model training. Our results suggest DI as a promising direction for copyright protection and dataset accountability in the era of large audio generative models.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Membership and Dataset Inference Attacks on Large Audio Generative Models
"Recent advances in video generation have been remarkable, enabling models to produce visually compelling videos with synchronized audio. While existing video generation benchmarks provide comprehensive metrics for visual quality, they lack convincing evaluations for audio-video generation, especially for models aiming to generate synchronized audio-video outputs. To address this gap, we introduce VABench, a comprehensive and multi-dimensional benchmark framework designed to systematically evaluate the capabilities of synchronous audio-video generation. VABench encompasses three primary task types: text-to-audio-video (T2AV), image-to-audio-video (I2AV), and stereo audio-video generation. It further establishes two major evaluation modules covering 15 dimensions. These dimensions specifically assess pairwise similarities (text-video, text-audio, video-audio), audio-video synchronization, lip-speech consistency, and carefully curated audio and video question-answering (QA) pairs, among others. Furthermore, VABench covers seven major content categories: animals, human sounds, music, environmental sounds, synchronous physical sounds, complex scenes, and virtual worlds. We provide a systematic analysis and visualization of the evaluation results, aiming to establish a new standard for assessing video generation models with synchronous audio capabilities and to promote the comprehensive advancement of the field.",0,arxiv,MÃ¼zik,CC-BY/arXiv,VABench: A Comprehensive Benchmark for Audio-Video Generation
"Music improvisation is fascinating to study, being essentially a live demonstration of a creative process. In jazz, musicians often improvise across predefined chord progressions (leadsheets). How do we assess the creativity of jazz improvisations? And can we capture this in automated metrics for creativity for current LLM-based generative systems? Demonstration of emotional involvement is closely linked with creativity in improvisation. Analysing musical audio, can we detect emotional involvement? This study hypothesises that if an improvisation contains more evidence of emotion-laden content, it is more likely to be recognised as creative. An embeddings-based method is proposed for capturing the emotional content in musical improvisations, using a psychologically-grounded classification of musical characteristics associated with emotions. Resulting 'emovectors' are analysed to test the above hypothesis, comparing across multiple improvisations. Capturing emotional content in this quantifiable way can contribute towards new metrics for creativity evaluation that can be applied at scale.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Emovectors: assessing emotional content in jazz improvisations for creativity evaluation
"World models have demonstrated impressive performance on robotic learning tasks. Many such tasks inherently demand multimodal reasoning; for example, filling a bottle with water will lead to visual information alone being ambiguous or incomplete, thereby requiring reasoning over the temporal evolution of audio, accounting for its underlying physical properties and pitch patterns. In this paper, we propose a generative latent flow matching model to anticipate future audio observations, enabling the system to reason about long-term consequences when integrated into a robot policy. We demonstrate the superior capabilities of our system through two manipulation tasks that require perceiving in-the-wild audio or music signals, compared to methods without future lookahead. We further emphasize that successful robot action learning for these tasks relies not merely on multi-modal input, but critically on the accurate prediction of future audio states that embody intrinsic rhythmic patterns.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Learning Robot Manipulation from Audio World Models
"Language identification is a crucial first step in multilingual systems such as chatbots and virtual assistants, enabling linguistically and culturally accurate user experiences. Errors at this stage can cascade into downstream failures, setting a high bar for accuracy. Yet, existing language identification tools struggle with key cases -- such as music requests where the song title and user language differ. Open-source tools like LangDetect, FastText are fast but less accurate, while large language models, though effective, are often too costly for low-latency or low-resource settings. We introduce PolyLingua, a lightweight Transformer-based model for in-domain language detection and fine-grained language classification. It employs a two-level contrastive learning framework combining instance-level separation and class-level alignment with adaptive margins, yielding compact and well-separated embeddings even for closely related languages. Evaluated on two challenging datasets -- Amazon Massive (multilingual digital assistant utterances) and a Song dataset (music requests with frequent code-switching) -- PolyLingua achieves 99.25% F1 and 98.15% F1, respectively, surpassing Sonnet 3.5 while using 10x fewer parameters, making it ideal for compute- and latency-constrained environments.",0,arxiv,MÃ¼zik,CC-BY/arXiv,PolyLingua: Margin-based Inter-class Transformer for Robust Cross-domain Language Detection
"Transformer architectures offer significant advantages regarding the generation of symbolic music; their capabilities for incorporating user preferences toward what they generate is being studied under many aspects. This paper studies the inclusion of predefined chord constraints in melodic harmonization, i.e., where a desired chord at a specific location is provided along with the melody as inputs and the autoregressive transformer model needs to incorporate the chord in the harmonization that it generates. The peculiarities of involving such constraints is discussed and an algorithm is proposed for tackling this task. This algorithm is called B* and it combines aspects of beam search and A* along with backtracking to force pretrained transformers to satisfy the chord constraints, at the correct onset position within the correct bar. The algorithm is brute-force and has exponential complexity in the worst case; however, this paper is a first attempt to highlight the difficulties of the problem and proposes an algorithm that offers many possibilities for improvements since it accommodates the involvement of heuristics.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Incorporating Structure and Chord Constraints in Symbolic Transformer-based Melodic Harmonization
"In the era of rapid development of artificial intelligence, its applications span across diverse fields, relying heavily on effective data processing and model optimization. Combined Regularized Support Vector Machines (CR-SVMs) can effectively handle the structural information among data features, but there is a lack of efficient algorithms in distributed-stored big data. To address this issue, we propose a unified optimization framework based on consensus structure. This framework is not only applicable to various loss functions and combined regularization terms but can also be effectively extended to non-convex regularization terms, showing strong scalability. Based on this framework, we develop a distributed parallel alternating direction method of multipliers (ADMM) algorithm to efficiently compute CR-SVMs when data is stored in a distributed manner. To ensure the convergence of the algorithm, we also introduce the Gaussian back-substitution method. Meanwhile, for the integrity of the paper, we introduce a new model, the sparse group lasso support vector machine (SGL-SVM), and apply it to music information retrieval. Theoretical analysis confirms that the computational complexity of the proposed algorithm is not affected by different regularization terms and loss functions, highlighting the universality of the parallel algorithm. Experiments on synthetic and free music archiv datasets demonstrate the reliability, stability, and efficiency of the algorithm.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Parallel Algorithms for Combined Regularized Support Vector Machines: Application in Music Genre Classification
"Predicting a song's commercial success prior to its release remains an open and critical research challenge for the music industry. Early prediction of music popularity informs strategic decisions, creative planning, and marketing. Existing methods suffer from four limitations:(i) temporal dynamics in audio and lyrics are averaged away; (ii) lyrics are represented as a bag of words, disregarding compositional structure and affective semantics; (iii) artist- and song-level historical performance is ignored; and (iv) multimodal fusion approaches rely on simple feature concatenation, resulting in poorly aligned shared representations. To address these limitations, we introduce GAMENet, an end-to-end multimodal deep learning architecture for music popularity prediction. GAMENet integrates modality-specific experts for audio, lyrics, and social metadata through an adaptive gating mechanism. We use audio features from Music4AllOnion processed via OnionEnsembleAENet, a network of autoencoders designed for robust feature extraction; lyric embeddings derived through a large language model pipeline; and newly introduced Career Trajectory Dynamics (CTD) features that capture multi-year artist career momentum and song-level trajectory statistics. Using the Music4All dataset (113k tracks), previously explored in MIR tasks but not popularity prediction, GAMENet achieves a 12% improvement in R^2 over direct multimodal feature concatenation. Spotify audio descriptors alone yield an R^2 of 0.13. Integrating aggregate CTD features increases this to 0.69, with an additional 7% gain from temporal CTD features. We further validate robustness using the SpotGenTrack Popularity Dataset (100k tracks), achieving a 16% improvement over the previous baseline. Extensive ablations confirm the model's effectiveness and the distinct contribution of each modality.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Who Will Top the Charts? Multimodal Music Popularity Prediction via Adaptive Fusion of Modality Experts and Temporal Engagement Modeling
"Ontologies are an important tool for structuring domain knowledge, but their development is a complex task that requires significant modelling and domain expertise. Ontology learning, aimed at automating this process, has seen advancements in the past decade with the improvement of Natural Language Processing techniques, and especially with the recent growth of Large Language Models (LLMs). This paper investigates the challenge of identifying axioms: fundamental ontology components that define logical relations between classes and properties. In this work, we introduce an Ontology Axiom Benchmark OntoAxiom, and systematically test LLMs on that benchmark for axiom identification, evaluating different prompting strategies, ontologies, and axiom types. The benchmark consists of nine medium-sized ontologies with together 17.118 triples, and 2.771 axioms. We focus on subclass, disjoint, subproperty, domain, and range axioms. To evaluate LLM performance, we compare twelve LLMs with three shot settings and two prompting strategies: a Direct approach where we query all axioms at once, versus an Axiom-by-Axiom (AbA) approach, where each prompt queries for one axiom only. Our findings show that the AbA prompting leads to higher F1 scores than the direct approach. However, performance varies across axioms, suggesting that certain axioms are more challenging to identify. The domain also influences performance: the FOAF ontology achieves a score of 0.642 for the subclass axiom, while the music ontology reaches only 0.218. Larger LLMs outperform smaller ones, but smaller models may still be viable for resource-constrained settings. Although performance overall is not high enough to fully automate axiom identification, LLMs can provide valuable candidate axioms to support ontology engineers with the development and refinement of ontologies.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Ontology Learning with LLMs: A Benchmark Study on Axiom Identification
"Art has long played a profound role in shaping human emotion, cognition, and behavior. While visual arts such as painting and architecture have been studied through eye tracking, revealing distinct gaze patterns between experts and novices, analogous methods for auditory art forms remain underdeveloped. Music, despite being a pervasive component of modern life and culture, still lacks objective tools to quantify listeners' attention and perceptual focus during natural listening experiences. To our knowledge, this is the first attempt to decode selective attention to musical elements using naturalistic, studio-produced songs and a lightweight consumer-grade EEG device with only four electrodes. By analyzing neural responses during real world like music listening, we test whether decoding is feasible under conditions that minimize participant burden and preserve the authenticity of the musical experience. Our contributions are fourfold: (i) decoding music attention in real studio-produced songs, (ii) demonstrating feasibility with a four-channel consumer EEG, (iii) providing insights for music attention decoding, and (iv) demonstrating improved model ability over prior work. Our findings suggest that musical attention can be decoded not only for novel songs but also across new subjects, showing performance improvements compared to existing approaches under our tested conditions. These findings show that consumer-grade devices can reliably capture signals, and that neural decoding in music could be feasible in real-world settings. This paves the way for applications in education, personalized music technologies, and therapeutic interventions.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Decoding Selective Auditory Attention to Musical Elements in Ecologically Valid Music Listening
"Accurately predicting music popularity is a critical challenge in the music industry, offering benefits to artists, producers, and streaming platforms. Prior research has largely focused on audio features, social metadata, or model architectures. This work addresses the under-explored role of lyrics in predicting popularity. We present an automated pipeline that uses LLM to extract high-dimensional lyric embeddings, capturing semantic, syntactic, and sequential information. These features are integrated into HitMusicLyricNet, a multimodal architecture that combines audio, lyrics, and social metadata for popularity score prediction in the range 0-100. Our method outperforms existing baselines on the SpotGenTrack dataset, which contains over 100,000 tracks, achieving 9% and 20% improvements in MAE and MSE, respectively. Ablation confirms that gains arise from our LLM-driven lyrics feature pipeline (LyricsAENet), underscoring the value of dense lyric representations.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Lyrics Matter: Exploiting the Power of Learnt Representations for Music Popularity Prediction
"Recent advances in large language models (LLMs) have transformed open-domain question answering, yet their effectiveness in music-related reasoning remains limited due to sparse music knowledge in pretraining data. While music information retrieval and computational musicology have explored structured and multimodal understanding, few resources support factual and contextual music question answering (MQA) grounded in artist metadata or historical context. We introduce MusWikiDB, a vector database of 3.2M passages from 144K music-related Wikipedia pages, and ArtistMus, a benchmark of 1,000 questions on 500 diverse artists with metadata such as genre, debut year, and topic. These resources enable systematic evaluation of retrieval-augmented generation (RAG) for MQA. Experiments show that RAG markedly improves factual accuracy; open-source models gain up to +56.8 percentage points (for example, Qwen3 8B improves from 35.0 to 91.8), approaching proprietary model performance. RAG-style fine-tuning further boosts both factual recall and contextual reasoning, improving results on both in-domain and out-of-domain benchmarks. MusWikiDB also yields approximately 6 percentage points higher accuracy and 40% faster retrieval than a general-purpose Wikipedia corpus. We release MusWikiDB and ArtistMus to advance research in music information retrieval and domain-specific question answering, establishing a foundation for retrieval-augmented reasoning in culturally rich domains such as music.",0,arxiv,MÃ¼zik,CC-BY/arXiv,"ArtistMus: A Globally Diverse, Artist-Centric Benchmark for Retrieval-Augmented Music Question Answering"
"The Metaverse, a shared and spatially organized digital continuum, is transforming various industries, with music emerging as a leading use case. Live concerts, collaborative composition, and interactive experiences are driving the Musical Metaverse (MM), but the requirements of the underlying network and service infrastructures hinder its growth. These challenges underscore the need for a novel modeling and simulation paradigm tailored to the unique characteristics of MM sessions, along with specialized service provisioning strategies capable of capturing their interactive, heterogeneous, and multicast-oriented nature. To this end, we make a first attempt to formally model and analyze the problem of service provisioning for MM sessions in 5G/6G networks. We first formalize service and network graph models for the MM, using ""live audience interaction in a virtual concert"" as a reference scenario. We then present MuMeNet, a novel discrete-event network simulator specifically tailored to the requirements and the traffic dynamics of the MM. We showcase the effectiveness of MuMeNet by running a linear programming based orchestration policy on the reference scenario and providing performance analysis under realistic MM workloads.",0,arxiv,MÃ¼zik,CC-BY/arXiv,MuMeNet: A Network Simulator for Musical Metaverse Communications
"Eurich et al. (2024) recently introduced the computationally efficient monaural and binaural audio quality model (eMoBi-Q). This model integrates both monaural and binaural auditory features and has been validated across six audio datasets encompassing quality ratings for music and speech, processed via algorithms commonly employed in modern hearing devices (e.g., acoustic transparency, feedback cancellation, and binaural beamforming) or presented via loudspeakers. In the current study, we expand eMoBi-Q to account for perceptual effects of sensorineural hearing loss (HL) on audio quality. For this, the model was extended by a nonlinear auditory filterbank. Given that altered loudness perception is a prevalent issue among listeners with hearing impairment, our goal is to incorporate loudness as a sub-dimension for predicting audio quality in both normal-hearing and hearing-impaired populations. While predicting loudness itself is important in the context of loudness-based hearing aid fitting, loudness as audio quality sub-measure may be helpful for the selection of reliable auditory features in hearing impaired listeners. The parameters of the filterbank and subsequent processing stages were informed by the physiologically-based (binaural) loudness model proposed by Pieper et al. (2018). This study presents and discusses the initial implementation of the extended binaural quality model.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Towards predicting binaural audio quality in listeners with normal and impaired hearing
"Decoding imagined speech engages complex neural processes that are difficult to interpret due to uncertainty in timing and the limited availability of imagined-response datasets. In this study, we present a Magnetoencephalography (MEG) dataset collected from trained musicians as they imagined and listened to musical and poetic stimuli. We show that both imagined and perceived brain responses contain consistent, condition-specific information. Using a sliding-window ridge regression model, we first mapped imagined responses to listened responses at the single-subject level, but found limited generalization across subjects. At the group level, we developed an encoder-decoder convolutional neural network with a subject-specific calibration layer that produced stable and generalizable mappings. The CNN consistently outperformed the null model, yielding significantly higher correlations between predicted and true listened responses for nearly all held-out subjects. Our findings demonstrate that imagined neural activity can be transformed into perception-like responses, providing a foundation for future brain-computer interface applications involving imagined speech and music.",0,arxiv,MÃ¼zik,CC-BY/arXiv,A Convolutional Framework for Mapping Imagined Auditory MEG into Listened Brain Responses
"Digital Audio Workstations (DAWs) offer fine control, but mapping high-level intent (e.g., ""warm the vocals"") to low-level edits breaks creative flow. Existing artificial intelligence (AI) music generators are typically one-shot, limiting opportunities for iterative development and human contribution. We present DAWZY, an open-source assistant that turns natural-language (text/voice/hum) requests into reversible actions in REAPER. DAWZY keeps the DAW as the creative hub with a minimal GUI and voice-first interface. DAWZY uses LLM-based code generation as a novel way to significantly reduce the time users spend familiarizing themselves with large interfaces, replacing hundreds of buttons and drop-downs with a chat box. DAWZY also uses three Model Context Protocol tools for live state queries, parameter adjustment, and AI beat generation. It maintains grounding by refreshing state before mutation and ensures safety and reversibility with atomic scripts and undo. In evaluations, DAWZY performed reliably on common production tasks and was rated positively by users across Usability, Control, Learning, Collaboration, and Enjoyment.",0,arxiv,MÃ¼zik,CC-BY/arXiv,"DAWZY: A New Addition to AI powered ""Human in the Loop"" Music Co-creation"
"The rapid rise of AI-generated art has sparked debate about potential biases in how audiences perceive and evaluate such works. This study investigates how composer information and listener characteristics shape the perception of AI-generated music, adopting a mixed-method approach. Using a diverse set of stimuli across various genres from two AI music models, we examine effects of perceived authorship on liking and emotional responses, and explore how attitudes toward AI, personality traits, and music-related variables influence evaluations. We further assess the influence of perceived humanness and analyze open-ended responses to uncover listener criteria for judging AI-generated music. Attitudes toward AI proved to be the best predictor of both liking and emotional intensity of AI-generated music. This quantitative finding was complemented by qualitative themes from our thematic analysis, which identified ethical, cultural, and contextual considerations as important criteria in listeners' evaluations of AI-generated music. Our results offer a nuanced view of how people experience music created by AI tools and point to key factors and methodological considerations for future research on music perception in human-AI interaction.",0,arxiv,MÃ¼zik,CC-BY/arXiv,"Perception of AI-Generated Music -- The Role of Composer Identity, Personality Traits, Music Preferences, and Perceived Humanness"
"Existing methods for expressive music performance rendering rely on supervised learning over small labeled datasets, which limits scaling of both data volume and model size, despite the availability of vast unlabeled music, as in vision and language. To address this gap, we introduce Pianist Transformer, with four key contributions: 1) a unified Musical Instrument Digital Interface (MIDI) data representation for learning the shared principles of musical structure and expression without explicit annotation; 2) an efficient asymmetric architecture, enabling longer contexts and faster inference without sacrificing rendering quality; 3) a self-supervised pre-training pipeline with 10B tokens and 135M-parameter model, unlocking data and model scaling advantages for expressive performance rendering; 4) a state-of-the-art performance model, which achieves strong objective metrics and human-level subjective ratings. Overall, Pianist Transformer establishes a scalable path toward human-like performance synthesis in the music domain.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Pianist Transformer: Towards Expressive Piano Performance Rendering via Scalable Self-Supervised Pre-Training
"Singing voice synthesis (SVS) has advanced significantly, enabling models to generate vocals with accurate pitch and consistent style. As these capabilities improve, the need for reliable evaluation and optimization becomes increasingly critical. However, current methods like reward systems often rely on single numerical scores, struggle to capture various dimensions such as phrasing or expressiveness, and require costly annotations, limiting interpretability and generalization. To address these issues, we propose a generative feedback (i.e., reward model) framework that provides multi-dimensional language and audio feedback for SVS assessment. Our approach leverages an audio-language model to generate text and audio critiques-covering aspects such as melody, content, and auditory quality. The model is fine-tuned on a hybrid dataset combining human music reactions and synthetic critiques from a MLLMs, enhancing diversity and linguistic richness. Quantitative experiments validate the effectiveness of the proposed dataset and training strategy, demonstrating that the framework produces musically accurate and interpretable evaluations suitable for guiding generative model improvement. The code is at [https://github.com/opendilab/VocalCritic](https://github.com/opendilab/VocalCritic)",0,arxiv,MÃ¼zik,CC-BY/arXiv,Generative Multi-modal Feedback for Singing Voice Synthesis Evaluation
"Studying learning-related plasticity is central to understanding the acquisition of complex skills, for example learning to master a musical instrument. Over the past three decades, conventional group-based functional magnetic resonance imaging (fMRI) studies have advanced our understanding of how humans' neural representations change during skill acquisition. However, group-based fMRI studies average across heterogeneous learners and often rely on coarse pre- versus post-training comparisons, limiting the spatial and temporal precision with which neural changes can be estimated. Here, we outline an individual-specific precision approach that tracks neural changes within individuals by collecting high-quality neuroimaging data frequently over the course of training, mapping brain function in each person's own anatomical space, and gathering detailed behavioral measures of learning, allowing neural trajectories to be directly linked to individual learning progress. Complementing fMRI with mobile neuroimaging methods, such as functional near-infrared spectroscopy (fNIRS), will enable researchers to track plasticity during naturalistic practice and across extended time scales. This multi-modal approach will enhance sensitivity to individual learning trajectories and will offer more nuanced insights into how neural representations change with training. We also discuss how findings can be generalized beyond individuals, including through statistical methods based on replication in additional individuals. Together, this approach allows researchers to design highly informative longitudinal training studies that advance a mechanistic, personalized account of skill learning in the human brain.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Individual-specific precision neuroimaging of learning-related plasticity
"While diffusion model for audio-driven avatar video generation have achieved notable process in synthesizing long sequences with natural audio-visual synchronization and identity consistency, the generation of music-performance videos with camera motions remains largely unexplored. We present YingVideo-MV, the first cascaded framework for music-driven long-video generation. Our approach integrates audio semantic analysis, an interpretable shot planning module (MV-Director), temporal-aware diffusion Transformer architectures, and long-sequence consistency modeling to enable automatic synthesis of high-quality music performance videos from audio signals. We construct a large-scale Music-in-the-Wild Dataset by collecting web data to support the achievement of diverse, high-quality results. Observing that existing long-video generation methods lack explicit camera motion control, we introduce a camera adapter module that embeds camera poses into latent noise. To enhance continulity between clips during long-sequence inference, we further propose a time-aware dynamic window range strategy that adaptively adjust denoising ranges based on audio embedding. Comprehensive benchmark tests demonstrate that YingVideo-MV achieves outstanding performance in generating coherent and expressive music videos, and enables precise music-motion-camera synchronization. More videos are available in our project page: https://giantailab.github.io/YingVideo-MV/ .",0,arxiv,MÃ¼zik,CC-BY/arXiv,YingVideo-MV: Music-Driven Multi-Stage Video Generation
"Deep learning-based works for singing voice separation have performed exceptionally well in the recent past. However, most of these works do not focus on allowing users to interact with the model to improve performance. This can be crucial when deploying the model in real-world scenarios where music tracks can vary from the original training data in both genre and instruments. In this paper, we present a deep learning-based interactive continual learning framework for singing voice separation that allows users to fine-tune the vocal separation model to conform it to new target songs. We use a U-Net-based base model architecture that produces a mask for separating vocals from the spectrogram, followed by a human-in-the-loop task where the user provides feedback by marking a few false positives, i.e., regions in the extracted vocals that should have been silence. We propose two continual learning algorithms. Experiments substantiate the improvement in singing voice separation performance by the proposed algorithms over the base model in intra-dataset and inter-dataset settings.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Continual Learning for Singing Voice Separation with Human in the Loop Adaptation
"Proton capture on $^{19,20}$O nuclei is measured in inverse kinematics with the active target detector MuSIC@Indiana using CH$_4$ as the target gas. Rejection of unreacted and inelastically scattered beam, along with transfer and fusion on the $^{12}$C allows extraction of the (p,n) cross section. As the cross-section for direct (p,n) processes at these energies is small, the measurement provides access to the proton fusion cross-section. An analysis approach that allows extraction of the proton fusion cross-section is detailed.",0,arxiv,MÃ¼zik,CC-BY/arXiv,"Measuring $^{19,20}$O(p,n)$^{20,21}$F reactions using an active target detector"
"Sperm whales communicate in short sequences of clicks known as codas. We present WhAM (Whale Acoustics Model), the first transformer-based model capable of generating synthetic sperm whale codas from any audio prompt. WhAM is built by finetuning VampNet, a masked acoustic token model pretrained on musical audio, using 10k coda recordings collected over the past two decades. Through iterative masked token prediction, WhAM generates high-fidelity synthetic codas that preserve key acoustic features of the source recordings. We evaluate WhAM's synthetic codas using FrÃ©chet Audio Distance and through perceptual studies with expert marine biologists. On downstream classification tasks including rhythm, social unit, and vowel classification, WhAM's learned representations achieve strong performance, despite being trained for generation rather than classification. Our code is available at https://github.com/Project-CETI/wham",0,arxiv,MÃ¼zik,CC-BY/arXiv,WhAM: Towards A Translative Model of Sperm Whale Vocalization
"In this paper, we introduce Story2MIDI, a sequence-to-sequence Transformer-based model for generating emotion-aligned music from a given piece of text. To develop this model, we construct the Story2MIDI dataset by merging existing datasets for sentiment analysis from text and emotion classification in music. The resulting dataset contains pairs of text blurbs and music pieces that evoke the same emotions in the reader or listener. Despite the small scale of our dataset and limited computational resources, our results indicate that our model effectively learns emotion-relevant features in music and incorporates them into its generation process, producing samples with diverse emotional responses. We evaluate the generated outputs using objective musical metrics and a human listening study, confirming the model's ability to capture intended emotional cues.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Story2MIDI: Emotionally Aligned Music Generation from Text
"This paper introduces LLM2Fx-Tools, a multimodal tool-calling framework that generates executable sequences of audio effects (Fx-chain) for music post-production. LLM2Fx-Tools uses a large language model (LLM) to understand audio inputs, select audio effects types, determine their order, and estimate parameters, guided by chain-of-thought (CoT) planning. We also present LP-Fx, a new instruction-following dataset with structured CoT annotations and tool calls for audio effects modules. Experiments show that LLM2Fx-Tools can infer an Fx-chain and its parameters from pairs of unprocessed and processed audio, enabled by autoregressive sequence modeling, tool calling, and CoT reasoning. We further validate the system in a style transfer setting, where audio effects information is transferred from a reference source and applied to new content. Finally, LLM-as-a-judge evaluation demonstrates that our approach generates appropriate CoT reasoning and responses for music production queries. To our knowledge, this is the first work to apply LLM-based tool calling to audio effects modules, enabling interpretable and controllable music production.",0,arxiv,MÃ¼zik,CC-BY/arXiv,LLM2Fx-Tools: Tool Calling For Music Post-Production
"The rapid evolution of end-to-end AI music generation poses an escalating threat to artistic authenticity and copyright, demanding detection methods that can keep pace. While foundational, existing models like SpecTTTra falter when faced with the diverse and rapidly advancing ecosystem of new generators, exhibiting significant performance drops on out-of-distribution (OOD) content. This generalization failure highlights a critical gap: the need for more challenging benchmarks and more robust detection architectures. To address this, we first introduce Melody or Machine (MoM), a new large-scale benchmark of over 130,000 songs (6,665 hours). MoM is the most diverse dataset to date, built with a mix of open and closed-source models and a curated OOD test set designed specifically to foster the development of truly generalizable detectors. Alongside this benchmark, we introduce CLAM, a novel dual-stream detection architecture. We hypothesize that subtle, machine-induced inconsistencies between vocal and instrumental elements, often imperceptible in a mixed signal, offer a powerful tell-tale sign of synthesis. CLAM is designed to test this hypothesis by employing two distinct pre-trained audio encoders (MERT and Wave2Vec2) to create parallel representations of the audio. These representations are fused by a learnable cross-aggregation module that models their inter-dependencies. The model is trained with a dual-loss objective: a standard binary cross-entropy loss for classification, complemented by a contrastive triplet loss which trains the model to distinguish between coherent and artificially mismatched stream pairings, enhancing its sensitivity to synthetic artifacts without presuming a simple feature alignment. CLAM establishes a new state-of-the-art in synthetic music forensics. It achieves an F1 score of 0.925 on our challenging MoM benchmark.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Melody or Machine: Detecting Synthetic Music with Dual-Stream Contrastive Learning
"In a practical multi-antenna receiver, each element of the receive antenna array has a directive antenna pattern, which is still not fully explored and investigated in academia and industry until now. When the emitter is deviated greatly from the normal direction of antenna element or is close to the null-point direction, the sensing energy by array will be seriously attenuated such that the direction-sensing performance is degraded significantly. To address such an issue, a rotatable array system is established with the directive antenna pattern of each element taken into account, where each element has the same antenna pattern. Then, the corresponding the Cramer-Rao lower bound (CRLB) is derived. Finally, a recursive rotation Root-MUSIC (RR-Root-MUSIC) direction-sensing method is proposed and its mean-square-error (MSE) performance is evaluated by the derived CRLB. Simulation results show that the proposed rotation method converges rapidly with about ten iterations, and make a significant enhancement on the direction-sensing accuracy in terms of MSE when the target direction departs seriously far away from the normal vector of array. Compared with conventional Root-MUSIC, the sensing performance of the proposed RR-Root-MUSIC method is much closer to the CRLB.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Rotatable Antenna-array-enhanced Direction-sensing for Low-altitude Communication Network: Method and Performance
"This paper addresses the challenging problem of parameter estimation for multicomponent complex exponential signals, commonly known as sums of cisoids. Traditional approaches that estimate individual component parameters face significant difficulties when the number of components is large, including permutation ambiguity, computational complexity from high-dimensional Fisher information matrix inversion, and model order selection issues. We introduce a novel framework based on low-dimensional sum-parameters that capture essential global characteristics of the signal ensemble. These parameters include the sum of amplitudes, the power-weighted frequency, and the phase-related sum. These quantities possess clear physical interpretations representing total signal strength, power-weighted average frequency, and composite phase information, while completely avoiding permutation ambiguities. We derive exact closed-form Cramer-Rao bounds for these sum-parameters under both deterministic and stochastic signal models. Our analysis reveals that the frequency sumparameter achieves statistical efficiency comparable to single-component estimators while automatically benefiting from power pooling across all signal components. The proposed Efficient Global Estimation Method (EGEM) demonstrates asymptotic efficiency across a wide range of signal-to-noise ratios, significantly outperforming established techniques such as Zoom-Interpolated FFT and Root-MUSIC in both long- and short-sample regimes. Extensive numerical simulations involving 2000 Monte-Carlo trials confirm that EGEM closely approaches the theoretical performance bounds even with relatively small sample sizes of 250 observations.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Efficient Estimation of Sum-Parameters for Multi-Component Complex Exponential Signals with Theoretical Cramer-Rao Bound Analysis
"In this paper, we propose a novel Multi-Modal Scene Graph with Kolmogorov-Arnold Expert Network for Audio-Visual Question Answering (SHRIKE). The task aims to mimic human reasoning by extracting and fusing information from audio-visual scenes, with the main challenge being the identification of question-relevant cues from the complex audio-visual content. Existing methods fail to capture the structural information within video, and suffer from insufficient fine-grained modeling of multi-modal features. To address these issues, we are the first to introduce a new multi-modal scene graph that explicitly models the objects and their relationship as a visually grounded, structured representation of the audio-visual scene. Furthermore, we design a Kolmogorov-Arnold Network~(KAN)-based Mixture of Experts (MoE) to enhance the expressive power of the temporal integration stage. This enables more fine-grained modeling of cross-modal interactions within the question-aware fused audio-visual representation, leading to capture richer and more nuanced patterns and then improve temporal reasoning performance. We evaluate the model on the established MUSIC-AVQA and MUSIC-AVQA v2 benchmarks, where it achieves state-of-the-art performance. Code and model checkpoints will be publicly released.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Multi-Modal Scene Graph with Kolmogorov-Arnold Experts for Audio-Visual Question Answering
"This work presents a proof of concept for MUSIC, a multi-purpose detector conceived for high-precision and ultra-high-energy physics studies in the challenging environment of $\sqrt{s}=10$ TeV muon-antimuon collisions. The detector features a central tracking system, electromagnetic and hadronic calorimeters, and dedicated muon detectors. This paper outlines the main design elements of each subdetector, with an emphasis on the effects of machine-induced backgrounds and the reconstruction strategies employed for key physics objects. Performance results for electrons, photons, muons, and jets are reported, and studies of jet flavour identification are discussed.",0,arxiv,MÃ¼zik,CC-BY/arXiv,MUSIC: A Multi-Purpose Detector Concept for Physics at the 10 TeV Muon Collider
"This paper studies the effects of directional antenna element complex gain patterns and nonidealities in direction of arrival (DoA) estimation. We compare sparse arrays and classical uniform linear arrays, harnessing EM simulation tools to accurately model the electromagnetic behavior of both patch and Vivaldi antenna element including mutual coupling effects. We show that with sparse array configurations, the performance impacts are significant in terms of DoA estimation accuracy and operable SNR ranges. Specifically, in the scenarios considered, both the usage of directional antenna elements and a sparse array result in over 90% reduction in average direction finding error, compared to a uniform omnidirectional array with the same number of elements (in this case eight), when estimating the directions of two sources using the MUSIC algorithm. For a fixed angular RMSE, the improvements in array sensitivity are shown to yield a 4 to 15-fold increase in one-way coverage distance (assuming free-space path loss). Among the studied options, the best performance was obtained using sparse arrays with either patch or Vivaldi elements for field of views of 100$^\circ$ or 120$^\circ$, respectively.",0,arxiv,MÃ¼zik,CC-BY/arXiv,DoA Estimation with Sparse Arrays: Effects of Antenna Element Patterns and Nonidealities
"With the rise of AI-generated content (AIGC), generating perceptually natural and feeling-aligned music from multimodal inputs has become a central challenge. Existing approaches often rely on explicit emotion labels that require costly annotation, underscoring the need for more flexible feeling-aligned methods. To support multimodal music generation, we construct ArtiCaps, a pseudo feeling-aligned image-music-text dataset created by semantically matching descriptions from ArtEmis and MusicCaps. We further propose Art2Music, a lightweight cross-modal framework that synthesizes music from artistic images and user comments. In the first stage, images and text are encoded with OpenCLIP and fused using a gated residual module; the fused representation is decoded by a bidirectional LSTM into Mel-spectrograms with a frequency-weighted L1 loss to enhance high-frequency fidelity. In the second stage, a fine-tuned HiFi-GAN vocoder reconstructs high-quality audio waveforms. Experiments on ArtiCaps show clear improvements in Mel-Cepstral Distortion, Frechet Audio Distance, Log-Spectral Distance, and cosine similarity. A small LLM-based rating study further verifies consistent cross-modal feeling alignment and offers interpretable explanations of matches and mismatches across modalities. These results demonstrate improved perceptual naturalness, spectral fidelity, and semantic consistency. Art2Music also maintains robust performance with only 50k training samples, providing a scalable solution for feeling-aligned creative audio generation in interactive art, personalized soundscapes, and digital art exhibitions.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Art2Music: Generating Music for Art Images with Multi-modal Feeling Alignment
"Separating the individual elements in a musical mixture is an essential process for music analysis and practice. While this is generally addressed using neural networks optimized to mask or transform the time-frequency representation of a mixture to extract the target sources, the flexibility and generalization capabilities of generative diffusion models are giving rise to a novel class of solutions for this complicated task. In this work, we explore singing voice separation from real music recordings using a diffusion model which is trained to generate the solo vocals conditioned on the corresponding mixture. Our approach improves upon prior generative systems and achieves competitive objective scores against non-generative baselines when trained with supplementary data. The iterative nature of diffusion sampling enables the user to control the quality-efficiency trade-off, and also refine the output when needed. We present an ablation study of the sampling algorithm, highlighting the effects of the user-configurable parameters.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Generating Separated Singing Vocals Using a Diffusion Model Conditioned on Music Mixtures
"This paper introduces The Spheres dataset, multitrack orchestral recordings designed to advance machine learning research in music source separation and related MIR tasks within the classical music domain. The dataset is composed of over one hour recordings of musical pieces performed by the ColibrÃ¬ Ensemble at The Spheres recording studio, capturing two canonical works - Tchaikovsky's Romeo and Juliet and Mozart's Symphony No. 40 - along with chromatic scales and solo excerpts for each instrument. The recording setup employed 23 microphones, including close spot, main, and ambient microphones, enabling the creation of realistic stereo mixes with controlled bleeding and providing isolated stems for supervised training of source separation models. In addition, room impulse responses were estimated for each instrument position, offering valuable acoustic characterization of the recording space. We present the dataset structure, acoustic analysis, and baseline evaluations using X-UMX based models for orchestral family separation and microphone debleeding. Results highlight both the potential and the challenges of source separation in complex orchestral scenarios, underscoring the dataset's value for benchmarking and for exploring new approaches to separation, localization, dereverberation, and immersive rendering of classical music.",0,arxiv,MÃ¼zik,CC-BY/arXiv,The Spheres Dataset: Multitrack Orchestral Recordings for Music Source Separation and Information Retrieval
"Older adults often experience increased difficulty in decision making due to age-related declines particularly in contexts that require information search or the generation of alternatives from memory. This study examined whether using generative AI for information search enhances choice satisfaction and reduces choice difficulty among older adults. A total of 130 participants (younger, n = 56; older, n = 74) completed a music-selection task under AI-use and AI-nonuse conditions across two contexts: previously experienced (road trip) and not previously experienced (space travel). In the AI-nonuse condition, participants generated candidate options from memory; in the AI-use condition, GPT-4o presented options tailored to individual preferences. Cognitive functions, including working memory, processing speed, verbal comprehension, and perceptual reasoning, were assessed. Results showed that AI use significantly reduced perceived choice difficulty across age groups, with larger benefits in unfamiliar contexts. Regarding cognitive function, among older adults, lower cognitive function was associated with fewer recalled options, higher choice difficulty, and lower satisfaction in the AI-nonuse condition; these associations were substantially attenuated when AI was used. These results demonstrate that generative AI can mitigate age-related cognitive constraints by reducing the cognitive load associated with information search during decision making. While the use of AI reduced perceived difficulty, choice satisfaction remained unchanged, suggesting that autonomy in decision making was preserved. These findings indicate that generative AI can support everyday decision making by compensating for the constraints in information search that older adults face due to cognitive decline.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Preference-Aligned Options from Generative AI Compensates for Age-Related Cognitive Decline in Decision Making
"Singing voice synthesis (SVS) and singing voice conversion (SVC) have achieved remarkable progress in generating natural-sounding human singing. However, existing systems are restricted to human timbres and have limited ability to synthesize voices outside the human range, which are increasingly demanded in creative applications such as video games, movies, and virtual characters. We introduce Non-Human Singing Generation (NHSG), covering non-human singing voice synthesis (NHSVS) and non-human singing voice conversion (NHSVC), as a novel machine learning task for generating musically coherent singing with non-human timbral characteristics. NHSG is particularly challenging due to the scarcity of non-human singing data, the lack of symbolic alignment, and the wide timbral gap between human and non-human voices. To address these challenges, we propose CartoonSing, a unified framework that integrates singing voice synthesis and conversion while bridging human and non-human singing generation. CartoonSing employs a two-stage pipeline: a score representation encoder trained with annotated human singing and a timbre-aware vocoder that reconstructs waveforms for both human and non-human audio. Experiments demonstrate that CartoonSing successfully generates non-human singing voices, generalizes to novel timbres, and extends conventional SVS and SVC toward creative, non-human singing generation.",0,arxiv,MÃ¼zik,CC-BY/arXiv,CartoonSing: Unifying Human and Nonhuman Timbres in Singing Generation
"Music-to-dance generation aims to translate auditory signals into expressive human motion, with broad applications in virtual reality, choreography, and digital entertainment. Despite promising progress, the limited generation efficiency of existing methods leaves insufficient computational headroom for high-fidelity 3D rendering, thereby constraining the expressiveness of 3D characters during real-world applications. Thus, we propose FlowerDance, which not only generates refined motion with physical plausibility and artistic expressiveness, but also achieves significant generation efficiency on inference speed and memory utilization . Specifically, FlowerDance combines MeanFlow with Physical Consistency Constraints, which enables high-quality motion generation with only a few sampling steps. Moreover, FlowerDance leverages a simple but efficient model architecture with BiMamba-based backbone and Channel-Level Cross-Modal Fusion, which generates dance with efficient non-autoregressive manner. Meanwhile, FlowerDance supports motion editing, enabling users to interactively refine dance sequences. Extensive experiments on AIST++ and FineDance show that FlowerDance achieves state-of-the-art results in both motion quality and generation efficiency. Code will be released upon acceptance.",0,arxiv,MÃ¼zik,CC-BY/arXiv,FlowerDance: MeanFlow for Efficient and Refined 3D Dance Generation
"With recent advances in automatic speech recognition (ASR), large language models (LLMs), and text-to-speech (TTS) technologies, spoken dialogue systems (SDS) have become widely accessible. However, most existing SDS are limited to conventional spoken responses. We present SingingSDS, a cascaded SDS that responds through singing rather than speaking, fostering more affective, memorable, and pleasurable interactions in character-based roleplay and interactive entertainment scenarios. SingingSDS employs a modular ASR-LLM-SVS pipeline and supports a wide range of configurations across character personas, ASR and LLM backends, SVS models, melody sources, and voice profiles, tailored to different needs in terms of latency, quality, and musical style. SingingSDS is available as a plug-and-play web demo, featuring modular, open-source code that supports customization and extension. Demo: https://huggingface.co/spaces/espnet/SingingSDS. Code: https://github.com/SingingSDS/SingingSDS.",0,arxiv,MÃ¼zik,CC-BY/arXiv,SingingSDS: A Singing-Capable Spoken Dialogue System for Conversational Roleplay Applications
"Extracting individual elements from music mixtures is a valuable tool for music production and practice. While neural networks optimized to mask or transform mixture spectrograms into the individual source(s) have been the leading approach, the source overlap and correlation in music signals poses an inherent challenge. Also, accessing all sources in the mixture is crucial to train these systems, while complicated. Attempts to address these challenges in a generative fashion exist, however, the separation performance and inference efficiency remain limited. In this work, we study the potential of diffusion models to advance toward bridging this gap, focusing on generative singing voice separation relying only on corresponding pairs of isolated vocals and mixtures for training. To align with creative workflows, we leverage latent diffusion: the system generates samples encoded in a compact latent space, and subsequently decodes these into audio. This enables efficient optimization and faster inference. Our system is trained using only open data. We outperform existing generative separation systems, and level the compared non-generative systems on a list of signal quality measures and on interference removal. We provide a noise robustness study on the latent encoder, providing insights on its potential for the task. We release a modular toolkit for further research on the topic.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Efficient and Fast Generative-Based Singing Voice Separation using a Latent Diffusion Model
"Duo-Tok is a source-aware dual-codebook tokenizer for vocal-accompaniment music that targets the growing tension between reconstruction quality and language-model (LM) learnability in modern lyrics-to-song systems. Existing codecs either prioritize high-fidelity reconstruction with difficult-to-model acoustic tokens or compress aggressively into semantic tokens that are LM-friendly but lossy, and they rarely make the tokenizer itself aware of dual-track structure. Duo-Tok follows a four-stage, SSL-centered pipeline: we first pretrain a BEST-RQ-style encoder on large-scale audio, then stabilize and factorize the representation with Gaussian replacement noise and multi-task supervision, before freezing the encoder to learn SimVQ-based dual codebooks with hard routing for vocals and accompaniment, and finally training latent diffusion decoders on top of the discrete tokens. Duo-Tok at 0.75 kbps shifts the empirical reconstruction-generation Pareto frontier, achieving the best music-tagging AP and the lowest vocabulary-normalized LM perplexity among compared codecs while maintaining reconstruction quality comparable to state-of-the-art music tokenizers.",0,arxiv,MÃ¼zik,CC-BY/arXiv,DUO-TOK: Dual-Track Semantic Music Tokenizer for Vocal-Accompaniment Generation
"Automatic Pitch Correction (APC) enhances vocal recordings by aligning pitch deviations with the intended musical notes. However, existing APC systems either rely on reference pitches, which limits their practical applicability, or employ simple pitch estimation algorithms that often fail to preserve expressiveness and naturalness. We propose BERT-APC, a novel reference-free APC framework that corrects pitch errors while maintaining the natural expressiveness of vocal performances. In BERT-APC, a novel stationary pitch predictor first estimates the perceived pitch of each note from the detuned singing voice. A context-aware note pitch predictor estimates the intended pitch sequence by leveraging a music language model repurposed to incorporate musical context. Finally, a note-level correction algorithm fixes pitch errors while preserving intentional pitch deviations for emotional expression. In addition, we introduce a learnable data augmentation strategy that improves the robustness of the music language model by simulating realistic detuning patterns. Compared to two recent singing voice transcription models, BERT-APC demonstrated superior performance in note pitch prediction, outperforming the second-best model, ROSVOT, by 10.49%p on highly detuned samples in terms of the raw pitch accuracy. In the MOS test, BERT-APC achieved the highest score of $4.32 \pm 0.15$, which is significantly higher than those of the widely-used commercial APC tools, AutoTune ($3.22 \pm 0.18$) and Melodyne ($3.08 \pm 0.18$), while maintaining a comparable ability to preserve expressive nuances. To the best of our knowledge, this is the first APC model that leverages a music language model to achieve reference-free pitch correction with symbolic musical context. The corrected audio samples of BERT-APC are available online.",0,arxiv,MÃ¼zik,CC-BY/arXiv,BERT-APC: A Reference-free Framework for Automatic Pitch Correction via Musical Context Inference
"Egocentric perception on smart glasses could transform how we learn new skills in the physical world, but automatic skill assessment remains a fundamental technical challenge. We introduce SkillSight for power-efficient skill assessment from first-person data. Central to our approach is the hypothesis that skill level is evident not only in how a person performs an activity (video), but also in how they direct their attention when doing so (gaze). Our two-stage framework first learns to jointly model gaze and egocentric video when predicting skill level, then distills a gaze-only student model. At inference, the student model requires only gaze input, drastically reducing power consumption by eliminating continuous video processing. Experiments on three datasets spanning cooking, music, and sports establish, for the first time, the valuable role of gaze in skill understanding across diverse real-world settings. Our SkillSight teacher model achieves state-of-the-art performance, while our gaze-only student variant maintains high accuracy using 73x less power than competing methods. These results pave the way for in-the-wild AI-supported skill learning.",0,arxiv,MÃ¼zik,CC-BY/arXiv,SkillSight: Efficient First-Person Skill Assessment with Gaze
"State-of-the-art symbolic music generation models have recently achieved remarkable output quality, yet explicit control over compositional features, such as tonal tension, remains challenging. We propose a novel approach that integrates a computational tonal tension model, based on tonal interval vector analysis, into a Transformer framework. Our method employs a two-level beam search strategy during inference. At the token level, generated candidates are re-ranked using model probability and diversity metrics to maintain overall quality. At the bar level, a tension-based re-ranking is applied to ensure that the generated music aligns with a desired tension curve. Objective evaluations indicate that our approach effectively modulates tonal tension, and subjective listening tests confirm that the system produces outputs that align with the target tension. These results demonstrate that explicit tension conditioning through a dual-level beam search provides a powerful and intuitive tool to guide AI-generated music. Furthermore, our experiments demonstrate that our method can generate multiple distinct musical interpretations under the same tension condition.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Explicit Tonal Tension Conditioning via Dual-Level Beam Search for Symbolic Music Generation
"Generation of dynamic, scalable multi-species bird soundscapes remains a significant challenge in computer music and algorithmic sound design. Birdsongs involve rapid frequency-modulated chirps, complex amplitude envelopes, distinctive acoustic patterns, overlapping calls, and dynamic inter-bird interactions, all of which require precise temporal and spatial control in 3D environments. Existing approaches, whether Digital Signal Processing (DSP)-based or data-driven, typically focus only on single species modeling, static call structures, or synthesis directly from recordings, and often suffer from noise, limited flexibility, or large data needs. To address these challenges, we present a novel, fully algorithm-driven framework that generates dynamic multi-species bird soundscapes using DSP-based chirp generation and 3D spatialization, without relying on recordings or training data. Our approach simulates multiple independently-moving birds per species along different moving 3D trajectories, supporting controllable chirp sequences, overlapping choruses, and realistic 3D motion in scalable soundscapes while preserving species-specific acoustic patterns. A visualization interface provides bird trajectories, spectrograms, activity timelines, and sound waves for analytical and creative purposes. Both visual and audio evaluations demonstrate the ability of the system to generate dense, immersive, and ecologically inspired soundscapes, highlighting its potential for computer music, interactive virtual environments, and computational bioacoustics research.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Dynamic Multi-Species Bird Soundscape Generation with Acoustic Patterning and 3D Spatialization
"Evaluating the aesthetic quality of generated songs is challenging due to the multi-dimensional nature of musical perception. We propose a robust music aesthetic evaluation framework that combines (1) multi-source multi-scale feature extraction to obtain complementary segment- and track-level representations, (2) a hierarchical audio augmentation strategy to enrich training data, and (3) a hybrid training objective that integrates regression and ranking losses for accurate scoring and reliable top-song identification. Experiments on the ICASSP 2026 SongEval benchmark demonstrate that our approach consistently outperforms baseline methods across correlation and top-tier metrics.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Multidimensional Music Aesthetic Evaluation via Semantically Consistent C-Mixup Augmentation
"Understanding complete musical scores requires reasoning over symbolic structures such as pitch, rhythm, harmony, and form. Despite the rapid progress of Large Language Models (LLMs) and Vision-Language Models (VLMs) in natural language and multimodal tasks, their ability to comprehend musical notation remains underexplored. We introduce Musical Score Understanding Benchmark (MSU-Bench), the first large-scale, human-curated benchmark for evaluating score-level musical understanding across both textual (ABC notation) and visual (PDF) modalities. MSU-Bench comprises 1,800 generative question-answer (QA) pairs drawn from works spanning Bach, Beethoven, Chopin, Debussy, and others, organised into four progressive levels of comprehension: Onset Information, Notation & Note, Chord & Harmony, and Texture & Form. Through extensive zero-shot and fine-tuned evaluations of over 15+ state-of-the-art (SOTA) models, we reveal sharp modality gaps, fragile level-wise success rates, and the difficulty of sustaining multilevel correctness. Fine-tuning markedly improves performance in both modalities while preserving general knowledge, establishing MSU-Bench as a rigorous foundation for future research at the intersection of Artificial Intelligence (AI), musicological, and multimodal reasoning.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Musical Score Understanding Benchmark: Evaluating Large Language Models' Comprehension of Complete Musical Scores
"Text-to-speech (TTS) and text-to-music (TTM) models face significant limitations in instruction-based control. TTS systems usually depend on reference audio for timbre, offer only limited text-level attribute control, and rarely support dialogue generation. TTM systems are constrained by input conditioning requirements that depend on expert knowledge annotations. The high heterogeneity of these input control conditions makes them difficult to joint modeling with speech synthesis. Despite sharing common acoustic modeling characteristics, these two tasks have long been developed independently, leaving open the challenge of achieving unified modeling through natural language instructions. We introduce InstructAudio, a unified framework that enables instruction-based (natural language descriptions) control of acoustic attributes including timbre (gender, age), paralinguistic (emotion, style, accent), and musical (genre, instrument, rhythm, atmosphere). It supports expressive speech, music, and dialogue generation in English and Chinese. The model employs joint and single diffusion transformer layers with a standardized instruction-phoneme input format, trained on 50K hours of speech and 20K hours of music data, enabling multi-task learning and cross-modal alignment. Fig. 1 visualizes performance comparisons with mainstream TTS and TTM models, demonstrating that InstructAudio achieves optimal results on most metrics. To our best knowledge, InstructAudio represents the first instruction-controlled framework unifying speech and music generation. Audio samples are available at: https://qiangchunyu.github.io/InstructAudio/",0,arxiv,MÃ¼zik,CC-BY/arXiv,InstructAudio: Unified speech and music generation with natural language instruction
"This study introduce GeeSanBhava, a high-quality data set of Sinhala song comments extracted from YouTube manually tagged using Russells Valence-Arousal model by three independent human annotators. The human annotators achieve a substantial inter-annotator agreement (Fleiss kappa = 84.96%). The analysis revealed distinct emotional profiles for different songs, highlighting the importance of comment based emotion mapping. The study also addressed the challenges of comparing comment-based and song-based emotions, mitigating biases inherent in user-generated content. A number of Machine learning and deep learning models were pre-trained on a related large data set of Sinhala News comments in order to report the zero-shot result of our Sinhala YouTube comment data set. An optimized Multi-Layer Perceptron model, after extensive hyperparameter tuning, achieved a ROC-AUC score of 0.887. The model is a three-layer MLP with a configuration of 256, 128, and 64 neurons. This research contributes a valuable annotated dataset and provides insights for future work in Sinhala Natural Language Processing and music emotion recognition.",0,arxiv,MÃ¼zik,CC-BY/arXiv,GeeSanBhava: Sentiment Tagged Sinhala Music Video Comment Data Set
"Explanation fidelity, which measures how accurately an explanation reflects a model's true reasoning, remains critically underexplored in recommender systems. We introduce SPINRec (Stochastic Path Integration for Neural Recommender Explanations), a model-agnostic approach that adapts path-integration techniques to the sparse and implicit nature of recommendation data. To overcome the limitations of prior methods, SPINRec employs stochastic baseline sampling: instead of integrating from a fixed or unrealistic baseline, it samples multiple plausible user profiles from the empirical data distribution and selects the most faithful attribution path. This design captures the influence of both observed and unobserved interactions, yielding more stable and personalized explanations. We conduct the most comprehensive fidelity evaluation to date across three models (MF, VAE, NCF), three datasets (ML1M, Yahoo! Music, Pinterest), and a suite of counterfactual metrics, including AUC-based perturbation curves and fixed-length diagnostics. SPINRec consistently outperforms all baselines, establishing a new benchmark for faithful explainability in recommendation. Code and evaluation tools are publicly available at https://github.com/DeltaLabTLV/SPINRec.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Fidelity-Aware Recommendation Explanations via Stochastic Path Integration
"Most applications of generative AI involve a sequential interaction in which a person inputs a prompt and waits for a response, and where reaction time and adaptivity are not important factors. In contrast, live jamming is a collaborative interaction that requires real-time coordination and adaptation without access to the other player's future moves, while preserving diversity to sustain a creative flow. Reinforcement learning post-training enables effective adaptation through on-policy interaction, yet it often reduces output diversity by exploiting coherence-based rewards. This collapse, known as ``reward hacking'', affects many RL post-training pipelines, but is especially harmful in live jamming, where musical creativity relies on dynamic variation and mutual responsiveness. In this paper, we propose a novel adversarial training method on policy-generated trajectories to mitigate reward hacking in RL post-training for melody-to-chord accompaniment. A co-evolving discriminator separates policy trajectories from the data distribution, while the policy maximizes the discriminator output in addition to coherence rewards to prevent collapse to trivial outputs. We evaluate accompaniment quality and output diversity in simulation with both fixed test melodies and learned melody agents, and we conduct a user study with the model deployed in a real-time interactive system with expert musicians. Quantitative evaluation and user feedback demonstrate improved output diversity, harmonic coherence, adaptation speed and user agency. Our results demonstrate a simple yet effective method to mitigate reward hacking in RL post-training of generative sequence models.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Generative Adversarial Post-Training Mitigates Reward Hacking in Live Human-AI Music Interaction
"Multi-label feature selection (FS) reduces the dimensionality of multi-label data by removing irrelevant, noisy, and redundant features, thereby boosting the performance of multi-label learning models. However, existing methods typically require centralized data, which makes them unsuitable for distributed and federated environments where each device/client holds its own local dataset. Additionally, federated methods often assume that clients have labeled data, which is unrealistic in cases where clients lack the expertise or resources to label task-specific data. To address these challenges, we propose a Semi-Supervised Federated Multi-Label Feature Selection method, called SSFMLFS, where clients hold only unlabeled data, while the server has limited labeled data. SSFMLFS adapts fuzzy information theory to a federated setting, where clients compute fuzzy similarity matrices and transmit them to the server, which then calculates feature redundancy and feature-label relevancy degrees. A feature graph is constructed by modeling features as vertices, assigning relevancy and redundancy degrees as vertex weights and edge weights, respectively. PageRank is then applied to rank the features by importance. Extensive experiments on five real-world datasets from various domains, including biology, images, music, and text, demonstrate that SSFMLFS outperforms other federated and centralized supervised and semi-supervised approaches in terms of three different evaluation metrics in non-IID data distribution setting.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Semi-Supervised Federated Multi-Label Feature Selection with Fuzzy Information Measures
"This paper investigates the emerging text-to-audio paradigm in artificial intelligence (AI), examining its transformative implications for musical creation, interpretation, and cognition. I explore the complex semantic and semiotic interplays that occur when descriptive natural language prompts are translated into nuanced sound objects across the text-to-audio modality. Drawing from structuralist and post-structuralist perspectives, as well as cognitive theories of schema dynamics and metacognition, the paper explores how these AI systems reconfigure musical signification processes and navigate established cognitive frameworks. The research analyzes some of the cognitive dynamics at play in AI-mediated musicking, including processes of schema assimilation and accommodation, metacognitive reflection, and constructive perception. The paper argues that text-to-audio AI models function as quasi-objects of musical signification, simultaneously stabilizing and destabilizing conventional forms while fostering new modes of listening and aesthetic reflexivity.Using Udio as a primary case study, this study explores how these models navigate the liminal spaces between linguistic prompts and sonic outputs. This process not only generates novel musical expressions but also prompts listeners to engage in forms of critical and ""structurally-aware listening."", encouraging a deeper understanding of music's structures, semiotic nuances, and the socio-cultural contexts that shape our musical cognition. The paper concludes by reflecting on the potential of text-to-audio AI models to serve as epistemic tools and quasi-objects, facilitating a significant shift in musical interactions and inviting users to develop a more nuanced comprehension of the cognitive and cultural foundations of music.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Semantic and Semiotic Interplays in Text-to-Audio AI: Exploring Cognitive Dynamics and Musical Interactions
"This paper presents a pedagogical and conceptual account of the course AI in Music and Sound: Modalities, Tools and Creative Applications, offered within the Music Informatics and Media Art module of an M.Sc. in Audio Communication. The course engaged students with a range of AI modalities such as symbolic composition, voice synthesis, timbre transfer, neural audio synthesis, and text-to-audio systems, combining theoretical reflection with practice-based experimentation. Its central pedagogical move is a paired-Ã©tudes design: each modality is approached first through its intended affordances and then through a deliberately reframed or ""misused"" exercise that surfaces representational limits and alternative behaviours. Framed by medium theory and post-structuralist inquiry, we treated AI as a transmodal conduit-a system that translates and perturbs musical signs across textual, symbolic, timbral and audio domains. Evidence from student work and reflection indicates growth in technical fluency, medium awareness, and critical literacy, alongside the cultivation of experimental method and process-oriented listening. The paper outlines the course architecture, assessment design, and representative projects, and distils a set of design patterns for AI-music pedagogy (eg., prompt-conditioned interplays and semantic destabilisation in text-to-audio; latent space materialism in timbre transfer). It concludes with pedagogical recommendations that integrate creative practice with medium awareness and with cultural-epistemic analysis of AI technologies, preparing students to participate in how AI is understood, developed, and deployed with creative communities.",0,arxiv,MÃ¼zik,CC-BY/arXiv,"AI in Music and Sound: Pedagogical Reflections, Post-Structuralist Approaches and Creative Outcomes in Seminar Practice"
"Text-to-audio (TTA) systems are rapidly transforming music creation and distribution, with platforms like Udio and Suno generating thousands of tracks daily and integrating into mainstream music platforms and ecosystems. These systems, trained on vast and largely undisclosed datasets, are fundamentally reshaping how music is produced, reproduced and consumed. This paper presents empirical evidence that artist-conditioned regions can be systematically microlocated through metatag-based prompt design, effectively enabling the spawning of artist-like content through strategic prompt engineering. Through systematic exploration of metatag-based prompt engineering techniques this research reveals how users can access the distinctive sonic signatures of specific artists, evidencing their inclusion in training datasets. Using descriptor constellations drawn from public music taxonomies, the paper demonstrates reproducible proximity to artists such as Bon Iver, Philip Glass, Panda Bear and William Basinski. The results indicate stable text-audio correspondences consistent with artist-specific training signals, enabling precise traversal of stylistic microlocations without explicitly naming artists. This capacity to summon artist-specific outputs shows that artists' creative works fuction as foundational material from which these systems generate new content, often without explicit consent or attribuition. Conceptually, the work clarifies how textual descriptors act as navigational cues in high-dimensional representation spaces; methodologically, it provides a replicable protocol for auditing stylistic inducibility. The findings raise immediate queestions for governance-attribution, consent and disclosure standards-and for creative practice, where induced stylistic proximity complicates boundaries between ownership, reproduction, imitation, creative agency and the ethics of algorithmic creation.",0,arxiv,MÃ¼zik,CC-BY/arXiv,The Artist is Present: Traces of Artists Resigind and Spawning in Text-to-Audio AI
"Recent advances in generative AI have made music generation a prominent research focus. However, many neural-based models rely on large datasets, raising concerns about copyright infringement and high-performance costs. In contrast, we propose MusicAIR, an innovative multimodal AI music generation framework powered by a novel algorithm-driven symbolic music core, effectively mitigating copyright infringement risks. The music core algorithms connect critical lyrical and rhythmic information to automatically derive musical features, creating a complete, coherent melodic score solely from the lyrics. The MusicAIR framework facilitates music generation from lyrics, text, and images. The generated score adheres to established principles of music theory, lyrical structure, and rhythmic conventions. We developed Generate AI Music (GenAIM), a web tool using MusicAIR for lyric-to-song, text-to-music, and image-to-music generation. In our experiments, we evaluated AI-generated music scores produced by the system using both standard music metrics and innovative analysis that compares these compositions with original works. The system achieves an average key confidence of 85%, outperforming human composers at 79%, and aligns closely with established music theory standards, demonstrating its ability to generate diverse, human-like compositions. As a co-pilot tool, GenAIM can serve as a reliable music composition assistant and a possible educational composition tutor while simultaneously lowering the entry barrier for all aspiring musicians, which is innovative and significantly contributes to AI for music generation.",0,arxiv,MÃ¼zik,CC-BY/arXiv,MusicAIR: A Multimodal AI Music Generation Framework Powered by an Algorithm-Driven Core
"This paper studies two structured approximation problems: (1) Recovering a corrupted low-rank Toeplitz matrix and (2) recovering the range of a Fourier matrix from a single observation. Both problems are computationally challenging because the structural constraints are difficult to enforce directly. We show that both tasks can be solved efficiently and optimally by applying the Gradient-MUSIC algorithm for spectral estimation. For a rank $r$ Toeplitz matrix ${\boldsymbol T}\in {\mathbb C}^{n\times n}$ that satisfies a regularity assumption and is corrupted by an arbitrary ${\boldsymbol E}\in {\mathbb C}^{n\times n}$ such that $\|{\boldsymbol E}\|_2\leq Î±n$, our algorithm outputs a Toeplitz matrix $\widehat{\boldsymbol T}$ of rank exactly $r$ such that $\|{\boldsymbol T}-\widehat{\boldsymbol T}\|_2 \leq C \sqrt r \, \|{\boldsymbol E}\|_2$, where $C,Î±>0$ are absolute constants. This performance guarantee is minimax optimal in $n$ and $\|{\boldsymbol E}\|_2$. We derive optimal results for the second problem as well. Our analysis provides quantitative connections between these two problems and spectral estimation. Our results are equally applicable to Hankel matrices with superficial modifications.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Structured Approximation of Toeplitz Matrices and Subspaces
"Device-guided music transfer adapts playback across unseen devices for users who lack them. Existing methods mainly focus on modifying the timbre, rhythm, harmony, or instrumentation to mimic genres or artists, overlooking the diverse hardware properties of the playback device (i.e., speaker). Therefore, we propose DeMT, which processes a speaker's frequency response curve as a line graph using a vision-language model to extract device embeddings. These embeddings then condition a hybrid transformer via feature-wise linear modulation. Fine-tuned on a self-collected dataset, DeMT enables effective speaker-style transfer and robust few-shot adaptation for unseen devices, supporting applications like device-style augmentation and quality enhancement.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Device-Guided Music Transfer
"Artificial neural networks (ANNs) are increasingly powerful models of brain computation, yet it remains unclear whether improving their task performance also makes their internal representations more similar to brain signals. To address this question in the auditory domain, we quantified the alignment between the internal representations of 36 different audio models and brain activity from two independent fMRI datasets. Using voxel-wise and component-wise regression, and representation similarity analysis (RSA), we found that recent self-supervised audio models with strong performance in diverse downstream tasks are better predictors of auditory cortex activity than older and more specialized models. To assess the quality of the audio representations, we evaluated these models in 6 auditory tasks from the HEAREval benchmark, spanning music, speech, and environmental sounds. This revealed strong positive Pearson correlations ($r>0.7$) between a model's overall task performance and its alignment with brain representations. Finally, we analyzed the evolution of the similarity between audio and brain representations during the pretraining of EnCodecMAE. We discovered that brain similarity increases progressively and emerges early during pretraining, despite the model not being explicitly optimized for this objective. This suggests that brain-like representations can be an emergent byproduct of learning to reconstruct missing information from naturalistic audio data.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Better audio representations are more brain-like: linking model-brain alignment with performance in downstream auditory tasks
"Audio-language pretraining holds promise for general-purpose audio understanding, yet remains underexplored compared to its vision counterpart. While vision-language models like CLIP serve as widely adopted foundations, existing audio-language models primarily excel at retrieval tasks with limited adoption as general-purpose encoders. We identify three key barriers: limited large-scale audio-text corpora, insufficient caption diversity, and lack of systematic exploration and evaluation. To this end, we introduce CaptionStew, a 10.7M caption dataset aggregating diverse open-source audio-text corpora across multiple domains and captioning styles. Using this resource, we conduct the first comprehensive evaluation comparing contrastive and captioning objectives for audio representation learning across speech, music, and environmental sound tasks. Our results demonstrate that audio-language pretraining yields competitive, transferable representations. Through systematic data-scaling experiments, we reveal complementary objective strengths: contrastive learning achieves superior data efficiency at smaller scales, while captioning demonstrates better scalability on language-involved audio understanding tasks. We also find that common supervised initialization practices provide diminishing returns at scale, challenging current approaches. These findings establish audio-language pretraining as a viable pathway toward general-purpose audio representations, guiding future research. To accelerate progress, we release data preparation recipes, training protocols, and pretrained models, paving the way toward universal audio understanding.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation
"Music Recommender Systems (MRS) have long relied on an information-retrieval framing, where progress is measured mainly through accuracy on retrieval-oriented subtasks. While effective, this reductionist paradigm struggles to address the deeper question of what makes a good recommendation, and attempts to broaden evaluation, through user studies or fairness analyses, have had limited impact. The emergence of Large Language Models (LLMs) disrupts this framework: LLMs are generative rather than ranking-based, making standard accuracy metrics questionable. They also introduce challenges such as hallucinations, knowledge cutoffs, non-determinism, and opaque training data, rendering traditional train/test protocols difficult to interpret. At the same time, LLMs create new opportunities, enabling natural-language interaction and even allowing models to act as evaluators.   This work argues that the shift toward LLM-driven MRS requires rethinking evaluation. We first review how LLMs reshape user modeling, item modeling, and natural-language recommendation in music. We then examine evaluation practices from NLP, highlighting methodologies and open challenges relevant to MRS. Finally, we synthesize insights-focusing on how LLM prompting applies to MRS, to outline a structured set of success and risk dimensions. Our goal is to provide the MRS community with an updated, pedagogical, and cross-disciplinary perspective on evaluation.",0,arxiv,MÃ¼zik,CC-BY/arXiv,"Music Recommendation with Large Language Models: Challenges, Opportunities, and Evaluation"
"Despite its potential, AI advances in music education are hindered by proprietary systems that limit the democratization of technology in this domain. In particular, AI-driven music difficulty adjustment is especially promising, as simplifying complex pieces can make music education more inclusive and accessible to learners of all ages and contexts. Nevertheless, recent efforts have relied on proprietary datasets, which prevents the research community from reproducing, comparing, or extending the current state of the art. In addition, while these generative methods offer great potential, most of them use the MIDI format, which, unlike others, such as MusicXML, lacks readability and layout information, thereby limiting their practical use for human performers. This work introduces a transformer-based method for adjusting the difficulty of MusicXML piano scores. Unlike previous methods, which rely on annotated datasets, we propose a synthetic dataset composed of pairs of piano scores ordered by estimated difficulty, with each pair comprising a more challenging and easier arrangement of the same piece. We generate these pairs by creating variations conditioned on the same melody and harmony and leverage pretrained models to assess difficulty and style, ensuring appropriate pairing. The experimental results illustrate the validity of the proposed approach, showing accurate control of playability and target difficulty, as highlighted through qualitative and quantitative evaluations. In contrast to previous work, we openly release all resources (code, dataset, and models), ensuring reproducibility while fostering open-source innovation to help bridge the digital divide.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Difficulty-Controlled Simplification of Piano Scores with Synthetic Data for Inclusive Music Education
"Recent advances in reasoning models have demonstrated remarkable success in text and vision domains through extended chain-of-thought deliberation. However, a perplexing phenomenon persists in audio language models: they consistently perform better with minimal or no reasoning, raising a fundamental question - can audio intelligence truly benefit from deliberate thinking? We introduce Step-Audio-R1, the first audio reasoning model that successfully unlocks reasoning capabilities in the audio domain. Through our proposed Modality-Grounded Reasoning Distillation (MGRD) framework, Step-Audio-R1 learns to generate audio-relevant reasoning chains that genuinely ground themselves in acoustic features rather than hallucinating disconnected deliberations. Our model exhibits strong audio reasoning capabilities, surpassing Gemini 2.5 Pro and achieving performance comparable to the state-of-the-art Gemini 3 Pro across comprehensive audio understanding and reasoning benchmarks spanning speech, environmental sounds, and music. These results demonstrate that reasoning is a transferable capability across modalities when appropriately anchored, transforming extended deliberation from a liability into a powerful asset for audio intelligence. By establishing the first successful audio reasoning model, Step-Audio-R1 opens new pathways toward building truly multimodal reasoning systems that think deeply across all sensory modalities.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Step-Audio-R1 Technical Report
"Recent advances in AI-based music generation have focused heavily on text-conditioned models, with less attention given to reference-based generation such as song adaptation. To support this line of research, we introduce LargeSHS, a large-scale dataset derived from SecondHandSongs, containing over 1.7 million metadata entries and approximately 900k publicly accessible audio links. Unlike existing datasets, LargeSHS includes structured adaptation relationships between musical works, enabling the construction of adaptation trees and performance clusters that represent cover song families. We provide comprehensive statistics and comparisons with existing datasets, highlighting the unique scale and richness of LargeSHS. This dataset paves the way for new research in cover song generation, reference-based music generation, and adaptation-aware MIR tasks.",0,arxiv,MÃ¼zik,CC-BY/arXiv,LargeSHS: A large-scale dataset of music adaptation
"In the study, the device of social robot was designed for visually impaired users, and along with a mobile application for provide functions to assist their lives. Both physical and mental conditions of visually impaired users are considered, and the mobile application provides functions: photo record, mood lift, greeting guest and today highlight. The application was designed for visually impaired users, and uses voice control to provide a friendly interface. Photo record function allows visually impaired users to capture image immediately when they encounter danger situations. Mood lift function accompanies visually impaired users by asking questions, playing music and reading articles. Greeting guest function answers to the visitors for the inconvenient physical condition of visually impaired users. In addition, today highlight function read news including weather forecast, daily horoscopes and daily reminder for visually impaired users. Multiple tools were adopted for developing the mobile application, and a website was developed for caregivers to check statues of visually impaired users and for marketing of the application.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Eye Care You: Voice Guidance Application Using Social Robot for Visually Impaired People
"Recent advances in generative AI for music have achieved remarkable fidelity and stylistic diversity, yet these systems often fail to align with nuanced human preferences due to the specific loss functions they use. This paper advocates for the systematic application of preference alignment techniques to music generation, addressing the fundamental gap between computational optimization and human musical appreciation. Drawing on recent breakthroughs including MusicRL's large-scale preference learning, multi-preference alignment frameworks like diffusion-based preference optimization in DiffRhythm+, and inference-time optimization techniques like Text2midi-InferAlign, we discuss how these techniques can address music's unique challenges: temporal coherence, harmonic consistency, and subjective quality assessment. We identify key research challenges including scalability to long-form compositions, reliability amongst others in preference modelling. Looking forward, we envision preference-aligned music generation enabling transformative applications in interactive composition tools and personalized music services. This work calls for sustained interdisciplinary research combining advances in machine learning, music-theory to create music AI systems that truly serve human creative and experiential needs.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Aligning Generative Music AI with Human Preferences: Methods and Challenges
"While Large Language Models (LLMs) make symbolic music generation increasingly accessible, producing music with distinctive composition and rich expressiveness remains a significant challenge. Many studies have introduced emotion models to guide the generative process. However, these approaches still fall short of delivering novelty and creativity. In the field of Music Information Retrieval (MIR), auditory perception is recognized as a key dimension of musical experience, offering insights into both compositional intent and emotional patterns. To this end, we propose a neural network named CPFG-Net, along with a transformation algorithm that maps perceptual feature values to chord representations, enabling melody harmonization. The system can controllably predict sequences of perceptual features and tonal structures from given melodies, and subsequently generate harmonically coherent chord progressions. Our network is trained on our newly constructed perceptual feature dataset BCPT-220K, derived from classical music. Experimental results show state-of-the-art perceptual feature prediction capability of our model as well as demonstrate our musical expressiveness and creativity in chord inference. This work offers a novel perspective on melody harmonization and contributes to broader music generation tasks. Our symbolic-based model can be easily extended to audio-based models.",0,arxiv,MÃ¼zik,CC-BY/arXiv,A Controllable Perceptual Feature Generative Model for Melody Harmonization via Conditional Variational Autoencoder
"Automatic Music Transcription (AMT) converts audio recordings into symbolic musical representations. Training deep neural networks (DNNs) for AMT typically requires strongly aligned training pairs with precise frame-level annotations. Since creating such datasets is costly and impractical for many musical contexts, weakly aligned approaches using segment-level annotations have gained traction. However, existing methods often rely on Dynamic Time Warping (DTW) or soft alignment loss functions, both of which still require local semantic correspondences, making them error-prone and computationally expensive. In this article, we introduce CountEM, a novel AMT framework that eliminates the need for explicit local alignment by leveraging note event histograms as supervision, enabling lighter computations and greater flexibility. Using an Expectation-Maximization (EM) approach, CountEM iteratively refines predictions based solely on note occurrence counts, significantly reducing annotation efforts while maintaining high transcription accuracy. Experiments on piano, guitar, and multi-instrument datasets demonstrate that CountEM matches or surpasses existing weakly supervised methods, improving AMT's robustness, scalability, and efficiency. Our project page is available at https://yoni-yaffe.github.io/count-the-notes.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Count The Notes: Histogram-Based Supervision for Automatic Music Transcription
"Large language models perform strongly on general tasks but remain constrained in specialized settings such as music, particularly in the music-entertainment domain, where corpus scale, purity, and the match between data and training objectives are critical. We address this by constructing a large, music-related natural language corpus (40B tokens) that combines open source and in-house data, and by implementing a domain-first data pipeline: a lightweight classifier filters and weights in-domain text, followed by multi-stage cleaning, de-duplication, and privacy-preserving masking. We further integrate multi-source music text with associated metadata to form a broader, better-structured foundation of domain knowledge. On the training side, we introduce reference-model (RM)-based token-level soft scoring for quality control: a unified loss-ratio criterion is used both for data selection and for dynamic down-weighting during optimization, reducing noise gradients and amplifying task-aligned signals, thereby enabling more effective music-domain continued pretraining and alignment. To assess factuality, we design the MusicSimpleQA benchmark, which adopts short, single-answer prompts with automated agreement scoring. Beyond the benchmark design, we conduct systematic comparisons along the axes of data composition. Overall, this work advances both the right corpus and the right objective, offering a scalable data-training framework and a reusable evaluation tool for building domain LLMs in the music field.",0,arxiv,MÃ¼zik,CC-BY/arXiv,MuCPT: Music-related Natural Language Model Continued Pretraining
"Robotic arm choreography often reproduces trajectories while missing cultural semantics. This study examines whether symbolic posture transfer with joint space compatible notation can preserve semantic fidelity on a six-degree-of-freedom arm and remain portable across morphologies. We implement ROPERA, a three-stage pipeline for encoding culturally codified postures, composing symbolic sequences, and decoding to servo commands. A scene from Kunqu opera, \textit{The Peony Pavilion}, serves as the material for evaluation. The procedure includes corpus-based posture selection, symbolic scoring, direct joint angle execution, and a visual layer with light painting and costume-informed colors. Results indicate reproducible execution with intended timing and cultural legibility reported by experts and audiences. The study points to non-anthropocentric cultural preservation and portable authoring workflows. Future work will design dance-informed transition profiles, extend the notation to locomotion with haptic, musical, and spatial cues, and test portability across platforms.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Translating Cultural Choreography from Humanoid Forms to Robotic Arm
"This paper presents an integrative review and experimental validation of artificial intelligence (AI) agents applied to music analysis and education. We synthesize the historical evolution from rule-based models to contemporary approaches involving deep learning, multi-agent architectures, and retrieval-augmented generation (RAG) frameworks. The pedagogical implications are evaluated through a dual-case methodology: (1) the use of generative AI platforms in secondary education to foster analytical and creative skills; (2) the design of a multiagent system for symbolic music analysis, enabling modular, scalable, and explainable workflows.   Experimental results demonstrate that AI agents effectively enhance musical pattern recognition, compositional parameterization, and educational feedback, outperforming traditional automated methods in terms of interpretability and adaptability. The findings highlight key challenges concerning transparency, cultural bias, and the definition of hybrid evaluation metrics, emphasizing the need for responsible deployment of AI in educational environments.   This research contributes to a unified framework that bridges technical, pedagogical, and ethical considerations, offering evidence-based guidance for the design and application of intelligent agents in computational musicology and music education.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Artificial Intelligence Agents in Music Analysis: An Integrative Perspective Based on Two Use Cases
"Despite the parallel challenges that audio and text domains face in evaluating generative model outputs, preference learning remains remarkably underexplored in audio applications. Through a PRISMA-guided systematic review of approximately 500 papers, we find that only 30 (6%) apply preference learning to audio tasks. Our analysis reveals a field in transition: pre-2021 works focused on emotion recognition using traditional ranking methods (rankSVM), while post-2021 studies have pivoted toward generation tasks employing modern RLHF frameworks. We identify three critical patterns: (1) the emergence of multi-dimensional evaluation strategies combining synthetic, automated, and human preferences; (2) inconsistent alignment between traditional metrics (WER, PESQ) and human judgments across different contexts; and (3) convergence on multi-stage training pipelines that combine reward signals. Our findings suggest that while preference learning shows promise for audio, particularly in capturing subjective qualities like naturalness and musicality, the field requires standardized benchmarks, higher-quality datasets, and systematic investigation of how temporal factors unique to audio impact preference learning frameworks.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Preference-Based Learning in Audio Applications: A Systematic Analysis
"Video-to-audio generation (V2A) is of increasing importance in domains such as film post-production, AR/VR, and sound design, particularly for the creation of Foley sound effects synchronized with on-screen actions. Foley requires generating audio that is both semantically aligned with visible events and temporally aligned with their timing. Yet, there is a mismatch between evaluation and downstream applications due to the absence of a benchmark tailored to Foley-style scenarios. We find that 74% of videos from past evaluation datasets have poor audio-visual correspondence. Moreover, they are dominated by speech and music, domains that lie outside the use case for Foley. To address this gap, we introduce FoleyBench, the first large-scale benchmark explicitly designed for Foley-style V2A evaluation. FoleyBench contains 5,000 (video, ground-truth audio, text caption) triplets, each featuring visible sound sources with audio causally tied to on-screen events. The dataset is built using an automated, scalable pipeline applied to in-the-wild internet videos from YouTube-based and Vimeo-based sources. Compared to past datasets, we show that videos from FoleyBench have stronger coverage of sound categories from a taxonomy specifically designed for Foley sound. Each clip is further labeled with metadata capturing source complexity, UCS/AudioSet category, and video length, enabling fine-grained analysis of model performance and failure modes. We benchmark several state-of-the-art V2A models, evaluating them on audio quality, audio-video alignment, temporal synchronization, and audio-text consistency. Samples are available at: https://gclef-cmu.org/foleybench",0,arxiv,MÃ¼zik,CC-BY/arXiv,FoleyBench: A Benchmark For Video-to-Audio Models
"In recent years, significant progress has been made in the field of deep learning for music demixing. However, there has been limited attention on real-time, low-latency music demixing, which holds potential for various applications, such as hearing aids, audio stream remixing, and live performances. Additionally, a notable tendency has emerged towards the development of larger models, limiting their applicability in certain scenarios. In this paper, we introduce a lightweight real-time low-latency model called Real-Time Single-Path TFC-TDF UNET (RT-STT), which is based on the Dual-Path TFC-TDF UNET (DTTNet). In RT-STT, we propose a feature fusion technique based on channel expansion. We also demonstrate the superiority of single-path modeling over dual-path modeling in real-time models. Moreover, we investigate the method of quantization to further reduce inference time. RT-STT exhibits superior performance with significantly fewer parameters and shorter inference times compared to state-of-the-art models.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Towards Practical Real-Time Low-Latency Music Source Separation
"The emotional and structural experience of music remains a significant accessibility challenge for the deaf and hard of hearing community. This paper introduces MUSTEM (Multisensorial Emotional Translation), a novel system designed to translate music into a rich, coherent, and scientifically-grounded sensory experience. We present a dual-modality approach addressing this challenge through two interconnected components. First, a low-cost, portable hardware prototype that performs real-time audio analysis, mapping distinct frequency bands (sub-bass, bass, mid-range, treble) to a four-channel vibrotactile system, allowing users to feel the music's rhythmic and foundational structure. Second, to overcome the processing limitations of embedded hardware, we developed a high-fidelity software simulation that demonstrates the full potential of the visual translation. This assistive dashboard decodes musical components - such as rhythm, harmony, and frequency spectrum - into an intuitive and educational visual interface. MUSTEM offers a comprehensive framework for sensory substitution, presenting a viable and accessible pathway for the deaf community to experience music not just as vibration, but as a structured, substantiated and emotionally resonant visual and tactile language. Preliminary feedback from seven deaf users suggests the system's spatial vibrotactile mapping is perceptible and engaging. All source code and hardware designs are released as open-source. Video demonstrations and open-source code are available on the project's official channel.",0,arxiv,MÃ¼zik,CC-BY/arXiv,MUSTEM: A Dual-Modality System for Vibrotactile and Visual Translation of Music as an Assistive Technology
"We propose a joint estimation method for the Direction-of-Arrival (DoA) and the Noise Covariance Matrix (NCM) tailored for beamforming applications. Building upon an existing NCM framework, our approach simplifies the estimation procedure by deriving an quasi-linear solution, instead of the traditional exhaustive search. Additionally, we introduce a novel DoA estimation technique that operates across all frequency bins, improving robustness in reverberant environments. Simulation results demonstrate that our method outperforms classical techniques, such as MUSIC, in mid- to high-angle scenarios, achieving lower angular errors and superior signal enhancement through beamforming. The proposed framework was also fared against other techniques for signal enhancement, having better noise rejection and interference canceling capabilities. These improvements are validated using both theoretical and empirical performance metrics.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Direction-of-Arrival and Noise Covariance Matrix joint estimation for beamforming
"We introduce Music Flamingo, a novel large audio-language model designed to advance music (including song) understanding in foundational audio models. While audio-language research has progressed rapidly, music remains challenging due to its dynamic, layered, and information-dense nature. Progress has been further limited by the difficulty of scaling open audio understanding models, primarily because of the scarcity of high-quality music data and annotations. As a result, prior models are restricted to producing short, high-level captions, answering only surface-level questions, and showing limited generalization across diverse musical cultures. To address these challenges, we curate MF-Skills, a large-scale dataset labeled through a multi-stage pipeline that yields rich captions and question-answer pairs covering harmony, structure, timbre, lyrics, and cultural context. We fine-tune an enhanced Audio Flamingo 3 backbone on MF-Skills and further strengthen multiple skills relevant to music understanding. To improve the model's reasoning abilities, we introduce a post-training recipe: we first cold-start with MF-Think, a novel chain-of-thought dataset grounded in music theory, followed by GRPO-based reinforcement learning with custom rewards. Music Flamingo achieves state-of-the-art results across 10+ benchmarks for music understanding and reasoning, establishing itself as a generalist and musically intelligent audio-language model. Beyond strong empirical results, Music Flamingo sets a new standard for advanced music understanding by demonstrating how models can move from surface-level recognition toward layered, human-like perception of songs. We believe this work provides both a benchmark and a foundation for the community to build the next generation of models that engage with music as meaningfully as humans do.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Music Flamingo: Scaling Music Understanding in Audio Language Models
"Video-to-Music generation seeks to generate musically appropriate background music that enhances audiovisual immersion for videos. However, current approaches suffer from two critical limitations: 1) incomplete representation of video details, leading to weak alignment, and 2) inadequate temporal and rhythmic correspondence, particularly in achieving precise beat synchronization. To address the challenges, we propose Video Echoed in Music (VeM), a latent music diffusion that generates high-quality soundtracks with semantic, temporal, and rhythmic alignment for input videos. To capture video details comprehensively, VeM employs a hierarchical video parsing that acts as a music conductor, orchestrating multi-level information across modalities. Modality-specific encoders, coupled with a storyboard-guided cross-attention mechanism (SG-CAtt), integrate semantic cues while maintaining temporal coherence through position and duration encoding. For rhythmic precision, the frame-level transition-beat aligner and adapter (TB-As) dynamically synchronize visual scene transitions with music beats. We further contribute a novel video-music paired dataset sourced from e-commerce advertisements and video-sharing platforms, which imposes stricter transition-beat synchronization requirements. Meanwhile, we introduce novel metrics tailored to the task. Experimental results demonstrate superiority, particularly in semantic relevance and rhythmic precision.",0,arxiv,MÃ¼zik,CC-BY/arXiv,"Video Echoed in Music: Semantic, Temporal, and Rhythmic Alignment for Video-to-Music Generation"
"Video-to-music (V2M) generation aims to create music that aligns with visual content. However, two main challenges persist in existing methods: (1) the lack of explicit rhythm modeling hinders audiovisual temporal alignments; (2) effectively integrating various visual features to condition music generation remains non-trivial. To address these issues, we propose Diff-V2M, a general V2M framework based on a hierarchical conditional diffusion model, comprising two core components: visual feature extraction and conditional music generation. For rhythm modeling, we begin by evaluating several rhythmic representations, including low-resolution mel-spectrograms, tempograms, and onset detection functions (ODF), and devise a rhythmic predictor to infer them directly from videos. To ensure contextual and affective coherence, we also extract semantic and emotional features. All features are incorporated into the generator via a hierarchical cross-attention mechanism, where emotional features shape the affective tone via the first layer, while semantic and rhythmic features are fused in the second cross-attention layer. To enhance feature integration, we introduce timestep-aware fusion strategies, including feature-wise linear modulation (FiLM) and weighted fusion, allowing the model to adaptively balance semantic and rhythmic cues throughout the diffusion process. Extensive experiments identify low-resolution ODF as a more effective signal for modeling musical rhythm and demonstrate that Diff-V2M outperforms existing models on both in-domain and out-of-domain datasets, achieving state-of-the-art performance in terms of objective metrics and subjective comparisons. Demo and code are available at https://Tayjsl97.github.io/Diff-V2M-Demo/.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Diff-V2M: A Hierarchical Conditional Diffusion Model with Explicit Rhythmic Modeling for Video-to-Music Generation
"Subjective room acoustics impressions play an important role for the performance and reception of music in concert venues and auralizations. Therefore, room acoustics since the 20th century dealt with the relationship between objective, acoustic parameters and subjective impressions of room acoustics. One common approach is to correlate acoustic measures with experts' subjective ratings of rooms as recalled from their long-term memory, and explain them using acoustical measures. Another approach is to let listeners rate auralized room acoustics on bipolar scales and find objective correlates. In this study, we present an alternative approach to characterizing the subjective impressions of room acoustics. We concolve music with binaural room impulse response measurements and utilize Multi Dimensional Scaling (MDS) to identify the perceptual dimensions of room acoustics. Results show that the perception of room acoustics has $5$ dimensions that can be explained by the (psycho-)acoustical measures echo density, fractal correlation dimension, roughness, loudness, and early decay time.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Non-verbal Perception of Room Acoustics using Multi Dimensional Scaling Metho
"The Next-generation Extended Wavelength Multi-band Sub/millimeter Inductance Camera (NEW-MUSIC), located on the Leighton Chajnantor Telescope (LCT), will be the first six-band trans-millimeter wave polarimeter. This paper proposes a broadband, hierarchical phased-array antenna with integrated band-defining filters necessary to realize NEW-MUSIC. It covers a spectral bandwidth of 2.4 octaves from 80~GHz to 420~GHz, a frequency range ideal for studying trans-millimeter emission from a range of time-domain sources, using the Sunyaev-Zeldovich effects to study hot plasmas in galaxy clusters and galaxies, and to observe dusty sources, from star-forming regions in our galaxy to high-redshift dusty, star-forming galaxies. To achieve these goals, three groups of superconducting lumped-element on-chip low-pass/band-pass filter-banks were designed to hierarchically sum the superconducting, broadband, non-resonant, slot-dipole antenna arrays and band-pass filter the trans-mm light before outputting it on microstripline to detectors (KIDs in the case of NEW-MUSIC).",0,arxiv,MÃ¼zik,CC-BY/arXiv,"Design of a Six-band, 2.4-Octave (80--420 GHz) Hierarchically Summed Phased-Array Slot-Dipole Antenna Array for NEW-MUSIC"
"We report on the optical characterization of the AlMn kinetic inductance detectors (KIDs) in development for use in the Next-generation Extended Wavelength-MUltiband Sub/millimeter Inductance Camera (NEW-MUSIC) on the Leighton Chajnantor Telescope (LCT). NEW-MUSIC will cover 80-420 GHz, split into six spectral bands, with polarimetry. This broad spectral coverage will enable study of a range of scientific topics such as the accretion and feedback in galaxies and galaxy cluster evolution via the Sunyaev-Zeldovich effect, the transient synchrotron emission from the explosive deaths of massive stars and other time-domain phenomena, and dusty sources from low to high redshift (with polarization). Al KIDs have already been demonstrated for bands 2-5. AlMn KIDs will be used for the 90~GHz band, as Al's pair-breaking energy is too high. However, AlMn has only barely been explored as a KID material. To this end, we first improved the modeling techniques used for Al KIDs within BCS theory by eliminating the use of analytical approximations for the expressions of the complex conductivity and found these changes reduced fit parameter degeneracy in the analysis of AlMn. Then, we tested the addition of a gap smearing parameter, a standard extension to BCS theory in use for high kinetic inductance materials, and found it did not improve the fits.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Improved Modeling of Quasi-Static Thermal and Optical Response of Lumped-Element Aluminum Manganese KIDs
"We present measurements of the low-frequency noise of microstrip-coupled, lumped-element aluminum kinetic inductance detectors that use hydrogenated amorphous silicon parallel-plate capacitors (Al/a-Si:H MS-PPC-LEKIDs), which are under development for the Next-generation Extended Wavelength Multiband Submillimeter Inductance Camera (NEW-MUSIC). We show that, under dark conditions, these devices are generation recombination (GR) noise dominated down to 0.1 Hz and, under optical load, they are likely dominated by GR and photon noise down to tenths of a Hz and possibly lower, both in spite of the use of a-Si:H PPCs. Our measurements set limits on the low-frequency two-level-system (TLS) noise of the a-Si:H material that are consistent with higher frequency measurements in the 0.1-10 kHz regime. These results establish that our MS-PPC-LEKID design for NEW-MUSIC will be photon-noise-limited under a range of observing conditions and, more generally, that a-Si:H PPC-KIDs are a viable new detector technology for even low modulation-rate applications such as astronomy.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Low-Frequency Noise Performance of Microstrip-Coupled Lumped-Element Aluminum KIDs using Hydrogenated Amorphous Silicon Parallel-Plate Capacitors for NEW-MUSIC
"We perform a study of electromagnetic radiation in heavy-ion collisions at Relativistic Heavy Ion Collider (RHIC) Beam Energy Scan (BES) and SPS energies using the iEBE-MUSIC framework, which includes 3D dynamical Monte Carlo Glauber initial conditions, MUSIC (3+1)D viscous relativistic hydrodynamics, and the UrQMD hadronic afterburner. The multistage modeling has been calibrated to hadronic data at RHIC-BES energies using a Bayesian analysis. Integrating the thermal photon emission rates with the medium evolution, we study the direct photon yield and elliptic flow and how they vary with collision energy and emission source. We compare with results obtained by the STAR and PHENIX Collaborations. We employ next-to-leading order thermal QCD dilepton emission rates to compute dilepton invariant mass spectra and extract the effective temperature of the quark-gluon plasma at different collision energies.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Electromagnetic Radiation from Baryon-Rich Matter in Heavy-Ion Collisions
"We evaluate five Transformer-based strategies for chord-conditioned melody and bass generation using a set of music theory-motivated metrics capturing pitch content, pitch interval size, and chord tone usage. The evaluated models include (1) no chord conditioning, (2) independent line chord-conditioned generation, (3) bass-first chord-conditioned generation, (4) melody-first chord-conditioned generation, and (5) chord-conditioned co-generation. We show that chord-conditioning improves the replication of stylistic pitch content and chord tone usage characteristics, particularly for the bass-first model.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Chord-conditioned Melody and Bass Generation
"Text-to-music generation technology is progressing rapidly, creating new opportunities for musical composition and editing. However, existing music editing methods often fail to preserve the source music's temporal structure, including melody and rhythm, when altering particular attributes like instrument, genre, and mood. To address this challenge, this paper conducts an in-depth probing analysis on attention maps within AudioLDM 2, a diffusion-based model commonly used as the backbone for existing music editing methods. We reveal a key finding: cross-attention maps encompass details regarding distinct musical characteristics, and interventions on these maps frequently result in ineffective modifications. In contrast, self-attention maps are essential for preserving the temporal structure of the source music during its conversion into the target music. Building upon this understanding, we present Melodia, a training-free technique that selectively manipulates self-attention maps in particular layers during the denoising process and leverages an attention repository to store source music information, achieving accurate modification of musical characteristics while preserving the original structure without requiring textual descriptions of the source music. Additionally, we propose two novel metrics to better evaluate music editing methods. Both objective and subjective experiments demonstrate that our approach achieves superior results in terms of textual adherence and structural integrity across various datasets. This research enhances comprehension of internal mechanisms within music generation models and provides improved control for music creation.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Melodia: Training-Free Music Editing Guided by Attention Probing in Diffusion Models
"Music mixing involves combining individual tracks into a cohesive mixture, a task characterized by subjectivity where multiple valid solutions exist for the same input. Existing automatic mixing systems treat this task as a deterministic regression problem, thus ignoring this multiplicity of solutions. Here we introduce MEGAMI (Multitrack Embedding Generative Auto MIxing), a generative framework that models the conditional distribution of professional mixes given unprocessed tracks. MEGAMI uses a track-agnostic effects processor conditioned on per-track generated embeddings, handles arbitrary unlabeled tracks through a permutation-equivariant architecture, and enables training on both dry and wet recordings via domain adaptation. Our objective evaluation using distributional metrics shows consistent improvements over existing methods, while listening tests indicate performances approaching human-level quality across diverse musical genres.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Automatic Music Mixing using a Generative Model of Effect Embeddings
"We summarize our recent investigations on how causality violations in Israel-Stewart-type relativistic viscous hydrodynamic simulations can give rise to both analytical and numerical instabilities. The classification of spacetime regions into causal and stable (""good""), acausal but stable (""bad""), and acausal and unstable (""ugly"") is reviewed. We compare the predictions of the MUSIC hydrodynamic solver with an analytical solution, and demonstrate how the acausality-driven instabilities develop in a simple one-dimensional scenario.",0,arxiv,MÃ¼zik,CC-BY/arXiv,What happens in hydrodynamic simulations of heavy-ion collisions when causality is violated?
"Many formal languages of contemporary mathematical music theory -- particularly those employing category theory -- are powerful but cumbersome: ideas that are conceptually simple frequently require expression through elaborate categorical constructions such as functor categories. This paper proposes a remedy in the form of a type-theoretic symbolic language that enables mathematical music theorists to build and reason about musical structures more intuitively, without relinquishing the rigor of their categorical foundations. Type theory provides a syntax in which elements, functions, and relations can be expressed in simple terms, while categorical semantics supplies their mathemusical interpretation. Within this system, reasoning itself becomes constructive: propositions and proofs are treated as objects, yielding a framework in which the formation of structures and the reasoning about them take place within the same mathematical language. The result is a concise and flexible formalism that restores conceptual transparency to mathemusical thought and supports new applications, illustrated here through the theory of voice-leading spaces.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Type Theory for the Working Mathematical Music Theorist
"Although a variety of transformers have been proposed for symbolic music generation in recent years, there is still little comprehensive study on how specific design choices affect the quality of the generated music. In this work, we systematically compare different datasets, model architectures, model sizes, and training strategies for the task of symbolic piano music generation. To support model development and evaluation, we examine a range of quantitative metrics and analyze how well they correlate with human judgment collected through listening studies. Our best-performing model, a 950M-parameter transformer trained on 80K MIDI files from diverse genres, produces outputs that are often rated as human-composed in a Turing-style listening survey.",0,arxiv,MÃ¼zik,CC-BY/arXiv,"Generating Piano Music with Transformers: A Comparative Study of Scale, Data, and Metrics"
"In this paper, we trace the evolution of Music Information Retrieval (MIR) over the past 25 years. While MIR gathers all kinds of research related to music informatics, a large part of it focuses on signal processing techniques for music data, fostering a close relationship with the IEEE Audio and Acoustic Signal Processing Technical Commitee. In this paper, we reflect the main research achievements of MIR along the three EDICS related to music analysis, processing and generation. We then review a set of successful practices that fuel the rapid development of MIR research. One practice is the annual research benchmark, the Music Information Retrieval Evaluation eXchange, where participants compete on a set of research tasks. Another practice is the pursuit of reproducible and open research. The active engagement with industry research and products is another key factor for achieving large societal impacts and motivating younger generations of students to join the field. Last but not the least, the commitment to diversity, equity and inclusion ensures MIR to be a vibrant and open community where various ideas, methodologies, and career pathways collide. We finish by providing future challenges MIR will have to face.",0,arxiv,MÃ¼zik,CC-BY/arXiv,"Twenty-Five Years of MIR Research: Achievements, Practices, Evaluations, and Future Challenges"
"Recent advances in latent diffusion models have demonstrated state-of-the-art performance in high-dimensional time-series data synthesis while providing flexible control through conditioning and guidance. However, existing methodologies primarily rely on musical context or natural language as the main modality of interacting with the generative process, which may not be ideal for expert users who seek precise fader-like control over specific musical attributes. In this work, we explore the application of denoising diffusion processes as plug-and-play latent constraints for unconditional symbolic music generation models. We focus on a framework that leverages a library of small conditional diffusion models operating as implicit probabilistic priors on the latents of a frozen unconditional backbone. While previous studies have explored domain-specific use cases, this work, to the best of our knowledge, is the first to demonstrate the versatility of such an approach across a diverse array of musical attributes, such as note density, pitch range, contour, and rhythm complexity. Our experiments show that diffusion-driven constraints outperform traditional attribute regularization and other latent constraints architectures, achieving significantly stronger correlations between target and generated attributes while maintaining high perceptual quality and diversity.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Conditional Diffusion as Latent Constraints for Controllable Symbolic Music Generation
"Music induced painting is a unique artistic practice, where visual artworks are created under the influence of music. Evaluating whether a painting faithfully reflects the music that inspired it poses a challenging perceptual assessment task. Existing methods primarily rely on emotion recognition models to assess the similarity between music and painting, but such models introduce considerable noise and overlook broader perceptual cues beyond emotion. To address these limitations, we propose a novel framework for music induced painting assessment that directly models perceptual coherence between music and visual art. We introduce MPD, the first large scale dataset of music painting pairs annotated by domain experts based on perceptual coherence. To better handle ambiguous cases, we further collect pairwise preference annotations. Building on this dataset, we present MPJudge, a model that integrates music features into a visual encoder via a modulation based fusion mechanism. To effectively learn from ambiguous cases, we adopt Direct Preference Optimization for training. Extensive experiments demonstrate that our method outperforms existing approaches. Qualitative results further show that our model more accurately identifies music relevant regions in paintings.",0,arxiv,MÃ¼zik,CC-BY/arXiv,MPJudge: Towards Perceptual Assessment of Music-Induced Paintings
"Explicit latent variable models provide a flexible yet powerful framework for data synthesis, enabling controlled manipulation of generative factors. With latent variables drawn from a tractable probability density function that can be further constrained, these models enable continuous and semantically rich exploration of the output space by navigating their latent spaces. Structured latent representations are typically obtained through the joint minimization of regularization loss functions. In variational information bottleneck models, reconstruction loss and Kullback-Leibler Divergence (KLD) are often linearly combined with an auxiliary Attribute-Regularization (AR) loss. However, balancing KLD and AR turns out to be a very delicate matter. When KLD dominates over AR, generative models tend to lack controllability; when AR dominates over KLD, the stochastic encoder is encouraged to violate the standard normal prior. We explore this trade-off in the context of symbolic music generation with explicit control over continuous musical attributes. We show that existing approaches struggle to jointly minimize both regularization objectives, whereas suitable attribute transformations can help achieve both controllability and regularization of the target latent dimensions.",0,arxiv,MÃ¼zik,CC-BY/arXiv,On the Joint Minimization of Regularization Loss Functions in Deep Variational Bayesian Methods for Attribute-Controlled Symbolic Music Generation
"Categorical predictors are omnipresent in everyday regression practice: in fact, most regression data involve some categorical predictors, and this tendency is increasing in modern applications with more complex structures and larger data sizes. However, including too many categories in a regression model would seriously hamper accuracy, as the information in the data is fragmented by the multitude of categories. In this paper, we introduce a systematic method to reduce the complexity of categorical predictors by adaptively collapsing categories in regressions, so as to enhance the performance of regression estimation. Our method is based on the {\em pairwise vector fused LASSO}, which automatically fuses the categories that bear a similar regression relation with the response. We develop our method under a wide class of regression models defined by a general loss function, which includes linear models and generalized linear models as special cases. We rigorously established the category collapsing consistency of our method, developed an Inexact Proximal Gradient Descent algorithm to implement it, and proved the feasibility and convergence of our algorithm. Through simulations and an application to Spotify music data, we demonstrate that our method can effectively reduce categorical complexity while improving prediction performance, making it a powerful tool for regression with mixed predictors.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Collapsing Categories for Regression with Mixed Predictors
"Large language models (LLMs) have advanced in text and vision, but their reasoning on audio remains limited. Most existing methods rely on dense audio embeddings, which are difficult to interpret and often fail on structured reasoning tasks. Caption-based approaches, introduced in recent benchmarks such as MMAU, improve performance by translating audio into text, yet still depend on dense embeddings as input, offering little insight when models fail.   We present SAR-LM, a symbolic audio reasoning pipeline that builds on this caption-based paradigm by converting audio into structured, human-readable features across speech, sound events, and music. These symbolic inputs support both reasoning and transparent error analysis, enabling us to trace failures to specific features. Across three benchmarks, MMAU, MMAR, and OmniBench, SAR-LM achieves competitive results, while prioritizing interpretability as its primary contribution.",0,arxiv,MÃ¼zik,CC-BY/arXiv,SAR-LM: Symbolic Audio Reasoning with Large Language Models
"Interfaces for contemporary large language, generative media, and perception AI models are often engineered for single user interaction. We investigate ritual as a design scaffold for developing collaborative, multi-user human-AI engagement. We consider the specific case of an immersive staging of the musical Xanadu performed at UCLA in Spring 2025. During a two-week run, over five hundred audience members contributed sketches and jazzercise moves that vision language models translated to virtual scenery elements and from choreographic prompts. This paper discusses four facets of interaction-as-ritual within the show: audience input as offerings that AI transforms into components of the ritual; performers as ritual guides, demonstrating how to interact with technology and sorting audience members into cohorts; AI systems as instruments ""played"" by the humans, in which sensing, generative components, and stagecraft create systems that can be mastered over time; and reciprocity of interaction, in which the show's AI machinery guides human behavior as well as being guided by humans, completing a human-AI feedback loop that visibly reshapes the virtual world. Ritual served as a frame for integrating linear narrative, character identity, music and interaction. The production explored how AI systems can support group creativity and play, addressing a critical gap in prevailing single user AI design paradigms.",0,arxiv,MÃ¼zik,CC-BY/arXiv,"AI as intermediary in modern-day ritual: An immersive, interactive production of the roller disco musical Xanadu at UCLA"
"Neural audio codecs have recently enabled high-fidelity reconstruction at high compression rates, especially for speech. However, speech and non-speech audio exhibit fundamentally different spectral characteristics: speech energy concentrates in narrow bands around pitch harmonics (80-400 Hz), while non-speech audio requires faithful reproduction across the full spectrum, particularly preserving higher frequencies that define timbre and texture. This poses a challenge: speech-optimized neural codecs suffer degradation on music or sound. Treating the full spectrum holistically is suboptimal: frequency bands have vastly different information density and perceptual importance by content type, yet full-band approaches apply uniform capacity across frequencies without accounting for these acoustic structures. To address this gap, we propose BSCodec (Band-Split Codec), a novel neural audio codec architecture that splits the spectral dimension into separate bands and compresses each band independently. Experimental results demonstrate that BSCodec achieves superior reconstruction over baselines across sound and music, while maintaining competitive quality in the speech domain, when trained on the same combined dataset of speech, music and sound. Downstream benchmark tasks further confirm that BSCodec shows strong potential for use in downstream applications.",0,arxiv,MÃ¼zik,CC-BY/arXiv,BSCodec: A Band-Split Neural Codec for High-Quality Universal Audio Reconstruction
"In recent years, the music research community has examined risks of AI models for music, with generative AI models in particular, raised concerns about copyright, deepfakes, and transparency. In our work, we raise concerns about cultural and genre biases in AI for music systems (music-AI systems) which affect stakeholders including creators, distributors, and listeners shaping representation in AI for music. These biases can misrepresent marginalized traditions, especially from the Global South, producing inauthentic outputs (e.g., distorted ragas) that reduces creators' trust on these systems. Such harms risk reinforcing biases, limiting creativity, and contributing to cultural erasure. To address this, we offer recommendations at dataset, model and interface level in music-AI systems.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Who Gets Heard? Rethinking Fairness in AI for Music Systems
"This work demonstrates ""slimmable Neural Amp Models"", whose size and computational cost can be changed without additional training and with negligible computational overhead, enabling musicians to easily trade off between the accuracy and compute of the models they are using. The method's performance is quantified against commonly-used baselines, and a real-time demonstration of the model in an audio effect plug-in is developed.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Slimmable NAM: Neural Amp Models with adjustable runtime computational cost
"Musical instrument classification is essential for music information retrieval (MIR) and generative music systems. However, research on non-Western traditions, particularly Persian music, remains limited. We address this gap by introducing a new dataset of isolated recordings covering seven traditional Persian instruments, two common but originally non-Persian instruments (i.e., violin, piano), and vocals. We propose a culturally informed data augmentation strategy that generates realistic polyphonic mixtures from monophonic samples. Using the MERT model (Music undERstanding with large-scale self-supervised Training) with a classification head, we evaluate our approach with out-of-distribution data which was obtained by manually labeling segments of traditional songs. On real-world polyphonic Persian music, the proposed method yielded the best ROC-AUC (0.795), highlighting complementary benefits of tonal and temporal coherence. These results demonstrate the effectiveness of culturally grounded augmentation for robust Persian instrument recognition and provide a foundation for culturally inclusive MIR and diverse music generation systems.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Persian Musical Instruments Classification Using Polyphonic Data Augmentation
"The proliferation of distorted, compressed, and manipulated music on modern media platforms like TikTok motivates the development of more robust audio fingerprinting techniques to identify the sources of musical recordings. In this paper, we develop and evaluate new neural audio fingerprinting techniques with the aim of improving their robustness. We make two contributions to neural fingerprinting methodology: (1) we use a pretrained music foundation model as the backbone of the neural architecture and (2) we expand the use of data augmentation to train fingerprinting models under a wide variety of audio manipulations, including time streching, pitch modulation, compression, and filtering. We systematically evaluate our methods in comparison to two state-of-the-art neural fingerprinting models: NAFP and GraFPrint. Results show that fingerprints extracted with music foundation models (e.g., MuQ, MERT) consistently outperform models trained from scratch or pretrained on non-musical audio. Segment-level evaluation further reveals their capability to accurately localize fingerprint matches, an important practical feature for catalog management.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Robust Neural Audio Fingerprinting using Music Foundation Models
"We argue that training autoencoders to reconstruct inputs from noised versions of their encodings, when combined with perceptual losses, yields encodings that are structured according to a perceptual hierarchy. We demonstrate the emergence of this hierarchical structure by showing that, after training an audio autoencoder in this manner, perceptually salient information is captured in coarser representation structures than with conventional training. Furthermore, we show that such perceptual hierarchies improve latent diffusion decoding in the context of estimating surprisal in music pitches and predicting EEG-brain responses to music listening. Pretrained weights are available on github.com/CPJKU/pa-audioic.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Perceptually Aligning Representations of Music via Noise-Augmented Autoencoders
"In this study, we introduce Emo100DB: a dataset consisting of improvised songs that were recorded and transcribed with emotion data based on Russell's circumplex model of emotion. The dataset was developed by collecting improvised songs that consist of melody, lyrics, and an instrumental accompaniment played, sung, and recorded by 20 young adults. Before recording each song, the participants were asked to report their emotional state, with the axes representing arousal and valence based on Russell's circumplex model of emotions. The dataset is organized into four emotion quadrants, and it includes the lyrics text and MIDI file of the melody extracted from the participant recordings, along with the original audio in WAV format. By providing an integrated composition of data and analysis, this study aims to offer a comprehensive dataset that allows for a diverse exploration of the relationship between music and emotion.",0,arxiv,MÃ¼zik,CC-BY/arXiv,EMO100DB: An Open Dataset of Improvised Songs with Emotion Data
"Music editing has emerged as an important and practical area of artificial intelligence, with applications ranging from video game and film music production to personalizing existing tracks according to user preferences. However, existing models face significant limitations, such as being restricted to editing synthesized music generated by their own models, requiring highly precise prompts, or necessitating task-specific retraining, thus lacking true zero-shot capability. leveraging recent advances in rectified flow and diffusion transformers, we introduce MusRec, a zero-shot text-to-music editing model capable of performing diverse editing tasks on real-world music efficiently and effectively. Experimental results demonstrate that our approach outperforms existing methods in preserving musical content, structural consistency, and editing fidelity, establishing a strong foundation for controllable music editing in real-world scenarios.",0,arxiv,MÃ¼zik,CC-BY/arXiv,MusRec: Zero-Shot Text-to-Music Editing via Rectified Flow and Diffusion Transformers
"We present MIDI-LLM, an LLM for generating multitrack MIDI music from free-form text prompts. Our approach expands a text LLM's vocabulary to include MIDI tokens, and uses a two-stage training recipe to endow text-to-MIDI abilities. By preserving the original LLM's parameter structure, we can directly leverage the vLLM library for accelerated inference. Experiments show that MIDI-LLM achieves higher quality, better text control, and faster inference compared to the recent Text2midi model. Live demo at https://midi-llm-demo.vercel.app.",0,arxiv,MÃ¼zik,CC-BY/arXiv,MIDI-LLM: Adapting Large Language Models for Text-to-MIDI Music Generation
"Emotions are fundamental to the creation and perception of music performances. However, achieving human-like expression and emotion through machine learning models for performance rendering remains a challenging task. In this work, we present SyMuPe, a novel framework for developing and training affective and controllable symbolic piano performance models. Our flagship model, PianoFlow, uses conditional flow matching trained to solve diverse multi-mask performance inpainting tasks. By design, it supports both unconditional generation and infilling of music performance features. For training, we use a curated, cleaned dataset of 2,968 hours of aligned musical scores and expressive MIDI performances. For text and emotion control, we integrate a piano performance emotion classifier and tune PianoFlow with the emotion-weighted Flan-T5 text embeddings provided as conditional inputs. Objective and subjective evaluations against transformer-based baselines and existing models show that PianoFlow not only outperforms other approaches, but also achieves performance quality comparable to that of human-recorded and transcribed MIDI samples. For emotion control, we present and analyze samples generated under different text conditioning scenarios. The developed model can be integrated into interactive applications, contributing to the creation of more accessible and engaging music performance systems.",0,arxiv,MÃ¼zik,CC-BY/arXiv,SyMuPe: Affective and Controllable Symbolic Music Performance
"This paper focuses on the often-overlooked aspect of perceived voice femininity in singing voices. While existing research has examined perceived voice femininity in speech, the same concept has not yet been studied in singing voice. The analysis of gender bias in music content could benefit from such study. To address this gap, we design a stimuli-based survey to measure perceived singing voice femininity (PSVF), and collect responses from 128 participants. Our analysis reveals intriguing insights into how PSVF varies across different demographic groups. Furthermore, we propose an automatic PSVF prediction model by fine-tuning an x-vector model, offering a novel tool for exploring gender stereotypes related to voices in music content analysis beyond binary sex classification. This study contributes to a deeper understanding of the complexities surrounding perceived femininity in singing voices by analyzing survey and proposes an automatic tool for future research.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Perceived Femininity in Singing Voice: Analysis and Prediction
"Endowing robot hands with human-level dexterity has been a long-standing goal in robotics. Bimanual robotic piano playing represents a particularly challenging task: it is high-dimensional, contact-rich, and requires fast, precise control. We present OmniPianist, the first agent capable of performing nearly one thousand music pieces via scalable, human-demonstration-free learning. Our approach is built on three core components. First, we introduce an automatic fingering strategy based on Optimal Transport (OT), allowing the agent to autonomously discover efficient piano-playing strategies from scratch without demonstrations. Second, we conduct large-scale Reinforcement Learning (RL) by training more than 2,000 agents, each specialized in distinct music pieces, and aggregate their experience into a dataset named RP1M++, consisting of over one million trajectories for robotic piano playing. Finally, we employ a Flow Matching Transformer to leverage RP1M++ through large-scale imitation learning, resulting in the OmniPianist agent capable of performing a wide range of musical pieces. Extensive experiments and ablation studies highlight the effectiveness and scalability of our approach, advancing dexterous robotic piano playing at scale.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Dexterous Robotic Piano Playing at Scale
"Audio denoising is critical in signal processing, enhancing intelligibility and fidelity for applications like restoring musical recordings. This paper presents a proof-of-concept for adapting a state-of-the-art neural audio codec, the Descript Audio Codec (DAC), for music denoising. This work overcomes the limitations of traditional architectures like U-Nets by training the model on a large-scale, custom-synthesized dataset built from diverse sources. Training is guided by a multi objective loss function that combines time-domain, spectral, and signal-level fidelity metrics. Ultimately, this paper aims to present a PoC for high-fidelity, generative audio restoration.",0,arxiv,MÃ¼zik,CC-BY/arXiv,ADNAC: Audio Denoiser using Neural Audio Codec
"While generative models for music composition are increasingly capable, their adoption by musicians is hindered by text-prompting, an asynchronous workflow disconnected from the embodied, responsive nature of instrumental performance. To address this, we introduce Aria-Duet, an interactive system facilitating a real-time musical duet between a human pianist and Aria, a state-of-the-art generative model, using a Yamaha Disklavier as a shared physical interface. The framework enables a turn-taking collaboration: the user performs, signals a handover, and the model generates a coherent continuation performed acoustically on the piano. Beyond describing the technical architecture enabling this low-latency interaction, we analyze the system's output from a musicological perspective, finding the model can maintain stylistic semantics and develop coherent phrasal ideas, demonstrating that such embodied systems can engage in musically sophisticated dialogue and open a promising new path for human-AI co-creation.",0,arxiv,MÃ¼zik,CC-BY/arXiv,The Ghost in the Keys: A Disklavier Demo for Human-AI Musical Co-Creativity
"While videos have become increasingly prevalent in delivering information across different educational and professional contexts, individuals with ADHD often face attention challenges when watching informational videos due to the dynamic, multimodal, yet potentially distracting video elements. To understand and address this critical challenge, we designed FocusView, a video customization interface that allows viewers with ADHD to customize informational videos from different aspects. We evaluated FocusView with 12 participants with ADHD and found that FocusView significantly improved the viewability of videos by reducing distractions. Through the study, we uncovered participants' diverse perceptions of video distractions (e.g., background music as a distraction vs. stimulation boost) and their customization preferences, highlighting unique ADHD-relevant needs in designing video customization interfaces (e.g., reducing the number of options to avoid distraction caused by customization itself). We further derived design considerations for future video customization systems for the ADHD community.",0,arxiv,MÃ¼zik,CC-BY/arXiv,FocusView: Understanding and Customizing Informational Video Watching Experiences for Viewers with ADHD
"Music language models (Music LMs), like vision language models, leverage multimodal representations to answer natural language queries about musical audio recordings. Although Music LMs are reportedly improving, we find that current evaluations fail to capture whether their answers are correct. Specifically, for all Music LMs that we examine, widely-used evaluation metrics such as BLEU, METEOR, and BERTScore fail to measure anything beyond linguistic fluency of the model's responses. To measure the true performance of Music LMs, we propose (1) a better general-purpose evaluation metric for Music LMs adapted to the music domain and (2) a factual evaluation framework to quantify the correctness of a Music LM's responses. Our framework is agnostic to the modality of the question-answering model and could be generalized to quantify performance in other open-ended question-answering domains. We use open datasets in our experiments and will release all code on publication.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Factual and Musical Evaluation Metrics for Music Language Models
"Gesture recognition is an essential component of human-computer interaction (HCI), facilitating seamless interconnectivity between users and computer systems without physical touch. This paper introduces an innovative application of vision-based dynamic gesture recognition (VDGR) for real-time music composition through gestures. To implement this application, we generate a custom gesture dataset that encompasses over 15000 samples across 21 classes, incorporating 7 musical notes each manifesting at three distinct pitch levels. To effectively deal with the modest volume of training data and to accurately discern and prioritize complex gesture sequences for music creation, we develop a multi-layer attention-based gated recurrent unit (MLA-GRU) model, in which gated recurrent unit (GRU) is used to learn temporal patterns from the observed sequence and an attention layer is employed to focus on musically pertinent gesture segments. Our empirical studies demonstrate that MLA-GRU significantly surpasses the classical GRU model, achieving a remarkable accuracy of 96.83% compared to the baseline's 86.7%. Moreover, our approach exhibits superior efficiency and processing speed, which are crucial for interactive applications. Using our proposed system, we believe that people will interact with music in a new and exciting way. It not only advances HCI experiences but also highlights MLA-GRU's effectiveness in scenarios demanding swift and precise gesture recognition.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Rhythm in the Air: Vision-based Real-Time Music Generation through Gestures
"Improvisation-the art of spontaneous creation that unfolds moment-to-moment without a scripted outcome-requires practitioners to continuously sense, adapt, and create anew. It is a fundamental mode of human creativity spanning music, dance, and everyday life. The open-ended nature of improvisation produces a stream of novel, unrepeatable moments-an aspect highly valued in artistic creativity. In parallel, open-endedness (OE)-a system's capacity for unbounded novelty and endless ""interestingness""-is exemplified in natural or cultural evolution and has been considered ""the last grand challenge"" in artificial life (ALife). The rise of generative AI now raises the question in computational creativity (CC) research: What makes a ""good"" improvisation for AI? Can AI learn to improvise in a genuinely open-ended way? In this work-in-progress paper, we report insights from in-depth interviews with 6 experts in improvisation across dance, music, and contact improvisation. We draw systemic connections between human improvisational arts and the design of future experiential AI agents that could improvise alone or alongside humans-or even with other AI agents-embodying qualities of improvisation drawn from practice: active listening (umwelt and awareness), being in the time (mindfulness and ephemerality), embracing the unknown (source of randomness and serendipity), non-judgmental flow (acceptance and dynamical stability, balancing structure and surprise (unpredictable criticality at edge of chaos), imaginative metaphor (synaesthesia and planning), empathy, trust, boundary, and care (mutual theory of mind), and playfulness and intrinsic motivation (maintaining interestingness).",0,arxiv,MÃ¼zik,CC-BY/arXiv,On Improvisation and Open-Endedness: Insights for Experiential AI
"WaveRoll is an interactive JavaScript library that enables comparative visualization and synchronized playback of multiple MIDI piano rolls on a browser. It addresses a specific evaluation need in Automatic Music Transcription (AMT), contrasting multiple MIDI outputs produced from the same input. The library displays multiple MIDI tracks on a single, time-aligned grid with synchronized audio, allowing users to compare pitch and timing, identify missed or extra notes, and observe onset and offset differences, as well as section-level patterns. We expect that such comparisons would assist in model evaluation and error analysis, and help readers to understand the model behavior better. The open-source library is available at https://github.com/crescent-stdio/wave-roll",0,arxiv,MÃ¼zik,CC-BY/arXiv,WaveRoll: JavaScript Library for Comparative MIDI Piano-Roll Visualization
"In this paper, we discuss the matching of the holographic equation of state with the equation of Hadron Resonance Gas for studying the nuclear matter properties within the framework of relativistic heavy-ion collisions. Machine learning methods are applied to the calibration of model's free parameters using the lattice QCD results for the physical values of quark masses. One of the most advanced procedures for matching is used with the function that approximate behavior of both models on particular limit adopted from NEOS equation. Final hadronic spectra are obtained within multi-staged numerical approach of the iEBE-MUSIC and SMASH-vHLLE packages. The code of relativistic hydrodynamics is modified by implementing a tabulated holographic equation of state, enabling simulations of quark-gluon plasma evolution with dynamically generated initial conditions via the 3D Monte Carlo Glauber Model and SMASH. Hybrid iSS+UrQMD and Hadron Sampler+SMASH approaches are utilized at the freeze-out stage.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Holographic equation of state matched with hadron gas equation as a tool for the study of the quark-gluon plasma evolution
"Understanding the structural and cognitive underpinnings of musical compositions remains a key challenge in music theory and computational musicology. While traditional methods focus on harmony and rhythm, cognitive models such as the Implication-Realization (I-R) model and Temporal Gestalt theory offer insight into how listeners perceive and anticipate musical structure. This study presents a graph-based computational approach that operationalizes these models by segmenting melodies into perceptual units and annotating them with I-R patterns. These segments are compared using Dynamic Time Warping and organized into k-nearest neighbors graphs to model intra- and inter-segment relationships.   Each segment is represented as a node in the graph, and nodes are further labeled with melodic expectancy values derived from Schellenberg's two-factor I-R model-quantifying pitch proximity and pitch reversal at the segment level. This labeling enables the graphs to encode both structural and cognitive information, reflecting how listeners experience musical tension and resolution.   To evaluate the expressiveness of these graphs, we apply the Weisfeiler-Lehman graph kernel to measure similarity between and within compositions. Results reveal statistically significant distinctions between intra- and inter-graph structures. Segment-level analysis via multidimensional scaling confirms that structural similarity at the graph level reflects perceptual similarity at the segment level. Graph2vec embeddings and clustering demonstrate that these representations capture stylistic and structural features that extend beyond composer identity.   These findings highlight the potential of graph-based methods as a structured, cognitively informed framework for computational music analysis, enabling a more nuanced understanding of musical structure and style through the lens of listener perception.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Representing Classical Compositions through Implication-Realization Temporal-Gestalt Graphs
"Recommender systems (RSs) are intelligent filtering methods that suggest items to users based on their inferred preferences, derived from their interaction history on the platform. Collaborative filtering-based RSs rely on users past interactions to generate recommendations. However, when a user is new to the platform, referred to as a cold-start user, there is no historical data available, making it difficult to provide personalized recommendations. To address this, rating elicitation techniques can be used to gather initial ratings or preferences on selected items, helping to build an early understanding of the user's tastes. Rating elicitation approaches are generally categorized into two types: non-personalized and personalized. Decision tree-based rating elicitation is a personalized method that queries users about their preferences at each node of the tree until sufficient information is gathered. In this paper, we propose an extension to the decision tree approach for rating elicitation in the context of music recommendation. Our method: (i) elicits not only item ratings but also preferences on attributes such as genres to better cluster users, and (ii) uses item pairs instead of single items at each node to more effectively learn user preferences. Experimental results demonstrate that both proposed enhancements lead to improved performance, particularly with a reduced number of queries.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Pairwise and Attribute-Aware Decision Tree-Based Preference Elicitation for Cold-Start Recommendation
"As people nowadays increasingly rely on artificial intelligence (AI) to curate information and make decisions, assigning the appropriate amount of trust in automated intelligent systems has become ever more important. However, current measurements of trust in automation still largely rely on self-reports that are subjective and disruptive to the user. Here, we take music recommendation as a model to investigate the neural and cognitive processes underlying trust in automation. We observed that system accuracy was directly related to users' trust and modulated the influence of recommendation cues on music preference. Modelling users' reward encoding process with a reinforcement learning model further revealed that system accuracy, expected reward, and prediction error were related to oscillatory neural activity recorded via EEG and changes in pupil diameter. Our results provide a neurally grounded account of calibrating trust in automation and highlight the promises of a multimodal approach towards developing trustable AI systems.",0,arxiv,MÃ¼zik,CC-BY/arXiv,"Inferring trust in recommendation systems from brain, behavioural, and physiological data"
"Text-to-audio models are a type of generative model that produces audio output in response to a given textual prompt. Although level generators and the properties of the functional content that they create (e.g., playability) dominate most discourse in procedurally generated content (PCG), games that emotionally resonate with players tend to weave together a range of creative and multimodal content (e.g., music, sounds, visuals, narrative tone), and multimodal models have begun seeing at least experimental use for this purpose. However, it remains unclear what exactly such models generate, and with what degree of variability and fidelity: audio is an extremely broad class of output for a generative system to target.   Within the PCG community, expressive range analysis (ERA) has been used as a quantitative way to characterize generators' output space, especially for level generators. This paper adapts ERA to text-to-audio models, making the analysis tractable by looking at the expressive range of outputs for specific, fixed prompts. Experiments are conducted by prompting the models with several standardized prompts derived from the Environmental Sound Classification (ESC-50) dataset. The resulting audio is analyzed along key acoustic dimensions (e.g., pitch, loudness, and timbre). More broadly, this paper offers a framework for ERA-based exploratory evaluation of generative audio models.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Expressive Range Characterization of Open Text-to-Audio Models
"The subject of this work is to check how different types of music affect human emotions. While listening to music, a subjective survey and brain activity measurements were carried out using an EEG helmet. The aim is to demonstrate the impact of different music genres on emotions. The research involved a diverse group of participants of different gender and musical preferences. This had the effect of capturing a wide range of emotional responses to music. After the experiment, a relationship analysis of the respondents' questionnaires with EEG signals was performed. The analysis revealed connections between emotions and observed brain activity.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Exploring the correlation between the type of music and the emotions evoked: A study using subjective questionnaires and EEG
"We have investigated the prospects of probing the five-dimensional $U(1)_{L_Î¼- L_Ï„}$ interactions in present and future muon dump experiments, namely, NA64$_Î¼$, M$^3$, MuSIC, and a future muon beam dump experiment. These experiments are classified into two categories: the first two can probe processes where feebly interacting massive particles go into invisible channels, while the latter two can probe processes where these states decay into muon pairs. These two types of experiments are complementary in that they allow exploration of different parameter regions of a model. In our scenario, the presence of multiple massive gauge bosons as Kaluza-Klein (KK) particles leads to an enhancement in the signal events compared to the corresponding four-dimensional scenario. In particular, the decay process into muon pairs enables mass reconstruction of the parent particle, making it possible to directly demonstrate the existence of multiple KK particles in at least some parameter regions. This can provide clear evidence that the origin of the $U(1)_{L_Î¼- L_Ï„}$ interaction lies in five dimensions. Furthermore, the muon $(g-2)$ value, which is now consistent with the SM, can be used to exclude specific parameter regions for new particles interacting with muons. We also carefully discuss the non-trivial effects arising from nonzero kinetic mixing.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Muon Beam Dump Experiments explicate five-dimensional nature of $U(1)_{L_Î¼-L_Ï„}$
"Ambiguities in data and problem constraints can lead to diverse, equally plausible outcomes for a machine learning task. In beat and downbeat tracking, for instance, different listeners may adopt various rhythmic interpretations, none of which would necessarily be incorrect. To address this, we propose a contrastive self-supervised pre-training approach that leverages multiple hypotheses about possible positive samples in the data. Our model is trained to learn representations compatible with different such hypotheses, which are selected with a knowledge-based scoring function to retain the most plausible ones. When fine-tuned on labeled data, our model outperforms existing methods on standard benchmarks, showcasing the advantages of integrating domain knowledge with multi-hypothesis selection in music representation learning in particular.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Controlling Contrastive Self-Supervised Learning with Knowledge-Driven Multiple Hypothesis: Application to Beat Tracking
"We study the problem of measuring the popularity of artists in music streaming platforms and the ensuing methods to compensate them (from the revenues platforms raise by charging users). We uncover the space of popularity indices upon exploring the implications of several axioms capturing principles with normative appeal. As a result, we characterize several families of indices. Some of them are intimately connected to the Shapley value, the central tool in cooperative game theory. Our characterizations might help to address the rising concern in the music industry to explore new methods that reward artists more appropriately. We actually connect our families to the new royalties models, recently launched by Spotify and Deezer.",0,arxiv,MÃ¼zik,CC-BY/arXiv,New methods to compensate artists in music streaming platforms
"Artificial intelligence systems based on large language models (LLMs) can now generate coherent text, music, and images, yet they operate without a persistent state: each inference reconstructs context from scratch. This paper introduces the Narrative Continuity Test (NCT) -- a conceptual framework for evaluating identity persistence and diachronic coherence in AI systems. Unlike capability benchmarks that assess task performance, the NCT examines whether an LLM remains the same interlocutor across time and interaction gaps. The framework defines five necessary axes -- Situated Memory, Goal Persistence, Autonomous Self-Correction, Stylistic & Semantic Stability, and Persona/Role Continuity -- and explains why current architectures systematically fail to support them. Case analyses (Character.\,AI, Grok, Replit, Air Canada) show predictable continuity failures under stateless inference. The NCT reframes AI evaluation from performance to persistence, outlining conceptual requirements for future benchmarks and architectural designs that could sustain long-term identity and goal coherence in generative models.",0,arxiv,MÃ¼zik,CC-BY/arXiv,The Narrative Continuity Test: A Conceptual Framework for Evaluating Identity Persistence in AI Systems
"Dance-to-music (D2M) generation aims to automatically compose music that is rhythmically and temporally aligned with dance movements. Existing methods typically rely on coarse rhythm embeddings, such as global motion features or binarized joint-based rhythm values, which discard fine-grained motion cues and result in weak rhythmic alignment. Moreover, temporal mismatches introduced by feature downsampling further hinder precise synchronization between dance and music. To address these problems, we propose \textbf{GACA-DiT}, a diffusion transformer-based framework with two novel modules for rhythmically consistent and temporally aligned music generation. First, a \textbf{genre-adaptive rhythm extraction} module combines multi-scale temporal wavelet analysis and spatial phase histograms with adaptive joint weighting to capture fine-grained, genre-specific rhythm patterns. Second, a \textbf{context-aware temporal alignment} module resolves temporal mismatches using learnable context queries to align music latents with relevant dance rhythm features. Extensive experiments on the AIST++ and TikTok datasets demonstrate that GACA-DiT outperforms state-of-the-art methods in both objective metrics and human evaluation. Project page: https://beria-moon.github.io/GACA-DiT/.",0,arxiv,MÃ¼zik,CC-BY/arXiv,GACA-DiT: Diffusion-based Dance-to-Music Generation with Genre-Adaptive Rhythm and Context-Aware Alignment
"We propose NanyinHGNN, a heterogeneous graph network model for generating Nanyin instrumental music. As a UNESCO-recognized intangible cultural heritage, Nanyin follows a heterophonic tradition centered around the pipa, where core melodies are notated in traditional notation while ornamentations are passed down orally, presenting challenges for both preservation and contemporary innovation. To address this, we construct a Pipa-Centric MIDI dataset, develop NanyinTok as a specialized tokenization method, and convert symbolic sequences into graph structures using a Graph Converter to ensure that key musical features are preserved. Our key innovation reformulates ornamentation generation as the creation of ornamentation nodes within a heterogeneous graph. First, a graph neural network generates melodic outlines optimized for ornamentations. Then, a rule-guided system informed by Nanyin performance practices refines these outlines into complete ornamentations without requiring explicit ornamentation annotations during training. Experimental results demonstrate that our model successfully generates authentic heterophonic ensembles featuring four traditional instruments. These findings validate that integrating domain-specific knowledge into model architecture can effectively mitigate data scarcity challenges in computational ethnomusicology.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Oral Tradition-Encoded NanyinHGNN: Integrating Nanyin Music Preservation and Generation through a Pipa-Centric Dataset
"Spatial Orchestra demonstrates how easy it is to play musical instruments using basic input like natural locomotion, which is accessible to most. Unlike many musical instruments, our work allows individuals of all skill levels to effortlessly create music by walking into virtual bubbles. Our Augmented Reality experience involves interacting with ever-shifting sound bubbles that the user engages with by stepping into color-coded bubbles within the assigned area using a standalone AR headset. Each bubble corresponds to a cello note, and omits sound from the center of the bubble, and lets the user hear and express in spatial audio, effectively transforming participants into musicians. This interactive element enables users to explore the intersection of spatial awareness, musical rhythm that extends to bodily expression through playful movements and dance-like gestures within the bubble-filled environment. This unique experience illuminates the intricate relationship between spatial awareness and the art of musical performance.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Spatial Orchestra: Locomotion Music Instruments through Spatial Exploration
"While sparse autoencoders (SAEs) successfully extract interpretable features from language models, applying them to audio generation faces unique challenges: audio's dense nature requires compression that obscures semantic meaning, and automatic feature characterization remains limited. We propose a framework for interpreting audio generative models by mapping their latent representations to human-interpretable acoustic concepts. We train SAEs on audio autoencoder latents, then learn linear mappings from SAE features to discretized acoustic properties (pitch, amplitude, and timbre). This enables both controllable manipulation and analysis of the AI music generation process, revealing how acoustic properties emerge during synthesis. We validate our approach on continuous (DiffRhythm-VAE) and discrete (EnCodec, WavTokenizer) audio latent spaces, and analyze DiffRhythm, a state-of-the-art text-to-music model, to demonstrate how pitch, timbre, and loudness evolve throughout generation. While our work is only done on audio modality, our framework can be extended to interpretable analysis of visual latent space generation models.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Learning Interpretable Features in Audio Latent Spaces via Sparse Autoencoders
"Audio autoencoders learn useful, compressed audio representations, but their non-linear latent spaces prevent intuitive algebraic manipulation such as mixing or scaling. We introduce a simple training methodology to induce linearity in a high-compression Consistency Autoencoder (CAE) by using data augmentation, thereby inducing homogeneity (equivariance to scalar gain) and additivity (the decoder preserves addition) without altering the model's architecture or loss function. When trained with our method, the CAE exhibits linear behavior in both the encoder and decoder while preserving reconstruction fidelity. We test the practical utility of our learned space on music source composition and separation via simple latent arithmetic. This work presents a straightforward technique for constructing structured latent spaces, enabling more intuitive and efficient audio processing.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Learning Linearity in Audio Consistency Autoencoders via Implicit Regularization
"Multidimensional hydrodynamical simulations have transformed the study of stellar interiors over the past few decades. Most codes developed during that time use the anelastic approximation, which fixes the thermal structure of simulations and filters out sound waves. Many of them also use explicit time integration, which imposes severe constraints on the time step of the simulations. In this context, MUSIC is developed to overcome these limitations. Its main scientific objective is to improve the phenomenological approaches used in 1D stellar evolution codes to describe major hydrodynamical and MHD processes. Here, we review recent applications of the MUSIC code, that focus mainly on convection, convective boundary mixing and waves in stars that possess convective cores, shells and/or envelopes.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Unveiling stellar (and planetary) internal dynamics with the fully compressible MUSIC code
"Generating full-length, high-quality songs is challenging, as it requires maintaining long-term coherence both across text and music modalities and within the music modality itself. Existing non-autoregressive (NAR) frameworks, while capable of producing high-quality songs, often struggle with the alignment between lyrics and vocal. Concurrently, catering to diverse musical preferences necessitates reinforcement learning from human feedback (RLHF). However, existing methods often rely on merging multiple models during multi-preference optimization, which results in significant performance degradation. To address these challenges, we introduce DiffRhythm 2, an end-to-end framework designed for high-fidelity, controllable song generation. To tackle the lyric alignment problem, DiffRhythm 2 employs a semi-autoregressive architecture based on block flow matching. This design enables faithful alignment of lyrics to singing vocals without relying on external labels and constraints, all while preserving the high generation quality and efficiency of NAR models. To make this framework computationally tractable for long sequences, we implement a music variational autoencoder (VAE) that achieves a low frame rate of 5 Hz while still enabling high-fidelity audio reconstruction. In addition, to overcome the limitations of multi-preference optimization in RLHF, we propose cross-pair preference optimization. This method effectively mitigates the performance drop typically associated with model merging, allowing for more robust optimization across diverse human preferences. We further enhance musicality and structural coherence by introducing stochastic block representation alignment loss.",0,arxiv,MÃ¼zik,CC-BY/arXiv,DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching
"Graph-based recommender systems are commonly trained in transductive settings, which limits their applicability to new users, items, or datasets. We propose NBF-Rec, a graph-based recommendation model that supports inductive transfer learning across datasets with disjoint user and item sets. Unlike conventional embedding-based methods that require retraining for each domain, NBF-Rec computes node embeddings dynamically at inference time. We evaluate the method on seven real-world datasets spanning movies, music, e-commerce, and location check-ins. NBF-Rec achieves competitive performance in zero-shot settings, where no target domain data is used for training, and demonstrates further improvements through lightweight fine-tuning. These results show that inductive transfer is feasible in graph-based recommendation and that interaction-level message passing supports generalization across datasets without requiring aligned users or items.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Inductive Transfer Learning for Graph-Based Recommenders
"Multimodal Large Language Models (LLMs) claim ""musical understanding"" via evaluations that conflate listening with score reading. We benchmark three SOTA LLMs (Gemini 2.5 Pro, Gemini 2.5 Flash, and Qwen2.5-Omni) across three core music skills: Syncopation Scoring, Transposition Detection, and Chord Quality Identification. Moreover, we separate three sources of variability: (i) perceptual limitations (audio vs. MIDI inputs), (ii) exposure to examples (zero- vs. few-shot manipulations), and (iii) reasoning strategies (Standalone, CoT, LogicLM). For the latter we adapt LogicLM, a framework combining LLMs with symbolic solvers to perform structured reasoning, to music. Results reveal a clear perceptual gap: models perform near ceiling on MIDI but show accuracy drops on audio. Reasoning and few-shot prompting offer minimal gains. This is expected for MIDI, where performance reaches saturation, but more surprising for audio, where LogicLM, despite near-perfect MIDI accuracy, remains notably brittle. Among models, Gemini Pro achieves the highest performance across most conditions. Overall, current systems reason well over symbols (MIDI) but do not yet ""listen"" reliably from audio. Our method and dataset make the perception-reasoning boundary explicit and offer actionable guidance for building robust, audio-first music systems.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Evaluating Multimodal Large Language Models on Core Music Perception Tasks
"Automatic Lyrics Transcription (ALT) for Vietnamese music presents unique challenges due to its tonal complexity and dialectal variations, but remains largely unexplored due to the lack of a dedicated dataset. Therefore, we curated the first large-scale Vietnamese ALT dataset (VietLyrics), comprising 647 hours of songs with line-level aligned lyrics and metadata to address these issues. Our evaluation of current ASRbased approaches reveal significant limitations, including frequent transcription errors and hallucinations in non-vocal segments. To improve performance, we fine-tuned Whisper models on the VietLyrics dataset, achieving superior results compared to existing multilingual ALT systems, including LyricWhiz. We publicly release VietLyrics and our models, aiming to advance Vietnamese music computing research while demonstrating the potential of this approach for ALT in low-resource language and music.",0,arxiv,MÃ¼zik,CC-BY/arXiv,VietLyrics: A Large-Scale Dataset and Models for Vietnamese Automatic Lyrics Transcription
"Music generation models can produce high-fidelity coherent accompaniment given complete audio input, but are limited to editing and loop-based workflows. We study real-time audio-to-audio accompaniment: as a model hears an input audio stream (e.g., a singer singing), it has to also simultaneously generate in real-time a coherent accompanying stream (e.g., a guitar accompaniment). In this work, we propose a model design considering inevitable system delays in practical deployment with two design variables: future visibility $t_f$, the offset between the output playback time and the latest input time used for conditioning, and output chunk duration $k$, the number of frames emitted per call. We train Transformer decoders across a grid of $(t_f,k)$ and show two consistent trade-offs: increasing effective $t_f$ improves coherence by reducing the recency gap, but requires faster inference to stay within the latency budget; increasing $k$ improves throughput but results in degraded accompaniment due to a reduced update rate. Finally, we observe that naive maximum-likelihood streaming training is insufficient for coherent accompaniment where future context is not available, motivating advanced anticipatory and agentic objectives for live jamming.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Streaming Generation for Music Accompaniment
"Recalling previously experienced movements is essential for a range of activities, including sports, music, and rehabilitation, yet little is known about the accuracy and decay of proprioceptive working memory. We examined how introducing a short-term memory component affected movement reproduction accuracy by comparing movement reproduction under two conditions: simultaneous reproduction (SimRep) and memorized reproduction (MemRep). In Experiment 1 (N = 191), participants felt a 5-s haptic trajectory with one hand and reproduced it with the other hand simultaneously or immediately after the template ended. Errors were greater in MemRep than SimRep (31.1 deg vs. 21.5 deg, p < 0.001). MemRep trajectories showed systematic temporal distortions: participants lagged fast movements and led slow ones (R = -0.32, p = 0.01), unlike the ~279 ms lag in SimRep. In Experiment 2 (N = 33), we varied template durations (2-8 s). Longer durations increased error for MemRep but not SimRep (p < 0.001). During MemRep, accuracy declined steadily, with replay-template correlations dropping from ~0.4 to ~0.1 over ~3 s, while SimRep correlations rose from ~0.25 to ~0.6. In ~10% of MemRep templates, participants moved in the wrong direction initially, especially for low-amplitude movements (p < 0.001). Templates with more than four movements showed element omission; after four movements had been reproduced participants ceased movement prematurely, affecting up to 40% of 8-s templates. These findings show that transferring proprioceptive experiences into working memory introduces systematic temporal and structural distortions. Accuracy decays within seconds, and memory span for movement trajectories was limited to four movements.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Limitations of Proprioceptive Working Memory
"Existing pitch curve generators face two main challenges: they often neglect singer-specific expressiveness, reducing their ability to capture individual singing styles. And they are typically developed as auxiliary modules for specific tasks such as pitch correction, singing voice synthesis, or voice conversion, which restricts their generalization capability. We propose StylePitcher, a general-purpose pitch curve generator that learns singer style from reference audio while preserving alignment with the intended melody. Built upon a rectified flow matching architecture, StylePitcher flexibly incorporates symbolic music scores and pitch context as conditions for generation, and can seamlessly adapt to diverse singing tasks without retraining. Objective and subjective evaluations across various singing tasks demonstrate that StylePitcher improves style similarity and audio quality while maintaining pitch accuracy comparable to task-specific baselines.",0,arxiv,MÃ¼zik,CC-BY/arXiv,StylePitcher: Generating Style-Following and Expressive Pitch Curves for Versatile Singing Tasks
"Virtual instrument generation requires maintaining consistent timbre across different pitches and velocities, a challenge that existing note-level models struggle to address. We present FlowSynth, which combines distributional flow matching (DFM) with test-time optimization for high-quality instrument synthesis. Unlike standard flow matching that learns deterministic mappings, DFM parameterizes the velocity field as a Gaussian distribution and optimizes via negative log-likelihood, enabling the model to express uncertainty in its predictions. This probabilistic formulation allows principled test-time search: we sample multiple trajectories weighted by model confidence and select outputs that maximize timbre consistency. FlowSynth outperforms the current state-of-the-art TokenSynth baseline in both single-note quality and cross-note consistency. Our approach demonstrates that modeling predictive uncertainty in flow matching, combined with music-specific consistency objectives, provides an effective path to professional-quality virtual instruments suitable for real-time performance.",0,arxiv,MÃ¼zik,CC-BY/arXiv,FlowSynth: Instrument Generation Through Distributional Flow Matching and Test-Time Search
"When following a sequence - such as reading a text or tracking a user's activity - one can measure how the ""dictionary"" of distinct elements (types) grows with the number of observations (tokens). When this growth follows a power law, it is referred to as Heaps' law, a regularity often associated with Zipf's law and frequently used to characterize human innovation and discovery processes. While random sampling from a Zipf-like distribution can reproduce Heaps' law, this connection relies on the assumption of temporal independence - an assumption often violated in real-world systems although frequently found in the literature. Here, we investigate how temporal correlations in token sequences affect the type-token curve. In systems like music listening and web browsing, domain-specific correlations in token ordering lead to systematic deviations from the Zipf-Heaps framework, effectively decoupling the type-token plot from the rank-frequency distribution. Using a minimal one-parameter model, we reproduce a wide variety of type-token trajectories, including the extremal cases that bound all possible behaviors compatible with a given frequency distribution. Our results demonstrate that type-token growth reflects not only the empirical distribution of type frequencies, but also the temporal structure of the sequence - a factor often overlooked in empirical applications of scaling laws to characterize human behavior.",0,arxiv,MÃ¼zik,CC-BY/arXiv,The dynamics of discovery and the Heaps-Zipf relationship
"Conventional Convolutional Neural Networks (CNNs) in the real domain have been widely used for audio classification. However, their convolution operations process multi-channel inputs independently, limiting the ability to capture correlations among channels. This can lead to suboptimal feature learning, particularly for complex audio patterns such as multi-channel spectrogram representations. Quaternion Convolutional Neural Networks (QCNNs) address this limitation by employing quaternion algebra to jointly capture inter-channel dependencies, enabling more compact models with fewer learnable parameters while better exploiting the multi-dimensional nature of audio signals. However, QCNNs exhibit higher computational complexity due to the overhead of quaternion operations, resulting in increased inference latency and reduced efficiency compared to conventional CNNs, posing challenges for deployment on resource-constrained platforms. To address this challenge, this study explores knowledge distillation (KD) and pruning, to reduce the computational complexity of QCNNs while maintaining performance. Our experiments on audio classification reveal that pruning QCNNs achieves similar or superior performance compared to KD while requiring less computational effort. Compared to conventional CNNs and Transformer-based architectures, pruned QCNNs achieve competitive performance with a reduced learnable parameter count and computational complexity. On the AudioSet dataset, pruned QCNNs reduce computational cost by 50\% and parameter count by 80\%, while maintaining performance comparable to the conventional CNNs. Furthermore, pruned QCNNs generalize well across multiple audio classification benchmarks, including GTZAN for music genre recognition, ESC-50 for environmental sound classification and RAVDESS for speech emotion recognition.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Compressing Quaternion Convolutional Neural Networks for Audio Classification
"Music representations are the backbone of modern recommendation systems, powering playlist generation, similarity search, and personalized discovery. Yet most embeddings offer little control for adjusting a single musical attribute, e.g., changing only the mood of a track while preserving its genre or instrumentation. In this work, we address the problem of controllable music retrieval through embedding-based transformation, where the objective is to retrieve songs that remain similar to a seed track but are modified along one chosen dimension. We propose a novel framework for mood-guided music embedding transformation, which learns a mapping from a seed audio embedding to a target embedding guided by mood labels, while preserving other musical attributes. Because mood cannot be directly altered in the seed audio, we introduce a sampling mechanism that retrieves proxy targets to balance diversity with similarity to the seed. We train a lightweight translation model using this sampling strategy and introduce a novel joint objective that encourages transformation and information preservation. Extensive experiments on two datasets show strong mood transformation performance while retaining genre and instrumentation far better than training-free baselines, establishing controllable embedding transformation as a promising paradigm for personalized music retrieval.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Controllable Embedding Transformation for Mood-Guided Music Retrieval
"In real-world singing voice conversion (SVC) applications, environmental noise and the demand for expressive output pose significant challenges. Conventional methods, however, are typically designed without accounting for real deployment scenarios, as both training and inference usually rely on clean data. This mismatch hinders practical use, given the inevitable presence of diverse noise sources and artifacts from music separation. To tackle these issues, we propose R2-SVC, a robust and expressive SVC framework. First, we introduce simulation-based robustness enhancement through random fundamental frequency ($F_0$) perturbations and music separation artifact simulations (e.g., reverberation, echo), substantially improving performance under noisy conditions. Second, we enrich speaker representation using domain-specific singing data: alongside clean vocals, we incorporate DNSMOS-filtered separated vocals and public singing corpora, enabling the model to preserve speaker timbre while capturing singing style nuances. Third, we integrate the Neural Source-Filter (NSF) model to explicitly represent harmonic and noise components, enhancing the naturalness and controllability of converted singing. R2-SVC achieves state-of-the-art results on multiple SVC benchmarks under both clean and noisy conditions.",0,arxiv,MÃ¼zik,CC-BY/arXiv,R2-SVC: Towards Real-World Robust and Expressive Zero-shot Singing Voice Conversion
"Music generation in the audio domain using artificial intelligence (AI) has witnessed steady progress in recent years. However for some instruments, particularly the guitar, controllable instrument synthesis remains limited in expressivity. We introduce GuitarFlow, a model designed specifically for electric guitar synthesis. The generative process is guided using tablatures, an ubiquitous and intuitive guitar-specific symbolic format. The tablature format easily represents guitar-specific playing techniques (e.g. bends, muted strings and legatos), which are more difficult to represent in other common music notation formats such as MIDI. Our model relies on an intermediary step of first rendering the tablature to audio using a simple sample-based virtual instrument, then performing style transfer using Flow Matching in order to transform the virtual instrument audio into more realistic sounding examples. This results in a model that is quick to train and to perform inference, requiring less than 6 hours of training data. We present the results of objective evaluation metrics, together with a listening test, in which we show significant improvement in the realism of the generated guitar audio from tablatures.",0,arxiv,MÃ¼zik,CC-BY/arXiv,GuitarFlow: Realistic Electric Guitar Synthesis From Tablatures via Flow Matching and Style Transfer
"Generative AI is reshaping music creation, but its rapid growth exposes structural gaps in attribution, rights management, and economic models. Unlike past media shifts, from live performance to recordings, downloads, and streaming, AI transforms the entire lifecycle of music, collapsing boundaries between creation, distribution, and monetization. However, existing streaming systems, with opaque and concentrated royalty flows, are ill-equipped to handle the scale and complexity of AI-driven production. We propose a content-based Music AI Agent architecture that embeds attribution directly into the creative workflow through block-level retrieval and agentic orchestration. Designed for iterative, session-based interaction, the system organizes music into granular components (Blocks) stored in BlockDB; each use triggers an Attribution Layer event for transparent provenance and real-time settlement. This framework reframes AI from a generative tool into infrastructure for a Fair AI Media Platform. By enabling fine-grained attribution, equitable compensation, and participatory engagement, it points toward a post-streaming paradigm where music functions not as a static catalog but as a collaborative and adaptive ecosystem.",0,arxiv,MÃ¼zik,CC-BY/arXiv,From Generation to Attribution: Music AI Agent Architectures for the Post-Streaming Era
"This paper investigates the design of channel estimation and 3D localization algorithms in a challenging scenario, where a sub-connected planar extremely large-scale multiple-input multiple-output (XL-MIMO) communicates with multi-antenna users. In the near field, the uplink MIMO channel is of full column rank and therefore can not be estimated effectively by applying existing codebooks that are designed for the far-field case or for the near-field case but limited to single antenna users. To solve this problem, we propose a three-stage algorithm aided by orthogonal matching pursuit (OMP) and sparse Bayesian learning (SBL). Specifically, we firstly partition the XL-MIMO into subarrays and use OMP to solve the compressed sensing (CS) problem about subarray channel estimation with the Discrete Fourier Transform (DFT)-based dictionary matrix. Secondly, exploiting the estimated subarray channels and employing one-dimensional multiple signal classification (MUSIC), we estimate the central location of the user array under the Least Squares (LS) criterion. Finally, we utilize the estimated central location to construct a refined location-aided dictionary matrix and obtain the MIMO channel estimation using SBL. Results exhibit the significant superiority of the proposed algorithm compared with several benchmarks, in terms of both the pilot overhead and estimation accuracy.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Near-Field 3D Localization and MIMO Channel Estimation with Sub-Connected Planar Arrays
"Lyric translation is a challenging task that requires balancing multiple musical constraints. Existing methods often rely on hand-crafted rules and sentence-level modeling, which restrict their ability to internalize musical-linguistic patterns and to generalize effectively at the paragraph level, where cross-line coherence and global rhyme are crucial. In this work, we propose LyriCAR, a novel framework for controllable lyric translation that operates in a fully unsupervised manner. LyriCAR introduces a difficulty-aware curriculum designer and an adaptive curriculum strategy, ensuring efficient allocation of training resources, accelerating convergence, and improving overall translation quality by guiding the model with increasingly complex challenges. Extensive experiments on the EN-ZH lyric translation task show that LyriCAR achieves state-of-the-art results across both standard translation metrics and multi-dimensional reward scores, surpassing strong baselines. Notably, the adaptive curriculum strategy reduces training steps by nearly 40% while maintaining superior performance. Code, data and model can be accessed at https://github.com/rle27/LyriCAR.",0,arxiv,MÃ¼zik,CC-BY/arXiv,LyriCAR: A Difficulty-Aware Curriculum Reinforcement Learning Framework For Controllable Lyric Translation
"This paper introduces OmniMotion-X, a versatile multimodal framework for whole-body human motion generation, leveraging an autoregressive diffusion transformer in a unified sequence-to-sequence manner. OmniMotion-X efficiently supports diverse multimodal tasks, including text-to-motion, music-to-dance, speech-to-gesture, and global spatial-temporal control scenarios (e.g., motion prediction, in-betweening, completion, and joint/trajectory-guided synthesis), as well as flexible combinations of these tasks. Specifically, we propose the use of reference motion as a novel conditioning signal, substantially enhancing the consistency of generated content, style, and temporal dynamics crucial for realistic animations. To handle multimodal conflicts, we introduce a progressive weak-to-strong mixed-condition training strategy. To enable high-quality multimodal training, we construct OmniMoCap-X, the largest unified multimodal motion dataset to date, integrating 28 publicly available MoCap sources across 10 distinct tasks, standardized to the SMPL-X format at 30 fps. To ensure detailed and consistent annotations, we render sequences into videos and use GPT-4o to automatically generate structured and hierarchical captions, capturing both low-level actions and high-level semantics. Extensive experimental evaluations confirm that OmniMotion-X significantly surpasses existing methods, demonstrating state-of-the-art performance across multiple multimodal tasks and enabling the interactive generation of realistic, coherent, and controllable long-duration motions.",0,arxiv,MÃ¼zik,CC-BY/arXiv,OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation
"This paper presents a novel micro-Doppler energy-based framework for robust multi-target vital signs monitoring using 77-GHz Frequency-Modulated Continuous-Wave (FMCW) radar. Unlike conventional phase-based methods that are susceptible to environmental noise, random body movements, and stringent calibration requirements, our approach exploits the energy variations in radar returns induced by cardiopulmonary activities. The proposed system integrates a comprehensive processing pipeline including space-time adaptive processing (STAP) for target detection and tracking, MUSIC algorithm for high-resolution angle estimation, and an innovative adaptive spectral filtering technique for vital signs extraction. We establish a rigorous mathematical framework that formalizes the relationship between micro-Doppler energy variations and physiological activities, enabling robust separation of closely spaced targets. The key innovation lies in the micro-Doppler energy extraction methodology that provides inherent robustness to phase noise and motion artifacts. Experimental results using millimeter-wave radar datasets demonstrate that the system can accurately detect and separate vital signs of up to four targets within \SI{5}{\meter} range, achieving mean absolute errors of \SI{1.2}beats per minute and \SI{2.3} beats per minute for respiration and heart rates, respectively. The proposed approach demonstrates superior performance compared to traditional phase-based methods, particularly in challenging multi-target scenarios with environmental noise and subject movement.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Micro-Doppler Energy-Based Robust Multi-Target Vital Signs Monitoring Using 77-GHz FMCW Radar with Spatiotemporal Adaptive Processing
"Timbre allows us to distinguish between sounds even when they share the same pitch and loudness, playing an important role in music, instrument recognition, and speech. Traditional approaches, such as frequency analysis or machine learning, often overlook subtle characteristics of sound. Topological Data Analysis (TDA) can capture complex patterns, but its application to timbre has been limited, partly because it is unclear how to represent sound effectively for TDA. In this study, we investigate how different time delay embeddings affect TDA results. Using both synthetic and real audio signals, we identify time delays that enhance the detection of harmonic structures. Our findings show that specific delays, related to fractions of the fundamental period, allow TDA to reveal key harmonic features and distinguish between integer and non-integer harmonics. The method is effective for synthetic and real musical instrument sounds and opens the way for future works, which could extend it to more complex sounds using higher-dimensional embeddings and additional persistence statistics.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Time delay embeddings to characterize the timbre of musical instruments using Topological Data Analysis: a study on synthetic and real data
"Controllable music generation remains a significant challenge, with existing methods often requiring model retraining or introducing audible artifacts. We introduce MusicRFM, a framework that adapts Recursive Feature Machines (RFMs) to enable fine-grained, interpretable control over frozen, pre-trained music models by directly steering their internal activations. RFMs analyze a model's internal gradients to produce interpretable ""concept directions"", or specific axes in the activation space that correspond to musical attributes like notes or chords. We first train lightweight RFM probes to discover these directions within MusicGen's hidden states; then, during inference, we inject them back into the model to guide the generation process in real-time without per-step optimization. We present advanced mechanisms for this control, including dynamic, time-varying schedules and methods for the simultaneous enforcement of multiple musical properties. Our method successfully navigates the trade-off between control and generation quality: we can increase the accuracy of generating a target musical note from 0.23 to 0.82, while text prompt adherence remains within approximately 0.02 of the unsteered baseline, demonstrating effective control with minimal impact on prompt fidelity. We release code to encourage further exploration on RFMs in the music domain.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Steering Autoregressive Music Generation with Recursive Feature Machines
"As emergent artificial intelligence technologies increasingly assert roles as assistants within intangible cultural heritage contexts, researchers and artists observe existing questions on the theme of agency negotiation, cultural resistance, and technical critique. This research interrogates power dynamics in human-AI sovereignty and entanglement for nomadic improvisational Dutar performance, a living cultural heritage through a long-necked lute from the Central Asia region. To investigate tensions between human agency and computational hegemony, the researcher and artists examined and iterated a feedback workflow that captures live performance data, processes digital transformations, and creates a real-time interactive art experience via immersive environments. Empirical data from artists and audience reveal modulations where musicians selectively embrace or reject algorithmic suggestions to preserve creative identity. The author concludes that decolonial potential requires redesigning tools or systems for cultural survivance, where technology becomes not merely a feedback environment but a site for decolonial praxis, challenging computational hegemony in digital ecosystems.",0,arxiv,MÃ¼zik,CC-BY/arXiv,When Strings Tug at Algorithm: Human-AI Sovereignty and Entanglement in Nomadic Improvisational Music Performance as a Decolonial Exploration
"Multimodal Large Language Models (MLLMs) have demonstrated capabilities in audio understanding, but current evaluations may obscure fundamental weaknesses in relational reasoning. We introduce the Music Understanding and Structural Evaluation (MUSE) Benchmark, an open-source resource with 10 tasks designed to probe fundamental music perception skills. We evaluate four SOTA models (Gemini Pro and Flash, Qwen2.5-Omni, and Audio-Flamingo 3) against a large human baseline (N=200). Our results reveal a wide variance in SOTA capabilities and a persistent gap with human experts. While Gemini Pro succeeds on basic perception, Qwen and Audio Flamingo 3 perform at or near chance, exposing severe perceptual deficits. Furthermore, we find Chain-of-Thought (CoT) prompting provides inconsistent, often detrimental results. Our work provides a critical tool for evaluating invariant musical representations and driving development of more robust AI systems.",0,arxiv,MÃ¼zik,CC-BY/arXiv,The MUSE Benchmark: Probing Music Perception and Auditory Relational Reasoning in Audio LLMS
"Recent advancements in song generation have shown promising results in generating songs from lyrics and/or global text prompts. However, most existing systems lack the ability to model the temporally varying attributes of songs, limiting fine-grained control over musical structure and dynamics. In this paper, we propose SegTune, a non-autoregressive framework for structured and controllable song generation. SegTune enables segment-level control by allowing users or large language models to specify local musical descriptions aligned to song sections.The segmental prompts are injected into the model by temporally broadcasting them to corresponding time windows, while global prompts influence the whole song to ensure stylistic coherence. To obtain accurate segment durations and enable precise lyric-to-music alignment, we introduce an LLM-based duration predictor that autoregressively generates sentence-level timestamped lyrics in LRC format. We further construct a large-scale data pipeline for collecting high-quality songs with aligned lyrics and prompts, and propose new evaluation metrics to assess segment-level alignment and vocal attribute consistency. Experimental results show that SegTune achieves superior controllability and musical coherence compared to existing baselines. See https://cai525.github.io/SegTune_demo for demos of our work.",0,arxiv,MÃ¼zik,CC-BY/arXiv,SegTune: Structured and Fine-Grained Control for Song Generation
"Conventional acoustic beamformers assume that noise is stationary within short time frames. This assumption prevents them from exploiting correlations between frequencies in almost-periodic noise sources such as musical instruments, fans, and engines. These signals exhibit periodically varying statistics and are better modeled as cyclostationary processes. This paper introduces the cyclic MVDR (cMVDR) beamformer, an extension of the conventional MVDR that leverages both spatial and spectral correlations to improve noise reduction, particularly in low-SNR scenarios. The method builds on frequency-shifted (FRESH) filtering, where shifted versions of the input are combined to attenuate or amplify components that are coherent across frequency. To address inharmonicity, where harmonic partials deviate from exact integer multiples of the fundamental frequency, we propose a data-driven strategy that estimates resonant frequencies via periodogram analysis and computes the frequency shifts from their spacing. Analytical and experimental results demonstrate that performance improves with increasing spectral correlation. On real recordings, the cMVDR achieves up to 5 dB gain in scale-invariant signal-to-distortion ratio (SI-SDR) over the MVDR and remains effective even with a single microphone. Code is available at https://github.com/Screeen/cMVDR.",0,arxiv,MÃ¼zik,CC-BY/arXiv,MVDR Beamforming for Cyclostationary Processes
"Estimating piano dynamic from audio recordings is a fundamental challenge in computational music analysis. In this paper, we propose an efficient multi-task network that jointly predicts dynamic levels, change points, beats, and downbeats from a shared latent representation. These four targets form the metrical structure of dynamics in the music score. Inspired by recent vocal dynamic research, we use a multi-scale network as the backbone, which takes Bark-scale specific loudness as the input feature. Compared to log-Mel as input, this reduces model size from 14.7 M to 0.5 M, enabling long sequential input. We use a 60-second audio length in audio segmentation, which doubled the length of beat tracking commonly used. Evaluated on the public MazurkaBL dataset, our model achieves state-of-the-art results across all tasks. This work sets a new benchmark for piano dynamic estimation and delivers a powerful and compact tool, paving the way for large-scale, resource-efficient analysis of musical expression.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Joint Estimation of Piano Dynamics and Metrical Structure with a Multi-task Multi-Scale Network
"We address the problem of estimating room impulse responses (RIRs) in noisy, uncontrolled environments where non-stationary sounds such as speech or footsteps corrupt conventional deconvolution. We propose AnyRIR, a non-intrusive method that uses music as the excitation signal instead of a dedicated test signal, and formulate RIR estimation as an L1-norm regression in the time-frequency domain. Solved efficiently with Iterative Reweighted Least Squares (IRLS) and Least-Squares Minimal Residual (LSMR) methods, this approach exploits the sparsity of non-stationary noise to suppress its influence. Experiments on simulated and measured data show that AnyRIR outperforms L2-based and frequency-domain deconvolution, under in-the-wild noisy scenarios and codec mismatch, enabling robust RIR estimation for AR/VR and related applications.",0,arxiv,MÃ¼zik,CC-BY/arXiv,AnyRIR: Robust Non-intrusive Room Impulse Response Estimation in the Wild
"Discrete representation learning has shown promising results across various domains, including generation and understanding in image, speech and language. Inspired by these advances, we propose MuseTok, a tokenization method for symbolic music, and investigate its effectiveness in both music generation and understanding tasks. MuseTok employs the residual vector quantized-variational autoencoder (RQ-VAE) on bar-wise music segments within a Transformer-based encoder-decoder framework, producing music codes that achieve high-fidelity music reconstruction and accurate understanding of music theory. For comprehensive evaluation, we apply MuseTok to music generation and semantic understanding tasks, including melody extraction, chord recognition, and emotion recognition. Models incorporating MuseTok outperform previous representation learning baselines in semantic understanding while maintaining comparable performance in content generation. Furthermore, qualitative analyses on MuseTok codes, using ground-truth categories and synthetic datasets, reveal that MuseTok effectively captures underlying musical concepts from large music collections.",0,arxiv,MÃ¼zik,CC-BY/arXiv,MuseTok: Symbolic Music Tokenization for Generation and Semantic Understanding
"While the ambient intelligence (AmI) systems we encounter in our daily lives, including security monitoring and energy-saving systems, typically serve pragmatic purposes, we wonder how we can design and implement ambient artificial intelligence experiences in public spaces that elicit deep human feelings of awe, wonder, and beauty. As a manifestation, we introduce Sound Clouds, an immersive art installation that generates live music based on participants' interaction with several human-height spheres. Our installation serves as a provocation into future ambient intelligence that provokes, not limits, the future possibilities of AmI.",0,arxiv,MÃ¼zik,CC-BY/arXiv,"Sound Clouds: Exploring ambient intelligence in public spaces to elicit deep human experience of awe, wonder, and beauty"
"EEG-based person identification enables applications in security, personalized brain-computer interfaces (BCIs), and cognitive monitoring. However, existing techniques often rely on deep learning architectures at high computational cost, limiting their scope of applications. In this study, we propose a novel EEG person identification approach using spiking neural networks (SNNs) with a lightweight spiking transformer for efficiency and effectiveness. The proposed SNN model is capable of handling the temporal complexities inherent in EEG signals. On the EEG-Music Emotion Recognition Challenge dataset, the proposed model achieves 100% classification accuracy with less than 10% energy consumption of traditional deep neural networks. This study offers a promising direction for energy-efficient and high-performance BCIs. The source code is available at https://github.com/PatrickZLin/Decode-ListenerIdentity.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Decoding Listeners Identity: Person Identification from EEG Signals Using a Lightweight Spiking Transformer
"The performance of deep learning models for music source separation heavily depends on training data quality. However, datasets are often corrupted by difficult-to-detect artifacts such as audio bleeding and label noise. Since the type and extent of contamination are typically unknown, cleaning methods targeting specific corruptions are often impractical. This paper proposes and evaluates two distinct, noise-agnostic data cleaning methods to address this challenge. The first approach uses data attribution via unlearning to identify and filter out training samples that contribute the least to producing clean outputs. The second leverages the FrÃ©chet Audio Distance to measure and remove samples that are perceptually dissimilar to a small and trusted clean reference set. On a dataset contaminated with a simulated distribution of real-world noise, our unlearning-based methods produced a cleaned dataset and a corresponding model that outperforms both the original contaminated data and the small clean reference set used for cleaning. This result closes approximately 66.7\% of the performance gap between the contaminated baseline and a model trained on the same dataset without any contamination. Unlike methods tailored for specific artifacts, our noise-agnostic approaches offer a more generic and broadly applicable solution for curating high-quality training data.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Towards Blind Data Cleaning: A Case Study in Music Source Separation
"Whole-body multi-modal human motion generation poses two primary challenges: creating an effective motion generation mechanism and integrating various modalities, such as text, speech, and music, into a cohesive framework. Unlike previous methods that usually employ discrete masked modeling or autoregressive modeling, we develop a continuous masked autoregressive motion transformer, where a causal attention is performed considering the sequential nature within the human motion. Within this transformer, we introduce a gated linear attention and an RMSNorm module, which drive the transformer to pay attention to the key actions and suppress the instability caused by either the abnormal movements or the heterogeneous distributions within multi-modalities. To further enhance both the motion generation and the multimodal generalization, we employ the DiT structure to diffuse the conditions from the transformer towards the targets. To fuse different modalities, AdaLN and cross-attention are leveraged to inject the text, speech, and music signals. Experimental results demonstrate that our framework outperforms previous methods across all modalities, including text-to-motion, speech-to-gesture, and music-to-dance. The code of our method will be made public.",0,arxiv,MÃ¼zik,CC-BY/arXiv,OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression
"Natural language offers a natural interface for humanoid robots, but existing language-guided humanoid locomotion pipelines remain cumbersome and untrustworthy. They typically decode human motion, retarget it to robot morphology, and then track it with a physics-based controller. However, this multi-stage process is prone to cumulative errors, introduces high latency, and yields weak coupling between semantics and control. These limitations call for a more direct pathway from language to action, one that eliminates fragile intermediate stages. Therefore, we present RoboGhost, a retargeting-free framework that directly conditions humanoid policies on language-grounded motion latents. By bypassing explicit motion decoding and retargeting, RoboGhost enables a diffusion-based policy to denoise executable actions directly from noise, preserving semantic intent and supporting fast, reactive control. A hybrid causal transformer-diffusion motion generator further ensures long-horizon consistency while maintaining stability and diversity, yielding rich latent representations for precise humanoid behavior. Extensive experiments demonstrate that RoboGhost substantially reduces deployment latency, improves success rates and tracking precision, and produces smooth, semantically aligned locomotion on real humanoids. Beyond text, the framework naturally extends to other modalities such as images, audio, and music, providing a universal foundation for vision-language-action humanoid systems.",0,arxiv,MÃ¼zik,CC-BY/arXiv,From Language to Locomotion: Retargeting-free Humanoid Control via Motion Latent Guidance
"The field of Optical Music Recognition (OMR) is currently hindered by the scarcity of real annotated data, particularly when dealing with handwritten historical musical scores. In similar fields, such as Handwritten Text Recognition, it was proven that synthetic examples produced with image generation techniques could help to train better-performing recognition architectures. This study explores the generation of realistic, handwritten-looking scores by implementing a music symbol-level Generative Adversarial Network (GAN) and assembling its output into a full score using the Smashcima engraving software. We have systematically evaluated the visual fidelity of these generated samples, concluding that the generated symbols exhibit a high degree of realism, marking significant progress in synthetic score generation.",0,arxiv,MÃ¼zik,CC-BY/arXiv,GAN-based Content-Conditioned Generation of Handwritten Musical Symbols
"Recent beat and downbeat tracking models (e.g., RNNs, TCNs, Transformers) output frame-level activations. We propose reframing this task as object detection, where beats and downbeats are modeled as temporal ""objects."" Adapting the FCOS detector from computer vision to 1D audio, we replace its original backbone with WaveBeat's temporal feature extractor and add a Feature Pyramid Network to capture multi-scale temporal patterns. The model predicts overlapping beat/downbeat intervals with confidence scores, followed by non-maximum suppression (NMS) to select final predictions. This NMS step serves a similar role to DBNs in traditional trackers, but is simpler and less heuristic. Evaluated on standard music datasets, our approach achieves competitive results, showing that object detection techniques can effectively model musical beats with minimal adaptation.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Beat Tracking as Object Detection
"Understanding and modeling the relationship between language and sound is critical for applications such as music information retrieval,text-guided music generation, and audio captioning. Central to these tasks is the use of joint language-audio embedding spaces, which map textual descriptions and auditory content into a shared embedding space. While multimodal embedding models such as MS-CLAP, LAION-CLAP, and MuQ-MuLan have shown strong performance in aligning language and audio, their correspondence to human perception of timbre, a multifaceted attribute encompassing qualities such as brightness, roughness, and warmth, remains underexplored. In this paper, we evaluate the above three joint language-audio embedding models on their ability to capture perceptual dimensions of timbre. Our findings show that LAION-CLAP consistently provides the most reliable alignment with human-perceived timbre semantics across both instrumental sounds and audio effects.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Do Joint Language-Audio Embeddings Encode Perceptual Timbre Semantics?
"The origins of consonance in human music has long been contested, and today there are three primary hypotheses: aversion to roughness, preference for harmonicity, and learned preferences from cultural exposure. While the evidence is currently insufficient to disentangle the contributions of these hypotheses, I propose several reasons why roughness is an especially promising area for future study. The aim of this review is to summarize and critically evaluate roughness theory and models, experimental data, to highlight areas that deserve further research. I identify 2 key areas: There are fundamental issues with the definition and interpretation of results due to tautology in the definition of roughness, and the lack of independence in empirical measurements. Despite extensive model development, there are many duplications and models have issues with data quality and overfitting. Future theory development should aim for model simplicity, and extra assumptions, features and parameters should be evaluated systematically. Model evaluation should aim to maximise the breadth of stimuli that are predicted.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Musical consonance: a review of theory and evidence on perception and preference of auditory roughness in humans and other animals
"Recent advances in unified multimodal models indicate a clear trend towards comprehensive content generation. However, the auditory domain remains a significant challenge, with music and speech often developed in isolation, hindering progress towards universal audio synthesis. This separation stems from inherent task conflicts and severe data imbalances, which impede the development of a truly unified audio generation model. To address this challenge, we propose UniMoE-Audio, a unified speech and music generation model within a novel Dynamic-Capacity Mixture-of-Experts (MoE) framework. Architecturally, UniMoE-Audio introduces a Top-P routing strategy for dynamic expert number allocation, and a hybrid expert design comprising routed experts for domain-specific knowledge, shared experts for domain-agnostic features, and null experts for adaptive computation skipping. To tackle data imbalance, we introduce a three-stage training curriculum: 1) Independent Specialist Training leverages original datasets to instill domain-specific knowledge into each ""proto-expert"" without interference; 2) MoE Integration and Warmup incorporates these specialists into the UniMoE-Audio architecture, warming up the gate module and shared expert using a subset of balanced dataset; and 3) Synergistic Joint Training trains the entire model end-to-end on the fully balanced dataset, fostering enhanced cross-domain synergy. Extensive experiments show that UniMoE-Audio not only achieves state-of-the-art performance on major speech and music generation benchmarks, but also demonstrates superior synergistic learning, mitigating the performance degradation typically seen in naive joint training. Our findings highlight the substantial potential of specialized MoE architecture and curated training strategies in advancing the field of universal audio generation. Homepage: https://mukioxun.github.io/Uni-MoE-site/home.html",0,arxiv,MÃ¼zik,CC-BY/arXiv,UniMoE-Audio: Unified Speech and Music Generation with Dynamic-Capacity MoE
"Music is both an auditory and an embodied phenomenon, closely linked to human motion and naturally expressed through dance. However, most existing audio representations neglect this embodied dimension, limiting their ability to capture rhythmic and structural cues that drive movement. We propose MotionBeat, a framework for motion-aligned music representation learning. MotionBeat is trained with two newly proposed objectives: the Embodied Contrastive Loss (ECL), an enhanced InfoNCE formulation with tempo-aware and beat-jitter negatives to achieve fine-grained rhythmic discrimination, and the Structural Rhythm Alignment Loss (SRAL), which ensures rhythm consistency by aligning music accents with corresponding motion events. Architecturally, MotionBeat introduces bar-equivariant phase rotations to capture cyclic rhythmic patterns and contact-guided attention to emphasize motion events synchronized with musical accents. Experiments show that MotionBeat outperforms state-of-the-art audio encoders in music-to-dance generation and transfers effectively to beat tracking, music tagging, genre and instrument classification, emotion recognition, and audio-visual retrieval. Our project demo page: https://motionbeat2025.github.io/.",0,arxiv,MÃ¼zik,CC-BY/arXiv,MotionBeat: Motion-Aligned Music Representation via Embodied Contrastive Learning and Bar-Equivariant Contact-Aware Encoding
"This work studies an integrated sensing and communication (ISAC) framework for targets that are spread both in the angle and range domains. We model each target using a cluster of rays parameterized by a specific density function, and propose a truncated Multiple Signal Classification (MUSIC) spread (TMS) algorithm to accurately estimate the parameters of the density function. Unlike the conventional MUSIC spread (CMS), TMS restricts the signal subspace rank based on the eigen decomposition of the received-signal autocorrelation. We also propose a discrete Fourier transform (DFT) based algorithm for estimating the distance and range spread of each target. Leveraging these estimates, we then develop a dynamic transmit beamforming algorithm that successfully illuminates multiple targets while also serving multiple downlink (DL) users. Simulation results demonstrate the superiority of our proposed algorithms over baseline schemes in both low and high signal-to-noise ratio (SNR) regimes as well as under a wide angular spread regime.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Enhanced Angle-Range Cluster Parameter Estimation in Full-Duplex ISAC Systems
"This paper presents the Deep learning-based Perceptual Audio Quality metric (DeePAQ) for evaluating general audio quality. Our approach leverages metric learning together with the music foundation model MERT, guided by surrogate labels, to construct an embedding space that captures distortion intensity in general audio. To the best of our knowledge, DeePAQ is the first in the general audio quality domain to leverage weakly supervised labels and metric learning for fine-tuning a music foundation model with Low-Rank Adaptation (LoRA), a direction not yet explored by other state-of-the-art methods. We benchmark the proposed model against state-of-the-art objective audio quality metrics across listening tests spanning audio coding and source separation. Results show that our method surpasses existing metrics in detecting coding artifacts and generalizes well to unseen distortions such as source separation, highlighting its robustness and versatility.",0,arxiv,MÃ¼zik,CC-BY/arXiv,DeePAQ: A Perceptual Audio Quality Metric Based On Foundational Models and Weakly Supervised Learning
"Recent advances in diffusion-based generative models have enabled high-quality text-to-audio synthesis, but fine-grained acoustic control remains a significant challenge in open-source research. We present Audio Palette, a diffusion transformer (DiT) based model that extends the Stable Audio Open architecture to address this ""control gap"" in controllable audio generation. Unlike prior approaches that rely solely on semantic conditioning, Audio Palette introduces four time-varying control signals, loudness, pitch, spectral centroid, and timbre, for precise and interpretable manipulation of acoustic features. The model is efficiently adapted for the nuanced domain of Foley synthesis using Low-Rank Adaptation (LoRA) on a curated subset of AudioSet, requiring only 0.85% of the original parameters to be trained. Experiments demonstrate that Audio Palette achieves fine-grained, interpretable control of sound attributes. Crucially, it accomplishes this novel controllability while maintaining high audio quality and strong semantic alignment to text prompts, with performance on standard metrics such as Frechet Audio Distance (FAD) and LAION-CLAP scores remaining comparable to the original baseline model. We provide a scalable, modular pipeline for audio research, emphasizing sequence-based conditioning, memory efficiency, and a three-scale classifier-free guidance mechanism for nuanced inference-time control. This work establishes a robust foundation for controllable sound design and performative audio synthesis in open-source settings, enabling a more artist-centric workflow in the broader context of music and sound information retrieval.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Audio Palette: A Diffusion Transformer with Multi-Signal Conditioning for Controllable Foley Synthesis
"Sampling, the technique of reusing pieces of existing audio tracks to create new music content, is a very common practice in modern music production. In this paper, we tackle the challenging task of automatic sample identification, that is, detecting such sampled content and retrieving the material from which it originates. To do so, we adopt a self-supervised learning approach that leverages a multi-track dataset to create positive pairs of artificial mixes, and design a novel contrastive learning objective. We show that such method significantly outperforms previous state-of-the-art baselines, that is robust to various genres, and that scales well when increasing the number of noise songs in the reference database. In addition, we extensively analyze the contribution of the different components of our training pipeline and highlight, in particular, the need for high-quality separated stems for this task.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Automatic Music Sample Identification with Multi-Track Contrastive Learning
"Music Source Restoration (MSR) extends source separation to realistic settings where signals undergo production effects (equalization, compression, reverb) and real-world degradations, with the goal of recovering the original unprocessed sources. Existing benchmarks cannot measure restoration fidelity: synthetic datasets use unprocessed stems but unrealistic mixtures, while real production datasets provide only already-processed stems without clean references. We present MSRBench, the first benchmark explicitly designed for MSR evaluation. MSRBench contains raw stem-mixture pairs across eight instrument classes, where mixtures are produced by professional mixing engineers. These raw-processed pairs enable direct evaluation of both separation accuracy and restoration fidelity. Beyond controlled studio conditions, the mixtures are augmented with twelve real-world degradations spanning analog artifacts, acoustic environments, and lossy codecs. Baseline experiments with U-Net and BSRNN achieve SI-SNR of -37.8 dB and -23.4 dB respectively, with perceptual quality (FAD CLAP) around 0.7-0.8, demonstrating substantial room for improvement and the need for restoration-specific architectures.",0,arxiv,MÃ¼zik,CC-BY/arXiv,MSRBench: A Benchmarking Dataset for Music Source Restoration
"Direction-of-Arrival (DOA) estimation in sensor arrays faces limitations under demanding conditions, including low signal-to-noise ratio, single-snapshot scenarios, coherent sources, and unknown source counts. Conventional beamforming suffers from sidelobe interference, adaptive methods (e.g., MVDR) and subspace algorithms (e.g., MUSIC) degrade with limited snapshots or coherent signals, while sparse-recovery approaches (e.g., L1-SVD) incur high computational complexity for large arrays. In this article, we construct the concept of the optimal spatial filter to solve the DOA estimation problem under demanding conditions by utilizing the sparsity of spatial signals. By utilizing the concept of the optimal spatial filter, we have transformed the DOA estimation problem into a solution problem for the optimal spatial filter. We propose the Spatial Signal Focusing and Noise Suppression (SSFNS) algorithm, which is a novel DOA estimation framework grounded in the theoretical existence of an optimal spatial filter, to solve for the optimal spatial filter and obtain DOA. Through experiments, it was found that the proposed algorithm is suitable for large aperture two-dimensional arrays and experiments have shown that our proposed algorithm performs better than other algorithms in scenarios with few snapshots or even a single snapshot, low signal-to-noise ratio, coherent signals, and unknown signal numbers in two-dimensional large aperture arrays.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Spatial Signal Focusing and Noise Suppression for Direction-of-Arrival Estimation in Large-Aperture 2D Arrays under Demanding Conditions
"Guitar tablature transcription consists in deducing the string and the fret number on which each note should be played to reproduce the actual musical part. This assignment should lead to playable string-fret combinations throughout the entire track and, in general, preserve parsimonious motion between successive combinations. Throughout the history of guitar playing, specific chord fingerings have been developed across different musical styles that facilitate common idiomatic voicing combinations and motion between them. This paper presents a method for assigning guitar tablature notation to a given MIDI-based musical part (possibly consisting of multiple polyphonic tracks), i.e. no information about guitar-idiomatic expressional characteristics is involved (e.g. bending etc.) The current strategy is based on machine learning and requires a basic assumption about how much fingers can stretch on a fretboard; only standard 6-string guitar tuning is examined. The proposed method also examines the transcription of music pieces that was not meant to be played or could not possibly be played by a guitar (e.g. potentially a symphonic orchestra part), employing a rudimentary method for augmenting musical information and training/testing the system with artificial data. The results present interesting aspects about what the system can achieve when trained on the initial and augmented dataset, showing that the training with augmented data improves the performance even in simple, e.g. monophonic, cases. Results also indicate weaknesses and lead to useful conclusions about possible improvements.",0,arxiv,MÃ¼zik,CC-BY/arXiv,A Machine Learning Approach for MIDI to Guitar Tablature Conversion
"Humans rely on multisensory integration to perceive spatial environments, where auditory cues enable sound source localization in three-dimensional space. Despite the critical role of spatial audio in immersive technologies such as VR/AR, most existing multimodal datasets provide only monaural audio, which limits the development of spatial audio generation and understanding. To address these challenges, we introduce MRSAudio, a large-scale multimodal spatial audio dataset designed to advance research in spatial audio understanding and generation. MRSAudio spans four distinct components: MRSLife, MRSSpeech, MRSMusic, and MRSSing, covering diverse real-world scenarios. The dataset includes synchronized binaural and ambisonic audio, exocentric and egocentric video, motion trajectories, and fine-grained annotations such as transcripts, phoneme boundaries, lyrics, scores, and prompts. To demonstrate the utility and versatility of MRSAudio, we establish five foundational tasks: audio spatialization, and spatial text to speech, spatial singing voice synthesis, spatial music generation and sound event localization and detection. Results show that MRSAudio enables high-quality spatial modeling and supports a broad range of spatial audio research. Demos and dataset access are available at https://mrsaudio.github.io.",0,arxiv,MÃ¼zik,CC-BY/arXiv,MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations
"Artificial Intelligence (AI) for music generation is undergoing rapid developments, with recent symbolic models leveraging sophisticated deep learning and diffusion model algorithms. One drawback with existing models is that they lack structural cohesion, particularly on harmonic-melodic structure. Furthermore, such existing models are largely ""black-box"" in nature and are not musically interpretable. This paper addresses these limitations via a novel generative music framework that incorporates concepts of Schenkerian analysis (SchA) in concert with a diffusion modeling framework. This framework, which we call ProGress (Prolongation-enhanced DiGress), adapts state-of-the-art deep models for discrete diffusion (in particular, the DiGress model of Vignac et al., 2023) for interpretable and structured music generation. Concretely, our contributions include 1) novel adaptations of the DiGress model for music generation, 2) a novel SchA-inspired phrase fusion methodology, and 3) a framework allowing users to control various aspects of the generation process to create coherent musical compositions. Results from human experiments suggest superior performance to existing state-of-the-art methods.",0,arxiv,MÃ¼zik,CC-BY/arXiv,ProGress: Structured Music Generation via Graph Diffusion and Hierarchical Music Analysis
"Highly-informed Expressive Performance Rendering (EPR) systems transform music scores with rich musical annotations into human-like expressive performance MIDI files. While these systems have achieved promising results, the availability of detailed music scores is limited compared to MIDI files and are less flexible to work with using a digital audio workstation (DAW). Recent advancements in low-informed EPR systems offer a more accessible alternative by directly utilizing score-derived MIDI as input, but these systems often exhibit suboptimal performance. Meanwhile, existing works are evaluated with diverse automatic metrics and data formats, hindering direct objective comparisons between EPR systems. In this study, we introduce Peransformer, a transformer-based low-informed EPR system designed to bridge the gap between low-informed and highly-informed EPR systems. Our approach incorporates a score-aware discriminator that leverages the underlying score-derived MIDI files and is trained on a score-to-performance paired, note-to-note aligned MIDI dataset. Experimental results demonstrate that Peransformer achieves state-of-the-art performance among low-informed systems, as validated by subjective evaluations. Furthermore, we extend existing automatic evaluation metrics for EPR systems and introduce generalized EPR metrics (GEM), enabling more direct, accurate, and reliable comparisons across EPR systems.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Peransformer: Improving Low-informed Expressive Performance Rendering with Score-aware Discriminator
"This paper introduces Chord Colourizer, a near real-time system that detects the musical key of an audio signal and visually represents it through a novel graphical user interface (GUI). The system assigns colours to musical notes based on Isaac Newton's original colour wheel, preserving historical links between pitch and hue, and also integrates an Arduino-controlled LED display using 3D-printed star-shaped diffusers to offer a physical ambient media representation. The method employs Constant-Q Transform (CQT) chroma features for chord estimation and visualization, followed by threshold-based filtering and tonal enhancement to isolate the root, third, and fifth. A confidence score is computed for each detection to ensure reliability, and only chords with moderate to very strong certainty are visualized. The graphical interface dynamically updates a colour-coded keyboard layout, while the LED display provides the same colour information via spatial feedback. This multi-modal system enhances user interaction with harmonic content, offering innovative possibilities for education and artistic performance. Limitations include slight latency and the inability to detect extended chords, which future development will aim to address through refined filtering, adaptive thresholds, and support for more complex harmonies such as sevenths and augmented chords. Future work will also explore integration with alternative visualization styles, and the comparison of audio analysis libraries to improve detection speed and precision. Plans also include formal user testing to evaluate perception, usability, and cross-cultural interpretations of colour-pitch mappings.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Chord Colourizer: A Near Real-Time System for Visualizing Musical Key
"Real-time music alignment, also known as score following, is a fundamental MIR task with a long history and is essential for many interactive applications. Despite its importance, there has not been a unified open framework for comparing models, largely due to the inherent complexity of real-time processing and the language- or system-dependent implementations. In addition, low compatibility with the existing MIR environment has made it difficult to develop benchmarks using large datasets available in recent years. While new studies based on established methods (e.g., dynamic programming, probabilistic models) have emerged, most evaluations compare models only within the same family or on small sets of test data. This paper introduces Matchmaker, an open-source Python library for real-time music alignment that is easy to use and compatible with modern MIR libraries. Using this, we systematically compare methods along two dimensions: music representations and alignment methods. We evaluated our approach on a large test set of solo piano music from the (n)ASAP, Batik, and Vienna4x22 datasets with a comprehensive set of metrics to ensure robust assessment. Our work aims to establish a benchmark framework for score-following research while providing a practical tool that developers can easily integrate into their applications.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Matchmaker: An Open-source Library for Real-time Piano Score Following and Systematic Evaluation
"This research investigates the feasibility of producing affordable, functional acoustic guitars using 3D printing, with a focus on producing structural designs with proper tonal performance. Conducted in collaboration with William Schiesser, the study uses a classical guitar model, chosen for its lower string tension, to evaluate the tonal characteristics of a 3D-printed prototype made from polylactic acid (PLA). Due to the build plate size constraints of the Prusa Mark 4 printer, the guitar body was divided into multiple sections joined with press-fit tolerances and minimal cyanoacrylate adhesive. CAD modeling in Fusion 360 ensured dimensional accuracy in press-fit connections and the overall assembly. Following assembly, the guitar was strung with nylon strings and tested using Audacity software to compare recorded frequencies and notes with standard reference values. Results showed large deviations in lower string frequencies, likely caused by the material choice utilized in printing. Accurate pitches were reached with all strings despite frequency differences through tuning, demonstrating that PLA and modern manufacturing methods can produce affordable, playable acoustic guitars despite inevitable challenges. Further research may investigate alternative plastics for superior frequency matching. This approach holds significant potential for expanding access to quality instruments while reducing reliance on endangered tonewoods, thereby encouraging both sustainable instrument production and increased musical participation. This also creates opportunities for disadvantaged communities where access to musical instruments remains a challenge.   Keywords: Luthiery, Stereolithography, 3D-Print, Guitar Making",0,arxiv,MÃ¼zik,CC-BY/arXiv,Production and Manufacturing of 3D Printed Acoustic Guitars
"Harnessing the power of LLMs requires a delicate dance between being helpful and harmless. This creates a fundamental tension between two competing challenges: vulnerability to adversarial attacks that elicit unsafe content, and a tendency for overrefusal on benign but sensitive prompts. Current approaches often navigate this dance with safeguard models that completely reject any content that contains unsafe portions. This approach cuts the music entirely-it may exacerbate overrefusals and fails to provide nuanced guidance for queries it refuses. To teach models a more coordinated choreography, we propose WaltzRL, a novel multi-agent reinforcement learning framework that formulates safety alignment as a collaborative, positive-sum game. WaltzRL jointly trains a conversation agent and a feedback agent, where the latter is incentivized to provide useful suggestions that improve the safety and helpfulness of the conversation agent's responses. At the core of WaltzRL is a Dynamic Improvement Reward (DIR) that evolves over time based on how well the conversation agent incorporates the feedback. At inference time, unsafe or overrefusing responses from the conversation agent are improved rather than discarded. The feedback agent is deployed together with the conversation agent and only engages adaptively when needed, preserving helpfulness and low latency on safe queries. Our experiments, conducted across five diverse datasets, demonstrate that WaltzRL significantly reduces both unsafe responses (e.g., from 39.0% to 4.6% on WildJailbreak) and overrefusals (from 45.3% to 9.9% on OR-Bench) compared to various baselines. By enabling the conversation and feedback agents to co-evolve and adaptively apply feedback, WaltzRL enhances LLM safety without degrading general capabilities, thereby advancing the Pareto front between helpfulness and harmlessness.",0,arxiv,MÃ¼zik,CC-BY/arXiv,The Alignment Waltz: Jointly Training Agents to Collaborate for Safety
"Audio-based lyrics matching can be an appealing alternative to other content-based retrieval approaches, but existing methods often suffer from limited reproducibility and inconsistent baselines. In this work, we introduce WEALY, a fully reproducible pipeline that leverages Whisper decoder embeddings for lyrics matching tasks. WEALY establishes robust and transparent baselines, while also exploring multimodal extensions that integrate textual and acoustic features. Through extensive experiments on standard datasets, we demonstrate that WEALY achieves a performance comparable to state-of-the-art methods that lack reproducibility. In addition, we provide ablation studies and analyses on language robustness, loss functions, and embedding strategies. This work contributes a reliable benchmark for future research, and underscores the potential of speech technologies for music information retrieval tasks.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Leveraging Whisper Embeddings for Audio-based Lyrics Matching
"Video-to-Audio generation has made remarkable strides in automatically synthesizing sound for video. However, existing evaluation metrics, which focus on semantic and temporal alignment, overlook a critical failure mode: models often generate acoustic events, particularly speech and music, that have no corresponding visual source. We term this phenomenon Insertion Hallucination and identify it as a systemic risk driven by dataset biases, such as the prevalence of off-screen sounds, that remains completely undetected by current metrics. To address this challenge, we first develop a systematic evaluation framework that employs a majority-voting ensemble of multiple audio event detectors. We also introduce two novel metrics to quantify the prevalence and severity of this issue: IH@vid (the fraction of videos with hallucinations) and IH@dur (the fraction of hallucinated duration). Building on this, we propose Posterior Feature Correction, a novel training-free inference-time method that mitigates IH. PFC operates in a two-pass process: it first generates an initial audio output to detect hallucinated segments, and then regenerates the audio after masking the corresponding video features at those timestamps. Experiments on several mainstream V2A benchmarks first reveal that state-of-the-art models suffer from severe IH. In contrast, our PFC method reduces both the prevalence and duration of hallucinations by over 50\% on average, without degrading, and in some cases even improving, conventional metrics for audio quality and temporal synchronization. Our work is the first to formally define, systematically measure, and effectively mitigate Insertion Hallucination, paving the way for more reliable and faithful V2A models.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Detecting and Mitigating Insertion Hallucination in Video-to-Audio Generation
"The rise of AI-generated music is diluting royalty pools and revealing structural flaws in existing remuneration frameworks, challenging the well-established artist compensation systems in the music industry. Existing compensation solutions, such as piecemeal licensing agreements, lack scalability and technical rigour, while current data attribution mechanisms provide only uncertain estimates and are rarely implemented in practice. This paper introduces a framework for a generative music infrastructure centred on direct attribution, transparent royalty distribution, and granular control for artists and rights' holders. We distinguish ontologically between the training set and the inference set, which allows us to propose two complementary forms of attribution: training-time attribution and inference-time attribution. We here favour inference-time attribution, as it enables direct, verifiable compensation whenever an artist's catalogue is used to condition a generated output. Besides, users benefit from the ability to condition generations on specific songs and receive transparent information about attribution and permitted usage. Our approach offers an ethical and practical solution to the pressing need for robust compensation mechanisms in the era of AI-generated music, ensuring that provenance and fairness are embedded at the core of generative systems.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Attribution-by-design: Ensuring Inference-Time Provenance in Generative Music Systems
"In Music Information Retrieval (MIR), modeling and transforming the tone of musical instruments, particularly electric guitars, has gained increasing attention due to the richness of the instrument tone and the flexibility of expression. Tone morphing enables smooth transitions between different guitar sounds, giving musicians greater freedom to explore new textures and personalize their performances. This study explores learning-based approaches for guitar tone morphing, beginning with LoRA fine-tuning to improve the model performance on limited data. Moreover, we introduce a simpler method, named spherical interpolation using Music2Latent. It yields significantly better results than the more complex fine-tuning approach. Experiments show that the proposed architecture generates smoother and more natural tone transitions, making it a practical and efficient tool for music production and real-time audio effects.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Guitar Tone Morphing by Diffusion-based Model
"Most current music source separation (MSS) methods rely on supervised learning, limited by training data quantity and quality. Though web-crawling can bring abundant data, platform-level track labeling often causes metadata mismatches, impeding accurate ""audio-label"" pair acquisition. To address this, we present ACMID: a dataset for MSS generated through web crawling of extensive raw data, followed by automatic cleaning via an instrument classifier built on a pre-trained audio encoder that filters and aggregates clean segments of target instruments from the crawled tracks, resulting in the refined ACMID-Cleaned dataset. Leveraging abundant data, we expand the conventional classification from 4-stem (Vocal/Bass/Drums/Others) to 7-stem (Piano/Drums/Bass/Acoustic Guitar/Electric Guitar/Strings/Wind-Brass), enabling high granularity MSS systems. Experiments on SOTA MSS model demonstrates two key results: (i) MSS model trained with ACMID-Cleaned achieved a 2.39dB improvement in SDR performance compared to that with ACMID-Uncleaned, demostrating the effectiveness of our data cleaning procedure; (ii) incorporating ACMID-Cleaned to training enhances MSS model's average performance by 1.16dB, confirming the value of our dataset. Our data crawling code, cleaning model code and weights are available at: https://github.com/scottishfold0621/ACMID.",0,arxiv,MÃ¼zik,CC-BY/arXiv,ACMID: Automatic Curation of Musical Instrument Dataset for 7-Stem Music Source Separation
"Processing long-form audio is a major challenge for Large Audio Language models (LALMs). These models struggle with the quadratic cost of attention ($O(N^2)$) and with modeling long-range temporal dependencies. Existing audio benchmarks are built mostly from short clips and do not evaluate models in realistic long context settings. To address this gap, we introduce AudioMarathon, a benchmark designed to evaluate both understanding and inference efficiency on long-form audio. AudioMarathon provides a diverse set of tasks built upon three pillars: long-context audio inputs with durations ranging from 90.0 to 300.0 seconds, which correspond to encoded sequences of 2,250 to 7,500 audio tokens, respectively, full domain coverage across speech, sound, and music, and complex reasoning that requires multi-hop inference. We evaluate state-of-the-art LALMs and observe clear performance drops as audio length grows. We also study acceleration techniques and analyze the trade-offs of token pruning and KV cache eviction. The results show large gaps across current LALMs and highlight the need for better temporal reasoning and memory-efficient architectures. We believe AudioMarathon will drive the audio and multimodal research community to develop more advanced audio understanding models capable of solving complex audio tasks.",0,arxiv,MÃ¼zik,CC-BY/arXiv,AudioMarathon: A Comprehensive Benchmark for Long-Context Audio Understanding and Efficiency in Audio LLMs
"In recent years, significant advances have been made in music source separation, with model architectures such as dual-path modeling, band-split modules, or transformer layers achieving comparably good results. However, these models often contain a significant number of parameters, posing challenges to devices with limited computational resources in terms of training and practical application. While some lightweight models have been introduced, they generally perform worse compared to their larger counterparts. In this paper, we take inspiration from these recent advances to improve a lightweight model. We demonstrate that with careful design, a lightweight model can achieve comparable SDRs to models with up to 13 times more parameters. Our proposed model, Moises-Light, achieves competitive results in separating four musical stems on the MUSDB-HQ benchmark dataset. The proposed model also demonstrates competitive scalability when using MoisesDB as additional training data.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Moises-Light: Resource-efficient Band-split U-Net For Music Source Separation
"This study introduces Mean Averaging Smoothed Product (MASP) Spectrum, which is a modified version of the Harmonic Product Spectrum, designed to enhance pitch estimation for many algorithm-wise deceptive frequency spectra that still lead clear pitches, for both harmonic and inharmonic cases. By introducing a global mean based smoothing for spectrum, the MASP algorithm diminishes the unwanted sensitivity of HPS for spectra with missing partials. The method exhibited robust pitch estimations consistent with perceptual expectations. Motivated upon the strong correlation between consonance and periodicity, the same algorithm is extended and, with the proposition of a harmonicity measure (H), used to evaluate musical consonance for two and three tones; yielding consonance hierarchies that align with perception and practice of music theory. These findings suggest that perception of pitch and consonance may share a similar underlying mechanism that depend on spectrum.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Pitch Estimation With Mean Averaging Smoothed Product Spectrum And Musical Consonance Evaluation Using MASP
"Automatic chord recognition (ACR) via deep learning models has gradually achieved promising recognition accuracy, yet two key challenges remain. First, prior work has primarily focused on audio-domain ACR, while symbolic music (e.g., score) ACR has received limited attention due to data scarcity. Second, existing methods still overlook strategies that are aligned with human music analytical practices. To address these challenges, we make two contributions: (1) we introduce POP909-CL, an enhanced version of POP909 dataset with tempo-aligned content and human-corrected labels of chords, beats, keys, and time signatures; and (2) We propose BACHI, a symbolic chord recognition model that decomposes the task into different decision steps, namely boundary detection and iterative ranking of chord root, quality, and bass (inversion). This mechanism mirrors the human ear-training practices. Experiments demonstrate that BACHI achieves state-of-the-art chord recognition performance on both classical and pop music benchmarks, with ablation studies validating the effectiveness of each module.",0,arxiv,MÃ¼zik,CC-BY/arXiv,BACHI: Boundary-Aware Symbolic Chord Recognition Through Masked Iterative Decoding on Pop and Classical Music
"Modulations are a critical part of sound design and music production, enabling the creation of complex and evolving audio. Modern synthesizers provide envelopes, low frequency oscillators (LFOs), and more parameter automation tools that allow users to modulate the output with ease. However, determining the modulation signals used to create a sound is difficult, and existing sound-matching / parameter estimation systems are often uninterpretable black boxes or predict high-dimensional framewise parameter values without considering the shape, structure, and routing of the underlying modulation curves. We propose a neural sound-matching approach that leverages modulation extraction, constrained control signal parameterizations, and differentiable digital signal processing (DDSP) to discover the modulations present in a sound. We demonstrate the effectiveness of our approach on highly modulated synthetic and real audio samples, its applicability to different DDSP synth architectures, and investigate the trade-off it incurs between interpretability and sound-matching accuracy. We make our code and audio samples available and provide the trained DDSP synths in a VST plugin.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Modulation Discovery with Differentiable Digital Signal Processing
"We propose the Segmented Full-Song Model (SFS) for symbolic full-song generation. The model accepts a user-provided song structure and an optional short seed segment that anchors the main idea around which the song is developed. By factorizing a song into segments and generating each one through selective attention to related segments, the model achieves higher quality and efficiency compared to prior work. To demonstrate its suitability for human-AI interaction, we further wrap SFS into a web application that enables users to iteratively co-create music on a piano roll with customizable structures and flexible ordering.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Segment-Factorized Full-Song Generation on Symbolic Piano Music
"Recent advances in text-to-music models have enabled coherent music generation from text prompts, yet fine-grained emotional control remains unresolved. We introduce LARA-Gen, a framework for continuous emotion control that aligns the internal hidden states with an external music understanding model through Latent Affective Representation Alignment (LARA), enabling effective training. In addition, we design an emotion control module based on a continuous valence-arousal space, disentangling emotional attributes from textual content and bypassing the bottlenecks of text-based prompting. Furthermore, we establish a benchmark with a curated test set and a robust Emotion Predictor, facilitating objective evaluation of emotional controllability in music generation. Extensive experiments demonstrate that LARA-Gen achieves continuous, fine-grained control of emotion and significantly outperforms baselines in both emotion adherence and music quality. Generated samples are available at https://nieeim.github.io/LARA-Gen/.",0,arxiv,MÃ¼zik,CC-BY/arXiv,LARA-Gen: Enabling Continuous Emotion Control for Music Generation Models via Latent Affective Representation Alignment
"Whereas chord transcription has received considerable attention during the past couple of decades, far less work has been devoted to transcribing and encoding the rhythmic patterns that occur in a song. The topic is especially relevant for instruments such as the rhythm guitar, which is typically played by strumming rhythmic patterns that repeat and vary over time. However, in many cases one cannot objectively define a single ""right"" rhythmic pattern for a given song section. To create a dataset with well-defined ground-truth labels, we asked expert musicians to transcribe the rhythmic patterns in 410 popular songs and record cover versions where the guitar tracks followed those transcriptions. To transcribe the strums and their corresponding rhythmic patterns, we propose a three-step framework. Firstly, we perform approximate stem separation to extract the guitar part from the polyphonic mixture. Secondly, we detect individual strums within the separated guitar audio, using a pre-trained foundation model (MERT) as a backbone. Finally, we carry out a pattern-decoding process in which the transcribed sequence of guitar strums is represented by patterns drawn from an expert-curated vocabulary. We show that it is possible to transcribe the rhythmic patterns of the guitar track in polyphonic music with quite high accuracy, producing a representation that is human-readable and includes automatically detected bar lines and time signature markers. We perform ablation studies and error analysis and propose a set of evaluation metrics to assess the accuracy and readability of the predicted rhythmic pattern sequence.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Transcribing Rhythmic Patterns of the Guitar Track in Polyphonic Music
"Automated video editing remains an underexplored task in the computer vision and multimedia domains, especially when contrasted with the growing interest in video generation and scene understanding. In this work, we address the specific challenge of editing multicamera recordings of classical music concerts by decomposing the problem into two key sub-tasks: when to cut and how to cut. Building on recent literature, we propose a novel multimodal architecture for the temporal segmentation task (when to cut), which integrates log-mel spectrograms from the audio signals, plus an optional image embedding, and scalar temporal features through a lightweight convolutional-transformer pipeline. For the spatial selection task (how to cut), we improve the literature by updating from old backbones, e.g. ResNet, with a CLIP-based encoder and constraining distractor selection to segments from the same concert. Our dataset was constructed following a pseudo-labeling approach, in which raw video data was automatically clustered into coherent shot segments. We show that our models outperformed previous baselines in detecting cut points and provide competitive visual shot selection, advancing the state of the art in multimodal automated video editing.",0,arxiv,MÃ¼zik,CC-BY/arXiv,When and How to Cut Classical Concerts? A Multimodal Automated Video Editing Approach
"We initiate the development of a new language and theory for quantum music, to which we refer as Quantum Concept Music (QCM). This new music formalism is based on Categorical Quantum Mechanics (CQM), and more specifically, its diagrammatic incarnation Quantum Picturalism (QPict), which is heavily based on ZX-calculus. In fact, it is naturally inherited from CQM/QPict. At its heart is the explicit notational representation of relations that exist within and between the key concepts of music composition, performance, and automation. QCM also enables one to directly translate quantum phenomena into music compositions in a both intuitively obvious, rigorous and mechanical manner.   Following this pattern, we propose a score for musicians interacting like a Bell-pair under measurement, and outline examples of how it could be live performed. While most of the Western classical music notation has heavily relied on linear representation of music - which does not always adequately capture the nature of music - our approach is distinct by highlighting the fundamental relational dimension of music. In addition, this quantum-based technique not only influences the music at the profound level of composition, but also has a direct impact on a live performance, and also provides a new template for automating music, e.g.~in the context of AI-generation.   All together, we initiate the creation of new music formalism that is powerful and efficient in capturing the interactive nature of music, both in terms of internal and external interactions, and goes beyond the boundaries of Western classical music notation, which allows to use it in many different genres and directions.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Quantum Concept Music Score from Quantum Picturalism: Musical Incarnation of a Bell-Pair under Measurements
"Immersion in virtual and augmented reality solutions is reliant on plausible spatial audio. However, plausibly representing a space for immersive audio often requires many individual acoustic measurements of source-microphone pairs with specialist spatial microphones, making the procedure time-consuming and expensive. In this study, we evaluate the plausibility of extrapolated and spatialised Room Impulse Responses (RIRs) by using a 3-Alternative Forced Choice (3AFC) listening test. The stimuli comprised of RIRs from three spaces convolved with speech, orchestral, and instrumental music. When asked to select which stimuli was artificial out of one extrapolated and two real stimuli, an overall accuracy of 38% was achieved from 20 participants (5 percentage points above the expected guessing rate). Given the listening test result, this study shows that it is possible to extrapolate plausible spatial RIRs from mono measurements, decreasing the need for time and specialist equipment in acoustic measurements.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Perceptual Evaluation of Extrapolated Spatial Room Impulse Responses From a Mono Source
"Multicellular tissues, such as the epithelium coating a developing embryo, often combine complex tissue shapes with heterogeneity in the spatial arrangement of individual cells. Discrete approximations, such as the cell vertex model, can accommodate these geometric features, but techniques for analysis of such models are underdeveloped. Here, we express differential operators defined on a network representing a monolayer of confluent cells in the framework of discrete exterior calculus, considering scalar fields defined over cell vertices and centres and vector fields defined over cell edges. We achieve this by defining Hodge stars, wedge products and musical isomorphisms that are appropriate for a disordered monolayer for which cell edges and links between cell centres are not orthogonal, as is generic for epithelia. We use this framework to evaluate the harmonic vector field arising in an ablated monolayer, demonstrating an approximate 1/\textit{r} scaling of the upper bound of the field's amplitude, where \textit{r} is the distance from the ablation. Using a vertex model that incorporates osmotic effects, we then calculate the mechanical response of a monolayer in a jammed state to ablation. Perturbation displacements exhibit long-range coherence, monopolar and quadrupolar features, and an approximate 1/\textit{r} near-hole upper-bound scaling, implicating the harmonic field. The upper bounds on perturbation stress amplitudes scale approximately like 1/\textit{r}$^2$, a feature relevant to long-range mechanical signalling.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Harmonic fields and the mechanical response of a cellular monolayer to ablation
"Music Emotion Recognition (MER) is a task deeply connected to human perception, relying heavily on subjective annotations collected from contributors. Prior studies tend to focus on specific musical styles rather than incorporating a diverse range of genres, such as rock and classical, within a single framework. In this paper, we address the task of recognizing emotion from audio content by investigating five datasets with dimensional emotion annotations -- EmoMusic, DEAM, PMEmo, WTC, and WCMED -- which span various musical styles. We demonstrate the problem of out-of-distribution generalization in a systematic experiment. By closely looking at multiple data and feature sets, we provide insight into genre-emotion relationships in existing data and examine potential genre dominance and dataset biases in certain feature representations. Based on these experiments, we arrive at a simple yet effective framework that combines embeddings extracted from the Jukebox model with chroma features and demonstrate how, alongside a combination of several diverse training sets, this permits us to train models with substantially improved cross-dataset generalization capabilities.",0,arxiv,MÃ¼zik,CC-BY/arXiv,A Study on the Data Distribution Gap in Music Emotion Recognition
"There is no doubt that there has been a drastic increase in abusive and sexually explicit content in music, particularly in Billboard Music Charts. However, there is a lack of studies that validate the trend for effective policy development, as such content has harmful behavioural changes in children and youths. In this study, we utilise deep learning methods to analyse songs (lyrics) from Billboard Charts of the United States in the last seven decades. We provide a longitudinal study using deep learning and language models and review the evolution of content using sentiment analysis and abuse detection, including sexually explicit content. Our results show a significant rise in explicit content in popular music from 1990 onwards. Furthermore, we find an increasing prevalence of songs with lyrics containing profane, sexually explicit, and otherwise inappropriate language. The longitudinal analysis of the ability of language models to capture nuanced patterns in lyrical content, reflecting shifts in societal norms and language use over time.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Language models for longitudinal analysis of abusive content in Billboard Music Charts
"The effectiveness of single-model sequential recommendation architectures, while scalable, is often limited when catering to ""power users"" in sparse or niche domains. Our previous research, PinnerFormerLite, addressed this by using a fixed weighted loss to prioritize specific domains. However, this approach can be sub-optimal, as a single, uniform weight may not be sufficient for domains with very few interactions, where the training signal is easily diluted by the vast, generic dataset.   This paper proposes a novel, data-driven approach: a Dynamic Weighted Loss function with comprehensive theoretical foundations and extensive empirical validation. We introduce an adaptive algorithm that adjusts the loss weight for each domain based on its sparsity in the training data, assigning a higher weight to sparser domains and a lower weight to denser ones. This ensures that even rare user interests contribute a meaningful gradient signal, preventing them from being overshadowed.   We provide rigorous theoretical analysis including convergence proofs, complexity analysis, and bounds analysis to establish the stability and efficiency of our approach. Our comprehensive empirical validation across four diverse datasets (MovieLens, Amazon Electronics, Yelp Business, LastFM Music) with state-of-the-art baselines (SIGMA, CALRec, SparseEnNet) demonstrates that this dynamic weighting system significantly outperforms all comparison methods, particularly for sparse domains, achieving substantial lifts in key metrics like Recall at 10 and NDCG at 10 while maintaining performance on denser domains and introducing minimal computational overhead.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Adaptive Weighted Loss for Sequential Recommendations on Sparse Domains
"This paper presents a novel approach to neural instrument sound synthesis using a two-stage semi-supervised learning framework capable of generating pitch-accurate, high-quality music samples from an expressive timbre latent space. Existing approaches that achieve sufficient quality for music production often rely on high-dimensional latent representations that are difficult to navigate and provide unintuitive user experiences. We address this limitation through a two-stage training paradigm: first, we train a pitch-timbre disentangled 2D representation of audio samples using a Variational Autoencoder; second, we use this representation as conditioning input for a Transformer-based generative model. The learned 2D latent space serves as an intuitive interface for navigating and exploring the sound landscape. We demonstrate that the proposed method effectively learns a disentangled timbre space, enabling expressive and controllable audio generation with reliable pitch conditioning. Experimental results show the model's ability to capture subtle variations in timbre while maintaining a high degree of pitch accuracy. The usability of our method is demonstrated in an interactive web application, highlighting its potential as a step towards future music production environments that are both intuitive and creatively empowering: https://pgesam.faresschulz.com",0,arxiv,MÃ¼zik,CC-BY/arXiv,Pitch-Conditioned Instrument Sound Synthesis From an Interactive Timbre Latent Space
"A quantum computing algorithm for rhythm generation is presented, which aims to expand and explore quantum computing applications in the arts, particularly in music. The algorithm maps quantum random walk trajectories onto a rhythmspace -- a 2D interface that interpolates rhythmic patterns. The methodology consists of three stages. The first stage involves designing quantum computing algorithms and establishing a mapping between the qubit space and the rhythmspace. To minimize circuit depth, a decomposition of a 2D quantum random walk into two 1D quantum random walks is applied. The second stage focuses on biasing the directionality of quantum random walks by introducing classical potential fields, adjusting the probability distribution of the wave function based on the position gradient within these fields. Four potential fields are implemented: a null potential, a linear field, a Gaussian potential, and a Gaussian potential under inertial dynamics. The third stage addresses the sonification of these paths by generating MIDI drum pattern messages and transmitting them to a Digital Audio Workstation (DAW). This work builds upon existing literature that applies quantum computing to simpler qubit spaces with a few positions, extending the formalism to a 2D x-y plane. It serves as a proof of concept for scalable quantum computing-based generative random walk algorithms in music and audio applications. Furthermore, the approach is applicable to generic multidimensional sound spaces, as the algorithms are not strictly constrained to rhythm generation and can be adapted to different musical structures.",0,arxiv,MÃ¼zik,CC-BY/arXiv,From Qubits to Rhythm: Exploring Quantum Random Walks in Rhythmspaces
"Evaluation for continuous piano pedal depth estimation tasks remains incomplete when relying only on conventional frame-level metrics, which overlook musically important features such as direction-change boundaries and pedal curve contours. To provide more interpretable and musically meaningful insights, we propose an evaluation framework that augments standard frame-level metrics with an action-level assessment measuring direction and timing using segments of press/hold/release states and a gesture-level analysis that evaluates contour similarity of each press-release cycle. We apply this framework to compare an audio-only baseline with two variants: one incorporating symbolic information from MIDI, and another trained in a binary-valued setting, all within a unified architecture. Results show that the MIDI-informed model significantly outperforms the others at action and gesture levels, despite modest frame-level gains. These findings demonstrate that our framework captures musically relevant improvements indiscernible by traditional metrics, offering a more practical and effective approach to evaluating pedal depth estimation models.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Evaluating High-Resolution Piano Sustain Pedal Depth Estimation with Musically Informed Metrics
"In this paper, we propose a method for numerical modeling of the nuclear matter properties within the framework of relativistic heavy-ion collisions using a holographic equation of state. Machine learning methods were applied to address the regression and optimization issues during the calibration of the relevant parameters using the LQCD results for quark masses that approximate the physical values. Numerical simulations are performed using the iEBE-MUSIC and vHLLE-SMASH frameworks, which incorporate certain relativistic hydrodynamics solvers. We modify the code by implementing a tabulated holographic equation of state, enabling simulations of quark-gluon plasma evolution with dynamically generated initial conditions via the 3D Monte Carlo Glauber Model and SMASH. Finally, the spectra of produced hadrons are computed using a hybrid iSS+UrQMD and Hadron Sampler+SMASH approaches at the freeze-out stage.12 p",0,arxiv,MÃ¼zik,CC-BY/arXiv,Application of the holographic equations of state for modeling experiments on heavy ion collisions
"The availability of large, unlabeled datasets across various domains has contributed to the development of a plethora of methods that learn representations for multiple target (downstream) tasks through self-supervised pre-training. In this work, we introduce CVSM (Contrastive Vocal Similarity Modeling), a contrastive self-supervised procedure for music signal representation learning in the audio domain that can be utilized for musical and vocal similarity modeling. Our method operates under a contrastive framework, maximizing the similarity between vocal excerpts and musical mixtures containing the same vocals; we devise both a label-informed protocol, leveraging artist identity information to sample the contrastive pairs, and a label-agnostic scheme, involving artificial mixture creation from randomly sampled vocal and accompaniment excerpts, which are paired with vocals from the same audio segment. We evaluate our proposed method in measuring vocal similarity both objectively, through linear probing on a suite of appropriate downstream tasks, and subjectively, via conducting a user study consisting of pairwise comparisons between different models in a recommendation-by-query setting. Our results indicate that the representations learned through CVSM are effective in musical and vocal similarity modeling, outperforming numerous baselines across both isolated vocals and complete musical mixtures. Moreover, while the availability of artist identity labels during pre-training leads to overall more consistent performance both in the evaluated downstream tasks and the user study, a label-agnostic CVSM variant incorporating hybrid pre-training with real and artificial mixtures achieves comparable performance to the label-informed one in artist identification and perceived vocal similarity.",0,arxiv,MÃ¼zik,CC-BY/arXiv,CVSM: Contrastive Vocal Similarity Modeling
"Music structure analysis (MSA) underpins music understanding and controllable generation, yet progress has been limited by small, inconsistent corpora. We present SongFormer, a scalable framework that learns from heterogeneous supervision. SongFormer (i) fuses short- and long-window self-supervised audio representations to capture both fine-grained and long-range dependencies, and (ii) introduces a learned source embedding to enable training with partial, noisy, and schema-mismatched labels. To support scaling and fair evaluation, we release SongFormDB, the largest MSA corpus to date (over 10k tracks spanning languages and genres), and SongFormBench, a 300-song expert-verified benchmark. On SongFormBench, SongFormer sets a new state of the art in strict boundary detection (HR.5F) and achieves the highest functional label accuracy, while remaining computationally efficient; it surpasses strong baselines and Gemini 2.5 Pro on these metrics and remains competitive under relaxed tolerance (HR3F). Code, datasets, and model are publicly available.",0,arxiv,MÃ¼zik,CC-BY/arXiv,SongFormer: Scaling Music Structure Analysis with Heterogeneous Supervision
"By observing the activities and relationships of musicians and sound designers to the activities of creation, performance, publishing and dissemination with artificial intelligence (AI), from two specialized forums between 2022 and 2024, this article proposes a lexicometric analysis of the representations linked to their use. Indeed, the machine, now equipped with artificial intelligences requiring new appropriations and enabling new mediations, constitutes new challenges for artists. To study these confrontations and new mediations, our approach mobilizes the theoretical framework of the Human-AI Musicking Framework, based on a lexicometric analysis of content. The aim is to clarify the present and future uses of AI from the interfaces, in the creation of sound and musical content, and to identify the obstacles, obstacles, brakes and limits to appropriation ``in the fact of making the content one's own and integrating it as a part of oneself'' (Bachimont and Crozat, 2004) in the context of a collaboration between musician and machine.",0,arxiv,MÃ¼zik,CC-BY/arXiv,The use of artificial intelligence in music creation: between interface and appropriation
"Music scores are used to precisely store music pieces for transmission and preservation. To represent and manipulate these complex objects, various formats have been tailored for different use cases. While music notation follows specific rules, digital formats usually enforce them leniently. Hence, digital music scores widely vary in quality, due to software and format specificity, conversion issues, and dubious user inputs. Problems range from minor engraving discrepancies to major notation mistakes. Yet, data quality is a major issue when dealing with musical information extraction and retrieval. We present an automated approach to detect notational errors, aiming at precisely localizing defects in scores. We identify two types of errors: i) rhythm/time inconsistencies in the encoding of individual musical elements, and ii) contextual errors, i.e. notation mistakes that break commonly accepted musical rules. We implement the latter using a modular state machine that can be easily extended to include rules representing the usual conventions from the common Western music notation. Finally, we apply this error-detection method to the piano score dataset ASAP. We highlight that around 40% of the scores contain at least one notational error, and manually fix multiple of them to enhance the dataset's quality.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Detecting Notational Errors in Digital Music Scores
"Automatic Music Transcription (AMT) has advanced significantly for the piano, but transcription for the guitar remains limited due to several key challenges. Existing systems fail to detect and annotate expressive techniques (e.g., slides, bends, percussive hits) and incorrectly map notes to the wrong string and fret combination in the generated tablature. Furthermore, prior models are typically trained on small, isolated datasets, limiting their generalizability to real-world guitar recordings. To overcome these limitations, we propose a four-stage end-to-end pipeline that produces detailed guitar tablature directly from audio. Our system consists of (1) Audio-to-MIDI pitch conversion through a piano transcription model adapted to guitar datasets; (2) MLP-based expressive technique classification; (3) Transformer-based string and fret assignment; and (4) LSTM-based tablature generation. To the best of our knowledge, this framework is the first to generate detailed tablature with accurate fingerings and expressive labels from guitar audio.",0,arxiv,MÃ¼zik,CC-BY/arXiv,TART: A Comprehensive Tool for Technique-Aware Audio-to-Tab Guitar Transcription
"Music performance is a distinctly human activity, intrinsically linked to the performer's ability to convey, evoke, or express emotion. Machines cannot perform music in the human sense; they can produce, reproduce, execute, or synthesize music, but they lack the capacity for affective or emotional experience. As such, music performance is an ideal candidate through which to explore aspects of collaboration between humans and machines. In this paper, we introduce the witheFlow system, designed to enhance real-time music performance by automatically modulating audio effects based on features extracted from both biosignals and the audio itself. The system, currently in a proof-of-concept phase, is designed to be lightweight, able to run locally on a laptop, and is open-source given the availability of a compatible Digital Audio Workstation and sensors.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Go witheFlow: Real-time Emotion Driven Audio Effects Modulation
"We present Timbru, a post-hoc audio watermarking model that achieves state-of-the-art robustness and imperceptibility trade-offs without training an embedder-detector model. Given any 44.1 kHz stereo music snippet, our method performs per-audio gradient optimization to add imperceptible perturbations in the latent space of a pretrained audio VAE, guided by a combined message and perceptual loss. The watermark can then be extracted using a pretrained CLAP model. We evaluate 16-bit watermarking on MUSDB18-HQ against AudioSeal, WavMark, and SilentCipher across common filtering, noise, compression, resampling, cropping, and regeneration attacks. Our approach attains the best average bit error rates, while preserving perceptual quality, demonstrating an efficient, dataset-free path to imperceptible audio watermarking.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Multi-bit Audio Watermarking
"While recent years have seen remarkable progress in music generation models, research on their biases across countries, languages, cultures, and musical genres remains underexplored. This gap is compounded by the lack of datasets and benchmarks that capture the global diversity of music. To address these challenges, we introduce GlobalDISCO, a large-scale dataset consisting of 73k music tracks generated by state-of-the-art commercial generative music models, along with paired links to 93k reference tracks in LAION-DISCO-12M. The dataset spans 147 languages and includes musical style prompts extracted from MusicBrainz and Wikipedia. The dataset is globally balanced, representing musical styles from artists across 79 countries and five continents. Our evaluation reveals large disparities in music quality and alignment with reference music between high-resource and low-resource regions. Furthermore, we find marked differences in model performance between mainstream and geographically niche genres, including cases where models generate music for regional genres that more closely align with the distribution of mainstream styles.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Bias beyond Borders: Global Inequalities in AI-Generated Music
"While the recent developments in large language models (LLMs) have successfully enabled generative recommenders with natural language interactions, their recommendation behavior is limited, leaving other simpler yet crucial components such as metadata or attribute filtering underutilized in the system. We propose an LLM-based music recommendation system with tool calling to serve as a unified retrieval-reranking pipeline. Our system positions an LLM as an end-to-end recommendation system that interprets user intent, plans tool invocations, and orchestrates specialized components: boolean filters (SQL), sparse retrieval (BM25), dense retrieval (embedding similarity), and generative retrieval (semantic IDs). Through tool planning, the system predicts which types of tools to use, their execution order, and the arguments needed to find music matching user preferences, supporting diverse modalities while seamlessly integrating multiple database filtering methods. We demonstrate that this unified tool-calling framework achieves competitive performance across diverse recommendation scenarios by selectively employing appropriate retrieval methods based on user queries, envisioning a new paradigm for conversational music recommendation systems.",0,arxiv,MÃ¼zik,CC-BY/arXiv,TalkPlay-Tools: Conversational Music Recommendation with LLM Tool Calling
"Directly learning to generate audio waveforms in an autoregressive manner is a challenging task, due to the length of the raw sequences and the existence of important structure on many different timescales. Traditional approaches based on recurrent neural networks, as well as causal convolutions and self-attention, have only had limited success on this task. However, recent work has shown that deep state space models, also referred to as linear RNNs, can be highly efficient in this context. In this work, we push the boundaries of linear RNNs applied to raw audio modeling, investigating the effects of different architectural choices and using context-parallelism to enable training on sequences up to one minute (1M tokens) in length. We present a model, HarmonicRNN, which attains state of the art log-likelihoods and perceptual metrics on small-scale datasets.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Linear RNNs for autoregressive generation of long music samples
"The study of art evolution has provided valuable insights into societal change, often revealing long-term patterns of simplification and transformation. Album covers represent a distinctive yet understudied form of visual art that has both shaped and been shaped by cultural, technological, and commercial dynamics over the past century. As highly visible artifacts at the intersection of art and commerce, they offer a unique lens through which to study cultural evolution. In this work, we examine the visual complexity of album covers spanning 75 years and 11 popular musical genres. Using a diverse set of computational measures that capture multiple dimensions of visual complexity, our analysis reveals a broad shift toward minimalism across most genres, with notable exceptions that highlight the heterogeneity of aesthetic trends. At the same time, we observe growing variance over time, with many covers continuing to display high levels of abstraction and intricacy. Together, these findings position album covers as a rich, quantifiable archive of cultural history and underscore the value of computational approaches in the systematic study of the arts, bridging quantitative analysis with aesthetic and cultural inquiry.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Disc-Cover Complexity Trends in Music Illustrations from Sinatra to Swift
"Low-latency symbolic music generation is essential for real-time improvisation and human-AI co-creation. Existing transformer-based models, however, face a trade-off between inference speed and musical quality. Traditional acceleration techniques such as embedding pooling significantly degrade quality, while recently proposed Byte Pair Encoding (BPE) methods - though effective on single-track piano data - suffer large performance drops in multi-track settings, as revealed by our analysis. We propose Attribute-Specialized Key-Value Head Sharing (AS-KVHS), adapted to music's structured symbolic representation, achieving about 30% inference speedup with only a negligible (about 0.4%) quality drop in objective evaluations and slight improvements in subjective listening tests. Our main contributions are (1) the first systematic study of BPE's generalizability in multi-track symbolic music, and (2) the introduction of AS-KVHS for low-latency symbolic music generation. Beyond these, we also release SAGE-Music, an open-source benchmark that matches or surpasses state-of-the-art models in generation quality.",0,arxiv,MÃ¼zik,CC-BY/arXiv,SAGE-Music: Low-Latency Symbolic Music Generation via Attribute-Specialized Key-Value Head Sharing
"We propose a design space for data melodification, where standard visualization idioms and fundamental data characteristics map to rhetorical devices of music for a more affective experience of data. Traditional data sonification transforms data into sound by mapping it to different parameters such as pitch, volume, and duration. Often and regrettably, this mapping leaves behind melody, harmony, rhythm and other musical devices that compose the centuries-long persuasive and expressive power of music. What results is the occasional, unintentional sense of tinnitus and horror film-like impending doom caused by a disconnect between the semantics of data and sound. Through this work we ask, can the aestheticization of sonification through (classical) music theory make data simultaneously accessible, meaningful, and pleasing to one's ears?",0,arxiv,MÃ¼zik,CC-BY/arXiv,Data Melodification FM: Where Musical Rhetoric Meets Sonification
"Existing multimodal audio generation models often lack precise user control, which limits their applicability in professional Foley workflows. In particular, these models focus on the entire video and do not provide precise methods for prioritizing a specific object within a scene, generating unnecessary background sounds, or focusing on the wrong objects. To address this gap, we introduce the novel task of video object segmentation-aware audio generation, which explicitly conditions sound synthesis on object-level segmentation maps. We present SAGANet, a new multimodal generative model that enables controllable audio generation by leveraging visual segmentation masks along with video and textual cues. Our model provides users with fine-grained and visually localized control over audio generation. To support this task and further research on segmentation-aware Foley, we propose Segmented Music Solos, a benchmark dataset of musical instrument performance videos with segmentation information. Our method demonstrates substantial improvements over current state-of-the-art methods and sets a new standard for controllable, high-fidelity Foley synthesis. Code, samples, and Segmented Music Solos are available at https://saganet.notion.site",0,arxiv,MÃ¼zik,CC-BY/arXiv,Video Object Segmentation-Aware Audio Generation
"In this work, we study the task of multi-singer separation in a cappella music, where the number of active singers varies across mixtures. To address this, we use a power set-based data augmentation strategy that expands limited multi-singer datasets into exponentially more training samples. To separate singers, we introduce SepACap, an adaptation of SepReformer, a state-of-the-art speaker separation model architecture. We adapt the model with periodic activations and a composite loss function that remains effective when stems are silent, enabling robust detection and separation. Experiments on the JaCappella dataset demonstrate that our approach achieves state-of-the-art performance in both full-ensemble and subset singer separation scenarios, outperforming spectrogram-based baselines while generalizing to realistic mixtures with varying numbers of singers.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Source Separation for A Cappella Music
"Interpretability is essential for deploying deep learning models in symbolic music analysis, yet most research emphasizes model performance over explanation. To address this, we introduce MUSE-Explainer, a new method that helps reveal how music Graph Neural Network models make decisions by providing clear, human-friendly explanations. Our approach generates counterfactual explanations by making small, meaningful changes to musical score graphs that alter a model's prediction while ensuring the results remain musically coherent. Unlike existing methods, MUSE-Explainer tailors its explanations to the structure of musical data and avoids unrealistic or confusing outputs. We evaluate our method on a music analysis task and show it offers intuitive insights that can be visualized with standard music tools such as Verovio.",0,arxiv,MÃ¼zik,CC-BY/arXiv,MUSE-Explainer: Counterfactual Explanations for Symbolic Music Graph Classification Models
"This paper investigates the importance of personal ownership in musical AI design, examining how practising musicians can maintain creative control over the compositional process. Through a four-week ecological evaluation, we examined how a music variation tool, reliant on the skill of musicians, functioned within a composition setting. Our findings demonstrate that the dependence of the tool on the musician's ability, to provide a strong initial musical input and to turn moments into complete musical ideas, promoted ownership of both the process and artefact. Qualitative interviews further revealed the importance of this personal ownership, highlighting tensions between technological capability and artistic identity. These findings provide insight into how musical AI can support rather than replace human creativity, highlighting the importance of designing tools that preserve the humanness of musical expression.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Supporting Creative Ownership through Deep Learning-Based Music Variation
"Recent advances in large language models (LLMs) have created new opportunities for symbolic music generation. However, existing formats such as MIDI, ABC, and MusicXML are either overly complex or structurally inconsistent, limiting their suitability for token-based learning architectures. To address these challenges, we propose HNote, a novel hexadecimal-based notation system extended from YNote, which encodes both pitch and duration within a fixed 32-unit measure framework. This design ensures alignment, reduces ambiguity, and is directly compatible with LLM architectures. We converted 12,300 Jiangnan-style songs generated from traditional folk pieces from YNote into HNote, and fine-tuned LLaMA-3.1(8B) using parameter-efficient LoRA. Experimental results show that HNote achieves a syntactic correctness rate of 82.5%, and BLEU and ROUGE evaluations demonstrate strong symbolic and structural similarity, producing stylistically coherent compositions. This study establishes HNote as an effective framework for integrating LLMs with cultural music modeling.",0,arxiv,MÃ¼zik,CC-BY/arXiv,HNote: Extending YNote with Hexadecimal Encoding for Fine-Tuning LLMs in Music Modeling
"Recent advances in AI music (AIM) generation services are currently transforming the music industry. Given these advances, understanding how humans perceive AIM is crucial both to educate users on identifying AIM songs, and, conversely, to improve current models. We present results from a listener-focused experiment aimed at understanding how humans perceive AIM. In a blind, Turing-like test, participants were asked to distinguish, from a pair, the AIM and human-made song. We contrast with other studies by utilizing a randomized controlled crossover trial that controls for pairwise similarity and allows for a causal interpretation. We are also the first study to employ a novel, author-uncontrolled dataset of AIM songs from real-world usage of commercial models (i.e., Suno). We establish that listeners' reliability in distinguishing AIM causally increases when pairs are similar. Lastly, we conduct a mixed-methods content analysis of listeners' free-form feedback, revealing a focus on vocal and technical cues in their judgments.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Echoes of Humanity: Exploring the Perceived Humanness of AI Music
"While research in AI methods for music generation and analysis has grown in scope and impact, AI researchers' engagement with the ethical consequences of this work has not kept pace. To encourage such engagement, many publication venues have introduced optional or required ethics statements for AI research papers. Though some authors use these ethics statements to critically engage with the broader implications of their research, we find that the majority of ethics statements in the AI music literature do not appear to be effectively utilized for this purpose. In this work, we conduct a review of ethics statements across ISMIR, NIME, and selected prominent works in AI music from the past five years. We then offer suggestions for both audio conferences and researchers for engaging with ethics statements in ways that foster meaningful reflection rather than formulaic compliance.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Ethics Statements in AI Music Papers: The Effective and the Ineffective
"Randomness plays a pivotal yet paradoxical role in computational music creativity: it can spark novelty, but unchecked chance risks incoherence. This paper presents a thematic review of contemporary AI music systems, examining how designers incorporate randomness and uncertainty into creative practice. I draw on the concept of structured uncertainty to analyse how stochastic processes are constrained within musical and interactive frameworks. Through a comparative analysis of six systems - Musika (Pasini and SchlÃ¼ter, 2022), MIDI-DDSP (Wu et al., 2021), Melody RNN (Magenta Project), RAVE (Caillon and Esling, 2021), Wekinator (Fiebrink and Cook, 2010), and Somax 2 (Borg, 2019) - we identify recurring design patterns that support musical coherence, user control, and co-creativity. To my knowledge, this is the first thematic review examining randomness in AI music through structured uncertainty, offering practical insights for designers and artists aiming to support expressive, collaborative, or improvisational interactions.",0,arxiv,MÃ¼zik,CC-BY/arXiv,The Shape of Surprise: Structured Uncertainty and Co-Creativity in AI Music Tools
"This paper presents the first step in a research project situated within the field of musical agents. The objective is to achieve, through training, the tuning of the desired musical relationship between a live musical input and a real-time generated musical output, through the curation of a database of separated tracks. We propose an architecture integrating a symbolic decision module capable of learning and exploiting musical relationships from such musical corpus. We detail an offline implementation of this architecture employing Transformers as the decision module, associated with a perception module based on Wav2Vec 2.0, and concatenative synthesis as audio renderer. We present a quantitative evaluation of the decision module's ability to reproduce learned relationships extracted during training. We demonstrate that our decision module can predict a coherent track B when conditioned by its corresponding ''guide'' track A, based on a corpus of paired tracks (A, B).",0,arxiv,MÃ¼zik,CC-BY/arXiv,Learning Relationships Between Separate Audio Tracks for Creative Applications
"Versatile audio super-resolution (SR) aims to predict high-frequency components from low-resolution audio across diverse domains such as speech, music, and sound effects. Existing diffusion-based SR methods often fail to produce semantically aligned outputs and struggle with consistent high-frequency reconstruction. In this paper, we propose SAGA-SR, a versatile audio SR model that combines semantic and acoustic guidance. Based on a DiT backbone trained with a flow matching objective, SAGA-SR is conditioned on text and spectral roll-off embeddings. Due to the effective guidance provided by its conditioning, SAGA-SR robustly upsamples audio from arbitrary input sampling rates between 4 kHz and 32 kHz to 44.1 kHz. Both objective and subjective evaluations show that SAGA-SR achieves state-of-the-art performance across all test cases. Sound examples and code for the proposed model are available online.",0,arxiv,MÃ¼zik,CC-BY/arXiv,SAGA-SR: Semantically and Acoustically Guided Audio Super-Resolution
"The advent of digital streaming platforms have recently revolutionized the landscape of music industry, with the ensuing digitalization providing structured data collections that open new research avenues for investigating popularity dynamics and mainstream success. The present work explored which determinants hold the strongest predictive influence for a track's inclusion in the Billboard Hot 100 charts, including streaming popularity, measurable audio signal attributes, and probabilistic indicators of human listening. The analysis revealed that popularity was by far the most decisive predictor of Billboard Hot 100 inclusion, with considerable contribution from instrumentalness, valence, duration and speechiness. Logistic Regression achieved 90.0% accuracy, with very high recall for charting singles (0.986) but lower recall for non-charting ones (0.813), yielding balanced F1-scores around 0.90. Random Forest slightly improved performance to 90.4% accuracy, maintaining near-perfect precision for non-charting singles (0.990) and high recall for charting ones (0.992), with F1-scores up to 0.91. Gradient Boosting (XGBoost) reached 90.3% accuracy, delivering a more balanced trade-off by improving recall for non-charting singles (0.837) while sustaining high recall for charting ones (0.969), resulting in F1-scores comparable to the other models.",0,arxiv,MÃ¼zik,CC-BY/arXiv,"Beyond the Hook: Predicting Billboard Hot 100 Chart Inclusion with Machine Learning from Streaming, Audio Signals, and Perceptual Features"
"Automatic Drum Transcription (ADT) remains a challenging task in MIR but recent advances allow accurate transcription of drum kits with up 5 classes - kick, snare, hi-hats, toms and cymbals - via the ADTOF package. In addition, several drum kit \emph{stem} separation models in the open source community support separation for more than 6 stem classes, including distinct crash and ride cymbals. In this work we explore the benefits of combining these tools to improve the realism of drum transcriptions. We describe a simple post-processing step which expands the transcription output from five to seven classes and furthermore, we are able to estimate MIDI velocity values based on the separated stems. Our solution achieves strong performance when assessed against a baseline of 8-class drum transcription and produces realistic MIDI transcriptions suitable for MIR or music production tasks.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Enhanced Automatic Drum Transcription via Drum Stem Source Separation
"Audio pretrained models are widely employed to solve various tasks in speech processing, sound event detection, or music information retrieval. However, the representations learned by these models are unclear, and their analysis mainly restricts to linear probing of the hidden representations. In this work, we explore the use of Sparse Autoencoders (SAEs) to analyze the hidden representations of pretrained models, focusing on a case study in singing technique classification. We first demonstrate that SAEs retain both information about the original representations and class labels, enabling their internal structure to provide insights into self-supervised learning systems. Furthermore, we show that SAEs enhance the disentanglement of vocal attributes, establishing them as an effective tool for identifying the underlying factors encoded in the representations.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Sparse Autoencoders Make Audio Foundation Models more Explainable
"This paper presents an unsupervised machine learning algorithm that identifies recurring patterns -- referred to as ``music-words'' -- from symbolic music data. These patterns are fundamental to musical structure and reflect the cognitive processes involved in composition. However, extracting these patterns remains challenging because of the inherent semantic ambiguity in musical interpretation. We formulate the task of music-word discovery as a statistical optimization problem and propose a two-stage Expectation-Maximization (EM)-based learning framework: 1. Developing a music-word dictionary; 2. Reconstructing the music data. When evaluated against human expert annotations, the algorithm achieved an Intersection over Union (IoU) score of 0.61. Our findings indicate that minimizing code length effectively addresses semantic ambiguity, suggesting that human optimization of encoding systems shapes musical semantics. This approach enables computers to extract ``basic building blocks'' from music data, facilitating structural analysis and sparse encoding. The method has two primary applications. First, in AI music, it supports downstream tasks such as music generation, classification, style transfer, and improvisation. Second, in musicology, it provides a tool for analyzing compositional patterns and offers insights into the principle of minimal encoding across diverse musical styles and composers.",0,arxiv,MÃ¼zik,CC-BY/arXiv,"Discovering ""Words"" in Music: Unsupervised Learning of Compositional Sparse Code for Symbolic Music"
"Music representation models are widely used for tasks such as tagging, retrieval, and music understanding. Yet, their potential to encode cultural bias remains underexplored. In this paper, we apply Concept Activation Vectors (CAVs) to investigate whether non-musical singer attributes - such as gender and language - influence genre representations in unintended ways. We analyze four state-of-the-art models (MERT, Whisper, MuQ, MuQ-MuLan) using the STraDa dataset, carefully balancing training sets to control for genre confounds. Our results reveal significant model-specific biases, aligning with disparities reported in MIR and music sociology. Furthermore, we propose a post-hoc debiasing strategy using concept vector manipulation, demonstrating its effectiveness in mitigating these biases. These findings highlight the need for bias-aware model design and show that conceptualized interpretability methods offer practical tools for diagnosing and mitigating representational bias in MIR.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Beyond Genre: Diagnosing Bias in Music Embeddings Using Concept Activation Vectors
"The generation of musically coherent and aesthetically pleasing harmony remains a significant challenge in the field of algorithmic composition. This paper introduces an innovative Agentic AI-enabled Higher Harmony Music Generator, a multi-agent system designed to create harmony in a collaborative and modular fashion. Our framework comprises four specialized agents: a Music-Ingestion Agent for parsing and standardizing input musical scores; a Chord-Knowledge Agent, powered by a Chord-Former (Transformer model), to interpret and provide the constituent notes of complex chord symbols; a Harmony-Generation Agent, which utilizes a Harmony-GPT and a Rhythm-Net (RNN) to compose a melodically and rhythmically complementary harmony line; and an Audio-Production Agent that employs a GAN-based Symbolic-to-Audio Synthesizer to render the final symbolic output into high-fidelity audio. By delegating specific tasks to specialized agents, our system effectively mimics the collaborative process of human musicians. This modular, agent-based approach allows for robust data processing, deep theoretical understanding, creative composition, and realistic audio synthesis, culminating in a system capable of generating sophisticated and contextually appropriate higher-voice harmonies for given melodies.",0,arxiv,MÃ¼zik,CC-BY/arXiv,An Agent-Based Framework for Automated Higher-Voice Harmony Generation
"This project presents an AI-based system for tone replication in music production, focusing on predicting EQ parameter settings directly from audio features. Unlike traditional audio-to-audio methods, our approach outputs interpretable parameter values (e.g., EQ band gains) that musicians can further adjust in their workflow. Using a dataset of piano recordings with systematically varied EQ settings, we evaluate both regression and neural network models. The neural network achieves a mean squared error of 0.0216 on multi-band tasks. The system enables practical, flexible, and automated tone matching for music producers and lays the foundation for extensions to more complex audio effects.",0,arxiv,MÃ¼zik,CC-BY/arXiv,From Sound to Setting: AI-Based Equalizer Parameter Prediction for Piano Tone Replication
"Audio generation, including speech, music and sound effects, has advanced rapidly in recent years. These tasks can be divided into two categories: time-aligned (TA) tasks, where each input unit corresponds to a specific segment of the output audio (e.g., phonemes aligned with frames in speech synthesis); and non-time-aligned (NTA) tasks, where such alignment is not available. Since modeling paradigms for the two types are typically different, research on different audio generation tasks has traditionally followed separate trajectories. However, audio is not inherently divided into such categories, making a unified model a natural and necessary goal for general audio generation. Previous unified audio generation works have adopted autoregressive architectures, while unified non-autoregressive approaches remain largely unexplored. In this work, we propose UniFlow-Audio, a universal audio generation framework based on flow matching. We propose a dual-fusion mechanism that temporally aligns audio latents with TA features and integrates NTA features via cross-attention in each model block. Task-balanced data sampling is employed to maintain strong performance across both TA and NTA tasks. UniFlow-Audio supports omni-modalities, including text, audio, and video. By leveraging the advantage of multi-task learning and the generative modeling capabilities of flow matching, UniFlow-Audio achieves strong results across 7 tasks using fewer than 8K hours of public training data and under 1B trainable parameters. Even the small variant with only ~200M trainable parameters shows competitive performance, highlighting UniFlow-Audio as a potential non-auto-regressive foundation model for audio generation. Code and models will be available at https://wsntxxn.github.io/uniflow_audio.",0,arxiv,MÃ¼zik,CC-BY/arXiv,UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities
"Statistical models and information theory have provided a useful set of tools for studying music from a quantitative perspective. These approaches have been employed to generate compositions, analyze structural patterns, and model cognitive processes that underlie musical perception. A common framework used in such studies is a Markov chain model, which models the probability of a musical event -- such as a note, chord, or rhythm -- based on a sequence of preceding events. While many studies focus on first-order models, relatively few have used more complex models to systematically compare across composers and compositional forms. In this study, we examine statistical dependencies in classical sonatas and quartets using higher-order Markov chains fit to sequences of top notes. Our data set of 605 MIDI files comprises piano sonatas and string quartets by Mozart, Haydn, Beethoven, and Schubert, from which we analyze sequences of top notes. We probe statistical dependencies using three distinct methods: Markov chain fits, time-delayed mutual information, and mixture transition distribution analysis. We find that, in general, the statistical dependencies in Mozart's music notably differ from that of the other three composers. Markov chain models of higher order provide significantly better fits than low-order models for Beethoven, Haydn, and Schubert, but not for Mozart. At the same time, we observe nuances across compositional forms and composers: for example, in the string quartets, certain metrics yield comparable results for Mozart and Beethoven. Broadly, our study extends the analysis of statistical dependencies in music, and highlights systematic distinctions in the predictability of sonatas and quartets from different classical composers. These findings motivate future work comparing across composers for other musical forms, or in other eras, cultures, or musical traditions.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Predictability and Statistical Memory in Classical Sonatas and Quartets
"Generating realistic, context-aware two-person motion conditioned on diverse modalities remains a central challenge in computer graphics, animation, and human-computer interaction. We introduce DualFlow, a unified and efficient framework for multi-modal two-person motion generation. DualFlow conditions 3D motion synthesis on diverse inputs, including text, music, and prior motion sequences. Leveraging rectified flow, it achieves deterministic straight-line sampling paths between noise and data, reducing inference time and mitigating error accumulation common in diffusion-based models. To enhance semantic grounding, DualFlow employs a Retrieval-Augmented Generation (RAG) module that retrieves motion exemplars using music features and LLM-based text decompositions of spatial relations, body movements, and rhythmic patterns. We use contrastive objective that further strengthens alignment with conditioning signals and introduce synchronization loss that improves inter-person coordination. Extensive evaluations across text-to-motion, music-to-motion, and multi-modal interactive benchmarks show consistent gains in motion quality, responsiveness, and efficiency. DualFlow produces temporally coherent and rhythmically synchronized motions, setting state-of-the-art in multi-modal human motion generation.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Unified Multi-Modal Interactive & Reactive 3D Motion Generation via Rectified Flow
"The human auditory cortex is topographically organized. Neurons with similar response properties are spatially clustered, forming smooth maps for acoustic features such as frequency in early auditory areas, and modular regions selective for music and speech in higher-order cortex. Yet, evaluations for current computational models of auditory perception do not measure whether such topographic structure is present in a candidate model. Here, we show that cortical topography is not present in the previous best-performing models at predicting human auditory fMRI responses. To encourage the emergence of topographic organization, we adapt a cortical wiring-constraint loss originally designed for visual perception. The new class of topographic auditory models, TopoAudio, are trained to classify speech, and environmental sounds from cochleagram inputs, with an added constraint that nearby units on a 2D cortical sheet develop similar tuning. Despite these additional constraints, TopoAudio achieves high accuracy on benchmark tasks comparable to the unconstrained non-topographic baseline models. Further, TopoAudio predicts the fMRI responses in the brain as well as standard models, but unlike standard models, TopoAudio develops smooth, topographic maps for tonotopy and amplitude modulation (common properties of early auditory representation, as well as clustered response modules for music and speech (higher-order selectivity observed in the human auditory cortex). TopoAudio is the first end-to-end biologically grounded auditory model to exhibit emergent topography, and our results emphasize that a wiring-length constraint can serve as a general-purpose regularization tool to achieve biologically aligned representations.",0,arxiv,MÃ¼zik,CC-BY/arXiv,End-to-end Topographic Auditory Models Replicate Signatures of Human Auditory Cortex
"Expressive performance rendering (EPR) and automatic piano transcription (APT) are fundamental yet inverse tasks in music information retrieval: EPR generates expressive performances from symbolic scores, while APT recovers scores from performances. Despite their dual nature, prior work has addressed them independently. In this paper we propose a unified framework that jointly models EPR and APT by disentangling note-level score content and global performance style representations from both paired and unpaired data. Our framework is built on a transformer-based sequence-to-sequence architecture and is trained using only sequence-aligned data, without requiring fine-grained note-level alignment. To automate the rendering process while ensuring stylistic compatibility with the score, we introduce an independent diffusion-based performance style recommendation module that generates style embeddings directly from score content. This modular component supports both style transfer and flexible rendering across a range of expressive styles. Experimental results from both objective and subjective evaluations demonstrate that our framework achieves competitive performance on EPR and APT tasks, while enabling effective content-style disentanglement, reliable style transfer, and stylistically appropriate rendering. Demos are available at https://jointpianist.github.io/epr-apt/",0,arxiv,MÃ¼zik,CC-BY/arXiv,Disentangling Score Content and Performance Style for Joint Piano Rendering and Transcription
"The exceptional reception of Pietro Metastasio's works during the eighteenth century, all over Europe and in the Iberian Peninsula in particular, is well documented. Due to that unparalleled success, it is possible to ascertain Spain and Portugal's participation in international, contemporary tastes and artistic webs, applicable to both composers and performers. However, this internationalisation needs to be nuanced, as some characteristics of the repertoire specifically written for the Peninsula indicate that their court audiences may have had expectations, both social and strictly musical, different from those of the public in opera theatres elsewhere in the continent. In this light, this article investigates in what ways the style of five composers in the international scene - Perez, Galuppi, Jommelli, Conforto, and Corselli - varied when commissioned to write opera seria for the Iberian courts. The statistical analysis of fifteen settings especially written for the court theatres in Madrid and Lisbon, in comparison to the average data extracted from a corpus of 2,404 arias from 126 versions of a select number of Metastasian librettos, allows us to evaluate some particular usages regarding key, metre, tempo, and treatment of the vocal part. In this manner, through quantitative analysis, this article places eighteenth-century Iberian music production and consumption in the context of European opera seria, while ultimately suggesting that its unique musical characteristics were also partly dependent on local musical customs, gender stereotypes, and personal idiosyncrasies alike.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Constructing Opera Seria in the Iberian Courts: Metastasian Repertoire for Spain and Portugal
"While automatic music transcription is well-established in music information retrieval, most models are limited to transcribing pitch and timing information from audio, and thus omit crucial expressive and instrument-specific nuances. One example is playing technique on the violin, which affords its distinct palette of timbres for maximal emotional impact. Here, we propose VioPTT (Violin Playing Technique-aware Transcription), a lightweight, end-to-end model that directly transcribes violin playing technique in addition to pitch onset and offset. Furthermore, we release MOSA-VPT, a novel, high-quality synthetic violin playing technique dataset to circumvent the need for manually labeled annotations. Leveraging this dataset, our model demonstrated strong generalization to real-world note-level violin technique recordings in addition to achieving state-of-the-art transcription performance. To our knowledge, VioPTT is the first to jointly combine violin transcription and playing technique prediction within a unified framework.",0,arxiv,MÃ¼zik,CC-BY/arXiv,VioPTT: Violin Technique-Aware Transcription from Synthetic Data Augmentation
"Symbolic music generation faces a fundamental trade-off between efficiency and quality. Fine-grained tokenizations achieve strong coherence but incur long sequences and high complexity, while compact tokenizations improve efficiency at the expense of intra-token dependencies. To address this, we adapt a delay-based scheduling mechanism (DP) that expands compound-like tokens across decoding steps, enabling autoregressive modeling of intra-token dependencies while preserving efficiency. Notably, DP is a lightweight strategy that introduces no additional parameters and can be seamlessly integrated into existing representations. Experiments on symbolic orchestral MIDI datasets show that our method improves all metrics over standard compound tokenizations and narrows the gap to fine-grained tokenizations.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Time-Shifted Token Scheduling for Symbolic Music Generation
"Guidance methods have demonstrated significant improvements in cross-modal audio generation, including text-to-audio (T2A) and video-to-audio (V2A) generation. The popularly adopted method, classifier-free guidance (CFG), steers generation by emphasizing condition alignment, enhancing fidelity but often at the cost of diversity. Recently, autoguidance (AG) has been explored for audio generation, encouraging the sampling to faithfully reconstruct the target distribution and showing increased diversity. Despite these advances, they usually rely on a single guiding principle, e.g., condition alignment in CFG or score accuracy in AG, leaving the full potential of guidance for audio generation untapped. In this work, we explore enriching the composition of the guidance method and present a mixture-of-guidance framework, AudioMoG. Within the design space, AudioMoG can exploit the complementary advantages of distinctive guiding principles by fulfilling their cumulative benefits. With a reduced form, AudioMoG can consider parallel complements or recover a single guiding principle, without sacrificing generality. We experimentally show that, given the same inference speed, AudioMoG approach consistently outperforms single guidance in T2A generation across sampling steps, concurrently showing advantages in V2A, text-to-music, and image generation. These results highlight a ""free lunch"" in current cross-modal audio generation systems: higher quality can be achieved through mixed guiding principles at the sampling stage without sacrificing inference efficiency. Demo samples are available at: https://audio-mog.github.io.",0,arxiv,MÃ¼zik,CC-BY/arXiv,AudioMoG: Guiding Audio Generation with Mixture-of-Guidance
"Text-to-music models have revolutionized the creative landscape, offering new possibilities for music creation. Yet their integration into musicians workflows remains underexplored. This paper presents a case study on how TTM models impact music production, based on a user study of their effect on producers creative workflows. Participants produce tracks using a custom tool combining TTM and source separation models. Semi-structured interviews and thematic analysis reveal key challenges, opportunities, and ethical considerations. The findings offer insights into the transformative potential of TTMs in music production, as well as challenges in their real-world integration.",0,arxiv,MÃ¼zik,CC-BY/arXiv,AI-Assisted Music Production: A User Study on Text-to-Music Models
"As large language models continue to develop, the feasibility and significance of text-based symbolic music tasks have become increasingly prominent. While symbolic music has been widely used in generation tasks, LLM capabilities in understanding and reasoning about symbolic music remain largely underexplored. To address this gap, we propose ABC-Eval, the first open-source benchmark dedicated to the understanding and instruction-following capabilities in text-based ABC notation scores. It comprises 1,086 test samples spanning 10 sub-tasks, covering scenarios from basic musical syntax comprehension to complex sequence-level reasoning. Such a diverse scope poses substantial challenges to models' ability to handle symbolic music tasks. We evaluated seven state-of-the-art LLMs on ABC-Eval, and the results reveal notable limitations in existing models' symbolic music processing capabilities. Furthermore, the consistent performance of individual baselines across different sub-tasks supports the reliability of our benchmark.",0,arxiv,MÃ¼zik,CC-BY/arXiv,ABC-Eval: Benchmarking Large Language Models on Symbolic Music Understanding and Instruction Following
"The growing capabilities of large language models and multimodal systems have spurred interest in voice-first AI assistants, yet existing benchmarks are inadequate for evaluating the full range of these systems' capabilities. We introduce VoiceAssistant-Eval, a comprehensive benchmark designed to assess AI assistants across listening, speaking, and viewing. VoiceAssistant-Eval comprises 10,497 curated examples spanning 13 task categories. These tasks include natural sounds, music, and spoken dialogue for listening; multi-turn dialogue, role-play imitation, and various scenarios for speaking; and highly heterogeneous images for viewing. To demonstrate its utility, we evaluate 21 open-source models and GPT-4o-Audio, measuring the quality of the response content and speech, as well as their consistency. The results reveal three key findings: (1) proprietary models do not universally outperform open-source models; (2) most models excel at speaking tasks but lag in audio understanding; and (3) well-designed smaller models can rival much larger ones. Notably, the mid-sized Step-Audio-2-mini (7B) achieves more than double the listening accuracy of LLaMA-Omni2-32B-Bilingual. However, challenges remain: multimodal (audio plus visual) input and role-play voice imitation tasks are difficult for current models, and significant gaps persist in robustness and safety alignment. VoiceAssistant-Eval identifies these gaps and establishes a rigorous framework for evaluating and guiding the development of next-generation AI assistants. Code and data will be released at https://mathllm.github.io/VoiceAssistantEval/ .",0,arxiv,MÃ¼zik,CC-BY/arXiv,"VoiceAssistant-Eval: Benchmarking AI Assistants across Listening, Speaking, and Viewing"
"The ability to reason from audio, including speech, paralinguistic cues, environmental sounds, and music, is essential for AI agents to interact effectively in real-world scenarios. Existing benchmarks mainly focus on static or single-scene settings and do not fully capture scenarios where multiple speakers, unfolding events, and heterogeneous audio sources interact. To address these challenges, we introduce MDAR, a benchmark for evaluating models on complex, multi-scene, and dynamically evolving audio reasoning tasks. MDAR comprises 3,000 carefully curated question-answer pairs linked to diverse audio clips, covering five categories of complex reasoning and spanning three question types. We benchmark 26 state-of-the-art audio language models on MDAR and observe that they exhibit limitations in complex reasoning tasks. On single-choice questions, Qwen2.5-Omni (open-source) achieves 76.67% accuracy, whereas GPT-4o Audio (closed-source) reaches 68.47%; however, GPT-4o Audio substantially outperforms Qwen2.5-Omni on the more challenging multiple-choice and open-ended tasks. Across all three question types, no model achieves 80% performance. These findings underscore the unique challenges posed by MDAR and its value as a benchmark for advancing audio reasoning research.Code and benchmark can be found at https://github.com/luckyerr/MDAR.",0,arxiv,MÃ¼zik,CC-BY/arXiv,MDAR: A Multi-scene Dynamic Audio Reasoning Benchmark
"Recently, Image-to-Music (I2M) generation has garnered significant attention, with potential applications in fields such as gaming, advertising, and multi-modal art creation. However, due to the ambiguous and subjective nature of I2M tasks, most end-to-end methods lack interpretability, leaving users puzzled about the generation results. Even methods based on emotion mapping face controversy, as emotion represents only a singular aspect of art. Additionally, most learning-based methods require substantial computational resources and large datasets for training, hindering accessibility for common users. To address these challenges, we propose the first Vision Language Model (VLM)-based I2M framework that offers high interpretability and low computational cost. Specifically, we utilize ABC notation to bridge the text and music modalities, enabling the VLM to generate music using natural language. We then apply multi-modal Retrieval-Augmented Generation (RAG) and self-refinement techniques to allow the VLM to produce high-quality music without external training. Furthermore, we leverage the generated motivations in text and the attention maps from the VLM to provide explanations for the generated results in both text and image modalities. To validate our method, we conduct both human studies and machine evaluations, where our method outperforms others in terms of music quality and music-image consistency, indicating promising results. Our code is available at https://github.com/RS2002/Image2Music .",0,arxiv,MÃ¼zik,CC-BY/arXiv,Zero-Effort Image-to-Music Generation: An Interpretable RAG-based VLM Approach
"We propose DAVIS, a Diffusion-based Audio-VIsual Separation framework that solves the audio-visual sound source separation task through generative learning. Existing methods typically frame sound separation as a mask-based regression problem, achieving significant progress. However, they face limitations in capturing the complex data distribution required for high-quality separation of sounds from diverse categories. In contrast, DAVIS circumvents these issues by leveraging potent generative modeling paradigms, specifically Denoising Diffusion Probabilistic Models (DDPM) and the more recent Flow Matching (FM), integrated within a specialized Separation U-Net architecture. Our framework operates by synthesizing the desired separated sound spectrograms directly from a noise distribution, conditioned concurrently on the mixed audio input and associated visual information. The inherent nature of its generative objective makes DAVIS particularly adept at producing high-quality sound separations for diverse sound categories. We present comparative evaluations of DAVIS, encompassing both its DDPM and Flow Matching variants, against leading methods on the standard AVE and MUSIC datasets. The results affirm that both variants surpass existing approaches in separation quality, highlighting the efficacy of our generative framework for tackling the audio-visual source separation task.",0,arxiv,MÃ¼zik,CC-BY/arXiv,High-Quality Sound Separation Across Diverse Categories via Visually-Guided Generative Modeling
"We propose AUV, a unified neural audio codec with a single codebook, which enables a favourable reconstruction of speech and further extends to general audio, including vocal, music, and sound. AUV is capable of tackling any 16 kHz mixed-domain audio segment at bit rates around 700 bps. To accomplish this, we guide the matryoshka codebook with nested domain-specific partitions, assigned with corresponding teacher models to perform distillation, all in a single-stage training. A conformer-style encoder-decoder architecture with STFT features as audio representation is employed, yielding better audio quality. Comprehensive evaluations demonstrate that AUV exhibits comparable audio reconstruction ability to state-of-the-art domain-specific single-layer quantizer codecs, showcasing the potential of audio universal vector quantization with a single codebook. The pre-trained model and demo samples are available at https://swivid.github.io/AUV/.",0,arxiv,MÃ¼zik,CC-BY/arXiv,AUV: Teaching Audio Universal Vector Quantization with Single Nested Codebook
"Automatic drum transcription (ADT) is traditionally formulated as a discriminative task to predict drum events from audio spectrograms. In this work, we redefine ADT as a conditional generative task and introduce Noise-to-Notes (N2N), a framework leveraging diffusion modeling to transform audio-conditioned Gaussian noise into drum events with associated velocities. This generative diffusion approach offers distinct advantages, including a flexible speed-accuracy trade-off and strong inpainting capabilities. However, the generation of binary onset and continuous velocity values presents a challenge for diffusion models, and to overcome this, we introduce an Annealed Pseudo-Huber loss to facilitate effective joint optimization. Finally, to augment low-level spectrogram features, we propose incorporating features extracted from music foundation models (MFMs), which capture high-level semantic information and enhance robustness to out-of-domain drum audio. Experimental results demonstrate that including MFM features significantly improves robustness and N2N establishes a new state-of-the-art performance across multiple ADT benchmarks.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Noise-to-Notes: Diffusion-based Generation and Refinement for Automatic Drum Transcription
"Current music generators capture local textures but often fail to model long-range structure, leading to off-beat outputs, weak section transitions, and limited editing capability. We present MusicWeaver, a music generation model conditioned on a beat-aligned structural plan. This plan serves as an editable intermediate between the input prompt and the generated music, preserving global form and enabling professional, localized edits. MusicWeaver consists of a planner, which translates prompts into a structural plan encoding musical form and compositional cues, and a diffusion-based generator, which synthesizes music under the plan's guidance. To assess generation and editing quality, we introduce two metrics: the Structure Coherence Score (SCS) for evaluating long-range form and timing, and the Edit Fidelity Score (EFS) for measuring the accuracy of realizing plan edits. Experiments demonstrate that MusicWeaver achieves state-of-the-art fidelity and controllability, producing music closer to human-composed works. Music results can be found on our project page: https://musicweaver.github.io/.",0,arxiv,MÃ¼zik,CC-BY/arXiv,MusicWeaver: Coherent Long-Range and Editable Music Generation from a Beat-Aligned Structural Plan
"An algorithm for deriving delay functions based on real examples of vibrato was recently introduced and can be used to perform a vibrato transfer, in which the vibrato pattern of a target signal is imparted onto an incoming sound using a delay line. The algorithm contains methods that computationally restrict a real-time implementation. Here, a real-time approximation is presented that incorporates an efficient fundamental frequency estimation algorithm and time-domain polyphase IIR filters that approximate an analytic signal. The vibrato transfer algorithm is further supplemented with a proposed method to transfer the amplitude modulation of the target sound, moving this method beyond the capabilities of typical delay-based vibrato effects. Modifications to the original algorithm for real-time use are detailed here and available as source code for an implementation as a VST plugin. This algorithm has applications as an audio effect in sound design, sound morphing, and real-time vibrato control of synthesized sounds.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Real-time implementation of vibrato transfer as an audio effect
"For example, in the chromatic circle, the twelve tones are represented by twelve points on a circle, and in Tonnetz, the relationships among harmonies are represented by a triangular lattice. Recently, we have shown that several arrangements of tones on the regular icosahedron can be associated with chromatic scales, whole-tone scales, major tones, and minor tones through the golden ratio. Here, we investigate another type of connection between music and the golden ratio. We show that there exists an arrangement of 7 tones on a golden triangle that can represent a given major/minor scale and its tonic, dominant, and subdominant chords by golden triangles. By applying this finding, we propose ``golden Tonnetz"" which represents all the major/minor scales and triads by the golden triangles or gnomons and also represents relative, parallel, and leading-tone exchange transformations in Neo-Riemannian theory by transformations among the golden triangles and gnomons",0,arxiv,MÃ¼zik,CC-BY/arXiv,Golden Tonnetz
"The presence of $Î±$ clustered structures in light nuclei can enhance the initial spatial anisotropies in relativistic nuclear collisions relative to those arising from nuclei with uniform density distributions. Thus, observables that are strongly sensitive to the initial geometry can be a more efficient probe of the clustered structures than observables dominated by final state dynamics. We investigate the collisions of $Î±$ clustered oxygen nuclei at $\sqrt{s_{NN}}=7$A TeV at the LHC using the GLISSANDO initial state model along with the MUSIC event-by-event hydrodynamical framework. The tetrahedral $Î±$ clustered structure of $^{16}$O leads to significantly larger initial triangular eccentricity $Îµ_3$ than collisions with uniform density distributions especially in the most central events. The spatial eccentricity $Îµ_2$ is found to be relatively less sensitive to the initial state clustered structure. The production of thermal photons is estimated to be only marginally influenced by clustering for both central as well as peripheral collisions. In contrast, the photon triangular flow coefficient $v_3(p_T)$ is strongly affected by initial state clustering resulting in substantially larger values in both central and peripheral collisions. An experimental determination of photon anisotropic flow together with the ratios of flow coefficients in $^{16}$O+$^{16}$O collisions therefore expected to provide valuable insight into the possible clustered structure in light nuclei and also to constrain parameters in theoretical modeling.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Probing Initial State Clustering through Photon Anisotropic Flow in 7A TeV $^{16}$O+$^{16}$O Collisions at the LHC
"We present AIBA (Attention-In-Band Alignment), a lightweight, training-free pipeline to quantify where text-to-audio diffusion models attend on the time-frequency (T-F) plane. AIBA (i) hooks cross-attention at inference to record attention probabilities without modifying weights; (ii) projects them to fixed-size mel grids that are directly comparable to audio energy; and (iii) scores agreement with instrument-band ground truth via interpretable metrics (T-F IoU/AP, frequency-profile correlation, and a pointing game). On Slakh2100 with an AudioLDM2 backbone, AIBA reveals consistent instrument-dependent trends (e.g., bass favoring low bands) and achieves high precision with moderate recall.",0,arxiv,MÃ¼zik,CC-BY/arXiv,AIBA: Attention-based Instrument Band Alignment for Text-to-Audio Diffusion
"Audio Large Language Models (Audio LLMs) enable human-like conversation about music, yet it is unclear if they are truly listening to the audio or just using textual reasoning, as recent benchmarks suggest. This paper investigates this issue by quantifying the contribution of each modality to a model's output. We adapt the MM-SHAP framework, a performance-agnostic score based on Shapley values that quantifies the relative contribution of each modality to a model's prediction. We evaluate two models on the MuChoMusic benchmark and find that the model with higher accuracy relies more on text to answer questions, but further inspection shows that even if the overall audio contribution is low, models can successfully localize key sound events, suggesting that audio is not entirely ignored. Our study is the first application of MM-SHAP to Audio LLMs and we hope it will serve as a foundational step for future research in explainable AI and audio.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Investigating Modality Contribution in Audio LLMs for Music
"Self-supervised audio-visual source separation leverages natural correlations between audio and vision modalities to separate mixed audio signals. In this work, we first systematically analyse the performance of existing multimodal fusion methods for audio-visual separation task, demonstrating that the performance of different fusion strategies is closely linked to the characteristics of the sound: middle fusion is better suited for handling short, transient sounds, while late fusion is more effective for capturing sustained and harmonically rich sounds. We thus propose a hierarchical fusion strategy that effectively integrates both fusion stages. In addition, training can be made easier by incorporating high-quality external audio representations, rather than relying solely on the audio branch to learn them independently. To explore this, we propose a representation alignment approach that aligns the latent features of the audio encoder with embeddings extracted from pre-trained audio models. Extensive experiments on MUSIC, MUSIC-21 and VGGSound datasets demonstrate that our approach achieves state-of-the-art results, surpassing existing methods under the self-supervised setting. We further analyse the impact of representation alignment on audio features, showing that it reduces modality gap between the audio and visual modalities.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Audio-Visual Separation with Hierarchical Fusion and Representation Alignment
"Music engagement spans diverse interactions with music, from selection and emotional response to its impact on behavior, identity, and social connections. Social media platforms provide spaces where such engagement can be observed in natural, unprompted conversations. Advances in natural language processing (NLP) and big data analytics make it possible to analyze these discussions at scale, extending music research to broader contexts. Reddit, in particular, offers anonymity that encourages diverse participation and yields rich discourse on music in ecological settings. Yet the scale of this data requires tools to extract, process, and analyze it effectively. We present Muse-it, a platform that retrieves comprehensive Reddit data centered on user-defined queries. It aggregates posts from across subreddits, supports topic modeling, temporal trend analysis, and clustering, and enables efficient study of large-scale discourse. Muse-it also identifies music-related hyperlinks (e.g., Spotify), retrieves track-level metadata such as artist, album, release date, genre, popularity, and lyrics, and links these to the discussions. An interactive interface provides dynamic visualizations of the collected data. Muse-it thus offers an accessible way for music researchers to gather and analyze big data, opening new avenues for understanding music engagement as it naturally unfolds online.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Muse-it: A Tool for Analyzing Music Discourse on Reddit
"Singing Voice Synthesis (SVS) aims to generate expressive vocal performances from structured musical inputs such as lyrics and pitch sequences. While recent progress in discrete codec-based speech synthesis has enabled zero-shot generation via in-context learning, directly extending these techniques to SVS remains non-trivial due to the requirement for precise melody control. In particular, prompt-based generation often introduces prosody leakage, where pitch information is inadvertently entangled within the timbre prompt, compromising controllability. We present CoMelSinger, a zero-shot SVS framework that enables structured and disentangled melody control within a discrete codec modeling paradigm. Built on the non-autoregressive MaskGCT architecture, CoMelSinger replaces conventional text inputs with lyric and pitch tokens, preserving in-context generalization while enhancing melody conditioning. To suppress prosody leakage, we propose a coarse-to-fine contrastive learning strategy that explicitly regularizes pitch redundancy between the acoustic prompt and melody input. Furthermore, we incorporate a lightweight encoder-only Singing Voice Transcription (SVT) module to align acoustic tokens with pitch and duration, offering fine-grained frame-level supervision. Experimental results demonstrate that CoMelSinger achieves notable improvements in pitch accuracy, timbre consistency, and zero-shot transferability over competitive baselines.",0,arxiv,MÃ¼zik,CC-BY/arXiv,CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance
"Conversational recommendation has advanced rapidly with large language models (LLMs), yet music remains a uniquely challenging domain where effective recommendations require reasoning over audio content beyond what text or metadata can capture. We present MusiCRS, the first benchmark for audio-centric conversational recommendation that links authentic user conversations from Reddit with corresponding audio tracks. MusiCRS contains 477 high-quality conversations spanning diverse genres (classical, hip-hop, electronic, metal, pop, indie, jazz) with 3,589 unique musical entities and audio grounding via YouTube links. MusiCRS enables evaluation across three input modality configurations: audio-only, query-only, and audio+query (multimodal), allowing systematic comparison of audio-LLMs, retrieval models, and traditional approaches. Our experiments reveal that current systems rely heavily on textual signals and struggle with nuanced audio reasoning. This exposes fundamental limitations in cross-modal knowledge integration where models excel at dialogue semantics but cannot effectively ground abstract musical concepts in actual audio content. To facilitate progress, we release the MusiCRS dataset (https://huggingface.co/datasets/rohan2810/MusiCRS), evaluation code (https://github.com/rohan2810/musiCRS), and comprehensive baselines.",0,arxiv,MÃ¼zik,CC-BY/arXiv,MusiCRS: Benchmarking Audio-Centric Conversational Recommendation
"This paper focuses on automatic music engraving, i.e., the creation of a humanly-readable musical score from musical content. This step is fundamental for all applications that include a human player, but it remains a mostly unexplored topic in symbolic music processing. In this work, we formalize the problem as a collection of interdependent subtasks, and propose a unified graph neural network (GNN) framework that targets the case of piano music and quantized symbolic input. Our method employs a multi-task GNN to jointly predict voice connections, staff assignments, pitch spelling, key signature, stem direction, octave shifts, and clef signs. A dedicated postprocessing pipeline generates print-ready MusicXML/MEI outputs. Comprehensive evaluation on two diverse piano corpora (J-Pop and DCML Romantic) demonstrates that our unified model achieves good accuracy across all subtasks, compared to existing systems that only specialize in specific subtasks. These results indicate that a shared GNN encoder with lightweight task-specific decoders in a multi-task setting offers a scalable and effective solution for automatic music engraving.",0,arxiv,MÃ¼zik,CC-BY/arXiv,EngravingGNN: A Hybrid Graph Neural Network for End-to-End Piano Score Engraving
"Neural audio codecs (NACs) achieve low-bitrate compression by learning compact audio representations, which can also serve as features for perceptual quality evaluation. We introduce DACe, an enhanced, higher-fidelity version of the Descript Audio Codec (DAC), trained on diverse real and synthetic tonal data with balanced sampling. We systematically compare FrÃ©chet Audio Distance (FAD) and Maximum Mean Discrepancy (MMD) on MUSHRA tests across speech, music, and mixed content. FAD consistently outperforms MMD, and embeddings from higher-fidelity NACs (such as DACe) show stronger correlations with human judgments. While CLAP LAION Music (CLAP-M) and OpenL3 Mel128 (OpenL3-128M) embeddings achieve higher correlations, NAC embeddings provide a practical zero-shot approach to audio quality assessment, requiring only unencoded audio for training. These results demonstrate the dual utility of NACs for compression and perceptually informed audio evaluation.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Towards Evaluating Generative Audio: Insights from Neural Audio Codec Embedding Distances
"Music Information Retrieval (MIR) encompasses a broad range of computational techniques for analyzing and understanding musical content, with recent deep learning advances driving substantial improvements. Building upon these advances, this paper explores how large language models (LLMs) can serve as an integrative bridge to connect and integrate information from multiple MIR tools, with a focus on enhancing automatic chord recognition performance. We present a novel approach that positions text-based LLMs as intelligent coordinators that process and integrate outputs from diverse state-of-the-art MIR tools-including music source separation, key detection, chord recognition, and beat tracking. Our method converts audio-derived musical information into textual representations, enabling LLMs to perform reasoning and correction specifically for chord recognition tasks. We design a 5-stage chain-of-thought framework that allows GPT-4o to systematically analyze, compare, and refine chord recognition results by leveraging music-theoretical knowledge to integrate information across different MIR components. Experimental evaluation on three datasets demonstrates consistent improvements across multiple evaluation metrics, with overall accuracy gains of 1-2.77% on the MIREX metric. Our findings demonstrate that LLMs can effectively function as integrative bridges in MIR pipelines, opening new directions for multi-tool coordination in music information retrieval tasks.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Enhancing Automatic Chord Recognition through LLM Chain-of-Thought Reasoning
"Heavy quarks are predominantly generated at the initial stage of relativistic heavy-ion collisions such that heavy flavor observables have the potential to provide information on the pre-equilibrium medium dynamics. In this study, we investigate the sensitivity of D-meson $R_{AA}$ and $v_2$ to early-time charm quark dynamics in Pb+Pb collisions at $\sqrt{s_{NN}}=5.02$ TeV. We employ the IP-Glasma+MUSIC+UrQMD framework to model the evolution of the bulk medium. Charm quarks are generated using PYTHIA with nuclear parton distribution functions and evolved using Langevin dynamics within MARTINI. We observe that even though there is significant momentum broadening in the earliest stage, D-meson $R_{AA}$ and $v_2$ are only weakly sensitive to pre-equilibrium interactions.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Charm quark evolution in the early stages of heavy-ion collisions
"The evaluation of audio fingerprinting at a realistic scale is limited by the scarcity of large public music databases. We present an audio-free approach that synthesises latent fingerprints which approximate the distribution of real fingerprints. Our method trains a Rectified Flow model on embeddings extracted by pre-trained neural audio fingerprinting systems. The synthetic fingerprints generated using our system act as realistic distractors and enable the simulation of retrieval performance at a large scale without requiring additional audio. We assess the fidelity of synthetic fingerprints by comparing the distributions to real data. We further benchmark the retrieval performances across multiple state-of-the-art audio fingerprinting frameworks by augmenting real reference databases with synthetic distractors, and show that the scaling trends obtained with synthetic distractors closely track those obtained with real distractors. Finally, we scale the synthetic distractor database to model retrieval performance for very large databases, providing a practical metric of system scalability that does not depend on access to audio corpora.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Scalable Evaluation for Audio Identification via Synthetic Latent Fingerprint Generation
"Although the dependence of convective core overshooting on mass has attracted much attention, no corresponding work exists for overshooting below a convective envelope. We aim to quantify this relationship for pre-main sequence stars of intermediate mass ranging from $1.2 M_{\mathsf{sun}}$ to $6 M_{\mathsf{sun}}$. These stars have a similar thermal and density structure, making this a suitable choice to isolate the effect of changing mass. We produce a series of two-dimensional global simulations of stars using MUSIC, a fully compressible, time-implicit hydrodynamics code. The stars that we select for this study are near the end of the pre-main sequence and are convectively unstable above 80% of their stellar radius; they thus have a convective envelope that is shallower than the current sun. For this series of stellar models, a simple scaling with luminosity, with a scaling exponent of 1/4, accounts for the increasing overshooting with stellar mass. This result has interesting similarities with the scaling found by Baraffe et al. [2023] for a range of intermediate mass and massive stars at the zero-age main sequence (ZAMS) that have convective cores.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Mass dependence of overshooting beneath convective envelopes
"This chapter and the experiments described within explore how `human entanglement' might be represented and even emulated by physical entanglement. To achieve this, a notion of `tonal centrality' between two musicians is captured via MIDI and passed as a parameter into a quantum simulation taking place on an embedded device (a Raspberry Pi Pico). The results of these simulations are then coded back into MIDI and sent to the players' instruments. The closer the musicians' tonality is, the more their instruments will be entangled in a $|Î¦^+ \rangle$ state, and the further away they are the more their instruments will be entangled in a $|Î¨^+ \rangle$ state. The intention is to create random parameters that are correlative - \emph{i.e.} the same on both instruments - or anti-correlative - \emph{i.e.} the bit-wise opposite of each other, influenced by the tonal relationship from the players. These random parameters sharing these particular properties add a new dimension for quantum-musical expression. This concept was realised experimentally, and the full code and sample outputs are provided. This work aims to pave the way for musicians to explore and experience quantum emulations of their own musical experiences, adding a new nuance and possibilities for the future of \emph{entangled ensembles.}",0,arxiv,MÃ¼zik,CC-BY/arXiv,Qubit Instrumentation of Entanglement
"The Dorabella cipher is an encrypted note written by English composer Edward Elgar, which has defied decipherment attempts for more than a century. While most proposed solutions are English texts, we investigate the hypothesis that Dorabella represents enciphered music. We weigh the evidence for and against the hypothesis, devise a simplified music notation, and attempt to reconstruct a melody from the cipher. Our tools are n-gram models of music which we validate on existing music corpora enciphered using monoalphabetic substitution. By applying our methods to Dorabella, we produce a decipherment with musical qualities, which is then transformed via artful composition into a listenable melody. Far from arguing that the end result represents the only true solution, we instead frame the process of decipherment as part of the composition process.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Dorabella Cipher as Musical Inspiration
"Modern multimodal large language models often claim ""video understanding,"" yet most evaluations use muted videos or simply discard audio. We ask a direct question: how much does audio actually matter for contemporary Video-LLMs and the benchmarks that certify them? We audit widely used suites and observe that many items are even solvable from a single frame, rendering audio largely redundant. Building on LLaVA-OneVision architecture, we attach a speech/audio encoder (e.g., Whisper) and analyze when audio helps, while addressing audio token explosion with a lightweight Mamba-based state-space token compressor. We find that audio yields minimal gains on recent video benchmarks but is decisive on curated, audio-sensitive subsets. To enable faithful evaluation, we release AVQA-Hard and Music-AVQA-Hard, our model, and code. Our findings surface a growing gap between current academic practice and real-world expectations, and provide practical tools for scalable audio-visual Video-LLMs. We will fully open-source our work at https://github.com/naver-ai/LLaVA-AV-SSM.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Does Audio Matter for Modern Video-LLMs and Their Benchmarks?
"Audio super-resolution (SR), i.e., upsampling the low-resolution (LR) waveform to the high-resolution (HR) version, has recently been explored with diffusion and bridge models, while previous methods often suffer from sub-optimal upsampling quality due to their uninformative generation prior. Towards high-quality audio super-resolution, we present a new system with latent bridge models (LBMs), where we compress the audio waveform into a continuous latent space and design an LBM to enable a latent-to-latent generation process that naturally matches the LR-toHR upsampling process, thereby fully exploiting the instructive prior information contained in the LR waveform. To further enhance the training results despite the limited availability of HR samples, we introduce frequency-aware LBMs, where the prior and target frequency are taken as model input, enabling LBMs to explicitly learn an any-to-any upsampling process at the training stage. Furthermore, we design cascaded LBMs and present two prior augmentation strategies, where we make the first attempt to unlock the audio upsampling beyond 48 kHz and empower a seamless cascaded SR process, providing higher flexibility for audio post-production. Comprehensive experimental results evaluated on the VCTK, ESC-50, Song-Describer benchmark datasets and two internal testsets demonstrate that we achieve state-of-the-art objective and perceptual quality for any-to-48kHz SR across speech, audio, and music signals, as well as setting the first record for any-to-192kHz audio SR. Demo at https://AudioLBM.github.io/.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Audio Super-Resolution with Latent Bridge Models
"This paper investigates single-snapshot direction-of-arrival (DOA) estimation and target localization with coherent sparse extremely large aperture arrays (ELAAs) in automotive radar applications. Far-field and near-field signal models are formulated for distributed bistatic configurations. To enable noncoherent processing, a single-snapshot MUSIC (SS-MUSIC) algorithm is proposed to fuse local spectra from individual subarrays and extended to near-field localization via geometric intersection. For coherent processing, a single-snapshot ESPRIT (SS-ESPRIT) method with ambiguity dealiasing is developed to fully exploit the aperture of sparse ELAAs for high-resolution angle estimation. Simulation results demonstrate that SS-ESPRIT provides superior angular resolution for closely spaced far-field targets, while SS-MUSIC offers robustness in near-field localization and flexibility in hybrid scenarios.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Single-Snapshot Localization Using Sparse Extremely Large Aperture Arrays
"Estimating the fundamental frequency, or melody, is a core task in Music Information Retrieval (MIR). Various studies have explored signal processing, machine learning, and deep-learning-based approaches, with a very recent focus on utilizing uncertainty in active learning settings for melody estimation. However, these approaches do not investigate the relative effectiveness of different uncertainties. In this work, we follow a framework that disentangles aleatoric and epistemic uncertainties to guide active learning for melody estimation. Trained on a source dataset, our model adapts to new domains using only a small number of labeled samples. Experimental results demonstrate that epistemic uncertainty is more reliable for domain adaptation with reduced labeling effort as compared to aleatoric uncertainty.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Improving Active Learning for Melody Estimation by Disentangling Uncertainties
"We propose a system to adapt a user's music to their exercise by aligning high-energy music segments with intense intervals of the workout. Listening to music during exercise can boost motivation and performance. However, the structure of the music may be different from the user's natural phases of rest and work, causing users to rest longer than needed while waiting for a motivational section, or lose motivation mid-work if the section ends too soon. To address this, our system, called RISE, automatically estimates the intense segments in music and uses component-based music rearrangement techniques to dynamically extend and shorten different segments of the user's song to fit the ongoing exercise routine. Our system takes as input the rest and work durations to guide adaptation. Currently, this is determined either via a pre-defined plan or manual input during the workout. We evaluated RISE with 12 participants and compared our system to a non-adaptive music baseline while exercising in our lab. Participants found our rearrangements keeps intensity estimation accurate, and many recalled moments when intensity alignment helped them push through their workout.",0,arxiv,MÃ¼zik,CC-BY/arXiv,RISE: Adaptive music playback for Realtime Intensity Synchronization with Exercise
"Video and audio inpainting for mixed audio-visual content has become a crucial task in multimedia editing recently. However, precisely removing an object and its corresponding audio from a video without affecting the rest of the scene remains a significant challenge. To address this, we propose VAInpaint, a novel pipeline that first utilizes a segmentation model to generate masks and guide a video inpainting model in removing objects. At the same time, an LLM then analyzes the scene globally, while a region-specific model provides localized descriptions. Both the overall and regional descriptions will be inputted into an LLM, which will refine the content and turn it into text queries for our text-driven audio separation model. Our audio separation model is fine-tuned on a customized dataset comprising segmented MUSIC instrument images and VGGSound backgrounds to enhance its generalization performance. Experiments show that our method achieves performance comparable to current benchmarks in both audio and video inpainting.",0,arxiv,MÃ¼zik,CC-BY/arXiv,VAInpaint: Zero-Shot Video-Audio inpainting framework with LLMs-driven Module
"Adapting learning materials to the level of skill of a student is important in education. In the context of music training, one essential ability is sight-reading -- playing unfamiliar scores at first sight -- which benefits from progressive and level-appropriate practice. However, creating exercises at the appropriate level of difficulty demands significant time and effort. We address this challenge as a controlled symbolic music generation task that aims to produce piano scores with a desired difficulty level. Controlling symbolic generation through conditioning is commonly done using control tokens, but these do not always have a clear impact on global properties, such as difficulty. To improve conditioning, we introduce an auxiliary optimization target for difficulty prediction that helps prevent conditioning collapse -- a common issue in which models ignore control signals in the absence of explicit supervision. This auxiliary objective helps the model to learn internal representations aligned with the target difficulty, enabling more precise and adaptive score generation. Evaluation with automatic metrics and expert judgments shows better control of difficulty and potential educational value. Our approach represents a step toward personalized music education through the generation of difficulty-aware practice material.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Difficulty-Aware Score Generation for Piano Sight-Reading
"This paper defines the novel task of drum-to-vocal percussion (VP) sound conversion. VP imitates percussion instruments through human vocalization and is frequently employed in contemporary a cappella music. It exhibits acoustic properties distinct from speech and singing (e.g., aperiodicity, noisy transients, and the absence of linguistic structure), making conventional speech or singing synthesis methods unsuitable. We thus formulate VP synthesis as a timbre transfer problem from drum sounds, leveraging their rhythmic and timbral correspondence. To support this formulation, we define three requirements for successful conversion: rhythmic fidelity, timbral consistency, and naturalness as VP. We also propose corresponding subjective evaluation criteria. We implement two baseline conversion methods using a neural audio synthesizer, the real-time audio variational autoencoder (RAVE), with and without vector quantization (VQ). Subjective experiments show that both methods produce plausible VP outputs, with the VQ-based RAVE model yielding more consistent conversion.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Drum-to-Vocal Percussion Sound Conversion and Its Evaluation Methodology
"In the development of spatial audio technologies, reliable and shared methods for evaluating audio quality are essential. Listening tests are currently the standard but remain costly in terms of time and resources. Several models predicting subjective scores have been proposed, but they do not generalize well to real-world signals. In this paper, we propose QASTAnet (Quality Assessment for SpaTial Audio network), a new metric based on a deep neural network, specialized on spatial audio (ambisonics and binaural). As training data is scarce, we aim for the model to be trainable with a small amount of data. To do so, we propose to rely on expert modeling of the low-level auditory system and use a neurnal network to model the high-level cognitive function of the quality judgement. We compare its performance to two reference metrics on a wide range of content types (speech, music, ambiance, anechoic, reverberated) and focusing on codec artifacts. Results demonstrate that QASTAnet overcomes the aforementioned limitations of the existing methods. The strong correlation between the proposed metric prediction and subjective scores makes it a good candidate for comparing codecs in their development.",0,arxiv,MÃ¼zik,CC-BY/arXiv,QASTAnet: A DNN-based Quality Metric for Spatial Audio
"A large-scale dataset is essential for training a well-generalized deep-learning model. Most such datasets are collected via scraping from various internet sources, inevitably introducing duplicated data. In the symbolic music domain, these duplicates often come from multiple user arrangements and metadata changes after simple editing. However, despite critical issues such as unreliable training evaluation from data leakage during random splitting, dataset duplication has not been extensively addressed in the MIR community. This study investigates the dataset duplication issues regarding Lakh MIDI Dataset (LMD), one of the largest publicly available sources in the symbolic music domain. To find and evaluate the best retrieval method for duplicated data, we employed the Clean MIDI subset of the LMD as a benchmark test set, in which different versions of the same songs are grouped together. We first evaluated rule-based approaches and previous symbolic music retrieval models for de-duplication and also investigated with a contrastive learning-based BERT model with various augmentations to find duplicate files. As a result, we propose three different versions of the filtered list of LMD, which filters out at least 38,134 samples in the most conservative settings among 178,561 files.",0,arxiv,MÃ¼zik,CC-BY/arXiv,On the de-duplication of the Lakh MIDI dataset
"Current methods for Music Structure Analysis (MSA) focus primarily on audio data. While symbolic music can be synthesized into audio and analyzed using existing MSA techniques, such an approach does not exploit symbolic music's rich explicit representation of pitch, timing, and instrumentation. A key subproblem of MSA is section boundary detection-determining whether a given point in time marks the transition between musical sections. In this paper, we study automatic section boundary detection for symbolic music. First, we introduce a human-annotated MIDI dataset for section boundary detection, consisting of metadata from 6134 MIDI files that we manually curated from the Lakh MIDI dataset. Second, we train a deep learning model to classify the presence of section boundaries within a fixed-length musical window. Our data representation involves a novel encoding scheme based on synthesized overtones to encode arbitrary MIDI instrumentations into 3-channel piano rolls. Our model achieves an F1 score of 0.77, improving over the analogous audio-based supervised learning approach and the unsupervised block-matching segmentation (CBM) audio approach by 0.22 and 0.31, respectively. We release our dataset, code, and models.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Barwise Section Boundary Detection in Symbolic Music Using Convolutional Neural Networks
"Piano cover generation aims to automatically transform a pop song into a piano arrangement. While numerous deep learning approaches have been proposed, existing models often fail to maintain structural consistency with the original song, likely due to the absence of beat-aware mechanisms or the difficulty of modeling complex rhythmic patterns. Rhythmic information is crucial, as it defines structural similarity (e.g., tempo, BPM) and directly impacts the overall quality of the generated music.   In this paper, we introduce Etude, a three-stage architecture consisting of Extract, strucTUralize, and DEcode stages. By pre-extracting rhythmic information and applying a novel, simplified REMI-based tokenization, our model produces covers that preserve proper song structure, enhance fluency and musical dynamics, and support highly controllable generation through style injection. Subjective evaluations with human listeners show that Etude substantially outperforms prior models, achieving a quality level comparable to that of human composers.",0,arxiv,MÃ¼zik,CC-BY/arXiv,"Etude: Piano Cover Generation with a Three-Stage Approach - Extract, strucTUralize, and DEcode"
"Music inpainting aims to reconstruct missing segments of a corrupted recording. While diffusion-based generative models improve reconstruction for medium-length gaps, they often struggle to preserve musical plausibility over multi-second gaps. We introduce Similarity-Guided Diffusion Posterior Sampling (SimDPS), a hybrid method that combines diffusion-based inference with similarity search. Candidate segments are first retrieved from a corpus based on contextual similarity, then incorporated into a modified likelihood that guides the diffusion process toward contextually consistent reconstructions. Subjective evaluation on piano music inpainting with 2-s gaps shows that the proposed SimDPS method enhances perceptual plausibility compared to unguided diffusion and frequently outperforms similarity search alone when moderately similar candidates are available. These results demonstrate the potential of a hybrid similarity approach for diffusion-based audio enhancement with long gaps.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Similarity-Guided Diffusion for Long-Gap Music Inpainting
"Existing digital mental wellness tools often overlook the nuanced emotional states underlying everyday challenges. For example, pre-sleep anxiety affects more than 1.5 billion people worldwide, yet current approaches remain largely static and ""one-size-fits-all"", failing to adapt to individual needs. In this work, we present EmoHeal, an end-to-end system that delivers personalized, three-stage supportive narratives. EmoHeal detects 27 fine-grained emotions from user text with a fine-tuned XLM-RoBERTa model, mapping them to musical parameters via a knowledge graph grounded in music therapy principles (GEMS, iso-principle). EmoHeal retrieves audiovisual content using the CLAMP3 model to guide users from their current state toward a calmer one (""match-guide-target""). A within-subjects study (N=40) demonstrated significant supportive effects, with participants reporting substantial mood improvement (M=4.12, p<0.001) and high perceived emotion recognition accuracy (M=4.05, p<0.001). A strong correlation between perceived accuracy and therapeutic outcome (r=0.72, p<0.001) validates our fine-grained approach. These findings establish the viability of theory-driven, emotion-aware digital wellness tools and provides a scalable AI blueprint for operationalizing music therapy principles.",0,arxiv,MÃ¼zik,CC-BY/arXiv,EmoHeal: An End-to-End System for Personalized Therapeutic Music Retrieval from Fine-grained Emotions
"Reverse engineering of music mixes aims to uncover how dry source signals are processed and combined to produce a final mix. We extend the prior works to reflect the compositional nature of mixing and search for a graph of audio processors. First, we construct a mixing console, applying all available processors to every track and subgroup. With differentiable processor implementations, we optimize their parameters with gradient descent. Then, we repeat the process of removing negligible processors and fine-tuning the remaining ones. This way, the quality of the full mixing console can be preserved while removing approximately two-thirds of the processors. The proposed method can be used not only to analyze individual music mixes but also to collect large-scale graph data that can be used for downstream tasks, e.g., automatic mixing. Especially for the latter purpose, efficient implementation of the search is crucial. To this end, we present an efficient batch-processing method that computes multiple processors in parallel. We also exploit the ""dry/wet"" parameter of the processors to accelerate the search. Extensive quantitative and qualitative analyses are conducted to evaluate the proposed method's performance, behavior, and computational cost.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Reverse Engineering of Music Mixing Graphs with Differentiable Processors and Iterative Pruning
"Source separation is a fundamental task in speech, music, and audio processing, and it also provides cleaner and larger data for training generative models. However, improving separation performance in practice often depends on increasingly large networks, inflating training and deployment costs. Motivated by recent advances in inference-time scaling for generative modeling, we propose Training-Time and Inference-Time Scalable Discriminative Source Separation (TISDiSS), a unified framework that integrates early-split multi-loss supervision, shared-parameter design, and dynamic inference repetitions. TISDiSS enables flexible speed-performance trade-offs by adjusting inference depth without retraining additional models. We further provide systematic analyses of architectural and training choices and show that training with more inference repetitions improves shallow-inference performance, benefiting low-latency applications. Experiments on standard speech separation benchmarks demonstrate state-of-the-art performance with a reduced parameter count, establishing TISDiSS as a scalable and practical framework for adaptive source separation. Code is available at https://github.com/WingSingFung/TISDiSS.",0,arxiv,MÃ¼zik,CC-BY/arXiv,TISDiSS: A Training-Time and Inference-Time Scalable Framework for Discriminative Source Separation
"We introduce Jamendo-QA, a large-scale dataset for Music Question Answering (Music-QA). The dataset is built on freely licensed tracks from the Jamendo platform and is automatically annotated using the Qwen-Omni model. Jamendo-QA provides question-answer pairs and captions aligned with music audio, enabling both supervised training and zero-shot evaluation. Our resource aims to fill the gap of music-specific QA datasets and foster further research in music understanding, retrieval, and generative applications. In addition to its scale, Jamendo-QA covers a diverse range of genres, instruments, and metadata attributes, allowing robust model benchmarking across varied musical contexts. We also provide detailed dataset statistics and highlight potential biases such as genre and gender imbalance to guide fair evaluation. We position Jamendo-QA as a scalable and publicly available benchmark that can facilitate future research in music understanding, multimodal modeling, and fair evaluation of music-oriented QA systems.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Jamendo-QA: A Large-Scale Music Question Answering Dataset
"Musicians and nonmusicians alike use rhythmic sound gestures, such as tapping and beatboxing, to express drum patterns. While these gestures effectively communicate musical ideas, realizing these ideas as fully-produced drum recordings can be time-consuming, potentially disrupting many creative workflows. To bridge this gap, we present TRIA (The Rhythm In Anything), a masked transformer model for mapping rhythmic sound gestures to high-fidelity drum recordings. Given an audio prompt of the desired rhythmic pattern and a second prompt to represent drumkit timbre, TRIA produces audio of a drumkit playing the desired rhythm (with appropriate elaborations) in the desired timbre. Subjective and objective evaluations show that a TRIA model trained on less than 10 hours of publicly-available drum data can generate high-quality, faithful realizations of sound gestures across a wide range of timbres in a zero-shot manner.",0,arxiv,MÃ¼zik,CC-BY/arXiv,The Rhythm In Anything: Audio-Prompted Drums Generation with Masked Language Modeling
"Piano performance is a multimodal activity that intrinsically combines physical actions with the acoustic rendition. Despite growing research interest in analyzing the multimodal nature of piano performance, the laborious process of acquiring large-scale multimodal data remains a significant bottleneck, hindering further progress in this field. To overcome this barrier, we present an integrated web toolkit comprising two graphical user interfaces (GUIs): (i) PiaRec, which supports the synchronized acquisition of audio, video, MIDI, and performance metadata. (ii) ASDF, which enables the efficient annotation of performer fingering from the visual data. Collectively, this system can streamline the acquisition of multimodal piano performance datasets.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Two Web Toolkits for Multimodal Piano Performance Dataset Acquisition and Fingering Annotation
"Audio effects (FX) such as reverberation, distortion, modulation, and dynamic range processing play a pivotal role in shaping emotional responses during music listening. While prior studies have examined links between low-level audio features and affective perception, the systematic impact of audio FX on emotion remains underexplored. This work investigates how foundation models - large-scale neural architectures pretrained on multimodal data - can be leveraged to analyze these effects. Such models encode rich associations between musical structure, timbre, and affective meaning, offering a powerful framework for probing the emotional consequences of sound design techniques. By applying various probing methods to embeddings from deep learning models, we examine the complex, nonlinear relationships between audio FX and estimated emotion, uncovering patterns tied to specific effects and evaluating the robustness of foundation audio models. Our findings aim to advance understanding of the perceptual impact of audio production practices, with implications for music cognition, performance, and affective computing.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Exploring How Audio Effects Alter Emotion with Foundation Models
"In pragmatic cluster randomized controlled trials (PCRCTs), the unit of randomization may be the healthcare provider. In these studies, noncompliance can occur at both the patient and cluster levels. Some studies measure cluster-level implementation using multiple continuous metrics while documenting individual binary compliance. The complier average causal effect estimates the intervention effects among individuals that comply with the assigned intervention. However, it does not account for compliance metrics at the cluster level. When compliance with the intervention is influenced by both providers and individuals, it can be scientifically beneficial to describe the effects of the intervention between all levels of compliance. We propose a Bayesian method for PCRCTs with one-sided binary noncompliance at the individual level and one-sided partial compliance at the cluster level. Our Bayesian model classifies clusters into latent compliance strata based on pretreatment characteristics, partial compliance status, and individual outcomes. Because compliance is only observed in the treatment arm, the method imputes unobserved compliance for control clusters and the individuals within them. This approach estimates finite and super-population estimands within strata defined by both cluster- and individual-level compliance. We apply this method to the METRIcAL trial, a multi-part, pragmatic cluster randomized trial evaluating the effects of a personalized music intervention on agitation in nursing home residents with dementia.",0,arxiv,MÃ¼zik,CC-BY/arXiv,A Latent Principal Stratification Method to Address One-Sided Cluster and Individual Noncompliance in Cluster RCTs
"Variational Autoencoders (VAEs) are essential for large-scale audio tasks like diffusion-based generation. However, existing open-source models often neglect auditory perceptual aspects during training, leading to weaknesses in phase accuracy and stereophonic spatial representation. To address these challenges, we propose Îµar-VAE, an open-source music signal reconstruction model that rethinks and optimizes the VAE training paradigm. Our contributions are threefold: (i) A K-weighting perceptual filter applied prior to loss calculation to align the objective with auditory perception. (ii) Two novel phase losses: a Correlation Loss for stereo coherence, and a Phase Loss using its derivatives--Instantaneous Frequency and Group Delay--for precision. (iii) A new spectral supervision paradigm where magnitude is supervised by all four Mid/Side/Left/Right components, while phase is supervised only by the LR components. Experiments show Îµar-VAE at 44.1kHz substantially outperforms leading open-source models across diverse metrics, showing particular strength in reconstructing high-frequency harmonics and the spatial characteristics.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Back to Ear: Perceptually Driven High Fidelity Music Reconstruction
"Music is characterized by aspects related to different modalities, such as the audio signal, the lyrics, or the music video clips. This has motivated the development of multimodal datasets and methods for Music Information Retrieval (MIR) tasks such as genre classification or autotagging. Music can be described at different levels of granularity, for instance defining genres at the level of artists or music albums. However, most datasets for multimodal MIR neglect this aspect and provide data at the level of individual music tracks. We aim to fill this gap by providing Music4All Artist and Album (Music4All A+A), a dataset for multimodal MIR tasks based on music artists and albums. Music4All A+A is built on top of the Music4All-Onion dataset, an existing track-level dataset for MIR tasks. Music4All A+A provides metadata, genre labels, image representations, and textual descriptors for 6,741 artists and 19,511 albums. Furthermore, since Music4All A+A is built on top of Music4All-Onion, it allows access to other multimodal data at the track level, including user--item interaction data. This renders Music4All A+A suitable for a broad range of MIR tasks, including multimodal music recommendation, at several levels of granularity. To showcase the use of Music4All A+A, we carry out experiments on multimodal genre classification of artists and albums, including an analysis in missing-modality scenarios, and a quantitative comparison with genre classification in the movie domain. Our experiments show that images are more informative for classifying the genres of artists and albums, and that several multimodal models for genre classification struggle in generalizing across domains. We provide the code to reproduce our experiments at https://github.com/hcai-mms/Music4All-A-A, the dataset is linked in the repository and provided open-source under a CC BY-NC-SA 4.0 license.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Music4All A+A: A Multimodal Dataset for Music Information Retrieval Tasks
"With the advent of extremely large-scale MIMO (XL-MIMO), mmWave/THz bands and ultra-wideband transmission, future 6G systems demand real-time positioning with centimeter or even millimeter level accuracy. This paper addresses the pronounced near-field beam squint problem caused by phase shifter based beamforming in wideband near-field scenarios and proposes a beam squint assisted joint angle-distance localization scheme. The key idea is to employ true-time-delay (TTD) units together with phase shifters (PS) to synthesize a controllable joint angle-distance (JAD) trajectory that establishes a unique mapping between subcarriers and spatial locations, enabling single scan acquisition of target angle and range. To implement this paradigm efficiently, we design a coarse to fine two stage estimator: a low complexity coarse stage based on subcarrier power peaks for user separation and candidate region selection, followed by a local high resolution refinement stage that applies spatial smoothing and near-field multiple signal classification (MUSIC) over multiple subcarriers and fuses the resulting spectra by geometric averaging to suppress spurious peaks. We theoretically prove the correctness and uniqueness of the MUSIC spatial spectrum peak under the proposed near-field steering model, and derive the CramÃ©r-Rao lower bound (CRLB) for joint angle-distance estimation. Simulation results in single and multi-user scenarios validate that the proposed method achieves very high accuracy and robustness, significantly outperforming conventional two-step approaches, and is promising for practical 6G sensing and localization deployments.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Beam Squint Assisted Joint Angle-Distance Localization for Near-Field Communications
"Although many models exist to detect singing voice deepfakes (SingFake), how these models operate, particularly with instrumental accompaniment, is unclear. We investigate how instrumental music affects SingFake detection from two perspectives. To investigate the behavioral effect, we test different backbones, unpaired instrumental tracks, and frequency subbands. To analyze the representational effect, we probe how fine-tuning alters encoders' speech and music capabilities. Our results show that instrumental accompaniment acts mainly as data augmentation rather than providing intrinsic cues (e.g., rhythm or harmony). Furthermore, fine-tuning increases reliance on shallow speaker features while reducing sensitivity to content, paralinguistic, and semantic information. These insights clarify how models exploit vocal versus instrumental cues and can inform the design of more interpretable and robust SingFake detection systems.",0,arxiv,MÃ¼zik,CC-BY/arXiv,How Does Instrumental Music Help SingFake Detection?
"Inverse medium scattering problems arise in many applications, but in practice, the measurement data are often restricted to a limited aperture by physical or experimental constraints. Classical sampling methods, such as MUSIC and the linear sampling method, are well understood for full-aperture data, yet their performance deteriorates severely under limited-aperture conditions, especially in the presence of noise. We propose a new sampling method tailored to the inverse medium problem with limited-aperture data. The method is motivated by the linear sampling framework and incorporates a weight function into the index function. The weight is designed so that the modified kernel reproduces the full-aperture behavior using only limited data, which both localizes oscillations and improves the conditioning of the far-field system, thereby yielding more accurate and stable reconstructions. We provide a theoretical justification of the method under the Born approximation and an efficient algorithm for computing the weight. Numerical experiments in two and three dimensions demonstrate that the proposed method achieves greater accuracy and robustness than existing sampling-type methods, particularly for noisy, limited-aperture data.",0,arxiv,MÃ¼zik,CC-BY/arXiv,A Weighted Sampling Method for Inverse Medium Problem with Limited Aperture
"Current recommendation systems often tend to overlook emotional context and rely on historical listening patterns or static mood tags. This paper introduces a novel music recommendation framework employing a variant of Wide and Deep Learning architecture that takes in real-time emotional states inferred directly from natural language as inputs and recommends songs that closely portray the mood. The system captures emotional contexts from user-provided textual descriptions by using transformer-based embeddings, which were finetuned to predict the emotional dimensions of valence-arousal. The deep component of the architecture utilizes these embeddings to generalize unseen emotional patterns, while the wide component effectively memorizes user-emotion and emotion-genre associations through cross-product features. Experimental results show that personalized music selections positively influence the user's emotions and lead to a significant improvement in emotional relevance.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Words to Waves: Emotion-Adaptive Music Recommendation System
"Music, as a structured yet perceptually rich experience, can be modeled as a network to uncover how humans encode and process auditory information. While network-based representations of music are increasingly common, the impact of feature selection on structural properties and cognitive alignment remains underexplored. In this study, we evaluated eight network models, each constructed from symbolic representations of piano compositions using distinct combinations of pitch, octave, duration, and interval, designed to be representative of existing approaches in the literature. By comparing these models through topological metrics, entropy analysis, and divergence with respect to inferred cognitive representations, we assessed both their structural and perceptual efficiency. Our findings reveal that simpler, feature-specific models better match human perception, whereas complex, multidimensional representations introduce cognitive inefficiencies. These results support the view that humans rely on modular, parallel cognitive networks--an architecture consistent with theories of predictive processing and free energy minimization. Moreover, we find that musical networks are structurally organized to guide attention toward transitions that are both uncertain and inferable. The resulting structure concentrates uncertainty in a few frequently visited nodes, creating local entropy gradients that alternate between stable and unpredictable regions, thereby enabling the expressive dynamics of tension and release that define the musical experience. These findings show that network structures make the organization of uncertainty in music observable, offering new insight into how patterned flows of expectation shape perception, and open new directions for studying how musical structures evolve across genres, cultures, and historical periods through the lens of network science.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Network representations reveal structured uncertainty in music
"Singing Accompaniment Generation (SAG) is the process of generating instrumental music for a given clean vocal input. However, existing SAG techniques use source-separated vocals as input and overfit to separation artifacts. This creates a critical train-test mismatch, leading to failure on clean, real-world vocal inputs. We introduce AnyAccomp, a framework that resolves this by decoupling accompaniment generation from source-dependent artifacts. AnyAccomp first employs a quantized melodic bottleneck, using a chromagram and a VQ-VAE to extract a discrete and timbre-invariant representation of the core melody. A subsequent flow-matching model then generates the accompaniment conditioned on these robust codes. Experiments show AnyAccomp achieves competitive performance on separated-vocal benchmarks while significantly outperforming baselines on generalization test sets of clean studio vocals and, notably, solo instrumental tracks. This demonstrates a qualitative leap in generalization, enabling robust accompaniment for instruments - a task where existing models completely fail - and paving the way for more versatile music co-creation tools. Demo audio and code: https://anyaccomp.github.io",0,arxiv,MÃ¼zik,CC-BY/arXiv,AnyAccomp: Generalizable Accompaniment Generation via Quantized Melodic Bottleneck
"AI-generated music may inadvertently replicate samples from the training data, raising concerns of plagiarism. Similarity measures can quantify such replication, thereby offering supervision and guidance for music generation models. Existing similarity measure methods for symbolic music mainly target melody repetition, leaving a gap in assessing complex music with rich textures and expressive performance characteristics. To address this gap, we introduce SSIMuse, the first adaptation of the Structural Similarity Index Measure (SSIM) from images to symbolic music. Specifically, we represent symbolic music as image-like piano rolls in binary and velocity-based forms. Build upon these representations, we reinterprete and suitably modify the SSIM components in the musical context to develop two variants, i.e., SSIMuse-B and SSIMuse-V, for evaluating data replication in composition and dynamic performance, respectively. Controlled experiments on synthetic samples from multiple datasets show that SSIMuse can reliably detect exact replication at a granularity of at least one bar. SSIMuse enables open evaluation of replication in music generation and draws attention to its broader ethical, social, legal, and economic implications. The code is available at https://github.com/Tayjsl97/SSIMuse.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Assessing Data Replication in Symbolic Music via Adapted Structural Similarity Index Measure
"Efficiently retrieving specific instrument timbres from audio mixtures remains a challenge in digital music production. This paper introduces a contrastive learning framework for musical instrument retrieval, enabling direct querying of instrument databases using a single model for both single- and multi-instrument sounds. We propose techniques to generate realistic positive/negative pairs of sounds for virtual musical instruments, such as samplers and synthesizers, addressing limitations in common audio data augmentation methods.   The first experiment focuses on instrument retrieval from a dataset of 3,884 instruments, using single-instrument audio as input. Contrastive approaches are competitive with previous works based on classification pre-training. The second experiment considers multi-instrument retrieval with a mixture of instruments as audio input. In this case, the proposed contrastive framework outperforms related works, achieving 81.7\% top-1 and 95.7\% top-5 accuracies for three-instrument mixtures.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Contrastive timbre representations for musical instrument and synthesizer retrieval
"In this paper, we present a method for conducting comparative corpus studies in musicology that reduces the time-consuming digitization process. Instead of encoding whole corpora of musical sources, we suggest sampling bars from these sources. We address the challenge of selecting representative samples and evaluate three different sampling methods. We used Beethoven's Bagatelles Op. 33 as a case study to find the method that works best in finding samples representative with respect to differences. We believe that this approach offers significant value to musicological research by enabling large-scale analyses and thereby statistically sound results. Moreover, we believe our work to be a valuable step toward understanding nineteenth-century editorial practices and enriching the field of scholarly editing of historical musical works.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Beyond Bars: Distribution of Edit Operations in Historical Prints
"In this work, we explore the use of Osu!, a community-based rhythm game, as an alternative source of beat and downbeat annotations. Osu! beatmaps are created and refined by a large, diverse community and span underrepresented genres such as anime, Vocaloid, and video game music. We introduce a pipeline for extracting annotations from Osu! beatmaps and partition them into meaningful subsets. Through manual analysis, we find that beatmaps with a single timing point or widely spaced multiple timing points (>=5 seconds apart) provide reliable annotations, while closely spaced timing points (<5 seconds apart) often require additional curation. We also observe high consistency across multiple annotations of the same song. This study demonstrates the potential of Osu! data as a scalable, diverse, and community-driven resource for MIR research. We release our pipeline and a high-quality subset osu2beat2025 to support further exploration: https://github.com/ziyunliu4444/osu2mir.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Osu2MIR: Beat Tracking Dataset Derived From Osu! Data
"Music learners can greatly benefit from tools that accurately detect errors in their practice. Existing approaches typically compare audio recordings to music scores using heuristics or learnable models. This paper introduces \textit{LadderSym}, a novel Transformer-based method for music error detection. \textit{LadderSym} is guided by two key observations about the state-of-the-art approaches: (1) late fusion limits inter-stream alignment and cross-modality comparison capability; and (2) reliance on score audio introduces ambiguity in the frequency spectrum, degrading performance in music with concurrent notes. To address these limitations, \textit{LadderSym} introduces (1) a two-stream encoder with inter-stream alignment modules to improve audio comparison capabilities and error detection F1 scores, and (2) a multimodal strategy that leverages both audio and symbolic scores by incorporating symbolic representations as decoder prompts, reducing ambiguity and improving F1 scores. We evaluate our method on the \textit{MAESTRO-E} and \textit{CocoChorales-E} datasets by measuring the F1 score for each note category. Compared to the previous state of the art, \textit{LadderSym} more than doubles F1 for missed notes on \textit{MAESTRO-E} (26.8\% $\rightarrow$ 56.3\%) and improves extra note detection by 14.4 points (72.0\% $\rightarrow$ 86.4\%). Similar gains are observed on \textit{CocoChorales-E}. This work introduces general insights about comparison models that could inform sequence evaluation tasks for reinforcement Learning, human skill assessment, and model evaluation.",0,arxiv,MÃ¼zik,CC-BY/arXiv,LadderSym: A Multimodal Interleaved Transformer for Music Practice Error Detection
"Traditional single-input single-output (SISO) systems face fundamental limitations in achieving accurate three-dimensional (3D) localization due to limited spatial degrees of freedom (DoF) and the adverse impact of multipath propagation. This paper proposes a novel fluid antenna system (FAS)-active reconfigurable intelligent surface (ARIS) framework that transforms multipath effects from a hindrance into a resource for enhanced localization. By synergistically combining the signal amplification capabilities of ARIS with the spatial diversity enabled by FAS, the proposed system achieves robust 3D user equipment (UE) positioning -- without relying on auxiliary information such as time-of-arrival (ToA) or frequency diversity. The system exploits both line-of-sight (LoS) and non-line-of-sight (NLoS) components through a tailored signal decoupling strategy. We design novel UE pilot sequences and ARIS phase configurations to effectively separate LoS and NLoS channels, enabling independent parameter estimation. A multi-stage estimation algorithm is then applied: the multiple signal classification (MUSIC) algorithm estimates angle-of-arrival (AoA) from the direct path, while maximum likelihood estimation with interior-point refinement recovers cascaded channel parameters from the reflected path. Finally, geometric triangulation using least-squares estimation determines the UE's 3D position based on the extracted AoA information. Comprehensive performance analysis, including the derivation of CramÃ©r-Rao bounds for both channel and position estimation, establishes theoretical benchmarks. Simulation results confirm that the proposed FAS-ARIS framework achieves near-optimal localization accuracy while maintaining robustness in rich multipath environments -- effectively turning conventional localization challenges into advantages.",0,arxiv,MÃ¼zik,CC-BY/arXiv,FAS-ARIS: Turning Multipath Challenges Into Localization Opportunities
"Multimodal music emotion analysis leverages both audio and MIDI modalities to enhance performance. While mainstream approaches focus on complex feature extraction networks, we propose that shortening the length of audio sequence features to mitigate redundancy, especially in contrast to MIDI's compact representation, may effectively boost task performance. To achieve this, we developed PoolingVQ by combining Vector Quantized Variational Autoencoder (VQVAE) with spatial pooling, which directly compresses audio feature sequences through codebook-guided local aggregation to reduce redundancy, then devised a two-stage co-attention approach to fuse audio and MIDI information. Experimental results on the public datasets EMOPIA and VGMIDI demonstrate that our multimodal framework achieves state-of-the-art performance, with PoolingVQ yielding effective improvement. Our proposed metho's code is available at Anonymous GitHub",0,arxiv,MÃ¼zik,CC-BY/arXiv,PoolingVQ: A VQVAE Variant for Reducing Audio Redundancy and Boosting Multi-Modal Fusion in Music Emotion Analysis
"We show that coherent, long-form musical composition can emerge from a decentralized swarm of identical, frozen foundation models that coordinate via stigmergic, peer-to-peer signals, without any weight updates. We compare a centralized multi-agent system with a global critic to a fully decentralized swarm in which bar-wise agents sense and deposit harmonic, rhythmic, and structural cues, adapt short-term memory, and reach consensus. Across symbolic, audio, and graph-theoretic analyses, the swarm yields superior quality while delivering greater diversity and structural variety and leads across creativity metrics. The dynamics contract toward a stable configuration of complementary roles, and self-similarity networks reveal a small-world architecture with efficient long-range connectivity and specialized bridging motifs, clarifying how local novelties consolidate into global musical form. By shifting specialization from parameter updates to interaction rules, shared memory, and dynamic consensus, MusicSwarm provides a compute- and data-efficient route to long-horizon creative structure that is immediately transferable beyond music to collaborative writing, design, and scientific discovery.",0,arxiv,MÃ¼zik,CC-BY/arXiv,MusicSwarm: Biologically Inspired Intelligence for Music Composition
"Online AI platforms for creating music from text prompts (AI music), such as Suno and Udio, are now being used by hundreds of thousands of users. Some AI music is appearing in advertising, and even charting, in multiple countries. How are these platforms being used? What subjects are inspiring their users? This article answers these questions for Suno and Udio using a large collection of songs generated by users of these platforms from May to October 2024. Using a combination of state-of-the-art text embedding models, dimensionality reduction and clustering methods, we analyze the prompts, tags and lyrics, and automatically annotate and display the processed data in interactive plots. Our results reveal prominent themes in lyrics, language preference, prompting strategies, as well as peculiar attempts at steering models through the use of metatags. To promote the musicological study of the developing cultural practice of AI-generated music we share our code and resources.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Data-Driven Analysis of Text-Conditioned AI-Generated Music: A Case Study with Suno and Udio
"Electronic Dance Music (EDM) classification typically relies on industry-defined taxonomies with numerous subgenres, yet the acoustic basis for these distinctions remains unclear. Current approaches use supervised learning with prescribed genre labels, assuming their validity without systematic evaluation. In this paper, we propose an unsupervised approach to discover the natural acoustic structure of EDM independent of commercial labels. Our method combines novel tempogram-based features capturing EDM's layered rhythmic patterns with multi-criteria feature selection. To validate that our findings reflect genuine acoustic structure rather than methodological artifacts, we compare our results against state-of-the-art pre-trained audio embeddings (MERT and CLAP). Both our feature space and embedding representations converge to 19-23 natural acoustic families compared to the prescribed 35, providing consistent evidence of significant overspecification in current EDM taxonomy by approximately one-third.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Acoustic Overspecification in Electronic Dance Music Taxonomy
"With the rapid advancement of Large Language Models (LLMs), AI-driven music generation has become a vibrant and fruitful area of research. However, the representation of musical data remains a significant challenge. To address this, a novel, machine-learning-friendly music notation system, YNote, was developed. This study leverages YNote to train an effective classification model capable of distinguishing whether a piece of music was composed by a human (Native), a rule-based algorithm (Algorithm Generated), or an LLM (LLM Generated). We frame this as a text classification problem, applying the Term Frequency-Inverse Document Frequency (TF-IDF) algorithm to extract structural features from YNote sequences and using the Synthetic Minority Over-sampling Technique (SMOTE) to address data imbalance. The resulting model achieves an accuracy of 98.25%, successfully demonstrating that YNote retains sufficient stylistic information for analysis. More importantly, the model can identify the unique "" technological fingerprints "" left by different AI generation techniques, providing a powerful tool for tracing the origins of AI-generated content.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Decoding Musical Origins: Distinguishing Human and AI Composers
"Beat and downbeat tracking, jointly referred to as Meter Tracking, is a fundamental task in Music Information Retrieval (MIR). Deep learning models have far surpassed traditional signal processing and classical machine learning approaches in this domain, particularly for Western (Eurogenetic) genres, where large annotated datasets are widely available. These systems, however, perform less reliably on underrepresented musical traditions. Carnatic music, a rich tradition from the Indian subcontinent, is renowned for its rhythmic intricacy and unique metrical structures (tÄlas). The most notable prior work on meter tracking in this context employed probabilistic Dynamic Bayesian Networks (DBNs). The performance of state-of-the-art (SOTA) deep learning models on Carnatic music, however, remains largely unexplored.   In this study, we evaluate two models for meter tracking in Carnatic music: the Temporal Convolutional Network (TCN), a lightweight architecture that has been successfully adapted for Latin rhythms, and Beat This!, a transformer-based model designed for broad stylistic coverage without the need for post-processing. Replicating the experimental setup of the DBN baseline on the Carnatic Music Rhythm (CMR$_f$) dataset, we systematically assess the performance of these models in a directly comparable setting. We further investigate adaptation strategies, including fine-tuning the models on Carnatic data and the use of musically informed parameters. Results show that while off-the-shelf models do not always outperform the DBN, their performance improves substantially with transfer learning, matching or surpassing the baseline. These findings indicate that SOTA deep learning models can be effectively adapted to underrepresented traditions, paving the way for more inclusive and broadly applicable meter tracking systems.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Revisiting Meter Tracking in Carnatic Music using Deep Learning Approaches
"Agentic AI has been standardized in industry as a practical paradigm for coordinating specialized models and tools to solve complex multimodal tasks. In this work, we present WeaveMuse, a multi-agent system for music understanding, symbolic composition, and audio synthesis. Each specialist agent interprets user requests, derives machine-actionable requirements (modalities, formats, constraints), and validates its own outputs, while a manager agent selects and sequences tools, mediates user interaction, and maintains state across turns. The system is extendable and deployable either locally, using quantization and inference strategies to fit diverse hardware budgets, or via the HFApi to preserve free community access to open models. Beyond out-of-the-box use, the system emphasizes controllability and adaptation through constraint schemas, structured decoding, policy-based inference, and parameter-efficient adapters or distilled variants that tailor models to MIR tasks. A central design goal is to facilitate intermodal interaction across text, symbolic notation and visualization, and audio, enabling analysis-synthesis-render loops and addressing cross-format constraints. The framework aims to democratize, implement, and make accessible MIR tools by supporting interchangeable open-source models of various sizes, flexible memory management, and reproducible deployment paths.",0,arxiv,MÃ¼zik,CC-BY/arXiv,WeaveMuse: An Open Agentic System for Multimodal Music Understanding and Generation
"While many text-to-audio systems produce monophonic or fixed-stereo outputs, generating audio with user-defined spatial properties remains a challenge. Existing deep learning-based spatialization methods often rely on latent-space manipulations, which can limit direct control over psychoacoustic parameters critical to spatial perception. To address this, we introduce STASE, a system that leverages a Large Language Model (LLM) as an agent to interpret spatial cues from text. A key feature of STASE is the decoupling of semantic interpretation from a separate, physics-based spatial rendering engine, which facilitates interpretable and user-controllable spatial reasoning. The LLM processes prompts through two main pathways: (i) Description Prompts, for direct mapping of explicit spatial information (e.g., ""place the lead guitar at 45Â° azimuth, 10 m distance""), and (ii) Abstract Prompts, where a Retrieval-Augmented Generation (RAG) module retrieves relevant spatial templates to inform the rendering. This paper details the STASE workflow, discusses implementation considerations, and highlights current challenges in evaluating generative spatial audio.",0,arxiv,MÃ¼zik,CC-BY/arXiv,STASE: A spatialized text-to-audio synthesis engine for music generation
"In this paper, we introduce YTCommentVerse, a large-scale multilingual and multi-category dataset of YouTube comments. It contains over 32 million comments from 178,000 videos contributed by more than 20 million unique users spanning 15 distinct YouTube content categories such as Music, News, Education and Entertainment. Each comment in the dataset includes video and comment IDs, user channel details, upvotes and category labels. With comments in over 50 languages, YTCommentVerse provides a rich resource for exploring sentiment, toxicity and engagement patterns across diverse cultural and topical contexts. This dataset helps fill a major gap in publicly available social media datasets particularly for analyzing video sharing platforms by combining multiple languages, detailed categories and other metadata.",0,arxiv,MÃ¼zik,CC-BY/arXiv,YTCommentVerse: A Multi-Category Multi-Lingual YouTube Comment Corpus
"We present a traditional approach to symbolic piano music continuation for the MIREX 2025 Symbolic Music Generation challenge. While computational music generation has recently focused on developing large foundation models with sophisticated architectural modifications, we argue that simpler approaches remain more effective for constrained, single-instrument tasks. We thus return to a simple, unaugmented next-token-prediction objective on tokenized raw MIDI, aiming to outperform large foundation models by using better data and better fundamentals. We release model weights and code at https://github.com/christianazinn/mirex2025.",0,arxiv,MÃ¼zik,CC-BY/arXiv,A Traditional Approach to Symbolic Piano Continuation
"This paper examines how musical symbolism is produced and circulated in online communities by combining content-based music analysis with a lightweight network perspective on lyrics. Using a curated corpus of 275 chart-topping songs enriched with audio descriptors (energy, danceability, loudness, liveness, valence, acousticness, speechiness, popularity) and full lyric transcripts, we build a reproducible pipeline that (i) quantifies temporal trends in sonic attributes, (ii) models lexical salience and co-occurrence, and (iii) profiles mood by genre. We find a decade-long decline in energy (79 -> 58) alongside a rise in danceability (59 -> 73); valence peaks in 2013 (63) and dips in 2014-2016 (42) before partially recovering. Correlation analysis shows strong coupling of energy with loudness (r = 0.74) and negative associations for acousticness with both energy (r = -0.54) and loudness (r = -0.51); danceability is largely orthogonal to other features (|r| < 0.20). Lyric tokenization (>114k tokens) reveals a pronoun-centric lexicon ""I/you/me/my"" and a dense co-occurrence structure in which interpersonal address anchors mainstream narratives. Mood differs systematically by style: R&B exhibits the highest mean valence (96), followed by K-Pop/Pop (77) and Indie/Pop (70), whereas Latin/Reggaeton is lower (37) despite high danceability. Read through a subcultural identity lens, these patterns suggest the mainstreaming of previously peripheral codes and a commercial preference for relaxed yet rhythmically engaging productions that sustain collective participation without maximal intensity. Methodologically, we contribute an integrated MIR-plus-network workflow spanning summary statistics, correlation structure, lexical co-occurrence matrices, and genre-wise mood profiling that is robust to modality sparsity and suitable for socially aware recommendation or community-level diffusion studies.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Unpacking Musical Symbolism in Online Communities: Content-Based and Network-Centric Approaches
"This study presents a novel Multi-Modal Graph Neural Network (MM-GNN) framework for socially aware music recommendation, designed to enhance personalization and foster community-based engagement. The proposed model introduces a fusion-free deep mutual learning strategy that aligns modality-specific representations from lyrics, audio, and visual data while maintaining robustness against missing modalities. A heterogeneous graph structure is constructed to capture both user-song interactions and user-user social relationships, enabling the integration of individual preferences with social influence. Furthermore, emotion-aware embeddings derived from acoustic and textual signals contribute to emotionally aligned recommendations. Experimental evaluations on benchmark datasets demonstrate that MM-GNN significantly outperforms existing state-of-the-art methods across various performance metrics. Ablation studies further validate the critical impact of each model component, confirming the effectiveness of the framework in delivering accurate and socially contextualized music recommendations.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Socially Aware Music Recommendation: A Multi-Modal Graph Neural Networks for Collaborative Music Consumption and Community-Based Engagement
"We present LubDubDecoder, a system that enables fine-grained monitoring of micro-cardiac vibrations associated with the opening and closing of heart valves across a range of hearables. Our system transforms the built-in speaker, the only transducer common to all hearables, into an acoustic sensor that captures the coarse ""lub-dub"" heart sounds, leverages their shared temporal and spectral structure to reconstruct the subtle seismocardiography (SCG) and gyrocardiography (GCG) waveforms, and extract the timing of key micro-cardiac events. In an IRB-approved feasibility study with 18 users, our system achieves correlations of 0.88-0.95 compared to chest-mounted reference measurements in within-user and cross-user evaluations, and generalizes to unseen hearables using a zero-effort adaptation scheme with a correlation of 0.91. Our system is robust across remounting sessions and music playback.",0,arxiv,MÃ¼zik,CC-BY/arXiv,LubDubDecoder: Bringing Micro-Mechanical Cardiac Monitoring to Hearables
"This paper addresses the Longest Filled Common Subsequence (LFCS) problem, a challenging NP-hard problem with applications in bioinformatics, including gene mutation prediction and genomic data reconstruction. Existing approaches, including exact, metaheuristic, and approximation algorithms, have primarily been evaluated on small-sized instances, which offer limited insights into their scalability. In this work, we introduce a new benchmark dataset with significantly larger instances and demonstrate that existing datasets lack the discriminative power needed to meaningfully assess algorithm performance at scale. To solve large instances efficiently, we utilize an adaptive Construct, Merge, Solve, Adapt (CMSA) framework that iteratively generates promising subproblems via component-based construction and refines them using feedback from prior iterations. Subproblems are solved using an external black-box solver. Extensive experiments on both standard and newly introduced benchmarks show that the proposed adaptive CMSA achieves state-of-the-art performance, outperforming five leading methods. Notably, on 1,510 problem instances with known optimal solutions, our approach solves 1,486 of them -- achieving over 99.9% optimal solution quality and demonstrating exceptional scalability. We additionally propose a novel application of LFCS for song identification from degraded audio excerpts as an engineering contribution, using real-world energy-profile instances from popular music. Finally, we conducted an empirical explainability analysis to identify critical feature combinations influencing algorithm performance, i.e., the key problem features contributing to success or failure of the approaches across different instance types are revealed.",0,arxiv,MÃ¼zik,CC-BY/arXiv,An Adaptive CMSA for Solving the Longest Filled Common Subsequence Problem with an Application in Audio Querying
"In the era of human-AI co-creation, the maxim ""knowing is easy, doing is hard"" is redefined. AI has the potential to ease execution, yet the essence of ""hard"" lies in who governs the translation from knowing to doing. Mainstream tools often centralize interpretive authority and homogenize expression, suppressing marginal voices. To address these challenges, we introduce the first systematic framework for redistributing authority in the knowing-doing cycle, built on three principles, namely contestability, agency, and plurality. Through interactive studies with 180 music practitioners, complemented by in-depth interviews, we demonstrate that these principles reshape human-AI authority relations and reactivate human creative expression. The findings establish a new paradigm for critical computing and human-AI co-creation that advances from critique to practice.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Who Decides How Knowing Becomes Doing? Redistributing Authority in Human-AI Music Co-Creation
"Adolescence is marked by strong creative impulses but limited strategies for structured expression, often leading to frustration or disengagement. While generative AI lowers technical barriers and delivers efficient outputs, its role in fostering adolescents' expressive growth has been overlooked. We propose MusicScaffold, the first adolescent-centered framework that repositions AI as a guide, coach, and partner, making expressive strategies transparent and learnable, and supporting autonomy. In a four-week study with middle school students (ages 12--14), MusicScaffold enhanced cognitive specificity, behavioral self-regulation, and affective confidence in music creation. By reframing generative AI as a scaffold rather than a generator, this work bridges the machine efficiency of generative systems with human growth in adolescent creative education.",0,arxiv,MÃ¼zik,CC-BY/arXiv,MusicScaffold: Bridging Machine Efficiency and Human Growth in Adolescent Creative Education through Generative AI
"This study investigates how listeners perceive consonance and dissonance in dyads composed of simple (sine) tones, focusing on the effects of frequency ratio ($R$) and mean frequency ($F$). Seventy adult participants - categorized by musical training, gender, and age group - rated randomly ordered dyads using binary preference responses (``like'' or ``dislike''). Dyads represented standard Western intervals but were constructed with sine tones rather than musical notes, preserving interval ratios while varying absolute pitch. Statistical analyses reveal a consistent decrease in preference with increasing mean frequency, regardless of interval class or participant group. Octaves, fifths, fourths, and sixths showed a nearly linear decline in preference with increasing $F$. Major seconds were among the least preferred. Musicians rated octaves and certain consonant intervals more positively than non-musicians, while gender and age groups exhibited different sensitivity to high frequencies. The findings suggest that both interval structure and pitch range shape the perception of consonance in simple-tone dyads, with possible psychoacoustic explanations involving frequency sensitivity and auditory fatigue at higher frequencies.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Psychoacoustic study of simple-tone dyads: frequency ratio and pitch
"This paper investigates automatic piano transcription based on computationally-efficient yet high-performant variants of the Transformer that can capture longer-term dependency over the whole musical piece. Recently, transformer-based sequence-to-sequence models have demonstrated excellent performance in piano transcription. These models, however, fail to deal with the whole piece at once due to the quadratic complexity of the self-attention mechanism, and music signals are thus typically processed in a sliding-window manner in practice. To overcome this limitation, we propose an efficient architecture with sparse attention mechanisms. Specifically, we introduce sliding-window self-attention mechanisms for both the encoder and decoder, and a hybrid global-local cross-attention mechanism that attends to various spans according to the MIDI token types. We also use a hierarchical pooling strategy between the encoder and decoder to further reduce computational load. Our experiments on the MAESTRO dataset showed that the proposed model achieved a significant reduction in computational cost and memory usage, accelerating inference speed, while maintaining transcription performance comparable to the full-attention baseline. This allows for training with longer audio contexts on the same hardware, demonstrating the viability of sparse attention for building efficient and high-performance piano transcription systems. The code is available at https://github.com/WX-Wei/efficient-seq2seq-piano-trans.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Efficient Transformer-Based Piano Transcription With Sparse Attention Mechanisms
"Objective assessment of source-separation systems still mismatches subjective human perception, especially when leakage and self-distortion interact. We introduce the Perceptual Separation (PS) and Perceptual Match (PM), the first pair of measures that functionally isolate these two factors. Our intrusive method begins with generating a bank of fundamental distortions for each reference waveform signal in the mixture. Distortions, references, and their respective system outputs from all sources are then independently encoded by a pre-trained self-supervised learning model. These representations are aggregated and projected onto a manifold via diffusion maps, which aligns Euclidean distances on the manifold with dissimilarities of the encoded waveforms. On this manifold, the PM measures the Mahalanobis distance from each output to its attributed cluster that consists of its reference and distortions embeddings, capturing self-distortion. The PS accounts for the Mahalanobis distance of the output to the attributed and to the closest non-attributed clusters, quantifying leakage. Both measures are differentiable and granular, operating at a resolution as low as 50 frames per second. We further derive, for both measures, deterministic error radius and non-asymptotic, high-probability confidence intervals (CIs). Experiments on English, Spanish, and music mixtures show that the PS and PM nearly always achieve the highest linear correlation coefficients with human mean-opinion scores than 14 competitors, reaching as high as 86.36% for speech and 87.21% for music. We observe, at worst, an error radius of 1.39% and a probabilistic 95% CI of 12.21% for these coefficients, which improves reliable and informed evaluation. Using mutual information, the measures complement each other most as their values decrease, suggesting they are jointly more informative as system performance degrades.",0,arxiv,MÃ¼zik,CC-BY/arXiv,MAPSS: Manifold-based Assessment of Perceptual Source Separation
"Abstract instructions in piano education, such as ""raise your wrist"" and ""relax your tension,"" lead to varying interpretations among learners, preventing instructors from effectively conveying their intended pedagogical guidance. To address this problem, this study conducted systematic interviews with a piano professor with 18 years teaching experience, and two researchers derived seven core need groups through cross-validation. Based on these findings, we developed a web-based dashboard prototype integrating video, motion capture, and musical scores, enabling instructors to provide concrete, visual feedback instead of relying solely on abstract verbal instructions. Technical feasibility was validated through 109 performance datasets.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Designing a Multimodal Viewer for Piano Performance Analysis -- a Pedagogy-First Approach
"The advent of quantum physics has revolutionized our understanding of the universe, replacing the deterministic framework of classical physics with a paradigm dominated by intrinsic randomness and quantum correlations. This shift has not only enabled groundbreaking technologies, such as quantum sensors, networks and computers, but has also unlocked entirely new possibilities for artistic expressions. In this paper, we explore the intersection of quantum mechanics and art, focusing on the use of quantum entanglement and inherent randomness as creative tools. Specifically, we present The Sound of Entanglement, a live musical performance driven by real-time measurements of entangled photons in a Bell test. By integrating the measured quantum correlations as a central compositional element and synchronizing live visuals with experimental data, the performance offers a unique and unrepeatable audiovisual experience that relies on quantum correlations which cannot be produced by any classical device. Through this fusion of science and art, we aim to provide a deeper appreciation of quantum phenomena while expanding the boundaries of creative expression.",0,arxiv,MÃ¼zik,CC-BY/arXiv,The Sound of Entanglement
"The multimodal nature of music performance has driven increasing interest in data beyond the audio domain within the music information retrieval (MIR) community. This paper introduces PianoVAM, a comprehensive piano performance dataset that includes videos, audio, MIDI, hand landmarks, fingering labels, and rich metadata. The dataset was recorded using a Disklavier piano, capturing audio and MIDI from amateur pianists during their daily practice sessions, alongside synchronized top-view videos in realistic and varied performance conditions. Hand landmarks and fingering labels were extracted using a pretrained hand pose estimation model and a semi-automated fingering annotation algorithm. We discuss the challenges encountered during data collection and the alignment process across different modalities. Additionally, we describe our fingering annotation method based on hand landmarks extracted from videos. Finally, we present benchmarking results for both audio-only and audio-visual piano transcription using the PianoVAM dataset and discuss additional potential applications.",0,arxiv,MÃ¼zik,CC-BY/arXiv,PianoVAM: A Multimodal Piano Performance Dataset
"The majority of research in recommender systems, be it algorithmic improvements, context-awareness, explainability, or other areas, evaluates these systems on datasets that capture user interaction over a relatively limited time span. However, recommender systems can very well be used continuously for extended time. Similarly so, user behavior may evolve over that extended time. Although media studies and psychology offer a wealth of research on the evolution of user preferences and behavior as individuals age, there has been scant research in this regard within the realm of user modeling and recommender systems. In this study, we investigate the evolution of user preferences and behavior using the LFM-2b dataset, which, to our knowledge, is the only dataset that encompasses a sufficiently extensive time frame to permit real longitudinal studies and includes age information about its users. We identify specific usage and taste preferences directly related to the age of the user, i.e., while younger users tend to listen broadly to contemporary popular music, older users have more elaborate and personalized listening habits. The findings yield important insights that open new directions for research in recommender systems, providing guidance for future efforts.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Soundtracks of Our Lives: How Age Influences Musical Preferences
"Audio and music generation systems have been remarkably developed in the music information retrieval (MIR) research field. The advancement of these technologies raises copyright concerns, as ownership and authorship of AI-generated music (AIGM) remain unclear. Also, it can be difficult to determine whether a piece was generated by AI or composed by humans clearly. To address these challenges, we aim to improve the accuracy of AIGM detection by analyzing the structural patterns of music segments. Specifically, to extract musical features from short audio clips, we integrated various pre-trained models, including self-supervised learning (SSL) models or an audio effect encoder, each within our suggested transformer-based framework. Furthermore, for long audio, we developed a segment transformer that divides music into segments and learns inter-segment relationships. We used the FakeMusicCaps and SONICS datasets, achieving high accuracy in both the short-audio and full-audio detection experiments. These findings suggest that integrating segment-level musical features into long-range temporal analysis can effectively enhance both the performance and robustness of AIGM detection systems.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Segment Transformer: AI-Generated Music Detection via Music Structural Analysis
"As a result of continuous advances in Music Information Retrieval (MIR) technology, generating and distributing music has become more diverse and accessible. In this context, interest in music intellectual property protection is increasing to safeguard individual music copyrights. In this work, we propose a system for detecting music plagiarism by combining various MIR technologies. We developed a music segment transcription system that extracts musically meaningful segments from audio recordings to detect plagiarism across different musical formats. With this system, we compute similarity scores based on multiple musical features that can be evaluated through comprehensive musical analysis. Our approach demonstrated promising results in music plagiarism detection experiments, and the proposed method can be applied to real-world music scenarios. We also collected a Similar Music Pair (SMP) dataset for musical similarity research using real-world cases. The dataset are publicly available.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Real-world Music Plagiarism Detection With Music Segment Transcription System
"Next to decision tree and k-nearest neighbours algorithms deep convolutional neural networks (CNNs) are widely used to classify audio data in many domains like music, speech or environmental sounds. To train a specific CNN various spectral and rhythm features like mel-scaled spectrograms, mel-frequency cepstral coefficients (MFCC), cyclic tempograms, short-time Fourier transform (STFT) chromagrams, constant-Q transform (CQT) chromagrams and chroma energy normalized statistics (CENS) chromagrams can be used as digital image input data for the neural network. The performance of these spectral and rhythm features for audio category level as well as audio class level classification is investigated in detail with a deep CNN and the ESC-50 dataset with 2,000 labeled environmental audio recordings using an end-to-end deep learning pipeline. The evaluated metrics accuracy, precision, recall and F1 score for multiclass classification clearly show that the mel-scaled spectrograms and the mel-frequency cepstral coefficients (MFCC) perform significantly better then the other spectral and rhythm features investigated in this research for audio classification tasks using deep CNNs.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Spectral and Rhythm Feature Performance Evaluation for Category and Class Level Audio Classification with Deep Convolutional Neural Networks
"We present a multimodal system for personalized music generation that integrates physiological sensing, LLM-based reasoning, and controllable audio synthesis. A millimeter-wave radar sensor non-invasively captures heart rate and respiration rate. These physiological signals, combined with environmental state, are interpreted by a reasoning agent to infer symbolic musical descriptors, such as tempo, mood intensity, and traditional Chinese pentatonic modes, which are then expressed as structured prompts to guide a diffusion-based audio model in synthesizing expressive melodies. The system emphasizes cultural grounding through tonal embeddings and enables adaptive, embodied music interaction. To evaluate the system, we adopt a research-creation methodology combining case studies, expert feedback, and targeted control experiments. Results show that physiological variations can modulate musical features in meaningful ways, and tonal conditioning enhances alignment with intended modal characteristics. Expert users reported that the system affords intuitive, culturally resonant musical responses and highlighted its potential for therapeutic and interactive applications. This work demonstrates a novel bio-musical feedback loop linking radar-based sensing, prompt reasoning, and generative audio modeling.",0,arxiv,MÃ¼zik,CC-BY/arXiv,BREATH: A Bio-Radar Embodied Agent for Tonal and Human-Aware Diffusion Music Generation
"Advances in neural network design and the availability of large-scale labeled datasets have driven major improvements in piano transcription. Existing approaches target either offline applications, with no restrictions on computational demands, or online transcription, with delays of 128-320 ms. However, most real-time musical applications require latencies below 30 ms. In this work, we investigate whether and how the current state-of-the-art online transcription model can be adapted for real-time piano transcription. Specifically, we eliminate all non-causal processing, and reduce computational load through shared computations across core model components and variations in model size. Additionally, we explore different pre- and postprocessing strategies, and related label encoding schemes, and discuss their suitability for real-time transcription. Evaluating the adaptions on the MAESTRO dataset, we find a drop in transcription accuracy due to strictly causal processing as well as a tradeoff between the preprocessing latency and prediction accuracy. We release our system as a baseline to support researchers in designing models towards minimum latency real-time transcription.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Exploring System Adaptations For Minimum Latency Real-Time Piano Transcription
"Music autotagging aims to automatically assign descriptive tags, such as genre, mood, or instrumentation, to audio recordings. Due to its challenges, diversity of semantic descriptions, and practical value in various applications, it has become a common downstream task for evaluating the performance of general-purpose music representations learned from audio data. We introduce a new benchmarking dataset based on the recently published MGPHot dataset, which includes expert musicological annotations, allowing for additional insights and comparisons with results obtained on common generic tag datasets. While MGPHot annotations have been shown to be useful for computational musicology, the original dataset neither includes audio nor provides evaluation setups for its use as a standardized autotagging benchmark. To address this, we provide a curated set of YouTube URLs with retrievable audio, and propose a train/val/test split for standardized evaluation, and precomputed representations for seven state-of-the-art models. Using these resources, we evaluated these models in MGPHot and standard reference tag datasets, highlighting key differences between expert and generic tag annotations. Altogether, our contributions provide a more advanced benchmarking framework for future research in music understanding.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Benchmarking Music Autotagging with MGPHot Expert Annotations vs. Generic Tag Datasets
"Audio Language Models (ALM) have emerged as the dominant paradigm for speech and music generation by representing audio as sequences of discrete tokens. Yet, unlike text tokens, which are invertible, audio tokens are extracted from lossy codecs with a limited bitrate. As a consequence, increasing audio quality requires generating more tokens, which imposes a trade-off between fidelity and computational cost. We address this issue by studying Continuous Audio Language Models (CALM). These models instantiate a large Transformer backbone that produces a contextual embedding at every timestep. This sequential information then conditions an MLP that generates the next continuous frame of an audio VAE through consistency modeling. By avoiding lossy compression, CALM achieves higher quality at lower computational cost than their discrete counterpart. Experiments on speech and music demonstrate improved efficiency and fidelity over state-of-the-art discrete audio language models, facilitating lightweight, high-quality audio generation. Samples are available at hf.co/spaces/kyutai/calm-samples",0,arxiv,MÃ¼zik,CC-BY/arXiv,Continuous Audio Language Models
"Recent years have seen a boom in computational approaches to music analysis, yet each one is typically tailored to a specific analytical domain. In this work, we introduce AnalysisGNN, a novel graph neural network framework that leverages a data-shuffling strategy with a custom weighted multi-task loss and logit fusion between task-specific classifiers to integrate heterogeneously annotated symbolic datasets for comprehensive score analysis. We further integrate a Non-Chord-Tone prediction module, which identifies and excludes passing and non-functional notes from all tasks, thereby improving the consistency of label signals. Experimental evaluations demonstrate that AnalysisGNN achieves performance comparable to traditional static-dataset approaches, while showing increased resilience to domain shifts and annotation inconsistencies across multiple heterogeneous corpora.",0,arxiv,MÃ¼zik,CC-BY/arXiv,AnalysisGNN: Unified Music Analysis with Graph Neural Networks
"From the mid-2000s to the 2010s, K-pop moved beyond its status as a regionally popular genre in Asia and established itself as a global music genre with enthusiastic fans around the world. However, little is known about how the vast number of music listeners across the globe have listened to and perceived K-pop. This study addresses this question by analyzing a large-scale listening dataset from Last.fm. An analysis of the distribution of play counts reveals that K-pop experienced a significant increase in plays between 2005 and 2019, largely supported by a small group of heavy listeners. The Gini coefficient in play counts is notably greater than that of existing mainstream genres and other growing niche genres. Furthermore, an analysis based on user-assigned genre tags quantitatively demonstrates that between 2005 and 2010, K-pop shed its status as a local Asian genre and established itself as a distinct music genre in its own right.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Unveiling the Listener Structure Underlying K-pop's Global Success: A Large-Scale Listening Data Analysis
"Spotify has recently introduced audiobooks as part of its catalog, complementing its music and podcast offering. Search is often the first entry point for users to access new items, and an important goal for Spotify is to support users in the exploration of the audiobook catalog. More specifically, we would like to enable users without a specific item in mind to broadly search by topic, genre, story tropes, decade, and discover audiobooks, authors and publishers they may like. To do this, we need to 1) inspire users to type more exploratory queries for audiobooks and 2) augment our retrieval systems to better deal with exploratory audiobook queries. This is challenging in a cold-start scenario, where we have a retrievabiliy bias due to the little amount of user interactions with audiobooks compared to previously available items such as music and podcast content. To address this, we propose AudioBoost, a system to boost audiobook retrievability in Spotify's Search via synthetic query generation. AudioBoost leverages Large Language Models (LLMs) to generate synthetic queries conditioned on audiobook metadata. The synthetic queries are indexed both in the Query AutoComplete (QAC) and in the Search Retrieval engine to improve query formulation and retrieval at the same time. We show through offline evaluation that synthetic queries increase retrievability and are of high quality. Moreover, results from an online A/B test show that AudioBoost leads to a +0.7% in audiobook impressions, +1.22% in audiobook clicks, and +1.82% in audiobook exploratory query completions.",0,arxiv,MÃ¼zik,CC-BY/arXiv,AudioBoost: Increasing Audiobook Retrievability in Spotify Search with Synthetic Query Generation
"Controllable Singing Voice Synthesis (SVS) aims to generate expressive singing voices reflecting user intent. While recent SVS systems achieve high audio quality, most rely on probabilistic modeling, limiting precise control over attributes such as dynamics. We address this by focusing on dynamic control--temporal loudness variation essential for musical expressiveness--and explicitly condition the SVS model on energy sequences extracted from ground-truth spectrograms, reducing annotation costs and improving controllability. We also propose a phoneme-level energy sequence for user-friendly control. To the best of our knowledge, this is the first attempt enabling user-driven dynamics control in SVS. Experiments show our method achieves over 50% reduction in mean absolute error of energy sequences for phoneme-level inputs compared to baseline and energy-predictor models, without compromising synthesis quality.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Controllable Singing Voice Synthesis using Phoneme-Level Energy Sequence
"AI music generation is rapidly emerging in the creative industries, enabling intuitive music generation from textual descriptions. However, these systems pose risks in exploitation of copyrighted creations, raising ethical and legal concerns. In this paper, we present preliminary results on the first application of machine unlearning techniques from an ongoing research to prevent inadvertent usage of creative content. Particularly, we explore existing methods in machine unlearning to a pre-trained Text-to-Music (TTM) baseline and analyze their efficacy in unlearning pre-trained datasets without harming model performance. Through our experiments, we provide insights into the challenges of applying unlearning in music generation, offering a foundational analysis for future works on the application of unlearning for music generative models.",0,arxiv,MÃ¼zik,CC-BY/arXiv,No Encore: Unlearning as Opt-Out in Music Generation
"Rapid and accurate body weight estimation is critical in emergency medical care, as it directly influences treatment decisions, such as drug dosing, defibrillation energy selection, and fluid resuscitation. Traditional methods such as stand-on scales, length-based tapes, or transfer-based weighing scales are often impractical for immobilized patients, inaccurate, or labor-intensive and time-consuming. This paper introduces MelodyBedScale, a non-intrusive and rapid on-bed weight estimation system that leverages bed vibration induced by music. The core insight is that body weight affects the vibration transfer function of the bed-body system, which is captured using vibration sensors placed on opposite sides of the bed. First, we identify weight-sensitive frequency bands and compose clinically acceptable soft, natural music with high signal energy in these frequency bands. This music is then played through a speaker mounted on the bed to induce bed vibrations. Additionally, to efficiently capture the complex weight-vibration relationship with limited data and enhance generalizability to unseen individuals and weights, we theoretically analyze the weight-vibration relationship and integrate the results into the activation functions of the neural network for physics-informed weight regression. We evaluated MelodyBedScale on both wooden and steel beds across 11 participants, achieving a mean absolute error of up to 1.55 kg.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Human Body Weight Estimation Through Music-Induced Bed Vibrations
"We introduce UniVerse-1, a unified, Veo-3-like model capable of simultaneously generating coordinated audio and video. To enhance training efficiency, we bypass training from scratch and instead employ a stitching of experts (SoE) technique. This approach deeply fuses the corresponding blocks of pre-trained video and music generation experts models, thereby fully leveraging their foundational capabilities. To ensure accurate annotations and temporal alignment for both ambient sounds and speech with video content, we developed an online annotation pipeline that processes the required training data and generates labels during training process. This strategy circumvents the performance degradation often caused by misalignment text-based annotations. Through the synergy of these techniques, our model, after being finetuned on approximately 7,600 hours of audio-video data, produces results with well-coordinated audio-visuals for ambient sounds generation and strong alignment for speech generation. To systematically evaluate our proposed method, we introduce Verse-Bench, a new benchmark dataset. In an effort to advance research in audio-video generation and to close the performance gap with state-of-the-art models such as Veo3, we make our model and code publicly available. We hope this contribution will benefit the broader research community. Project page: https://dorniwang.github.io/UniVerse-1/.",0,arxiv,MÃ¼zik,CC-BY/arXiv,UniVerse-1: Unified Audio-Video Generation via Stitching of Experts
"The introduction of Artificial Intelligence (AI) into the domains of traditional art (visual arts, performing arts, and crafts) has sparked a complicated discussion about whether this might be an agent of disruption or an enhancement of our traditional art forms. This paper looks at the duality of AI, exploring the ways that recent technologies like Generative Adversarial Networks and Diffusion Models, and text-to-image generators are changing the fields of painting, sculpture, calligraphy, dance, music, and the arts of craft. Using examples and data, we illustrate the ways that AI can democratize creative expression, improve productivity, and preserve cultural heritage, while also examining the negative aspects, including: the threats to authenticity within art, ethical concerns around data, and issues including socio-economic factors such as job losses. While we argue for the context-dependence of the impact of AI (the potential for creative homogenization and the devaluation of human agency in artmaking), we also illustrate the potential for hybrid practices featuring AI in cuisine, etc. We advocate for the development of ethical guidelines, collaborative approaches, and inclusive technology development. In sum, we are articulating a vision of AI in which it amplifies our innate creativity while resisting the displacement of the cultural, nuanced, and emotional aspects of traditional art. The future will be determined by human choices about how to govern AI so that it becomes a mechanism for artistic evolution and not a substitute for the artist's soul.",0,arxiv,MÃ¼zik,CC-BY/arXiv,The Impact of Artificial Intelligence on Traditional Art Forms: A Disruption or Enhancement
"The emotional content of song lyrics plays a pivotal role in shaping listener experiences and influencing musical preferences. This paper investigates the task of multi-label emotional attribution of song lyrics by predicting six emotional intensity scores corresponding to six fundamental emotions. A manually labeled dataset is constructed using a mean opinion score (MOS) approach, which aggregates annotations from multiple human raters to ensure reliable ground-truth labels. Leveraging this dataset, we conduct a comprehensive evaluation of several publicly available large language models (LLMs) under zero-shot scenarios. Additionally, we fine-tune a BERT-based model specifically for predicting multi-label emotion scores. Experimental results reveal the relative strengths and limitations of zero-shot and fine-tuned models in capturing the nuanced emotional content of lyrics. Our findings highlight the potential of LLMs for emotion recognition in creative texts, providing insights into model selection strategies for emotion-based music information retrieval applications. The labeled dataset is available at https://github.com/LLM-HITCS25S/LyricsEmotionAttribution.",0,arxiv,MÃ¼zik,CC-BY/arXiv,From Joy to Fear: A Benchmark of Emotion Estimation in Pop Song Lyrics
"An original serious game prototype named 'Puzzlegram' is created for the elderly demographic in group settings as the target players. Puzzlegram is precisely designed to accentuate memory, auditory interaction as well as haptic response to visual signals with the use of music. Music is introduced as a key component for establishing the game design that provides a source of meaningful contextualization (familiar music from the past) for setting the game mechanics, which facilitated the construction of the serious game design process. The discussion topics raised include the need to design serious games for fostering meaningful interactions, as well as developing a thorough framework for constructing purposeful design for serious games. A potential integral of artificial intelligence to Puzzlegram may involve assigning a novel dimension to its existing problem solving task by adapting to varying states of cognitive function for monitoring purposes based on an individual's interaction with the game.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Puzzlegram: a Serious Game Designed for the Elderly in Group Settings
"Spotify's Home page features a variety of content types, including music, podcasts, and audiobooks. However, historical data is heavily skewed toward music, making it challenging to deliver a balanced and personalized content mix. Moreover, users' preference towards different content types may vary depending on the time of day, the day of week, or even the device they use. We propose a calibration method that leverages contextual bandits to dynamically learn each user's optimal content type distribution based on their context and preferences. Unlike traditional calibration methods that rely on historical averages, our approach boosts engagement by adapting to how users interests in different content types varies across contexts. Both offline and online results demonstrate improved precision and user engagement with the Spotify Home page, in particular with under-represented content types such as podcasts.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Calibrated Recommendations with Contextual Bandits
"This paper investigates GrooveTransformer, a real-time rhythm generation system, through the postphenomenological framework of Variational Cross-Examination (VCE). By reflecting on its deployment across three distinct artistic contexts, we identify three stabilities: an autonomous drum accompaniment generator, a rhythmic control voltage sequencer in Eurorack format, and a rhythm driver for a harmonic accompaniment system. The versatility of its applications was not an explicit goal from the outset of the project. Thus, we ask: how did this multistability emerge? Through VCE, we identify three key contributors to its emergence: the affordances of system invariants, the interdisciplinary collaboration, and the situated nature of its development. We conclude by reflecting on the viability of VCE as a descriptive and analytical method for Digital Musical Instrument (DMI) design, emphasizing its value in uncovering how technologies mediate, co-shape, and are co-shaped by users and contexts.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Exploring Situated Stabilities of a Rhythm Generation System through Variational Cross-Examination
"The attention mechanism in a Transformer architecture matches key to query based on both content -- the what -- and position in a sequence -- the where. We present an analysis indicating that what and where are entangled in the popular RoPE rotary position embedding. This entanglement can impair performance particularly when decisions require independent matches on these two factors. We propose an improvement to RoPE, which we call Polar Coordinate Position Embeddings or PoPE, that eliminates the what-where confound. PoPE is far superior on a diagnostic task requiring indexing solely by position or by content. On autoregressive sequence modeling in music, genomic, and natural language domains, Transformers using PoPE as the positional encoding scheme outperform baselines using RoPE with respect to evaluation loss (perplexity) and downstream task performance. On language modeling, these gains persist across model scale, from 124M to 774M parameters. Crucially, PoPE shows strong zero-shot length extrapolation capabilities, whereas RoPE's performance degrades significantly on longer sequences at test time without fine tuning or the use of position-interpolation methods.",0,arxiv,MÃ¼zik,CC-BY/arXiv,"Decoupling the ""What"" and ""Where"" With Polar Coordinate Positional Embeddings"
"Music Information Retrieval (MIR) systems are highly vulnerable to adversarial attacks that are often imperceptible to humans, primarily due to a misalignment between model feature spaces and human auditory perception. Existing defenses and perceptual metrics frequently fail to adequately capture these auditory nuances, a limitation supported by our initial listening tests showing low correlation between common metrics and human judgments. To bridge this gap, we introduce Perceptually-Aligned MERT Transformer (PAMT), a novel framework for learning robust, perceptually-aligned music representations. Our core innovation lies in the psychoacoustically-conditioned sequential contrastive transformer, a lightweight projection head built atop a frozen MERT encoder. PAMT achieves a Spearman correlation coefficient of 0.65 with subjective scores, outperforming existing perceptual metrics. Our approach also achieves an average of 9.15\% improvement in robust accuracy on challenging MIR tasks, including Cover Song Identification and Music Genre Classification, under diverse perceptual adversarial attacks. This work pioneers architecturally-integrated psychoacoustic conditioning, yielding representations significantly more aligned with human perception and robust against music adversarial attacks.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Training a Perceptual Model for Evaluating Auditory Similarity in Music Adversarial Attack
"Music adversarial attacks have garnered significant interest in the field of Music Information Retrieval (MIR). In this paper, we present Music Adversarial Inpainting Attack (MAIA), a novel adversarial attack framework that supports both white-box and black-box attack scenarios. MAIA begins with an importance analysis to identify critical audio segments, which are then targeted for modification. Utilizing generative inpainting models, these segments are reconstructed with guidance from the output of the attacked model, ensuring subtle and effective adversarial perturbations. We evaluate MAIA on multiple MIR tasks, demonstrating high attack success rates in both white-box and black-box settings while maintaining minimal perceptual distortion. Additionally, subjective listening tests confirm the high audio fidelity of the adversarial samples. Our findings highlight vulnerabilities in current MIR systems and emphasize the need for more robust and secure models.",0,arxiv,MÃ¼zik,CC-BY/arXiv,MAIA: An Inpainting-Based Approach for Music Adversarial Attacks
"We investigate how machine learning models acquire the ability to compose music and how musical information is internally represented within such models. We develop a composition algorithm based on a restricted Boltzmann machine (RBM), a simple generative model capable of producing musical pieces of arbitrary length. We convert musical scores into piano-roll image representations and train the RBM in an unsupervised manner. We confirm that the trained RBM can generate new musical pieces; however, by analyzing the model's responses and internal structure, we find that the learned information is not stored in a form directly interpretable by humans. This study contributes to a better understanding of how machine learning models capable of music composition may internally represent musical structure and highlights issues related to the interpretability of generative models in creative tasks.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Learning and composing of classical music using restricted Boltzmann machines
"With the rapid expansion of user bases on short video platforms, personalized recommendation systems are playing an increasingly critical role in enhancing user experience and optimizing content distribution. Traditional interest modeling methods often rely on unimodal data, such as click logs or text labels, which limits their ability to fully capture user preferences in a complex multimodal content environment. To address this challenge, this paper proposes a multimodal foundation model-based framework for user interest modeling and behavior analysis. By integrating video frames, textual descriptions, and background music into a unified semantic space using cross-modal alignment strategies, the framework constructs fine-grained user interest vectors. Additionally, we introduce a behavior-driven feature embedding mechanism that incorporates viewing, liking, and commenting sequences to model dynamic interest evolution, thereby improving both the timeliness and accuracy of recommendations. In the experimental phase, we conduct extensive evaluations using both public and proprietary short video datasets, comparing our approach against multiple mainstream recommendation algorithms and modeling techniques. Results demonstrate significant improvements in behavior prediction accuracy, interest modeling for cold-start users, and recommendation click-through rates. Moreover, we incorporate interpretability mechanisms using attention weights and feature visualization to reveal the model's decision basis under multimodal inputs and trace interest shifts, thereby enhancing the transparency and controllability of the recommendation system.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Multimodal Foundation Model-Driven User Interest Modeling and Behavior Analysis on Short Video Platforms
"Recent advances in Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities across various vision-language tasks. However, their reasoning abilities in the multimodal symbolic music domain remain largely unexplored. We introduce WildScore, the first in-the-wild multimodal symbolic music reasoning and analysis benchmark, designed to evaluate MLLMs' capacity to interpret real-world music scores and answer complex musicological queries. Each instance in WildScore is sourced from genuine musical compositions and accompanied by authentic user-generated questions and discussions, capturing the intricacies of practical music analysis. To facilitate systematic evaluation, we propose a systematic taxonomy, comprising both high-level and fine-grained musicological ontologies. Furthermore, we frame complex music reasoning as multiple-choice question answering, enabling controlled and scalable assessment of MLLMs' symbolic music understanding. Empirical benchmarking of state-of-the-art MLLMs on WildScore reveals intriguing patterns in their visual-symbolic reasoning, uncovering both promising directions and persistent challenges for MLLMs in symbolic music reasoning and analysis. We release the dataset and code.",0,arxiv,MÃ¼zik,CC-BY/arXiv,WildScore: Benchmarking MLLMs in-the-Wild Symbolic Music Reasoning
"Solo piano music, despite being a single-instrument medium, possesses significant expressive capabilities, conveying rich semantic information across genres, moods, and styles. However, current general-purpose music representation models, predominantly trained on large-scale datasets, often struggle to captures subtle semantic distinctions within homogeneous solo piano music. Furthermore, existing piano-specific representation models are typically unimodal, failing to capture the inherently multimodal nature of piano music, expressed through audio, symbolic, and textual modalities. To address these limitations, we propose PianoBind, a piano-specific multimodal joint embedding model. We systematically investigate strategies for multi-source training and modality utilization within a joint embedding framework optimized for capturing fine-grained semantic distinctions in (1) small-scale and (2) homogeneous piano datasets. Our experimental results demonstrate that PianoBind learns multimodal representations that effectively capture subtle nuances of piano music, achieving superior text-to-music retrieval performance on in-domain and out-of-domain piano datasets compared to general-purpose music joint embedding models. Moreover, our design choices offer reusable insights for multimodal representation learning with homogeneous datasets beyond piano music.",0,arxiv,MÃ¼zik,CC-BY/arXiv,PianoBind: A Multimodal Joint Embedding Model for Pop-piano Music
"Enhancing the ability of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) to interpret sheet music is a crucial step toward building AI musicians. However, current research lacks both evaluation benchmarks and training data for sheet music reasoning. Inspired by mathematics, where simple operations yield infinite verifiable problems, we introduce a novel approach that treats core music theory rules, such as those governing beats and intervals, as programmatic functions to systematically synthesize a vast and diverse corpus of sheet music reasoning problems. This approach allows us to introduce a data synthesis framework that generates verifiable sheet music questions in both textual and visual modalities, leading to the Synthetic Sheet Music Reasoning Benchmark (SSMR-Bench) and a complementary training set. Evaluation results on SSMR-Bench highlight the key role reasoning plays in interpreting sheet music, while also pointing out the ongoing challenges in understanding sheet music in a visual format. By leveraging synthetic data for RLVR, all models show significant improvements on the SSMR-Bench. Additionally, they also demonstrate considerable advancements on previously established human-crafted benchmarks, such as MusicTheoryBench and the music subset of MMMU. Finally, our results show that the enhanced reasoning ability can also facilitate music composition.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Towards an AI Musician: Synthesizing Sheet Music Problems for Musical Reasoning
"Human motion video generation has garnered significant research interest due to its broad applications, enabling innovations such as photorealistic singing heads or dynamic avatars that seamlessly dance to music. However, existing surveys in this field focus on individual methods, lacking a comprehensive overview of the entire generative process. This paper addresses this gap by providing an in-depth survey of human motion video generation, encompassing over ten sub-tasks, and detailing the five key phases of the generation process: input, motion planning, motion video generation, refinement, and output. Notably, this is the first survey that discusses the potential of large language models in enhancing human motion video generation. Our survey reviews the latest developments and technological trends in human motion video generation across three primary modalities: vision, text, and audio. By covering over two hundred papers, we offer a thorough overview of the field and highlight milestone works that have driven significant technological breakthroughs. Our goal for this survey is to unveil the prospects of human motion video generation and serve as a valuable resource for advancing the comprehensive applications of digital humans. A complete list of the models examined in this survey is available in Our Repository https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Human Motion Video Generation: A Survey
"A guitar string represents a wave, and by associating a qubit to each of its playable states we get a quantum wave. This is the principle behind our Quantum Guitar: `quantising all strings of a guitar'. In order to achieve this quantisation, we couple a guitar with Moth's Actias quantum synth, and for qubit manipulations including measurements the musician uses foot controllers - hence using all four limbs like a drummer. Our Quantum Guitar also allows for smooth continuous transition from `quantum' to `classical' sound, and vice versa. We have used our Quantum Guitar in several live performances in a variety of venues, playing a number different musical styles, hence demonstrating that it is a very uniquely versatile and reliable instrument. For example, in some performances Quantum Guitar was used in industrial music, by the band Black Tish, who are currently recording an album with it. In other performances Quantum Guitar represented a qubit within a sonified Bell-pair under measurement, the other qubit being mentally realised with a Grand Piano. Classical-quantum and quantum-classical transitions prove particularly useful in musical performance. A link to a demo video of Quantum Guitar is provided.",0,arxiv,MÃ¼zik,CC-BY/arXiv,A Quantum Guitar
"We present a system for automatic multi-axis perceptual quality prediction of generative audio, developed for Track 2 of the AudioMOS Challenge 2025. The task is to predict four Audio Aesthetic Scores--Production Quality, Production Complexity, Content Enjoyment, and Content Usefulness--for audio generated by text-to-speech (TTS), text-to-audio (TTA), and text-to-music (TTM) systems. A main challenge is the domain shift between natural training data and synthetic evaluation data. To address this, we combine BEATs, a pretrained transformer-based audio representation model, with a multi-branch long short-term memory (LSTM) predictor and use a triplet loss with buffer-based sampling to structure the embedding space by perceptual similarity. Our results show that this improves embedding discriminability and generalization, enabling domain-robust audio quality assessment without synthetic training data.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Improving Perceptual Audio Aesthetic Assessment via Triplet Loss and Self-Supervised Embeddings
"Using the Bayesian calibrated iEBE-MUSIC framework, we compute the production of electromagnetic radiation from hot hadronic matter at finite baryon density. Results for thermal photon and thermal dilepton yields are obtained by folding in-medium emission rates with posterior-sampled backgrounds evolved hydrodynamically. We consider different photon sources and analyze the collision-energy dependence of the thermal-to-prompt photon ratio. The sensitivity of the dilepton spectra to the pre-equilibrium stage is explored by considering different initialization procedures. Finally, we examine the impact on dilepton spectra of choosing parameter sets stemming from different Bayesian data analyses.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Electromagnetic radiation from Quark-Gluon Plasma at finite baryon density
"As augmented reality (AR) becomes increasingly prevalent in mobile and context-aware applications, the role of auditory cues in guiding users through physical environments is becoming critical. This study investigates the effectiveness and user experience of various categories of audio cues, including fully non-verbal sounds and speech-derived Spearcons, during outdoor navigation tasks using the Meta Quest 3 headset. Twenty participants navigated five outdoor routes using audio-only cue types: Artificial Sounds, Nature Sounds, Spearcons, Musical Instruments, and Auditory Icons. Subjective evaluations were collected to assess the perceived effectiveness and user experience of each sound type. Results revealed significant differences in perceived novelty and stimulation across sound types. Artificial Sounds and Musical Instruments were rated higher than Spearcons in novelty, while Artificial Sounds were also rated higher than Spearcons in stimulation. Overall preference was evenly split between Nature Sounds and Artificial Sounds. These findings suggest that incorporating aspects of novelty and user engagement in auditory feedback design may enhance the effectiveness of AR navigation systems.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Finding My Way: Influence of Different Audio Augmented Reality Navigation Cues on User Experience and Subjective Usefulness
"Multimodal Large Language Models (MLLMs) have been widely applied in speech and music. This tendency has led to a focus on audio tokenization for Large Models (LMs). Unlike semantic-only text tokens, audio tokens must both capture global semantic content and preserve fine-grained acoustic details. Moreover, they provide a discrete method for speech and music that can be effectively integrated into MLLMs. However, existing research is unsuitable in the definitions of semantic tokens and acoustic tokens. In addition, the evaluation of different codecs typically concentrates on specific domains or tasks, such as reconstruction or Automatic Speech Recognition (ASR) task, which prevents fair and comprehensive comparisons. To address these problems, this paper provides suitable definitions for semantic and acoustic tokens and introduces a systematic evaluation framework. This framework allows for a comprehensive assessment of codecs' capabilities which evaluate across four dimensions: audio reconstruction metric, codebook index (ID) stability, decoder-only transformer perplexity, and performance on downstream probe tasks. Our results show the correctness of the provided suitable definitions and the correlation among reconstruction metrics, codebook ID stability, downstream probe tasks and perplexity.",0,arxiv,MÃ¼zik,CC-BY/arXiv,AudioCodecBench: A Comprehensive Benchmark for Audio Codec Evaluation
"Multimodal fusion is a multimedia technique that has become popular in the wide range of tasks where image information is accompanied by a signal/audio. The latter may not convey highly semantic information, such as speech or music, but some measures such as audio signal recorded by mics in the goal to detect rail structure elements or defects. While classical detection approaches such as You Only Look Once (YOLO) family detectors can be efficiently deployed for defect detection on the image modality, the single modality approaches remain limited. They yield an overdetection in case of the appearance similar to normal structural elements. The paper proposes a new multimodal fusion architecture built on the basis of domain rules with YOLO and Vision transformer backbones. It integrates YOLOv8n for rapid object detection with a Vision Transformer (ViT) to combine feature maps extracted from multiple layers (7, 16, and 19) and synthesised audio representations for two defect classes: rail Rupture and Surface defect. Fusion is performed between audio and image. Experimental evaluation on a real-world railway dataset demonstrates that our multimodal fusion improves precision and overall accuracy by 0.2 points compared to the vision-only approach. Student's unpaired t-test also confirms statistical significance of differences in the mean accuracy.",0,arxiv,MÃ¼zik,CC-BY/arXiv,FusWay: Multimodal hybrid fusion approach. Application to Railway Defect Detection
"This paper presents a systematic theoretical performance analysis of the Real-Valued root-MUSIC (RV-root-MUSIC) algorithm under non-asymptotic conditions. A well-known limitation of RV-root-MUSIC is the estimation ambiguity caused by mirror roots, which are typically suppressed using conventional beamforming (CBF). By leveraging the equivalent subspace constructed through the conjugate extension method and exploiting the equivalence of perturbations for true and mirror roots, this work provides a comprehensive study of three key aspects: noise subspace perturbation, true-root perturbation, and mirror-root perturbation. A statistical model is established, and generalized perturbation expressions are derived. Monte Carlo simulations confirm the correctness and effectiveness of the theoretical results. The analysis provides a rigorous foundation for parameter optimization in Direction-of-Arrival (DOA) estimation, with applications in radar, wireless communications, and intelligent sensing.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Non-Asymptotic Performance Analysis of DOA Estimation Based on Real-Valued Root-MUSIC
"This paper presents a comparative analysis of machine learning methodologies for automatic music genre classification. We evaluate the performance of classical classifiers, including Support Vector Machines (SVM) and ensemble methods, trained on a comprehensive set of hand-crafted audio features, against a Convolutional Neural Network (CNN) operating on Mel spectrograms. The study is conducted on the widely-used GTZAN dataset. Our findings demonstrate a noteworthy result: the SVM, leveraging domain-specific feature engineering, achieves superior classification accuracy compared to the end-to-end CNN model. We attribute this outcome to the data-constrained nature of the benchmark dataset, where the strong inductive bias of engineered features provides a regularization effect that mitigates the risk of overfitting inherent in high-capacity deep learning models. This work underscores the enduring relevance of traditional feature extraction in practical audio processing tasks and provides a critical perspective on the universal applicability of deep learning, especially for moderately sized datasets.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Music Genre Classification Using Machine Learning Techniques
"Audio Chord Estimation (ACE) holds a pivotal role in music information research, having garnered attention for over two decades due to its relevance for music transcription and analysis. Despite notable advancements, challenges persist in the task, particularly concerning unique characteristics of harmonic content, which have resulted in existing systems' performances reaching a glass ceiling. These challenges include annotator subjectivity, where varying interpretations among annotators lead to inconsistencies, and class imbalance within chord datasets, where certain chord classes are over-represented compared to others, posing difficulties in model training and evaluation. As a first contribution, this paper presents an evaluation of inter-annotator agreement in chord annotations, using metrics that extend beyond traditional binary measures. In addition, we propose a consonance-informed distance metric that reflects the perceptual similarity between harmonic annotations. Our analysis suggests that consonance-based distance metrics more effectively capture musically meaningful agreement between annotations. Expanding on these findings, we introduce a novel ACE conformer-based model that integrates consonance concepts into the model through consonance-based label smoothing. The proposed model also addresses class imbalance by separately estimating root, bass, and all note activations, enabling the reconstruction of chord labels from decomposed outputs.",0,arxiv,MÃ¼zik,CC-BY/arXiv,From Discord to Harmony: Decomposed Consonance-based Training for Improved Audio Chord Estimation
"This is the summary paper for the AudioMOS Challenge 2025, the very first challenge for automatic subjective quality prediction for synthetic audio. The challenge consists of three tracks. The first track aims to assess text-to-music samples in terms of overall quality and textual alignment. The second track is based on the four evaluation dimensions of Meta Audiobox Aesthetics, and the test set consists of text-to-speech, text-to-audio, and text-to-music samples. The third track focuses on synthetic speech quality assessment in different sampling rates. The challenge attracted 24 unique teams from both academia and industry, and improvements over the baselines were confirmed. The outcome of this challenge is expected to facilitate development and progress in the field of automatic evaluation for audio generation systems.",0,arxiv,MÃ¼zik,CC-BY/arXiv,The AudioMOS Challenge 2025
"We are interested in audio systems capable of performing a differentiated processing of stationary backgrounds and isolated acoustic events within an acoustic scene, whether for applying specific processing methods to each part or for focusing solely on one while ignoring the other. Such systems have applications in real-world scenarios, including robust adaptive audio rendering systems (e.g., EQ or compression), plosive attenuation in voice mixing, noise suppression or reduction, robust acoustic event classification or even bioacoustics. To this end, we introduce IS${}^3$, a neural network designed for Impulsive--Stationary Sound Separation, that isolates impulsive acoustic events from the stationary background using a deep filtering approach, that can act as a pre-processing stage for the above-mentioned tasks. To ensure optimal training, we propose a sophisticated data generation pipeline that curates and adapts existing datasets for this task. We demonstrate that a learning-based approach, build on a relatively lightweight neural architecture and trained with well-designed and varied data, is successful in this previously unaddressed task, outperforming the Harmonic--Percussive Sound Separation masking method, adapted from music signal processing research, and wavelet filtering on objective separation metrics.",0,arxiv,MÃ¼zik,CC-BY/arXiv,IS${}^3$ : Generic Impulsive--Stationary Sound Separation in Acoustic Scenes using Deep Filtering
"The success of the generative model has gained unprecedented attention in the music generation area. Transformer-based architectures have set new benchmarks for model performance. However, their practical adoption is hindered by some critical challenges: the demand for massive computational resources and inference time, due to their large number of parameters. These obstacles make them infeasible to deploy on edge devices, such as smartphones and wearables, with limited computational resources. In this work, we present TinyMusician, a lightweight music generation model distilled from MusicGen (a State-of-the-art music generation model). TinyMusician integrates two innovations: (i) Stage-mixed Bidirectional and Skewed KL-Divergence and (ii) Adaptive Mixed-Precision Quantization. The experimental results demonstrate that TinyMusician retains 93% of the MusicGen-Small performance with 55% less model size. TinyMusician is the first mobile-deployable music generation model that eliminates cloud dependency while maintaining high audio fidelity and efficient resource usage",0,arxiv,MÃ¼zik,CC-BY/arXiv,TinyMusician: On-Device Music Generation with Knowledge Distillation and Mixed Precision Quantization
"In this paper, we address the challenge of Optical Music Recognition (OMR) for handwritten jazz lead sheets, a widely used musical score type that encodes melody and chords. The task is challenging due to the presence of chords, a score component not handled by existing OMR systems, and the high variability and quality issues associated with handwritten images. Our contribution is two-fold. We present a novel dataset consisting of 293 handwritten jazz lead sheets of 163 unique pieces, amounting to 2021 total staves aligned with Humdrum **kern and MusicXML ground truth scores. We also supply synthetic score images generated from the ground truth. The second contribution is the development of an OMR model for jazz lead sheets. We discuss specific tokenisation choices related to our kind of data, and the advantages of using synthetic scores and pretrained models. We publicly release all code, data, and models.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Optical Music Recognition of Jazz Lead Sheets
"Recent advances in text-to-music (TTM) generation have enabled controllable and expressive music creation using natural language prompts. However, the emotional fidelity of TTM systems remains largely underexplored compared to human preference or text alignment. In this study, we introduce AImoclips, a benchmark for evaluating how well TTM systems convey intended emotions to human listeners, covering both open-source and commercial models. We selected 12 emotion intents spanning four quadrants of the valence-arousal space, and used six state-of-the-art TTM systems to generate over 1,000 music clips. A total of 111 participants rated the perceived valence and arousal of each clip on a 9-point Likert scale. Our results show that commercial systems tend to produce music perceived as more pleasant than intended, while open-source systems tend to perform the opposite. Emotions are more accurately conveyed under high-arousal conditions across all models. Additionally, all systems exhibit a bias toward emotional neutrality, highlighting a key limitation in affective controllability. This benchmark offers valuable insights into model-specific emotion rendering characteristics and supports future development of emotionally aligned TTM systems.",0,arxiv,MÃ¼zik,CC-BY/arXiv,AImoclips: A Benchmark for Evaluating Emotion Conveyance in Text-to-Music Generation
"Text-to-music models capture broad attributes such as instrumentation or mood, but fine-grained stylistic control remains an open challenge. Existing stylization methods typically require retraining or specialized conditioning, which complicates reproducibility and limits policy compliance when artist names are restricted. We study whether lightweight, human-readable modifiers sampled from a large language model can provide a policy-robust alternative for stylistic control. Using MusicGen-small, we evaluate two artists: Billie Eilish (vocal pop) and Ludovico Einaudi (instrumental piano). For each artist, we use fifteen reference excerpts and evaluate matched seeds under three conditions: baseline prompts, artist-name prompts, and five descriptor sets. All prompts are generated using a large language model. Evaluation uses both VGGish and CLAP embeddings with distributional and per-clip similarity measures, including a new min-distance attribution metric. Results show that artist names are the strongest control signal across both artists, while name-free descriptors recover much of this effect. This highlights that existing safeguards such as the restriction of artist names in music generation prompts may not fully prevent style imitation. Cross-artist transfers reduce alignment, showing that descriptors encode targeted stylistic cues. We also present a descriptor table across ten contemporary artists to illustrate the breadth of the tokens. Together these findings define the name-free gap, the controllability difference between artist-name prompts and policy-compliant descriptors, shown through a reproducible evaluation protocol for prompt-level controllability.",0,arxiv,MÃ¼zik,CC-BY/arXiv,The Name-Free Gap: Policy-Aware Stylistic Control in Music Generation
"In this study, we introduce the EEG brain mapping technique based on the Duffing oscillator. For this purpose, we used EEG signals recorded from two musicians and two audience members in a music experiment. The Duffing oscillator was used to detect weak signals in the EEG signals. The frequency values of the weak signal were searched between 12 Hz and 37 Hz in all EEG channels of the participants. Then, topographical maps of the brain were generated according to the numbers of these weak signals detected. The results show that Duffing oscillator-based weak signal detection is a very effective tool for brain mapping. It shows that this method has great potential to help study various pathological conditions and understand the cognitive and behavioural functions of the human brain.",0,arxiv,MÃ¼zik,CC-BY/arXiv,EEG Brain mapping based on the Duffing oscillator
"Real-time frequency analysis of musical instruments, such as the piano, is an essential feature in areas like electronic tuners, music visualizers, and live sound monitoring. Traditional methods often rely on software-based digital signal processing (DSP), which may introduce latency and require significant computational power. In contrast, hardware platforms such as FPGAs (Field Programmable Gate Arrays) offer the ability to perform such analyses with greater speed and determinism due to their parallel processing capabilities. The primary objective of this project was to analyze analog audio signals from a digital piano using an FPGA-based real-time Fast Fourier Transform (FFT) system.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Real-Time Piano Note Frequency Detection Using FPGA and FFT Core
"Despite recent progress, Graphic User Interface (GUI) agents powered by Large Language Models (LLMs) struggle with complex mobile tasks due to limited app-specific knowledge. While UI Transition Graphs (UTGs) offer structured navigation representations, they are underutilized due to poor extraction and inefficient integration. We introduce KG-RAG, a Knowledge Graph-driven Retrieval-Augmented Generation framework that transforms fragmented UTGs into structured vector databases for efficient real-time retrieval. By leveraging an intent-guided LLM search method, KG-RAG generates actionable navigation paths, enhancing agent decision-making. Experiments across diverse mobile apps show that KG-RAG outperforms existing methods, achieving a 75.8% success rate (8.9% improvement over AutoDroid), 84.6% decision accuracy (8.1% improvement), and reducing average task steps from 4.5 to 4.1. Additionally, we present KG-Android-Bench and KG-Harmony-Bench, two benchmarks tailored to the Chinese mobile ecosystem for future research. Finally, KG-RAG transfers to web/desktop (+40% SR on Weibo-web; +20% on QQ Music-desktop), and a UTG cost ablation shows accuracy saturates at ~4h per complex app, enabling practical deployment trade-offs.",0,arxiv,MÃ¼zik,CC-BY/arXiv,KG-RAG: Enhancing GUI Agent Decision-Making via Knowledge Graph-Driven Retrieval-Augmented Generation
"Existing AI Music composition tools are limited in generation duration, musical quality, and controllability. We introduce CoComposer, a multi-agent system that consists of five collaborating agents, each with a task based on the traditional music composition workflow. Using the AudioBox-Aesthetics system, we experimentally evaluate CoComposer on four compositional criteria. We test with three LLMs (GPT-4o, DeepSeek-V3-0324, Gemini-2.5-Flash), and find (1) that CoComposer outperforms existing multi-agent LLM-based systems in music quality, and (2) compared to a single-agent system, in production complexity. Compared to non- LLM MusicLM, CoComposer has better interpretability and editability, although MusicLM still produces better music.",0,arxiv,MÃ¼zik,CC-BY/arXiv,CoComposer: LLM Multi-agent Collaborative Music Composition
"We consider a specific scenario of text aggregation, in the realm of musical harmonization. Musical harmonization shares similarities with text aggregation, however the language of harmony is more structured than general text. Concretely, given a set of harmonization suggestions for a given musical melody, our interest lies in devising aggregation algorithms that yield an harmonization sequence that satisfies the following two key criteria: (1) an effective representation of the collective suggestions; and (2) an harmonization that is musically coherent. We present different algorithms for the aggregation of harmonies given by a group of agents and analyze their complexities. The results indicate that the Kemeny and plurality-based algorithms are most effective in assessing representation and maintaining musical coherence.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Algorithms for Collaborative Harmonization
"Mechanical metamaterials are increasingly attracting interest in engineering applications due to their unique mechanical properties and lightweight nature. This study develops a novel sound-based representation to characterize the topologies of mechanical metamaterials, including spinodal designs and porous cellular structures. Two distinct frameworks are introduced: an image-based approach, where material topologies are divided into grids and their relative densities are mapped to evolving melodies, and a numerical simulation approach, where finite element analysis (FEA) visualizes mechanical responses as color-coded images, translated into unique musical compositions. By applying the Fast Fourier Transform (FFT), the generated melodies are analyzed as frequency plots, revealing distinct acoustic signatures for each material topology. This innovative approach not only distinguishes between different metamaterial designs but also provides an intuitive, auditory tool for material characterization. The results demonstrate the potential of sound-based representations to complement traditional methods in materials modeling, offering new avenues for design and analysis.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Shape-to-Music: A Musical Representation for Structural Topologies of Mechanical Metamaterials
"This paper explores the deployment of mm-wave Frequency Modulated Continuous Wave (FMCW) radar for vital sign detection across multiple scenarios. We focus on overcoming the limitations of traditional sensing methods by enhancing signal processing techniques to capture subtle physiological changes effectively. Our study introduces novel adaptations of the Prony and MUSIC algorithms tailored for real-time heart and respiration rate monitoring, significantly advancing the accuracy and reliability of non-contact vital sign monitoring using radar technologies. Notably, these algorithms demonstrate a robust ability to suppress noise and harmonic interference. For instance, the mean absolute errors (MAE) for MUSIC and Prony in heart rate detection are 1.8 and 0.81, respectively, while for respiration rate, the MAEs are 1.01 and 0.8, respectively. These results underscore the potential of FMCW radar as a reliable, non-invasive solution for continuous vital sign monitoring in healthcare settings, particularly in clinical and emergency scenarios where traditional contact-based monitoring is impractical.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Breaking Barriers in Health Monitoring: Multi-Scenario Vital Sign Detection Using Mm-Wave MIMO FMCW Radar
"Existing state-of-the-art symbolic music generation models predominantly adopt autoregressive or hierarchical autoregressive architectures, modelling symbolic music as a sequence of attribute tokens with unidirectional temporal dependencies, under the assumption of a fixed, strict dependency structure among these attributes. However, we observe that using different attributes as the initial token in these models leads to comparable performance. This suggests that the attributes of a musical note are, in essence, a concurrent and unordered set, rather than a temporally dependent sequence. Based on this insight, we introduce Amadeus, a novel symbolic music generation framework. Amadeus adopts a two-level architecture: an autoregressive model for note sequences and a bidirectional discrete diffusion model for attributes. To enhance performance, we propose Music Latent Space Discriminability Enhancement Strategy(MLSDES), incorporating contrastive learning constraints that amplify discriminability of intermediate music representations. The Conditional Information Enhancement Module (CIEM) simultaneously strengthens note latent vector representation via attention mechanisms, enabling more precise note decoding. We conduct extensive experiments on unconditional and text-conditioned generation tasks. Amadeus significantly outperforms SOTA models across multiple metrics while achieving at least 4$\times$ speed-up. Furthermore, we demonstrate training-free, fine-grained note attribute control feasibility using our model. To explore the upper performance bound of the Amadeus architecture, we compile the largest open-source symbolic music dataset to date, AMD (Amadeus MIDI Dataset), supporting both pre-training and fine-tuning.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Amadeus: Autoregressive Model with Bidirectional Attribute Modelling for Symbolic Music
"Large Language Models (LLMs) are increasingly used for recommendation tasks due to their general-purpose capabilities. While LLMs perform well in rich-context settings, their behavior in cold-start scenarios, where only limited signals such as age, gender, or language are available, raises fairness concerns because they may rely on societal biases encoded during pretraining. We introduce a benchmark specifically designed to evaluate fairness in zero-context recommendation. Our modular pipeline supports configurable recommendation domains and sensitive attributes, enabling systematic and flexible audits of any open-source LLM. Through evaluations of state-of-the-art models (Gemma 3 and Llama 3.2), we uncover consistent biases across recommendation domains (music, movies, and colleges) including gendered and cultural stereotypes. We also reveal a non-linear relationship between model size and fairness, highlighting the need for nuanced analysis.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Revealing Potential Biases in LLM-Based Recommender Systems in the Cold Start Setting
"In music recommendation systems, multimodal interest learning is pivotal, which allows the model to capture nuanced preferences, including textual elements such as lyrics and various musical attributes such as different instruments and melodies. Recently, methods that incorporate multimodal content features through semantic IDs have achieved promising results. However, existing methods suffer from two critical limitations: 1) intra-modal semantic degradation, where residual-based quantization processes gradually decouple discrete IDs from original content semantics, leading to semantic drift; and 2) inter-modal modeling gaps, where traditional fusion strategies either overlook modal-specific details or fail to capture cross-modal correlations, hindering comprehensive user interest modeling. To address these challenges, we propose a novel multimodal recommendation framework with two stages. In the first stage, our Progressive Semantic Residual Quantization (PSRQ) method generates modal-specific and modal-joint semantic IDs by explicitly preserving the prefix semantic feature. In the second stage, to model multimodal interest of users, a Multi-Codebook Cross-Attention (MCCA) network is designed to enable the model to simultaneously capture modal-specific interests and perceive cross-modal correlations. Extensive experiments on multiple real-world datasets demonstrate that our framework outperforms state-of-the-art baselines. This framework has been deployed on one of China's largest music streaming platforms, and online A/B tests confirm significant improvements in commercial metrics, underscoring its practical value for industrial-scale recommendation systems.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Progressive Semantic Residual Quantization for Multimodal-Joint Interest Modeling in Music Recommendation
"We present the IRMA Dataset (Iranian Radif MIDI Audio), a multi-level, open-access corpus designed for the computational study of Iranian classical music, with a particular emphasis on the radif, a structured repertoire of modal-melodic units central to pedagogy and performance. The dataset combines symbolic MIDI representations, phrase-level audio-MIDI alignment, musicological transcriptions in PDF format, and comparative tables of theoretical information curated from a range of performers and scholars. We outline the multi-phase construction process, including segment annotation, alignment methods, and a structured system of identifier codes to reference individual musical units. The current release includes the complete radif of Karimi; MIDI files and metadata from Mirza Abdollah's radif; selected segments from the vocal radif of Davami, as transcribed by Payvar and Fereyduni; and a dedicated section featuring audio-MIDI examples of tahrir ornamentation performed by prominent 20th-century vocalists. While the symbolic and analytical components are released under an open-access license (CC BY-NC 4.0), some referenced audio recordings and third-party transcriptions are cited using discographic information to enable users to locate the original materials independently, pending copyright permission. Serving both as a scholarly archive and a resource for computational analysis, this dataset supports applications in ethnomusicology, pedagogy, symbolic audio research, cultural heritage preservation, and AI-driven tasks such as automatic transcription and music generation. We welcome collaboration and feedback to support its ongoing refinement and broader integration into musicological and machine learning workflows.",0,arxiv,MÃ¼zik,CC-BY/arXiv,The IRMA Dataset: A Structured Audio-MIDI Corpus for Iranian Classical Music
"Generative artificial intelligence in music has made significant strides, yet it still falls short of the substantial achievements seen in natural language processing, primarily due to the limited availability of music data. Knowledge-informed approaches have been shown to enhance the performance of music generation models, even when only a few pieces of musical knowledge are integrated. This paper seeks to leverage comprehensive music theory in AI-driven music generation tasks, such as algorithmic composition and style transfer, which traditionally require significant manual effort with existing techniques. We introduce a novel automatic music lexicon construction model that generates a lexicon, named CompLex, comprising 37,432 items derived from just 9 manually input category keywords and 5 sentence prompt templates. A new multi-agent algorithm is proposed to automatically detect and mitigate hallucinations. CompLex demonstrates impressive performance improvements across three state-of-the-art text-to-music generation models, encompassing both symbolic and audio-based methods. Furthermore, we evaluate CompLex in terms of completeness, accuracy, non-redundancy, and executability, confirming that it possesses the key characteristics of an effective lexicon.",0,arxiv,MÃ¼zik,CC-BY/arXiv,CompLex: Music Theory Lexicon Constructed by Autonomous Agents for Automatic Music Generation
"Question-answering (QA) is a natural approach for humans to understand a piece of music audio. However, for machines, accessing a large-scale dataset covering diverse aspects of music is crucial, yet challenging, due to the scarcity of publicly available music data of this type. This paper introduces MQAD, a music QA dataset built on the Million Song Dataset (MSD), encompassing a rich array of musical features, including beat, chord, key, structure, instrument, and genre -- across 270,000 tracks, featuring nearly 3 million diverse questions and captions. MQAD distinguishes itself by offering detailed time-varying musical information such as chords and sections, enabling exploration into the inherent structure of music within a song. To compile MQAD, our methodology leverages specialized Music Information Retrieval (MIR) models to extract higher-level musical features and Large Language Models (LLMs) to generate natural language QA pairs. Then, we leverage a multimodal LLM that integrates the LLaMA2 and Whisper architectures, along with novel subjective metrics to assess the performance of MQAD. In experiments, our model trained on MQAD demonstrates advancements over conventional music audio captioning approaches. The dataset and code are available at https://github.com/oyzh888/MQAD.",0,arxiv,MÃ¼zik,CC-BY/arXiv,MQAD: A Large-Scale Question Answering Dataset for Training Music Large Language Models
"Accurate and real-time monophonic pitch estimation in noisy conditions, particularly on resource-constrained devices, remains an open challenge in audio processing. We present \emph{SwiftF0}, a novel, lightweight neural model that sets a new state-of-the-art for monophonic pitch estimation. Through training on diverse speech, music, and synthetic datasets with extensive data augmentation, SwiftF0 achieves robust generalization across acoustic domains while maintaining computational efficiency. SwiftF0 achieves a 91.80\% harmonic mean (HM) at 10 dB SNR, outperforming baselines like CREPE by over 12 percentage points and degrading by only 2.3 points from clean audio. SwiftF0 requires only 95,842 parameters and runs approximately 42x faster than CREPE on CPU, making it ideal for efficient, real-time deployment. To address the critical lack of perfectly accurate ground truth pitch in speech corpora (which typically rely on algorithmic estimators or laryngograph signals), we introduce \emph{SpeechSynth}. This synthetic speech dataset, generated by a phoneme-level TTS model, provides exact, on-demand ground-truth pitch curves, enabling more robust model training and evaluation. Furthermore, we propose a unified metric, combining six complementary performance measures for comprehensive and reliable pitch evaluation, and release an open-source pitch benchmark suite. A live demo of SwiftF0 is available at https://swift-f0.github.io/, the source code at https://github.com/lars76/swift-f0, and the benchmark framework at https://github.com/lars76/pitch-benchmark.",0,arxiv,MÃ¼zik,CC-BY/arXiv,SwiftF0: Fast and Accurate Monophonic Pitch Detection
"Music serves as a powerful reflection of individual identity, often aligning with deeper psychological traits. Prior research has established correlations between musical preferences and personality, while separate studies have demonstrated that personality is detectable through linguistic analysis. Our study bridges these two research domains by investigating whether individuals' musical preferences leave traces in their spontaneous language through the lens of the Big Five personality traits (Openness, Conscientiousness, Extroversion, Agreeableness, and Neuroticism). Using a carefully curated dataset of over 500,000 text samples from nearly 5,000 authors with reliably identified musical preferences, we build advanced models to assess personality characteristics. Our results reveal significant personality differences across fans of five musical genres. We release resources for future research at the intersection of computational linguistics, music psychology and personality analysis.",0,arxiv,MÃ¼zik,CC-BY/arXiv,On the Interplay between Musical Preferences and Personality through the Lens of Language
"Despite significant advancements in music generation systems, the methodologies for evaluating generated music have not progressed as expected due to the complex nature of music, with aspects such as structure, coherence, creativity, and emotional expressiveness. In this paper, we shed light on this research gap, introducing a detailed taxonomy for evaluation metrics for both audio and symbolic music representations. We include a critical review identifying major limitations in current evaluation methodologies which includes poor correlation between objective metrics and human perception, cross-cultural bias, and lack of standardization that hinders cross-model comparisons. Addressing these gaps, we further propose future research directions towards building a comprehensive evaluation framework for music generation evaluation.",0,arxiv,MÃ¼zik,CC-BY/arXiv,A Survey on Evaluation Metrics for Music Generation
"Generating coherent and diverse human dances from music signals has gained tremendous progress in animating virtual avatars. While existing methods support direct dance synthesis, they fail to recognize that enabling users to edit dance movements is far more practical in real-world choreography scenarios. Moreover, the lack of high-quality dance datasets incorporating iterative editing also limits addressing this challenge. To achieve this goal, we first construct DanceRemix, a large-scale multi-turn editable dance dataset comprising the prompt featuring over 25.3M dance frames and 84.5K pairs. In addition, we propose a novel framework for iterative and editable dance generation coherently aligned with given music signals, namely DanceEditor. Considering the dance motion should be both musical rhythmic and enable iterative editing by user descriptions, our framework is built upon a prediction-then-editing paradigm unifying multi-modal conditions. At the initial prediction stage, our framework improves the authority of generated results by directly modeling dance movements from tailored, aligned music. Moreover, at the subsequent iterative editing stages, we incorporate text descriptions as conditioning information to draw the editable results through a specifically designed Cross-modality Editing Module (CEM). Specifically, CEM adaptively integrates the initial prediction with music and text prompts as temporal motion cues to guide the synthesized sequences. Thereby, the results display music harmonics while preserving fine-grained semantic alignment with text descriptions. Extensive experiments demonstrate that our method outperforms the state-of-the-art models on our newly collected DanceRemix dataset. Code is available at https://lzvsdy.github.io/DanceEditor/.",0,arxiv,MÃ¼zik,CC-BY/arXiv,DanceEditor: Towards Iterative Editable Music-driven Dance Generation with Open-Vocabulary Descriptions
"Dileptons serve as a clean and penetrating probe of the Quark--Gluon Plasma created in high-energy heavy-ion collisions. In this work, we investigate thermal dilepton spectra and their elliptic flow through the dynamical conductivity that governs the production rate. The conductivity is obtained from the trace of the spectral function within relativistic kinetic theory using the Relaxation Time Approximation. This allows us to derive for the first time an analytical expression for the dilepton rate with explicit dependence on the relaxation time of quark-antiquark interactions. We find a non-monotonic dependence of the dilepton rate on the relaxation time and compare the resulting transverse momentum, invariant mass spectra and elliptic flow with previous quantum field theory results. The spectra and elliptic flow are obtained by integrating the rate over the full spacetime volume of the evolving medium, using temperature and flow profiles from realistic MUSIC hydrodynamic simulations without considering the effect of magnetic fields in the profiles itself. However, we study the role of an external space-time dependent magnetic field by making the conductivity anisotropic. At small relaxation times, magnetic fields have negligible impact, while for larger relaxation times and stronger initial fields, modifications of up to $\sim$20\% appear in both spectra and elliptic flow. Assuming instead a constant magnetic field of $\sim 1\,m_Ï€^2$ at large relaxation times yields more modest effects, with changes of about 10\% in spectra and 5\% in elliptic flow.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Probing Dynamical Electrical Conductivity via Dilepton Emission: A Kinetic theory approach
"We introduce Multimodal DuetDance (MDD), a diverse multimodal benchmark dataset designed for text-controlled and music-conditioned 3D duet dance motion generation. Our dataset comprises 620 minutes of high-quality motion capture data performed by professional dancers, synchronized with music, and detailed with over 10K fine-grained natural language descriptions. The annotations capture a rich movement vocabulary, detailing spatial relationships, body movements, and rhythm, making MDD the first dataset to seamlessly integrate human motions, music, and text for duet dance generation. We introduce two novel tasks supported by our dataset: (1) Text-to-Duet, where given music and a textual prompt, both the leader and follower dance motion are generated (2) Text-to-Dance Accompaniment, where given music, textual prompt, and the leader's motion, the follower's motion is generated in a cohesive, text-aligned manner. We include baseline evaluations on both tasks to support future research.",0,arxiv,MÃ¼zik,CC-BY/arXiv,MDD: A Dataset for Text-and-Music Conditioned Duet Dance Generation
"Controllable human voice generation, particularly for expressive domains like singing, remains a significant challenge. This paper introduces Vevo2, a unified framework for controllable speech and singing voice generation. To tackle issues like the scarcity of annotated singing data and to enable flexible controllability, Vevo2 introduces two audio tokenizers: (1) a unified music-notation-free prosody tokenizer that captures prosody and melody from speech, singing, and even instrumental sounds, and (2) a unified content-style tokenizer that encodes linguistic content, prosody, and style for both speech and singing, while enabling timbre disentanglement. Vevo2 consists of an auto-regressive (AR) content-style modeling stage, which aims to enable controllability over text, prosody, and style, as well as a flow-matching acoustic modeling stage that allows for timbre control. Particularly, during the speech-singing joint training of the AR model, we propose both explicit and implicit prosody learning strategies to bridge speech and singing voice. Moreover, to further enhance the Vevo2's ability to follow text and prosody, we design a multi-objective post-training task that integrates both intelligibility and prosody similarity alignment. Experimental results show that the unified modeling in Vevo2 brings mutual benefits to both speech and singing voice generation. Additionally, Vevo2's effectiveness across a wide range of synthesis, conversion, and editing tasks for both speech and singing further demonstrates its strong generalization ability and versatility. Audio samples are are available at https://versasinger.github.io/.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Vevo2: A Unified and Controllable Framework for Speech and Singing Voice Generation
"Large language models have been widely evaluated on tasks such as comprehension, summarization, code generation, etc. However, their performance on graduate-level, culturally grounded questions in the Indian context remains largely unexplored. Existing Indian benchmarks emphasise basic fact-orientated queries that offer limited assessment of a deeper disciplinary understanding tailored to the Indian setting. In this paper, we present ParamBench, consisting of more than 17K questions in the Hindi language, comprising questionnaires from 21 diverse subjects. These questions are primarily derived from a nationwide graduate-level entrance examination covering topics such as history, music, instruments, yoga, literature, philosophy, law, etc.~ specifically for the Indian context. Additionally, we assess the ability of LLMs to handle diverse question formats - such as list-based matching, assertion-reason pairs, and sequence ordering - alongside conventional multiple-choice questions. We evaluated the performance of more than 16 open source LLMs on this benchmark, observing that Gemma3-27B attains the highest overall accuracy of 56.4\%. Furthermore, subject-wise analysis indicates that even for the best-performing LLMs, performance remains weak on topics such as music, classical instruments, and law, underscoring persistent challenges in culturally grounded reasoning. The dataset and source code is present at https://github.com/ayushbits/ParamBench.",0,arxiv,MÃ¼zik,CC-BY/arXiv,ParamBench: A Graduate-Level Benchmark for Evaluating LLM Understanding on Indic Subjects
"Conventional music visualisation systems rely on handcrafted ad hoc transformations of shapes and colours that offer only limited expressiveness. We propose two novel pipelines for automatically generating music videos from any user-specified, vocal or instrumental song using off-the-shelf deep learning models. Inspired by the manual workflows of music video producers, we experiment on how well latent feature-based techniques can analyse audio to detect musical qualities, such as emotional cues and instrumental patterns, and distil them into textual scene descriptions using a language model. Next, we employ a generative model to produce the corresponding video clips. To assess the generated videos, we identify several critical aspects and design and conduct a preliminary user evaluation that demonstrates storytelling potential, visual coherency and emotional alignment with the music. Our findings underscore the potential of latent feature techniques and deep generative models to expand music visualisation beyond traditional approaches.",0,arxiv,MÃ¼zik,CC-BY/arXiv,From Sound to Sight: Towards AI-authored Music Videos
"The integration of Large Language Models (LLMs) into recommender systems has enabled zero-shot, personality-based personalization through prompt-based interactions, offering a new paradigm for user-centric recommendations. However, incorporating user personality traits via the OCEAN model highlights a critical tension between achieving psychological alignment and ensuring demographic fairness. To address this, we propose PerFairX, a unified evaluation framework designed to quantify the trade-offs between personalization and demographic equity in LLM-generated recommendations. Using neutral and personality-sensitive prompts across diverse user profiles, we benchmark two state-of-the-art LLMs, ChatGPT and DeepSeek, on movie (MovieLens 10M) and music (Last.fm 360K) datasets. Our results reveal that personality-aware prompting significantly improves alignment with individual traits but can exacerbate fairness disparities across demographic groups. Specifically, DeepSeek achieves stronger psychological fit but exhibits higher sensitivity to prompt variations, while ChatGPT delivers stable yet less personalized outputs. PerFairX provides a principled benchmark to guide the development of LLM-based recommender systems that are both equitable and psychologically informed, contributing to the creation of inclusive, user-centric AI applications in continual learning contexts.",0,arxiv,MÃ¼zik,CC-BY/arXiv,PerFairX: Is There a Balance Between Fairness and Personality in Large Language Model Recommendations?
"We introduce a new music source separation model tailored for accurate vocal isolation. Unlike Transformer-based approaches, which often fail to capture intermittently occurring vocals, our model leverages Mamba2, a recent state space model, to better capture long-range temporal dependencies. To handle long input sequences efficiently, we combine a band-splitting strategy with a dual-path architecture. Experiments show that our approach outperforms recent state-of-the-art models, achieving a cSDR of 11.03 dB-the best reported to date-and delivering substantial gains in uSDR. Moreover, the model exhibits stable and consistent performance across varying input lengths and vocal occurrence patterns. These results demonstrate the effectiveness of Mamba-based models for high-resolution audio processing and open up new directions for broader applications in audio research.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Mamba2 Meets Silence: Robust Vocal Source Separation for Sparse Regions
"Audio comprehension-including speech, non-speech sounds, and music-is essential for achieving human-level intelligence. Consequently, AI agents must demonstrate holistic audio understanding to qualify as generally intelligent. However, evaluating auditory intelligence comprehensively remains challenging. To address this gap, we introduce MMAU-Pro, the most comprehensive and rigorously curated benchmark for assessing audio intelligence in AI systems. MMAU-Pro contains 5,305 instances, where each instance has one or more audios paired with human expert-generated question-answer pairs, spanning speech, sound, music, and their combinations. Unlike existing benchmarks, MMAU-Pro evaluates auditory intelligence across 49 unique skills and multiple complex dimensions, including long-form audio comprehension, spatial audio reasoning, multi-audio understanding, among others. All questions are meticulously designed to require deliberate multi-hop reasoning, including both multiple-choice and open-ended response formats. Importantly, audio data is sourced directly ``from the wild"" rather than from existing datasets with known distributions. We evaluate 22 leading open-source and proprietary multimodal AI models, revealing significant limitations: even state-of-the-art models such as Gemini 2.5 Flash and Audio Flamingo 3 achieve only 59.2% and 51.7% accuracy, respectively, approaching random performance in multiple categories. Our extensive analysis highlights specific shortcomings and provides novel insights, offering actionable perspectives for the community to enhance future AI systems' progression toward audio general intelligence. The benchmark and code is available at https://sonalkum.github.io/mmau-pro.",0,arxiv,MÃ¼zik,CC-BY/arXiv,MMAU-Pro: A Challenging and Comprehensive Benchmark for Holistic Evaluation of Audio General Intelligence
"Eye-movement related artifacts including blinks and saccades are significantly larger in amplitude than cortical activity as recorded by scalp electroencephalography (EEG), but are typically discarded in EEG studies focusing on cognitive mechanisms as explained by cortical source activity. Accumulating evidence however indicates that spontaneous eye blinks are not necessarily random, and can be modulated by attention and cognition beyond just physiological necessities. In this exploratory analysis we reanalyze a public EEG dataset of musicians listening to or imagining music (Bach chorales) while simultaneously reading from a sheet of music. We ask whether blink timing in reading music, accompanied by listening or imagery, is sufficient to uniquely identify the music being read from a given score. Intra-subject blink counts and timing are compared across trials using a spike train distance metric (Victor and Purpura, 1997). One-trial-left-out cross-validation is used to identify the music being read with above chance level accuracy (best subject: 56\%, chance: 25\%), where accuracy is seen to vary with subject, condition, and a tunable cost factor for time shifts. Future studies may consider incorporating eye blink contributions to brain decoding, especially in wearables where eye blinks could be easier to record than EEG given their higher amplitudes.",0,arxiv,MÃ¼zik,CC-BY/arXiv,EEG Blink Artifacts Can Identify Read Music in Listening and Imagery
"With the growing demand for short videos and personalized content, automated Video Log (Vlog) generation has become a key direction in multimodal content creation. Existing methods mostly rely on predefined scripts, lacking dynamism and personal expression. Therefore, there is an urgent need for an automated Vlog generation approach that enables effective multimodal collaboration and high personalization. To this end, we propose PersonaVlog, an automated multimodal stylized Vlog generation framework that can produce personalized Vlogs featuring videos, background music, and inner monologue speech based on a given theme and reference image. Specifically, we propose a multi-agent collaboration framework based on Multimodal Large Language Models (MLLMs). This framework efficiently generates high-quality prompts for multimodal content creation based on user input, thereby improving the efficiency and creativity of the process. In addition, we incorporate a feedback and rollback mechanism that leverages MLLMs to evaluate and provide feedback on generated results, thereby enabling iterative self-correction of multimodal content. We also propose ThemeVlogEval, a theme-based automated benchmarking framework that provides standardized metrics and datasets for fair evaluation. Comprehensive experiments demonstrate the significant advantages and potential of our framework over several baselines, highlighting its effectiveness and great potential for generating automated Vlogs.",0,arxiv,MÃ¼zik,CC-BY/arXiv,PersonaVlog: Personalized Multimodal Vlog Generation with Multi-Agent Collaboration and Iterative Self-Correction
"Automatic music transcription (AMT) has achieved remarkable progress for instruments such as the piano, largely due to the availability of large-scale, high-quality datasets. In contrast, violin AMT remains underexplored due to limited annotated data. A common approach is to fine-tune pretrained models for other downstream tasks, but the effectiveness of such transfer remains unclear in the presence of timbral and articulatory differences. In this work, we investigate whether training from scratch on a medium-scale violin dataset can match the performance of fine-tuned piano-pretrained models. We adopt a piano transcription architecture without modification and train it on the MOSA dataset, which contains about 30 hours of aligned violin recordings. Our experiments on URMP and Bach10 show that models trained from scratch achieved competitive or even superior performance compared to fine-tuned counterparts. These findings suggest that strong violin AMT is possible without relying on pretrained piano representations, highlighting the importance of instrument-specific data collection and augmentation strategies.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Is Transfer Learning Necessary for Violin Transcription?
"Masked latent prediction has emerged as a leading paradigm in self-supervised learning (SSL), especially for general audio and music representation learning. While recent methods have demonstrated strong performance, the role of the predictor module used at the output of such SSL systems remains mainly overlooked, despite being crucial for solving the pretext task at hand. In particular, this module should be able to deal with the ambiguity inherent in audio content, especially when it is composed of multiple sound sources. This work proposes a novel enhancement: integrating Multiple Choice Learning (MCL) to explicitly model prediction ambiguity and improve representation quality. We build on top of the recently proposed MATPAC system, improving its prediction and unsupervised classification pretext tasks with MCL. We extensively evaluate our method, MATPAC++, through both linear probing across multiple downstream tasks and fine-tuning on AudioSet, employing a unified protocol that enables rigorous and fair comparisons with state-of-the-art SSL approaches. Results show that our proposal achieves state-of-the-art when fine-tuned on AudioSet and overall state-of-the-art scores on downstream tasks. Additionally, we examine domain specialisation by training exclusively on music data, where our model achieves state-of-the-art performance with significantly improved efficiency.",0,arxiv,MÃ¼zik,CC-BY/arXiv,MATPAC++: Enhanced Masked Latent Prediction for Self-Supervised Audio Representation Learning
"Recently, motivated by the outstanding achievements of diffusion models, the diffusion process has been employed to strengthen representation learning in recommendation systems. Most diffusion-based recommendation models typically utilize standard Gaussian noise in symmetric forward and reverse processes in continuous data space. Nevertheless, the samples derived from recommendation systems inhabit a discrete data space, which is fundamentally different from the continuous one. Moreover, Gaussian noise has the potential to corrupt personalized information within latent representations. In this work, we propose a novel and effective method, named Asymmetric Diffusion Recommendation Model (AsymDiffRec), which learns forward and reverse processes in an asymmetric manner. We define a generalized forward process that simulates the missing features in real-world recommendation samples. The reverse process is then performed in an asymmetric latent feature space. To preserve personalized information within the latent representation, a task-oriented optimization strategy is introduced. In the serving stage, the raw sample with missing features is regarded as a noisy input to generate a denoising and robust representation for the final prediction. By equipping base models with AsymDiffRec, we conduct online A/B tests, achieving improvements of +0.131% and +0.166% in terms of users' active days and app usage duration respectively. Additionally, the extended offline experiments also demonstrate improvements. AsymDiffRec has been implemented in the Douyin Music App.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Asymmetric Diffusion Recommendation Model
"We present TalkPlayData 2, a synthetic dataset for multimodal conversational music recommendation generated by an agentic data pipeline. In the proposed pipeline, multiple large language model (LLM) agents are created under various roles with specialized prompts and access to different parts of information, and the chat data is acquired by logging the conversation between the Listener LLM and the Recsys LLM. To cover various conversation scenarios, for each conversation, the Listener LLM is conditioned on a finetuned conversation goal. Finally, all the LLMs are multimodal with audio and images, allowing a simulation of multimodal recommendation and conversation. In the LLM-as-a-judge and subjective evaluation experiments, TalkPlayData 2 achieved the proposed goal in various aspects related to training a generative recommendation model for music. TalkPlayData 2 and its generation code are released at https://talkpl.ai/talkplaydata2.",0,arxiv,MÃ¼zik,CC-BY/arXiv,TalkPlayData 2: An Agentic Synthetic Data Pipeline for Multimodal Conversational Music Recommendation
"Current approaches to music emotion annotation remain heavily reliant on manual labelling, a process that imposes significant resource and labour burdens, severely limiting the scale of available annotated data. This study examines the feasibility and reliability of employing a large language model (GPT-4o) for music emotion annotation. In this study, we annotated GiantMIDI-Piano, a classical MIDI piano music dataset, in a four-quadrant valence-arousal framework using GPT-4o, and compared against annotations provided by three human experts. We conducted extensive evaluations to assess the performance and reliability of GPT-generated music emotion annotations, including standard accuracy, weighted accuracy that accounts for inter-expert agreement, inter-annotator agreement metrics, and distributional similarity of the generated labels.   While GPT's annotation performance fell short of human experts in overall accuracy and exhibited less nuance in categorizing specific emotional states, inter-rater reliability metrics indicate that GPT's variability remains within the range of natural disagreement among experts. These findings underscore both the limitations and potential of GPT-based annotation: despite its current shortcomings relative to human performance, its cost-effectiveness and efficiency render it a promising scalable alternative for music emotion annotation.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Exploring the Feasibility of LLMs for Automated Music Emotion Annotation
"Integrated sensing and communication (ISAC) is a promising candidate technology for 6G due to its improvement in spectral efficiency and energy efficiency. Orthogonal frequency division multiplexing (OFDM) signal is a mainstream candidate ISAC waveform. However, there are inter-symbol interference (ISI) and inter-carrier interference (ICI) when the round-trip delay exceeds the cyclic prefix (CP) duration for OFDM signals, which limits the maximum sensing range of ISAC system. When detecting a long-range target, the wide beam inevitably covers the close-range target, of which the echo's power is much larger than that of the long-range target. In order to tackle the above problem, a multiple signal classification (MUSIC) and least squares (LS)-based spatial signal separation method is proposed to separate the echo signals reflected from different targets. Moreover, a coherent compensation-based sensing signal processing method at the receiver is proposed to enhance the signal to interference plus noise power ratio (SINR) of the OFDM block for generating the range-Doppler map (RDM) with higher SINR. Simulation results reveal that the proposed method greatly enhances the SINR of RDM by 10 dB for a target at 500 m compared with two-dimensional fast Fourier transform (2D-FFT) method. Besides, the detection probability is also significantly improved compared to the benchmarking method.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Coherent Compensation-Based Sensing for Long-Range Targets in Integrated Sensing and Communication System
"In practical teaching, we observe that few students thoroughly read or fully comprehend the information provided in traditional, text-based course syllabi. As a result, essential details, such as course policies and learning outcomes, are frequently overlooked. To address this challenge, in this paper, we propose a novel approach leveraging AI-generated singing and virtual avatars to present syllabi in a format that is more visually appealing, engaging, and memorable. Especially, we leveraged the open-source tool, HeyGem, to transform textual syllabi into audiovisual presentations, in which digital avatars perform the syllabus content as songs. The proposed approach aims to stimulate students' curiosity, foster emotional connection, and enhance retention of critical course information. Student feedback indicated that AI-sung syllabi significantly improved awareness and recall of key course information.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Singing Syllabi with Virtual Avatars: Enhancing Student Engagement Through AI-Generated Music and Digital Embodiment
"The interplay between leisure activities, stress management methods, studying methods, and methods of learning new things is crucial and affects performance in all aspects of life. On the other hand, data science and statistics are rapidly growing fields with high demands across universities. Thus, this study aimed to identify the similarities and dissimilarities between the four dimensions: leisure activities, stress management methods, studying methods and methods of learning new things. The participants of this study were first-year undergraduates studying statistics at one of the universities in Sri Lanka. There were 117 students in the sample (female-65, male-52). A self-reported questionnaire was used to collect data. First, individual responses for each question under each dimension were visualized using tile maps separately for males and females to identify similarities and dissimilarities in responses. Next, individuals were clustered based on the responses for each dimension separately. Finally, all resulting clusters were re-clustered to identify the relationships between the dimensions. In all cluster analyses, we used Jaccard distance with hierarchical clustering using the complete linkage method. The results were visualized using tile maps. Across all four dimensions we considered, the top activities were either listening to music or lectures and watching videos or TV shows, suggesting that individuals are introverts and passive learners. There was no strong relationship between these dimensions. By identifying these clusters and relationships, educators can tailor instructional approaches to enhance engagement and effectiveness in diverse learning environments.",0,arxiv,MÃ¼zik,CC-BY/arXiv,"Relationship Between Leisure Activities, Stress Management Methods, Study Methods, and Methods of Learning New Things Among First-Year Statistics Students"
"Fluid antenna (FA) technology has emerged as a promising approach in wireless communications due to its capability of providing increased degrees of freedom (DoFs) and exceptional design flexibility. This paper addresses the challenge of direction-of-arrival (DOA) estimation for aligned received signals (ARS) and non-aligned received signals (NARS) by designing two specialized uniform FA structures under time-constrained mobility. For ARS scenarios, we propose a fully movable antenna configuration that maximizes the virtual array aperture, whereas for NARS scenarios, we design a structure incorporating a fixed reference antenna to reliably extract phase information from the signal covariance. To overcome the limitations of large virtual arrays and limited sample data inherent in time-varying channels (TVC), we introduce two novel DOA estimation methods: TMRLS-MUSIC for ARS, combining Toeplitz matrix reconstruction (TMR) with linear shrinkage (LS) estimation, and TMR-MUSIC for NARS, utilizing sub-covariance matrices to construct virtual array responses. Both methods employ Nystrom approximation to significantly reduce computational complexity while maintaining estimation accuracy. Theoretical analyses and extensive simulation results demonstrate that the proposed methods achieve underdetermined DOA estimation using minimal FA elements, outperform conventional methods in estimation accuracy, and substantially reduce computational complexity.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Fluid Antenna Enabled Direction-of-Arrival Estimation Under Time-Constrained Mobility
"Estimating human dance motion is a challenging task with various industrial applications. Recently, many efforts have focused on predicting human dance motion using either egocentric video or music as input. However, the task of jointly estimating human motion from both egocentric video and music remains largely unexplored. In this paper, we aim to develop a new method that predicts human dance motion from both egocentric video and music. In practice, the egocentric view often obscures much of the body, making accurate full-pose estimation challenging. Additionally, incorporating music requires the generated head and body movements to align well with both visual and musical inputs. We first introduce EgoAIST++, a new large-scale dataset that combines both egocentric views and music with more than 36 hours of dancing motion. Drawing on the success of diffusion models and Mamba on modeling sequences, we develop an EgoMusic Motion Network with a core Skeleton Mamba that explicitly captures the skeleton structure of the human body. We illustrate that our approach is theoretically supportive. Intensive experiments show that our method clearly outperforms state-of-the-art approaches and generalizes effectively to real-world data.",0,arxiv,MÃ¼zik,CC-BY/arXiv,EgoMusic-driven Human Dance Motion Estimation with Skeleton Mamba
"Computational analysis of folk song audio is challenging due to structural irregularities and the need for manual annotation. We propose a method for automatic motive segmentation in Korean folk songs by fine-tuning a speech transcription model on audio lyric with motif boundary annotation. Applying this to 856 songs, we extracted motif count and duration entropy as structural features. Statistical analysis revealed that these features vary systematically according to the social function of the songs. Songs associated with collective labor, for instance, showed different structural patterns from those for entertainment or personal settings. This work offers a scalable approach for quantitative structural analysis of oral music traditions.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Motive-level Analysis of Form-functions Association in Korean Folk song
"Beat tracking is a widely researched topic in music information retrieval. However, current beat tracking methods face challenges due to the scarcity of labeled data, which limits their ability to generalize across diverse musical styles and accurately capture complex rhythmic structures. To overcome these challenges, we propose a novel beat tracking paradigm BeatFM, which introduces a pre-trained music foundation model and leverages its rich semantic knowledge to improve beat tracking performance. Pre-training on diverse music datasets endows music foundation models with a robust understanding of music, thereby effectively addressing these challenges. To further adapt it for beat tracking, we design a plug-and-play multi-dimensional semantic aggregation module, which is composed of three parallel sub-modules, each focusing on semantic aggregation in the temporal, frequency, and channel domains, respectively. Extensive experiments demonstrate that our method achieves state-of-the-art performance in beat and downbeat tracking across multiple benchmark datasets.",0,arxiv,MÃ¼zik,CC-BY/arXiv,BeatFM: Improving Beat Tracking with Pre-trained Music Foundation Model
"Fine-tuning pre-trained foundation models has made significant progress in music information retrieval. However, applying these models to beat tracking tasks remains unexplored as the limited annotated data renders conventional fine-tuning methods ineffective. To address this challenge, we propose HingeNet, a novel and general parameter-efficient fine-tuning method specifically designed for beat tracking tasks. HingeNet is a lightweight and separable network, visually resembling a hinge, designed to tightly interface with pre-trained foundation models by using their intermediate feature representations as input. This unique architecture grants HingeNet broad generalizability, enabling effective integration with various pre-trained foundation models. Furthermore, considering the significance of harmonics in beat tracking, we introduce harmonic-aware mechanism during the fine-tuning process to better capture and emphasize the harmonic structures in musical signals. Experiments on benchmark datasets demonstrate that HingeNet achieves state-of-the-art performance in beat and downbeat tracking",0,arxiv,MÃ¼zik,CC-BY/arXiv,HingeNet: A Harmonic-Aware Fine-Tuning Approach for Beat Tracking
"We study how musicians use artificial intelligence (AI) across formats like singles, albums, performances, installations, voices, ballets, operas, or soundtracks. We collect 337 music artworks and categorize them based on AI usage: AI composition, co-composition, sound design, lyrics generation, and translation. We find that AI is employed as a co-creative tool, as an artistic medium, and in live performances and installations. Innovative uses of AI include exploring uncanny aesthetics, multilingual and multigenre song releases, and new formats such as online installations. This research provides a comprehensive overview of current AI music practices, offering insights into emerging artistic trends and the challenges faced by AI musicians.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Music and Artificial Intelligence: Artistic Trends
"Evaluating audio generation systems, including text-to-music (TTM), text-to-speech (TTS), and text-to-audio (TTA), remains challenging due to the subjective and multi-dimensional nature of human perception. Existing methods treat mean opinion score (MOS) prediction as a regression problem, but standard regression losses overlook the relativity of perceptual judgments. To address this limitation, we introduce QAMRO, a novel Quality-aware Adaptive Margin Ranking Optimization framework that seamlessly integrates regression objectives from different perspectives, aiming to highlight perceptual differences and prioritize accurate ratings. Our framework leverages pre-trained audio-text models such as CLAP and Audiobox-Aesthetics, and is trained exclusively on the official AudioMOS Challenge 2025 dataset. It demonstrates superior alignment with human evaluations across all dimensions, significantly outperforming robust baseline models.",0,arxiv,MÃ¼zik,CC-BY/arXiv,QAMRO: Quality-aware Adaptive Margin Ranking Optimization for Human-aligned Assessment of Audio Generation Systems
"We propose a universal physical mechanism for the emergence of 1/f fluctuations, observed across a wide range of systems. In particular, we verify this on acoustic cases. The mechanism is based on amplitude modulation (AM) and demodulation (DM), where the 1/f spectral law arises not in the raw waveform but in its demodulated amplitude envelope. Two distinct yet complementary processes generate the required AM: (i) stochastic synchronization among oscillators, modeled via an extended Kuramoto framework that captures perpetual synchronization-desynchronization cycles, and (ii) frequency-selective resonance, modeled by spectral accumulation of eigenmodes in acoustic or structural environments. Numerical simulations demonstrate that both mechanisms, acting separately or in combination, robustly produce 1/f spectra over several decades when DM is applied, and that the classical Kuramoto critical point is not necessary for their emergence. We demonstrate the cross-domain relevance of this AM/DM framework through analyses of musical performances, seismic records, and astrophysical time series, revealing a common underlying structure. This work establishes demodulation as a general route to 1/f fluctuations, providing a simple and scalable explanation for its ubiquity in both natural and engineered systems.    Keywords: 1/f fluctuation, amplitude modulation, synchronization, resonance, Kuramoto model, music, natural noise, demodulation",0,arxiv,MÃ¼zik,CC-BY/arXiv,Dynamic Synchronization and Resonance as a Universal Origin of 1/f Fluctuations -- Amplitude Modulation Across Music and Nature
"AI systems for music generation are increasingly common and easy to use, granting people without any musical background the ability to create music. Because of this, generative-AI has been marketed and celebrated as a means of democratizing music making. However, inclusivity often functions as marketable rhetoric rather than a genuine guiding principle in these industry settings. In this paper, we look at four generative-AI music making systems available to the public as of mid-2025 (AIVA, Stable Audio, Suno, and Udio) and track how they are rhetoricized by their developers, and received by users. Our aim is to investigate ideologies that are driving the early-stage development and adoption of generative-AI in music making, with a particular focus on democratization. A combination of autoethnography and digital ethnography is used to examine patterns and incongruities in rhetoric when positioned against product functionality. The results are then collated to develop a nuanced, contextual discussion. The shared ideology we map between producers and consumers is individualist, globalist, techno-liberal, and ethically evasive. It is a 'total ideology' which obfuscates individual responsibility, and through which the nature of music and musical practice is transfigured to suit generative outcomes.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Opening Musical Creativity? Embedded Ideologies in Generative-AI Music Systems
"Despite recent advances, long-sequence video generation frameworks still suffer from significant limitations: poor assistive capability, suboptimal visual quality, and limited expressiveness. To mitigate these limitations, we propose MAViS, a multi-agent collaborative framework designed to assist in long-sequence video storytelling by efficiently translating ideas into visual narratives. MAViS orchestrates specialized agents across multiple stages, including script writing, shot designing, character modeling, keyframe generation, video animation, and audio generation. In each stage, agents operate under the 3E Principle -- Explore, Examine, and Enhance -- to ensure the completeness of intermediate outputs. Considering the capability limitations of current generative models, we propose the Script Writing Guidelines to optimize compatibility between scripts and generative tools. Experimental results demonstrate that MAViS achieves state-of-the-art performance in assistive capability, visual quality, and video expressiveness. Its modular framework further enables scalability with diverse generative models and tools. With just a brief idea description, MAViS enables users to rapidly explore diverse visual storytelling and creative directions for sequential video generation by efficiently producing high-quality, complete long-sequence videos. To the best of our knowledge, MAViS is the only framework that provides multimodal design output -- videos with narratives and background music.",0,arxiv,MÃ¼zik,CC-BY/arXiv,MAViS: A Multi-Agent Framework for Long-Sequence Video Storytelling
"Personalised music-based interventions offer a powerful means of supporting motor rehabilitation by dynamically tailoring auditory stimuli to provide external timekeeping cues, modulate affective states, and stabilise gait patterns. Generalisable Brain-Computer Interfaces (BCIs) thus hold promise for adapting these interventions across individuals. However, inter-subject variability in EEG signals, further compounded by movement-induced artefacts and motor planning differences, hinders the generalisability of BCIs and results in lengthy calibration processes. We propose Individual Tangent Space Alignment (ITSA), a novel pre-alignment strategy incorporating subject-specific recentering, distribution matching, and supervised rotational alignment to enhance cross-subject generalisation. Our hybrid architecture fuses Regularised Common Spatial Patterns (RCSP) with Riemannian geometry in parallel and sequential configurations, improving class separability while maintaining the geometric structure of covariance matrices for robust statistical computation. Using leave-one-subject-out cross-validation, `ITSA' demonstrates significant performance improvements across subjects and conditions. The parallel fusion approach shows the greatest enhancement over its sequential counterpart, with robust performance maintained across varying data conditions and electrode configurations. The code will be made publicly available at the time of publication.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Cross-Subject and Cross-Montage EEG Transfer Learning via Individual Tangent Space Alignment and Spatial-Riemannian Feature Fusion
"Automatic transcription of acoustic guitar fingerpicking performances remains a challenging task due to the scarcity of labeled training data and legal constraints connected with musical recordings. This work investigates a procedural data generation pipeline as an alternative to real audio recordings for training transcription models. Our approach synthesizes training data through four stages: knowledge-based fingerpicking tablature composition, MIDI performance rendering, physical modeling using an extended Karplus-Strong algorithm, and audio augmentation including reverb and distortion. We train and evaluate a CRNN-based note-tracking model on both real and synthetic datasets, demonstrating that procedural data can be used to achieve reasonable note-tracking results. Finetuning with a small amount of real data further enhances transcription accuracy, improving over models trained exclusively on real recordings. These results highlight the potential of procedurally generated audio for data-scarce music information retrieval tasks.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Exploring Procedural Data Generation for Automatic Acoustic Guitar Fingerpicking Transcription
"Automatic transcription of guitar strumming is an underrepresented and challenging task in Music Information Retrieval (MIR), particularly for extracting both strumming directions and chord progressions from audio signals. While existing methods show promise, their effectiveness is often hindered by limited datasets. In this work, we extend a multimodal approach to guitar strumming transcription by introducing a novel dataset and a deep learning-based transcription model. We collect 90 min of real-world guitar recordings using an ESP32 smartwatch motion sensor and a structured recording protocol, complemented by a synthetic dataset of 4h of labeled strumming audio. A Convolutional Recurrent Neural Network (CRNN) model is trained to detect strumming events, classify their direction, and identify the corresponding chords using only microphone audio. Our evaluation demonstrates significant improvements over baseline onset detection algorithms, with a hybrid method combining synthetic and real-world data achieving the highest accuracy for both strumming action detection and chord classification. These results highlight the potential of deep learning for robust guitar strumming transcription and open new avenues for automatic rhythm guitar analysis.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Joint Transcription of Acoustic Guitar Strumming Directions and Chords
"MIDI is a modern standard for storing music, recording how musical notes are played. Many piano performances have corresponding MIDI scores available online. Some of these are created by the original performer, recording on an electric piano alongside the audio, while others are through manual transcription. In recent years, automatic music transcription (AMT) has rapidly advanced, enabling machines to transcribe MIDI from audio. However, these transcriptions often require further correction. Assuming a perfect timing correction, we focus on the loudness correction in terms of MIDI velocity (a parameter in MIDI for loudness control). This task can be approached through score-informed MIDI velocity estimation, which has undergone several developments. While previous approaches introduced specifically built models to re-estimate MIDI velocity, thereby replacing AMT estimates, we propose a BiLSTM correction module to refine AMT-estimated velocity. Although we did not reach state-of-the-art performance, we validated our method on the well-known AMT system, the high-resolution piano transcription (HPT), and achieved significant improvements.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Score-Informed BiLSTM Correction for Refining MIDI Velocity in Automatic Piano Transcription
"Modern music producers commonly use MIDI (Musical Instrument Digital Interface) to store their musical compositions. However, MIDI files created with digital software may lack the expressive characteristics of human performances, essentially leaving the velocity parameter - a control for note loudness - undefined, which defaults to a flat value. The task of filling MIDI velocity is termed MIDI velocity prediction, which uses regression models to enhance music expressiveness by adjusting only this parameter. In this paper, we introduce the U-Net, a widely adopted architecture in image colorization, to this task. By conceptualizing MIDI data as images, we adopt window attention and develop a custom loss function to address the sparsity of MIDI-converted images. Current dataset availability restricts our experiments to piano data. Evaluated on the MAESTRO v3 and SMD datasets, our proposed method for filling MIDI velocity outperforms previous approaches in both quantitative metrics and qualitative listening tests.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Filling MIDI Velocity using U-Net Image Colorizer
"Existing emotion recognition methods mainly focus on enhancing performance by employing complex deep models, typically resulting in significantly higher model complexity. Although effective, it is also crucial to ensure the reliability of the final decision, especially for noisy, corrupted and out-of-distribution data. To this end, we propose a novel emotion recognition method called trusted emotion recognition (TER), which utilizes uncertainty estimation to calculate the confidence value of predictions. TER combines the results from multiple modalities based on their confidence values to output the trusted predictions. We also provide a new evaluation criterion to assess the reliability of predictions. Specifically, we incorporate trusted precision and trusted recall to determine the trusted threshold and formulate the trusted Acc. and trusted F1 score to evaluate the model's trusted performance. The proposed framework combines the confidence module that accordingly endows the model with reliability and robustness against possible noise or corruption. The extensive experimental results validate the effectiveness of our proposed model. The TER achieves state-of-the-art performance on the Music-video, achieving 82.40% Acc. In terms of trusted performance, TER outperforms other methods on the IEMOCAP and Music-video, achieving trusted F1 scores of 0.7511 and 0.9035, respectively.",0,arxiv,MÃ¼zik,CC-BY/arXiv,A Trustworthy Method for Multimodal Emotion Recognition
"The FMCW radars are widely used for automotive radar systems. The basic idea for FMCW radars is to generate a linear frequency ramp as transmit signal. The difference frequency, (i.e., beat frequency) between the transmitted and received signal is determined after down conversion. The FFT operation on beat frequency signal can recognize targets at different range and velocity. Increasing demand on safety functionality leads to the Direction of Arrival (DOA) estimation to resolve two closely located targets. Consequently, the problem of angle estimation for 77GHz FMCW automotive radar simulated data has been investigated in this term project. In particular, we examined the performances of FFT, MUSIC and compressed sensing in angle estimation task, and it was found that although FFT is the fastest algorithm, it has very poor angular resolution when compared with others which are both super resolution algorithms. The code for this project report is available at https://github.com/ekurtgl/FMCW-MIMO-Radar-Simulation.",0,arxiv,MÃ¼zik,CC-BY/arXiv,Direction of Arrival Estimation with Virtual Antenna Array Using FMCW Radar Simulated Data
"We consider Colouring on graphs that are $H$-subgraph-free for some fixed graph $H$, i.e., graphs that do not contain $H$ as a subgraph. It is known that even $3$-Colouring is NP-complete for $H$-subgraph-free graphs whenever $H$ has a cycle; or a vertex of degree at least $5$; or a component with two vertices of degree $4$, while Colouring is polynomial-time solvable for $H$-subgraph-free graphs if $H$ is a forest of maximum degree at most $3$, in which each component has at most one vertex of degree $3$. For connected graphs $H$, this means that it remains to consider when $H$ is tree of maximum degree $4$ with exactly one vertex of degree $4$, or a tree of maximum degree $3$ with at least two vertices of degree $3$. We let $H$ be a so-called subdivided ""H""-graph, which is either a subdivided $\mathbb{H}_0$: a tree of maximum degree $4$ with exactly one vertex of degree $4$ and no vertices of degree $3$, or a subdivided $\mathbb{H}_1$: a tree of maximum degree $3$ with exactly two vertices of degree $3$. In the literature, only a limited number of polynomial-time and NP-completeness results for these cases are known. We develop new polynomial-time techniques that allow us to determine the complexity of Colouring on $H$-subgraph-free graphs for all the remaining subdivided ""H""-graphs, so we fully classify both cases. As a consequence, the complexity of Colouring on $H$-subgraph-free graphs has now been settled for all connected graphs $H$ except when $H$ is a tree of maximum degree $4$ with exactly one vertex of degree $4$ and at least one vertex of degree $3$; or a tree of maximum degree $3$ with at least three vertices of degree $3$. We also employ our new techniques to obtain the same new polynomial-time results for another classic graph problem, namely Stable Cut.",0,arxiv,Matematik,CC-BY/arXiv,Colouring Graphs Without a Subdivided H-Graph: A Full Complexity Classification
"The study of eigenvalue multiplicities plays a central role in the spectral theory of signed graphs, extending several classical results from the unsigned setting. While most existing work focuses on the nullity of a signed graph (the multiplicity of the eigenvalue $0$), much less is known for arbitrary eigenvalues. In this paper, we establish a sharp upper bound for the multiplicity $m(G_Ïƒ, Î»)$ of any real eigenvalue $Î»$ of a connected signed graph $G_Ïƒ$ in terms of its girth. Our main result shows that \[ m(G_Ïƒ, Î») \le n - g(G_Ïƒ) + 2, \] where $n$ is the number of vertices and $g(G_Ïƒ)$ is the girth. We prove that equality holds if and only if $G_Ïƒ$ is switching equivalent to one of the following extremal families: \begin{itemize}   \item[(i)] a balanced complete graph with $Î»= -1$;   \item[(ii)] an antibalanced complete graph with $Î»= 1$; or   \item[(iii)] a balanced complete bipartite graph with $Î»= 0$. \end{itemize} This fully extends and generalizes the known result for the nullity case ($Î»= 0$), originally due to Wu et al.\ (2022), to the entire eigenvalue spectrum. Our approach combines Cauchy interlacing, switching equivalence, and a structural analysis of induced cycles in signed graphs. We also provide a characterization of eigenvalues with multiplicity $1$ and $2$ for signed cycles, and include examples illustrating the sharpness and spectral behavior of the extremal families.",0,arxiv,Matematik,CC-BY/arXiv,Multiplicity Bounds for Arbitrary Eigenvalues of Connected Signed Graphs
"An edge-colored hypergraph is called \emph{a rainbow hypergraph} if all the colors on its edges are distinct. Given two positive integers $n,r$ and an $r$-uniform hypergraph $\mathcal{G}$, the anti-Ramsey number $ar_r(n,\mathcal{G})$ is defined to be the minimum number of colors $t$ such that there exists a rainbow copy of $\mathcal{G}$ in any exactly $t$-edge-coloring of the complete $r$-uniform hypergraph of order $n$. Let $ \mathcal{F}_k $ denote the 3-graph ($k$-star) consisting of $k$ edges sharing exactly one vertex. Tang, Li and Yan \cite{YTG} determined the value of $ar_3(n,\mathcal{F}_3)$ when $n\geq 20$. In this paper, we determine the anti-Ramsey number $ar_3(n,\mathcal{F}_{k+1})$, where $k\geq 3$ and $n> \frac{5}{2}k^3+\frac{15}{2}k^2+26k-3$.",0,arxiv,Matematik,CC-BY/arXiv,Anti-Ramsey Number of Stars in 3-uniform hypergraphs
"The Weil-Petersson volume of genus-g hyperbolic surfaces with geodesic boundaries is known since work of Mirzakhani to be polynomial in the boundary lengths. We provide a bijective proof of this fact in the genus-0 case in the presence of a distinguished cusp. It is based on a generalization of a recent tree bijection, by the first author and Curien, to the setting with geodesic boundaries, requiring an extension of the Bowditch-Epstein-Penner spine construction. As an application of our tree bijection we establish an explicit formula for the distance-dependent three-point function, which records an exact metric statistic measuring the difference of two geodesic distances among a triple of distinguished cusps in a Weil-Petersson random surface. We conclude with a discussion of the relevance of this function to the topological recursion of Weil-Petersson volumes and metric properties of Weil-Petersson random surfaces with many boundaries or cusps.",0,arxiv,Matematik,CC-BY/arXiv,A tree bijection for the moduli space of genus-0 hyperbolic surfaces with boundaries
"In this extended abstract, we study special tropical prevarieties which we call Coxeter Dressians. They arise from equations capturing a generalization of valuated symmetric basis exchange for Coxeter matroids. In particular, we study subdivisions of the associated Coxeter matroid polytopes. We show that the subdivisions induced by points of the Coxeter Dressian consist of cells which are strong Coxeter matroidal. This generalizes well-known results in type $A$ to other Lie types. Finally, we implement explicit computations of Coxeter Dressians in OSCAR.",0,arxiv,Matematik,CC-BY/arXiv,Minuscule Coxeter Dressians
"In this paper, we study the maximum order $v(k,Î¸)$ of a connected $k$-regular graph whose second largest eigenvalue is at most $Î¸$. From Alon-Boppana and Serre, we know that $v(k,Î¸)$ is finite when $Î¸< 2\sqrt{k-1}$ while the work of Marcus, Spielman, and Srivastava implies that $v(k,Î¸)$ is infinite if $Î¸\geq 2\sqrt{k-1}$. CioabÄƒ, Koolen, Nozaki, and Vermette obtained a general upper bound on $v(k, Î¸)$ via Nozaki's linear programming bound and determined many values of $v(k,Î¸)$. The graphs attaining this bound are distance-regular and are called Moore polygons. Damerell and Georgiacodis proved that there are no Moore polygons of diameter $6$ or more. For smaller diameters, there are infinitely many Moore polygons.   We complement these results by proving two nonexistence results for Moore polygons with specific parameters. We also determine new values of $v(k,Î¸)$: $v(4, \sqrt{2}) = 14$ and $v(5, \sqrt{2}) = v(5,\sqrt{5}-1)=16$. The former is achieved by the co-Heawood graph, and the latter by the folded $5$-cube. We verify that any connected $5$-regular graph with second eigenvalue $Î»_2$ exceeding $1$ satisfies $Î»_2 \geq \sqrt{5} - 1$, and that the unique $5$-regular graph attaining equality in this bound has $10$ vertices. We prove a stronger form of a 2015 conjecture of Kolokolnikov related to the second eigenvalue of cubic graphs of given order, and observe that other recent results on the second eigenvalue of regular graphs are consequences of the general upper bound theorem on $v(k,Î¸)$ mentioned above.",0,arxiv,Matematik,CC-BY/arXiv,The non-existence of some Moore polygons and spectral Moore bounds
"This paper is devoted to the neighborhood complexes of the induced $k$-independent graphs. Inspired by the surprising correspondence between total $k$-cut complex of $n$-cycle $C_n$ and neighborhood complex of stable Kneser graph $SG(n,k)$, we anticipate that the homotopy type of total cut complexes may have some relationships with the neighborhood complexes of induced $k$-independent graphs. We investigated the homotopy type of some total cut complexes and neighborhood complexes of some other graphs, using techniques from algebraic topology and discrete Morse theory.",0,arxiv,Matematik,CC-BY/arXiv,Neighborhood Complexes of induced $k$-independent graphs
"We prove that for any coloring of the naturals using two colors there are monochromatic sets of the form $\{x,y,xy,x+iy:i\leq k\}$ and $\{x,y,x^y,xy^i:i\leq k\}$ for any $k$.",0,arxiv,Matematik,CC-BY/arXiv,"Sums, products, and exponents in two-colorings of the naturals"
"The \emph{chromatic number} of a hypergraph is the smallest number of colors needed to color the vertices such that no edge of at least two vertices is monochromatic. Given a family of geometric objects $\mathcal{F}$ that covers a subset $S$ of the Euclidean space, we can associate it with a hypergraph whose vertex set is $\mathcal F$ and whose edges are those subsets ${\mathcal{F}'}\subset \mathcal F$ for which there exists a point $p\in S$ such that ${\mathcal F}'$ consists of precisely those elements of $\mathcal{F}$ that contain $p$. The question whether $\mathcal F$ can be split into 2 coverings is equivalent to asking whether the chromatic number of the hypergraph is equal to 2.   There are a number of competing notions of the chromatic number that lead to deep combinatorial questions already for abstract hypergraphs. In this paper, we concentrate on \emph{geometrically defined} (in short, \emph{geometric}) hypergraphs, and survey many recent coloring results related to them. In particular, we study and survey the following problem, dual to the above covering question. Given a set of points $S$ in the Euclidean space and a family $\mathcal{F}$ of geometric objects of a fixed type, define a hypergraph ${\mathcal H}_m$ on the point set $S$, whose edges are the subsets of $S$ that can be obtained as the intersection of $S$ with a member of $\mathcal F$ and have at least $m$ elements. Is it true that if $m$ is large enough, then the chromatic number of ${\mathcal H}_m$ is equal to 2?",0,arxiv,Matematik,CC-BY/arXiv,Coloring Geometric Hypergraphs: A Survey
"Havet and ThomassÃ© proved that every tournament of order $n\geq 8$ contains every oriented Hamiltonian path, which was conjectured by Rosenfeld. Recently, it was shown that in any tournament $T$ of order $n\geq 8$, there exists an arc $e$ such that $T-e$ contains any oriented Hamiltonian path. A natural extension of this problem is to study the stability of this property under arbitrary arc deletion. In this paper, we prove that every arc $e$ in a tournament $T$ of order $n\geq 8$ satisfies that $T-e$ contains every oriented Hamiltonian path, except for some explicitly described exceptions.",0,arxiv,Matematik,CC-BY/arXiv,Oriented Hamiltonian Paths in Tournaments: Stability under Arc Deletion
"In this paper, we establish that the class of $\{P_6, (2,2)\text{-broom}\}$-free graphs contains a subclass $\mathcal{L}_i$, defined by certain cutset conditions, whose chromatic number admits a linear $Ï‡$-bound. Building on recent results showing that broom-free graphs excluding $K_d(t)$ as a subgraph admit a polynomial bound in~$t$ on their chromatic number (A broom is obtained from a path with one end $v$ by adding leaves adjacent to $v$), we extend this result to the hereditary class $\mathcal{H}$ of $C_4$-free and \emph{$p$-flag}-free graphs (where a \emph{$p$-flag} is a triangle with an attached $p$-path). We show that if $G \in \mathcal{H}$ is $B^{+}(p+2, t-1)$-free (for $p \ge 2$ and $t \ge 3$, that is, if it excludes a generalized broom with an additional leaf), and does not contain $K_d(t)$ as a subgraph, then $Ï‡(G)$ is polynomially bounded in $t$. Furthermore, for the subclass of $\mathcal{H}$ excluding $K_3(t)$ as a subgraph, we prove that $Ï‡(G)$ is linearly $Ï‡$-bounded in $Ï‰(G)$.",0,arxiv,Matematik,CC-BY/arXiv,Towards Esperet's Conjecture: Polynomial $Ï‡$-Bounds for Structured Graph Classes
"In the paper [J. Graph Theory (2023) 102:458-471, the Esperet's conjecture has been posed: Every $Ï‡$-bounded hereditary class is poly-$Ï‡$-bounded]. This conjecture was first posed in [Habilitation Thesis, UniversitÃ© Grenoble Alpes, 24, 2017]. This is adapted from the GyÃ¡rfÃ¡s--Sumner's conjecture which has been asserted in [The Theory and Applications of Graphs, (G. Chartrand, ed.), John Wiley & Sons, New York, 1981, pp. 557-576].   Although the Esperet's conjecture is false in general, in this study we consider an analogue of Esperet's conjecture as follows: Let $C$ be a hereditary class of graphs, and $d \ge 1$. Suppose that there is a function $f$ such that $Ï‡(G) \le f(Ï„_d(G))$ for each $G \in C$. Can we always choose $f$ to be a polynomial? We investigate this conjecture by focusing on specific classes of graphs. This work identifies hereditary graph classes that do not contain specific induced subdivisions of claws and confirms that they adhere to the stated conjecture.",0,arxiv,Matematik,CC-BY/arXiv,On the analogue of Esperet's conjecture: Characterizing hereditary classes
"We introduce Magic Gems, a geometric representation of magic squares as three-dimensional polyhedra. By mapping an n x n magic square onto a centered coordinate grid with cell values as vertical displacements, we construct a point cloud whose convex hull defines the Magic Gem. This reveals a connection between magic square constraints and statistical structure: we prove that magic squares have vanishing covariances between position and value. We introduce a covariance energy functional -- the sum of squared covariances with row, column, and diagonal indicator variables -- and prove for n=3 (via exhaustive enumeration) that its zeros are precisely the magic squares. Large-scale sampling for n=4,5 (460+ million arrangements) provides strong numerical evidence that this characterization extends to larger orders. Perturbation analysis demonstrates that magic squares are isolated local minima. The representation is invariant under dihedral symmetry D_4, yielding canonical geometric objects for equivalence classes.",0,arxiv,Matematik,CC-BY/arXiv,Magic Gems: A Polyhedral Framework for Magic Squares
"We define the Uniform Random Walk (URW) on a connected, locally finite graph as the weak limit of the uniform walk of length $n$ starting at a fixed vertex. When the limit exists, it is necessarily Markovian and is independent of the starting point. For a finite graph, URW equals the Maximal Entropy Random Walk (MERW).   We investigate the existence and phase transitions of URW for loop perturbed regular graphs and their limits. It turns out that for a sequence of finite graphs, it is the global spectral theory of the limiting graphing that governs the behavior of the finite MERWs.   In the delocalized phase, we use a ""membrane argument"", showing that the principal eigenfunction of an expander graphing is stable under a small diagonal perturbation. This gives us: 1) The existence of URW on leaves; 2) The URW is a unique entropy maximizer; 3) The MERW of a finite graph sequence Benjamini-Schramm converges to the URW of the limiting graphing.   In the localized phase, the environment seen by the particle takes the role of a finite stationary measure. We show that for canopy trees, the URW exists, is transient and maximizes entropy. We also show that for large finite graphs where most vertices have a fixed degree, localization of MERW is governed by the adjacency norm.",0,arxiv,Matematik,CC-BY/arXiv,"The Uniform Random Walk on graphs, loop processes and graphings"
"We study a conductance-weighted Nash--Williams density for a finite simple undirected graph $G=(V,E,c)$ with a conductance assignment $c:E\to[0,\infty)$: \[ A_c(G):=\max\bigl\{ D_c(H): H\subseteq G\text{ connected}, |V(H)|\ge 2 \bigr\},\qquad D_c(H):=\frac{\sum_{e\in E(H)}c(e)}{|V(H)|-1}. \] This functional reduces to the classical Nash--Williams density when $c\equiv 1$, isomorphism invariant, monotone under subgraphs and edge additions, positively homogeneous, and convex. We prove sharp global bounds \[ \max_{e\in E}c(e)\le A_c(G)\le\sum_{e\in E}c(e), \] with attainment by some connected subgraph. On the analytic side, we introduce a local variant and derive conductance--resistance inequalities using effective resistances in the ambient network. If $R_G(e)$ denotes the effective resistance between the endpoints of $e$ in $G$, we show that every connected $H\subseteq G$ satisfies \[ \sum_{e\in E(H)}c(e)\,R_G(e)\le |V(H)|-1, \] which in turn yields the Cauchy--Schwarz inequality \[ D_c(H)\le\sqrt{\frac{\sum_{e\in E(H)}c(e)/R_G(e)}{|V(H)|-1}} \] and hence an explicit resistance-based upper bound on $A_c(G)$. On the structural side, we describe the algebraic behavior of $A_c(G)$. We show that under edge-disjoint union, $A_c(G)$ behaves as a max invariant: for a finite disjoint union of weighted graphs one has $A_c(G)= \max_i A_{c_i}(G_i)$. In particular, disjoint union induces a commutative idempotent monoid structure at the level of isomorphism classes, with $A_c(G)$ idempotent with respect to this operation.",0,arxiv,Matematik,CC-BY/arXiv,On Weighted Arboricity: Conductance-Resistance Bounds and Monoid Structure
"Cluster algebras, introduced by Fomin and Zelevinsky through the process of quiver mutation, have become central objects in modern algebra and geometry, linking combinatorial constructions with diverse mathematical domains such as Teichmuller theory, total positivity, and even theoretical physics. Building on foundational work by Fomin, Shapiro, and Thurston connecting cluster algebras to triangulated surfaces, recent research has extended mutation theory to infinite settings, including the infinity-gon and more general marked surfaces. In this paper, we develop a purely combinatorial framework for mutation of infinite quivers, independent of but compatible with these topological constructions. By formalizing infinite quivers as limits of embedded finite quivers, we establish a consistent definition of mutation that generalizes prior surface-based results. We then apply this framework to extend the notion of reddening sequences, special mutation sequences with significant algebraic consequences, from the finite to the infinite setting. Our approach not only unifies previous topological and combinatorial perspectives but also provides a technical foundation for further generalizations of cluster algebra theory in the infinite case.",0,arxiv,Matematik,CC-BY/arXiv,Reddening sequences and mutation of infinite quivers
"The Prague dimension of a graph $G$ is defined as the minimum number of complete graphs whose direct product contains $G$ as an induced subgraph. Introduced in the 1970s by NeÅ¡etÅ™il, Pultr, and RÃ¶dl -- and motivated by the work of Dushnik and Miller, as well as by the induced Ramsey theorem -- determining the Prague dimension of a graph is a notoriously hard problem. In this paper, we show that for all $\varepsilon > 0$ and $p$ such that $ n^{-1+\varepsilon} \le p \le n^{-\varepsilon}$, with high probability the Prague dimension of $G_{n,p}$ is $Î˜_{\varepsilon}(pn)$, which improves upon a recent result by Molnar, RÃ¶dl, Sales and Schacht.   Inspired by the work of Bennett and Bohman, our approach centres on analysing a random greedy process that builds an independent set of size $Î©(p^{-1}\log pn)$ by iteratively selecting vertices uniformly at random from the common non-neighbourhood of those already chosen. Using the differential equation method, we show that every non-edge is essentially equally likely to be covered by this process, which is key to establishing our bound.",0,arxiv,Matematik,CC-BY/arXiv,On the Prague dimension of sparse random graphs
"We study the appearance of Hamilton $\ell$-cycles in dense $k$-uniform hypergraphs when $\ell \leq k-2$ and $k-\ell$ does not divide $k$. Our main result reduces this problem to the robust existence of a connected $\ell$-cycle tiling in host graph families that are approximately closed under subsampling. As an application, we determine the minimum $d$-degree threshold for $d=k-2$ and all $1 \leq \ell \leq k-2$ when $k - \ell$ does not divide $k$. We also reduce the case $\ell < d$ entirely to the corresponding (non-connected) $\ell$-cycle tiling problem. In addition, our outcomes lead to counting and random robust versions of these results. The proofs are based on the recently introduced method of blow-up covers and thus avoid the use of the Regularity Lemma and the Absorption Method.",0,arxiv,Matematik,CC-BY/arXiv,Loose Hamiltonicity
"A group is R-harmonious if there exists a permutation $g_1,g_2,\ldots, g_{n-1}$ of the non-identity elements of $G$ such that the consecutive products $g_1g_2$, $g_2g_3$, $\ldots, g_{n-1}g_1$ also form a permutation of the non-identity elements, where $n=|G|$. We investigate R-harmonious groups via cyclic and split extensions. Among our results, we prove that every group of odd-order not divisible by 3 is R-harmonious.",0,arxiv,Matematik,CC-BY/arXiv,R-harmonious groups
"G.-Pechenik-Pfannerer-Striker-Swanson applied hourglass plabic graphs to construct web bases for spaces of tensor invariants of fundamental representations of $U_q(\mathfrak{sl}_4)$, extending Kuperberg's celebrated basis for $U_q(\mathfrak{sl}_3)$. We give several combinatorial characterizations of basis webs in the kernel of the projection to invariants in a tensor product of arbitrary (type $1$) irreducibles. We apply this to show that the nonzero images of basis webs form a basis (a property shared with Lusztig's dual canonical basis) yielding distinguished clasped web bases for each such tensor product.",0,arxiv,Matematik,CC-BY/arXiv,Clasped web bases from hourglass plabic graphs
"In 2011, Dyer published a series of conjectures on the weak order of Coxeter groups. One of these conjectures stated that the inversion set of the join of two elements in a Coxeter group is equal to some ""closure"" of the union of their inversion sets. In this paper we show that this ""closure"" is in fact a preclosure, which we call the Bruhat preclosure, but is a closure whenever our underlying set is an inversion set. By performing the Bruhat preclosure an infinite number of times we obtain a closure which we call the infinite Bruhat closure. We show in a uniform way that Dyer's conjecture is true when using the infinite Bruhat closure (instead of Bruhat preclosure) if the join exists between two elements. Finally, we end by showing in type A, the Bruhat preclosure is a closure thus giving a (second) proof that Dyer's conjecture is true in type A.",0,arxiv,Matematik,CC-BY/arXiv,Bruhat Preclosure
"For a finite set of points $V=\{v_1, \dots, v_m\}$ in Euclidean space $\mathbb{R}^d$ and a point $r \in \mathbb{R}^d$, a subset $S \subset V$ is called $r$-balanced if $\mathrm{relint}(\mathrm{conv}(S)) \cap r \neq \emptyset$. In the case when $r$ is a point in the relative interior of the whole set $\mathrm{conv}(V)$, we prove that the poset of all balanced subsets, excluding the whole set $V$, is homotopy equivalent to the sphere of dimension $m-k-2$, where $k$ is the dimension of the affine hull of $V$.",0,arxiv,Matematik,CC-BY/arXiv,On the Homotopy Type of Balanced subsets
"This paper presents combinatorial facts dealing with the number of unlabeled partially ordered sets (posets) refined by the number of arcs in the Hasse diagram (sequence A342447 in OEIS). The main result is that the differences with respect to the number of points in this sequence become stationary if the number of points is sufficiently high. These differences are proposed as the new sequence A376894. In addition, the underlying combinatorial and graph theoretical arguments were used to extend some further OEIS sequences.",0,arxiv,Matematik,CC-BY/arXiv,On the Number of Posets
"We present a new construction of triple arrays by combining a symmetric 2-design with a resolution of another 2-design. This is the first general method capable of producing non-extremal triple arrays. We call the triple arrays which can be obtained in this way resolvable. We employ the construction to produce the first examples of $(21 \times 15, 63)$-triple arrays, and enumerate all resolvable $(7 \times 15, 35)$-triple arrays, of which there was previously only a single known example. An infinite subfamily of Paley triple arrays turns out to be resolvable.   We also introduce a new intermediate object, unordered triple arrays, that are to triple arrays what symmetric 2-designs are to Youden rectangles, and propose a strengthening of Agrawal's long-standing conjecture on the existence of extremal triple arrays. For small parameters, we completely enumerate all unordered triple arrays, and use this data to corroborate the new conjecture. We construct several infinite families of resolvable unordered triple arrays, and, in particular, show that all $((q + 1) \times q^2, q(q + 1))$-triple arrays are resolvable and are in correspondence with finite affine planes of order $q$.",0,arxiv,Matematik,CC-BY/arXiv,Resolvable Triple Arrays
"Raimi's classical theorem establishes a partition of the natural numbers with a remarkable unavoidability property: for every finite coloring of $\mathbb{N}$, there is a color class whose translate meets both parts of the partition in infinitely many points. Recently, Kang, Koh, and Tran have extended this phenomenon to the circle group, proving that there exists a measurable partition of the circle such that every finite measurable cover admits a rotation whose image meets each part of the partition in positive measure. This paper shows that this phenomenon extends beyond compact abelian groups to a wide class of non-group geometric surfaces that still exhibit \textit{a hidden one-dimensional symmetry}. Specifically, we establish analogs of Raimi's theorem for three families of surfaces (with their natural surface measures): the unit sphere $S^{n-1} \subset \mathbb{R}^n$, rotational power surfaces (such as cones and paraboloids), and circular cylindrical surfaces. The common feature is that each of these surfaces carries a natural measure-preserving action of the circle group by rotation in a fixed plane and admits a measurable trivialization as a product $C \times Y$. This circle-bundle structure allows the measurable Raimi partition on the base circle to be lifted to an unavoidable partition on the manifold. Our approach is unified through a general circle-bundle theorem, which reduces all three geometric cases to verifying suitable equivariance and product disintegration properties of the surface measure.",0,arxiv,Matematik,CC-BY/arXiv,Raimi's theorem for manifolds with circle symmetry
"It is known that the Ehrhart polynomials of cross-polytopes, as well as of pyramids over them, have positive coefficients. We give a combinatorial proof of this fact by showing that a scaled version of the Ehrhart polynomials are generating functions for certain colored permutations. This answers a question posed by Stanley.",0,arxiv,Matematik,CC-BY/arXiv,Interpreting the Ehrhart coefficients of cross-polytopes
"In this paper, we investigate the ideals of semidirect products of L-algebras and the structure of simple L-algebras. We provide a precise characterization of the ideals of semidirect products and describe the structure of their prime spectrum. Furthermore, we introduce a family of finite simple L-algebras and prove that every simple linear L-algebra belongs to this family. We also show that the family we construct coincides with the class of simple algebras in a certain subclass of finite CKL-algebras. As an application, we use these results to give a clear description of linear Hilbert algebras and their symmetric semidirect products.",0,arxiv,Matematik,CC-BY/arXiv,L-algebras and their ideals: from simplicity to semidirect products
"In this article we will introduce a central problem in additive combinatorics, which arised from the famous van der Waerden theorem and an early conjecture of ErdÅ‘s and TurÃ¡n. The first important theorem was due to Roth in 1953. There were a number of generalized or improved results afterwards, which we call Roth-type theorems. We will list them and try to give concise expositions to the ideas in some of the proofs without much prior knowledge.",0,arxiv,Matematik,CC-BY/arXiv,Roth-type theorems in additive combinatroics
"We consider domino tilings of the Aztec diamond. Using the Domino Shuffling algorithm introduced by Elkies, Kuperberg, Larsen, and Propp in arXiv:math/9201305, we are able to generate domino tilings uniformly at random. In this paper, we investigate the probability of finding a domino at a specific position in such a random tiling. We prove that this placement probability is always equal to $1/4$ plus a rational function, whose shape depends on the location of the domino, multiplied by a position-independent factor that involves only the size of the diamond. This result leads to significantly more compact explicit counting formulas compared to previous findings. As a direct application, we derive explicit counting formulas for the domino tilings of Aztec diamonds with $2\times 2$-square holes at arbitrary positions.",0,arxiv,Matematik,CC-BY/arXiv,The 1/4-phenomenon of placement probabilities of tilings in the Aztec diamond
"We present a common sufficient condition for the total positivity of combinatorial triangles and their reversals, as well as the real-rootedness of generating functions of the rows. The proof technique is to construct a unified planar network that represent the matrix, its reversal, and the Toeplitz matrices of rows, respectively, when selecting different sets of sources and sinks. These results can be applied to the exponential Riordan arrays, the iteration matrices and the $n$-recursive matrices. As consequences, we prove the total positivity and real-rootedness properties associated to many well-known combinatorial numbers, including the Stirling numbers of both kinds (of type A and type B), the Lah numbers, the idempotent numbers, the Delannoy numbers, and the derangement numbers of type A and type B.",0,arxiv,Matematik,CC-BY/arXiv,A unified planar network approach to total positivity of combinatorial matrices and real-rootedness of polynomials
"Let $G_{\mathbb{R}}$ be a Lie group of Hermitian type, and $L(Î»)$ a highest weight Harish-Chandra module of $G_{\mathbb{R}}$ with highest weight $Î»$. In this article, we exhibit a bijection between the set of connected Dynkin subdiagrams containing the noncompact simple root and the set of unitary highest weight modules $L(-wÏ-Ï)$, where $Ï$ is half the sum of positive roots. We find that $L(-wÏ-Ï)$ is unitary if and only if the Schubert variety $X(w)$ is smooth. We also give the cardinality of the set of unitary highest weight modules $L(-wÏ-Ï)$ for each Kazhdan-Lusztig right cell.",0,arxiv,Matematik,CC-BY/arXiv,Unitarity of highest weight Harish-Chandra modules and smoothness of Schubert varieties
"We classify all wormhole singularities, i.e. cyclic quotient surface singularities admitting at least two extremal P-resolutions, thereby solving an open problem posed by UrzÃºa. Our approach introduces a new combinatorial framework based on what we call the coherent graph of a framed triangulated polygon. As an application, we give an alternative proof of the Hacking-Tevelev-UrzÃºa theorem on the maximum number of extremal P-resolutions of a cyclic quotient singularity.",0,arxiv,Matematik,CC-BY/arXiv,Classification of wormhole singularities
"Billey-Postnikov (BP) decompositions govern when Schubert varieties $X(w)$ decompose as bundles of smaller Schubert varieties. We further develop the theory of BP decompositions and show that, in finite type, they can be recognized by pattern conditions and are indexed by the order ideals of a poset $\mathsf{bp}(w)$ that we introduce; we conjecture that this holds in any Coxeter group. We then apply BP decompositions to show that, when $X(w)$ is rationally smooth and $W$ simply laced, the Schubert structure constants $c_{uv}^w$ satisfy a triangularity property, yielding a canonical involution on the Schubert cells of $X(w)$ respecting PoincarÃ© duality. We also classify the rationally smooth Bruhat intervals in finite type (other than $E$) which admit generalized Lehmer codes, answering questions and conjectures of Billey-Fan-Losonczy, Bolognini-Sentinelli, and Bishop-MiliÄ‡eviÄ‡-Thomas. Finally, we show that rationally smooth Schubert varieties in infinite type need not have Grassmannian BP decompositions, disproving conjectures of Richmond-Slofstra and Oh-Richmond.",0,arxiv,Matematik,CC-BY/arXiv,"Billey-Postnikov posets, rationally smooth Schubert varieties, and PoincarÃ© duality"
A paradoxical idea in quantum transport is that attaching weakly-coupled edges to a large base graph creates high-fidelity quantum state transfer. We provide a mathematical treatment that rigorously prove this folklore idea. Our proofs are elementary and build upon the Feshbach-Schur method from perturbation theory. We also show the idea is effective in circumventing Anderson localization in spin chains and finding speedups in hitting times useful for quantum search.,0,arxiv,Matematik,CC-BY/arXiv,The strength of weak coupling
"The online Ramsey turnaround game is a game between two players, Builder and Painter, on a board of $n$ vertices using $3$ colors, for a fixed graph $H$ on at most $n$ vertices. The goal of Painter is to force a monochromatic copy of $H$, the goal of Builder is to avoid this as long as possible. In each round of the game, Builder exposes one new edge and is allowed to forbid the usage of one color for Painter to color this newly exposed edge, and Painter colors the edge according to this restriction. The game is over as soon as Painter manages to achieve a monochromatic copy of $H$. For sufficiently large $n$, we consider the smallest number $f(n, H)$ of edges so that Painter can always win after $f(n, H)$ edges have been exposed by Builder. In addition, we define $f(H)$ to be the smallest $n$ such that Painter can always win on a clique with $n$ vertices. We give bounds for both functions and show that this problem is closely related to other concepts in extremal graph theory, such as polychromatic colorings, set-coloring Ramsey numbers, chromatic Ramsey numbers, and 2-color TurÃ¡n numbers.",0,arxiv,Matematik,CC-BY/arXiv,Online Ramsey turnaround numbers
"The Hermitian adjacency matrices of digraphs based on the sixth root of unity were introduced in [B. Mohar, A new kind of Hermitian matrices for digraphs, Linear Alg. Appl. (2020)]. They appear to be the most natural choice for the spectral theory of digraphs. Undirected graphs have adjacency spectrum symmetric about 0 if and only if they are bipartite. The situation is more complex for the Hermitian spectra of digraphs. In this paper we study non-bipartite oriented graphs with symmetric Hermitian spectra. Our main result concerns the extremal problem of maximizing the density of spectrally symmetric oriented graphs. The maximum possible density is shown to be between 13/18} and 10/11. Furthermore, we give a necessary condition for an oriented graph to be spectrally symmetric based on the adjacency spectrum of the underlying graph. This allows us to show that line graphs of sufficiently dense graphs do not admit spectrally symmetric orientations. We also show how to construct infinite families of spectrally symmetric graphs using 1-sums.",0,arxiv,Matematik,CC-BY/arXiv,Spectrally symmetric orientations of graphs
"Albertson conjectured that every graph with chromatic number $r$ has crossing number at least the crossing number of the complete graph $K_r$. This conjecture was proved for $r\le 12$ by Albertson, Cranston, and Fox; for $r\le 16$ by BarÃ¡t and TÃ³th; and for $r\le 18$ by Ackerman. Here we verify it for $r\le 24$; we also greatly restrict the possibilities for counterexamples when $r\in\{25,26\}$. In addition, we strengthen earlier work bounding the order of a minimum counterexample for each choice of $r$: we exclude the possibility that $|G|\ge 2.82r$ and exclude the possibility that $1.228r\le |G|\le 1.768r$. Finally, as $r$ grows, we extend the lower end of this range of excluded orders for a minimum counterexample. In particular: if $r\ge 125{,}000$, then we exclude the possibility that $1.10r\le |G|\le 1.768r$; and if $r\ge 825{,}000$, then we exclude the possibility that $1.05r\le |G|\le 1.768r$.",0,arxiv,Matematik,CC-BY/arXiv,Progress on Albertson's Conjecture
"We harness both human ingenuity and the power of symbolic computation to study the number of coin tosses until reaching $n$ Heads or $m$ Tails. We also talk about the closely related problem of reaching $n$ Heads and $m$ Tails. This paper is accompanied by a Maple package that enables fast computation of expectations, variances, and higher moments of these quantities.",0,arxiv,Matematik,CC-BY/arXiv,How many coin tosses would you need until you get $n$ Heads or $m$ Tails?
"Around 2007, Warnaar proved four identities related to Nahm sums associated with twice the inverse of the Cartan matrix of type $D_k$. Three of these had been conjectured by Flohr, Grabow, and Koehn, while special cases of two of the identities were first conjectured in 1993 by Kedem, Klassen, McCoy, and Melzer. Warnaar's proof relies on a multi-sum identity from Andrews' proof of the Andrews-Gordon identities. We give a new proof of all four identities using the theory of Bailey pairs. Furthermore, we establish a parametric generalization of two of the identities and provide two distinct proofs of this generalization.",0,arxiv,Matematik,CC-BY/arXiv,Nahm sum identities for Cartan matrices of type $D_k$
"Generalized splines are a simultaneous generalization of GKM theory -- which studies equivariant cohomology -- and classical splines, which provide piecewise approximations of functions. Generalized splines can also be understood via schemes, with the interpolation constraints -- or so-called GKM-condition -- encoded by gluing along certain closed subschemes. This view provides a local-global principle, with the local pictures retaining the generalized spline structure. Consequently, the behavior of generalized splines over local rings controls certain global phenomena, such as projectivity and often freeness.   We introduce an interface between the homological study of local rings and the combinatorial study of generalized splines. We identify precisely how the generalized spline structure coordinates with the existing homological local ring machinery. This is accomplished by two exact sequences that provide a regulatory structure on the local cohomology of a generalized spline module. As an application, we use this to prove that for any edge-labeled graph $G$ with principal ideal labels, and any Cohen-Macaulay ring $R$ of Krull dimension 2 at each maximal ideal, the module of splines $R_G$ is free, provided it has finite projective dimension. As a special case, this implies every generalized spline module over $k[x,y]$ with principal edge labels is free.",0,arxiv,Matematik,CC-BY/arXiv,The local homological structure of generalized splines
"We give an expansion in $1/N$ and $Î²$ of the cumulants of power sums of the particles of the $Î²$-ensemble. This new expansion is obtained using the tridiagonal model of Dumitriu and Edelman. The coefficients of the expansion are expressed in terms of suitably labelled maps introduced by Bouttier, Fusy, and Guitter. Our expansion is of a different nature than the one obtained by LaCroix in is study of the $b$-conjecture of Goulden and Jackson, and involves only orientable maps. We are able to relate bijectively the first two orders of our expansion to the one of LaCroix using a novel many-to-one mapping that relates suitably labelled planar maps with two minima and maps on the projective plane.",0,arxiv,Matematik,CC-BY/arXiv,Enumeration of maps with the Dumitriu-Edelman model
"For any $k\ge 3$ and $\ell \in [k-1]$ such that $(k,\ell) \ne (3,1)$, we show that any sufficiently large $k$-graph $G$ must contain a Hamilton $\ell$-cycle provided that it has no isolated vertices and every set of $k-1$ vertices contained in an edge is contained in at least $\left(1 - \frac{1}{\lfloor{\frac{k}{k-\ell}\rfloor}(k-\ell)}\right)n - (k - 3)$ edges. We also show that this bound is tight for infinitely many values of $k$ and $\ell$ and is off by at most $1$ for all others, and is hence essentially optimal. This improves an asymptotic version of this result due to Mycroft and ZÃ¡rate-GuerÃ©n, and the case $\ell = k-1$ completely resolves a conjecture of Illingworth, Lang, MÃ¼yesser, Parczyk and Sgueglia.   These results support the utility of $\textit{minimum}$ $\textit{supported}$ $\textit{co-degree}$ conditions in a $k$-graph, a recently introduced variant of the standard notion of minimum co-degree applicable to $k$-graphs with non-trivial strong independent sets. Our proof techniques involve a novel blow-up tiling framework introduced by Lang, avoiding traditional approaches using the regularity and blow-up lemmas.",0,arxiv,Matematik,CC-BY/arXiv,Exact supported co-degree bounds for Hamilton cycles
"Chifan-Ioana (2010) implies that, for any factor of IID percolation on any nonamenable Cayley graph $G$, there is a countable set of (strong) indistinguishability classes for non-hyperfinite clusters. We introduce quantitative strengthenings, called (qI) and (qSI): for $Î·$-non-hyperfinite clusters, there are at most $M(G,Î·)<\infty$ (strong) indistinguishability classes, for any FIID percolation.   We first show that (qI) and (qSI) for any $G$ are equivalent to the ``sparse implies thin'' property (SiT): any FIID percolation with $Î·$-non-hyperfinite clusters has density at least $c(G,Î·)>0$. Also, (SiT) is independent of the finite generating set of a group. We prove, using entropy inequalities, that (SiT) holds for free groups, even for weak FIIDs. On the other hand, recent work of JardÃ³n-SÃ¡nchez, Mellick, Poulin, and WrÃ³bel implies that (SiT) fails for weak FIIDs on non-exact, i.e., not property (A) groups.   Furthermore, (SiT) implies that the Bernoulli graphing over any non-hyperfinite FIID cluster is strongly ergodic, and that indistinguishability for non-hyperfinite FIID clusters is equivalent to strong indistinguishability. These results follow from the work of Chifan-Ioana for every nonamenable Cayley graph, but with non-probabilistic proofs.   We also prove, again using entropy inequalities, this time for all nonamenable Cayley graphs, that any FIID percolation with high enough expected degree must have a density close to 1, and there must be a single indistinguishability class of such clusters. On Kazhdan groups, there must be a single such cluster.   Our results have finite counterparts: in any large girth $d$-regular graph sequence, any FIID subgraph of average degree at least $2+Î´$ must have density at least $c(d,Î´)>0$. In the uniform random d-regular graph $G_{n,d}$, this holds for every subgraph of average degree at least $2+Î´$.",0,arxiv,Matematik,CC-BY/arXiv,Quantitative indistinguishability and sparse and dense clusters in factor of IID percolations
"Given a connected graph $G$, the equidistant dimension of $G$ represents the cardinality of the smallest set of vertices $S$ of $G$ such that for any two vertices $x,y\notin S$ there is at least one vertex in $S$ equidistant to both $x,y$ in terms of distances. In this article, we compute the equidistant dimension of some Cartesian product graphs including two-dimensional Hamming graphs, some hypercubes, prisms of cycle, and squared grid graphs.",0,arxiv,Matematik,CC-BY/arXiv,Equidistant dimension of Cartesian product graphs
"We consider the spherical variety of quadratic forms over a quadratically closed field of characteristic 2, and determine its orbits for the action of the Borel subgroup of upper triangular matrices. We exhibit a connection between these orbits and the Catalan triangle numbers. In addition, we describe explicitly a natural Weyl group action on the set of Borel orbit double covers",0,arxiv,Matematik,CC-BY/arXiv,On Borel orbits of quadratic forms in characteristic 2
"The classical spectral TurÃ¡n problem is to determine the maximum spectral radius of an $F$-free graph of order $n$. This paper extends this framework to signed graphs. Let $\mathcal{C}_r^-$ be the set of all unbalanced signed graphs with underlying graphs $C_r$. Wang, Hou and Li [Linear Algebra Appl, 681 (2024) 47-65] previously determined the spectral TurÃ¡n number of $\mathcal{C}_{3}^{-}$. In the present work, we characterize the extremal graphs that achieve the maximum index among all unbalanced signed graphs of order $n$ that are $t\mathcal{C}{3}^{-}$-free for $t\geq 2$. Furthermore, for $t\geq 3$, we identify the graphs with the second maximum index among all $t\mathcal{C}{3}^{-}$-free unbalanced signed graphs of fixed order $n$.",0,arxiv,Matematik,CC-BY/arXiv,The index of $t\mathcal{C}_{3}^{-}$-free signed graphs
"This paper proves a stability result for a variation of the ErdÅ‘s-Ko-Rado theorem in the context of polynomials over finite fields. Let $\mathcal F$ be a family of polynomials of degree at most $k \geq 3$ in $\mathbb F_q[X]$. Call $\mathcal F$ intersecting if for any two polynomials $f, g$ in $\mathcal F$, there exists a point $x \in \mathbb F_q$ for which $f(x) = g(x)$. An intersecting family is called a star if it consists of all polynomials $f$ with ${\rm deg } f \leq k$ such that $f(x) = y$ for some fixed points $x, y \in \mathbb F_q$. In this paper we prove that if $\mathcal F$ is an intersecting family with $|\mathcal F| \geq \frac 1{\sqrt 2} q^k + \mathcal O(q^{k-1})$, then $\mathcal F$ is contained in a star. In fact, we prove that this is still true if we also evaluate the polynomials ""at infinity"", which is equivalent to studying the problem for homogeneous bivariate polynomials.   The proof technique extends to a general framework for intersection problems of linear codes $C$. One has to investigate the geometry of the projective system $\mathcal S$ associated to $C$. If the hyperplanes that don't intersect $\mathcal S$ are well spread out with respect to the points not on $\mathcal S$, then one obtains stability results, showing that any intersecting family of reasonably large size is contained in a star.",0,arxiv,Matematik,CC-BY/arXiv,Intersection problems for linear codes and polynomials over finite fields
"Building on the work of Gabriel Conant, we investigate the enumeration problems of finite distance monoids by applying the decomposition of Archimedean classes and studying their internal arithmetic progressions. Specifically, we first determine the exact value of $DM(n,2)$, which denotes the number of distance monoids on $n$ non-zero elements with Archimedean complexity $2$. This computation allows us to resolve a conjecture of Conant, establishing that the total number $DM(n)$ of distance monoids grows at least exponentially in $n$. Furthermore, we study the asymptotic behavior of $DM(n,n-k)$ for fixed $k$, proving that $DM(n,n-k) = O(n^k)$ and providing an exact formula for $DM(n,n-2)$.",0,arxiv,Matematik,CC-BY/arXiv,Enumeration of Finite Distance Monoids
"Patterns are words with terminals and variables. The language of a pattern is the set of words obtained by uniformly substituting all variables with words that contain only terminals. In their original definition, patterns only allow for multiple distinct occurrences of some variables to be related by the equality relation, represented by using the same variable multiple times. In an extended notion, called relational patterns and relational pattern languages, variables may be related by arbitrary other relations. We extend the ongoing investigation of the main decision problems for patterns (namely, the membership problem, the inclusion problem, and the equivalence problem) to relational pattern languages under a wide range of individual relations. It is shown show that - even for many much simpler or less restrictive relations - the complexity and (un)decidability characteristics of these problems do not change compared to the classical case where variables are related only by equality.",0,arxiv,Matematik,CC-BY/arXiv,An Analysis of Decision Problems for Relational Pattern Languages under Various Constraints
"We prove a lower bound theorem for the number of $k$-faces ($1\le k\le d-2$) in a $d$-dimensional polytope $P$ (or $d$-polytope) with up to $3d-1$ vertices. Previous lower bound theorems for $d$-polytopes with few vertices concern those with at most $2d$ vertices, $2d+1$ vertices, and $2d+2$ vertices.   If $P$ has exactly $d+2$ facets and $2d+\ell$ vertices ($\ell\ge 1$), the lower bound is tight for certain combinations of $d$ and $\ell$. When $P$ has at least $d+3$ facets and $2d+\ell$ vertices ($\ell\ge 1$), the lower bound remains tight up to $\ell=d-1$, and equality for some $1\le k\le d-2$ is attained only when $P$ has precisely $d+3$ facets.   We exhibit at least one minimiser for each number of vertices between $2d+1$ and $3d-1$, including two distinct minimisers with $2d+2$ vertices and three with $3d-2$ vertices.",0,arxiv,Matematik,CC-BY/arXiv,A lower bound theorem for $d$-polytopes with at most $3d-1$ vertices
"The aim of this study is to show that harmonic geometric polynomials can be represented in terms of geometric polynomials. This problem was first considered by Keller [14]; however, the corresponding coefficients were not fully determined. In the present work, we provide several explicit representations of harmonic geometric polynomials in terms of geometric polynomials. Moreover, several applications of one of these representations are subsequently developed. In particular, we obtain a generalization of the classical identity for the harmonic numbers, compute an integral involving harmonic geometric polynomials and an integral involving products of harmonic geometric and geometric polynomials in terms of Bernoulli numbers. These integral formulas lead to new explicit expressions for Bernoulli numbers. In addition, we give several recurrence relations for harmonic geometric polynomials and evaluate a finite sum involving harmonic numbers and positive powers of integers.",0,arxiv,Matematik,CC-BY/arXiv,Harmonic Geometric Polynomials via Geometric Polynomials and Their Applications
"In this work we establish a connection between copositivity, that is, nonnegativity on the positive orthant, of sparse real Laurent polynomials and discriminants. Specifically, we consider Laurent polynomials in the positive orthant with fixed support and fixed coefficient signs. We provide a criterion to decide whether a given polynomial is copositive that is based in determining the intersection points of the signed discriminant and a path going through the coefficients of the polynomial. If the signed support satisfies a combinatorial condition termed nonseparability, we show additionally that this intersection consists of one point, and that tracking one path in homotopy continuation methods suffices to decide upon copositivity.   Building on these results, we show that any copositive polynomial with nonseparable signed support can be decomposed into a sum of nonnegative circuit polynomials, generalising thereby previously known supports having this property.",0,arxiv,Matematik,CC-BY/arXiv,"Copositivity, discriminants and nonseparable signed supports"
"This paper studies short brooms in edge-chromatic critical graphs. We prove that for any short broom in a $Î”$-critical graph, at most one color is missing at more than one vertex. Moreover, this color (if exists) is missing at exactly two vertices. Applying this result, we verify the Vertex-splitting Conjecture for graphs with $Î”\geq 2(n-1)/3$ and the Overfull Conjecture for $Î”$-critical graphs satisfying $Î”\geq (2n+5Î´-12)/3$.",0,arxiv,Matematik,CC-BY/arXiv,Short Brooms in Edge-chromatic Critical Graphs
"The representation theory of left regular band semigroup algebras is well-studied and known to have close connections with combinatorial topology, as established in the work of Margolis--Saliola--Steinberg ('15, '21). In this paper, we investigate the representation theory of the invariant subalgebras of left regular band semigroup algebras carrying the action of a finite group through the lens of group-equivariant combinatorial topology.   We characterize when the invariant subalgebra is semisimple or commutative and examine the equivariant structure of the Peirce components of the semigroup algebra. For CW left regular bands, we interpret these Peirce components in terms of the equivariant topology of intervals in the support semilattice, yielding the Cartan invariants of the invariant subalgebras of left regular bands associated to CAT(0)-cube complexes. We also give a topological formula for the Peirce components for left regular bands with hereditary algebras. Finally, in specializing to left regular bands associated to geometric lattices, we explore generalizations of the DesarmÃ©niÃ©n--Wachs derangement representation and their connections to Markov chains.",0,arxiv,Matematik,CC-BY/arXiv,Left regular bands with symmetry
"Let $G$ be a simple graph with order $n$, maximum degree $\D(G)$, minimum degree $Î´(G)$ and chromatic index $Ï‡'(G)$, respectively. A graph $G$ is called {\em $\D$-critical} if $Ï‡'(G)=\D(G)+1$ and $Ï‡'(H)\textless Ï‡'(G)$ for every proper subgraph $H$ of $G$, and $G$ is overfull if $\left|E(G)\right|>Î”(G)\lfloor n/2\rfloor$. In 1986, Chetwynd and Hilton proposed the Overfull Conjecture: Every $\D$-critical graph $G$ with $\D(G)\textgreater\frac{n}{3}$ is overfull. The Overfull Conjecture has many implications, such as that it implies a polynomial-time algorithm for determining the chromatic index of graphs $G$ with $\D(G)\textgreater\frac{n}{3}$, and implies several longstanding conjectures in the area of graph edge coloring. Recently, Cao, Chen, Jing and Shan (SIAM J. Discrete Math. 2022) verified the Overfull Conjecture for $\D(G)-7Î´(G)/4\ge (3n-17)/4$. In this paper, we improve it for $\D(G)-5Î´(G)/3\ge (2n-7)/3$.",0,arxiv,Matematik,CC-BY/arXiv,A new improvement to the Overfull Conjecture
"A Catalan word is one on the alphabet of positive integers starting with $1$ in which each subsequent letter is at most one more than its predecessor. Let $\mathcal{C}_n$ denote the set of Catalan words of length $n$. In this paper, we give combinatorial proofs of explicit formulas for the sums of several parameter values taken over all the members of $\mathcal{C}_n$. In particular, we find such proofs for the parameters tracking the number of symmetric or $\ell$-valleys, which was previously requested by Baril et al. Further, we find a combinatorial explanation of a related Catalan number identity whose proof was also requested. To carry out our arguments, we consider corresponding statistics on Dyck paths and find the cardinality of certain sets of marked Dyck paths wherein one or more of the steps is distinguished from all others.",0,arxiv,Matematik,CC-BY/arXiv,Combinatorial proofs of totals of some statistics on Catalan words
"We establish explicit formulas for Bell numbers and graphical Stirling numbers of complete multipartite graphs, complete bipartite graphs with removed perfect matchings, and Mycielskian trees. For complete multipartite graphs $K(n_1,\ldots,n_\ell)$, we provide a simplified proof that $B(G) = \prod_{i=1}^\ell \bell{n_i}$. We derive $B(K_{n,n} - M) = \sum_{k=0}^{n} \binom{n}{k} \bell{k}^2$ for removed perfect matching $M$, and for Mycielskian star graphs, $B(M(St_n); 3) = 2^n + 1$ and $B(M(St_n); 2n) = 2n^2 - 3n + 3$. Results extend to Mycielskians of arbitrary trees. Our computational verifications establish links between graphical Bell numbers and fundamental sequences in combinatorics and pattern avoidance, including identification of several OEIS entries: A000051, A096376, A116735, A384980, A384981, A384988, A385432, and A385437.",0,arxiv,Matematik,CC-BY/arXiv,Bell Numbers and Stirling Numbers of the Mycielskian of Trees
"In this paper, we characterize graphs with circular chromatic number less than 3 in terms of certain balancing labellings studied in the context of signed graphs. In fact, we construct a signed graph which is universal for all such labellings of graphs with circular chromatic number less than $3$, and is closely related to the generic circular triangle-free graph studied by Bodirsky and GuzmÃ¡n-Pro. Moreover, our universal structure gives rise to a representation of the relation algebra $56_{65}$. We then use this representation to show that the network satisfaction problem described by this relation algebra belongs to NP. This concludes the full classification of the existence of a universal square representation, as well as the complexity of the corresponding network satisfaction problem, for relation algebras with at most four atoms.",0,arxiv,Matematik,CC-BY/arXiv,"Circular Chromatic Numbers, Balanceability, Relation Algebras, and Network Satisfaction Problems"
"Komjath studied the list chromatic number of infinite graphs and introduced the notion of restricted list chromatic number. For a graph $X=(V_X,E_X)$ and a cardinal $Îº$, we say that $X$ is restricted list colorable for $Îº$ if for every $L:V_X\to[Îº]^Îº$ there is a choice function $c$ of $L$ such that $c(v)\neq c(w)$ whenever ${v,w}\in E_X$. In this paper, we discuss a variation, stationary list colorability for $Îº$, obtained by replacing $[Îº]^Îº$ with the set of all stationary subsets of $Îº$. We compare the stationary list colorability with other coloring properties. Among other things, we prove that the stationary list colorability is essentially different from other coloring properties including the restricted list colorability. We also prove the consistency result showing that for some $Îº<Î»$, restricted and stationary list colorability at $Îº$ do not imply the corresponding properties at $Î»$.",0,arxiv,Matematik,CC-BY/arXiv,Stationary list colorings
"We formulate a combinatorial version of the Intersection Hodge Conjecture for projective toric varieties. The conjecture asserts that the subspace of rational Hodge classes in the intersection cohomology $IH^*(X_Î£)$ is generated by the classes of algebraic cycles. We define the space of combinatorial Hodge classes, $Hdg^k_{\mathrm{comb}}(Î£) \subset IH^{2k}_{\mathrm{comb}}(Î£, \mathbb{Q})$, using the combinatorial intersection cohomology theory for fans developed by Barthel, Brasselet, Fieseler, and Kaup. We conjecture that this space is spanned by the combinatorial cycle classes corresponding to torus-invariant subvarieties. We verify this conjecture for all projective toric varieties of dimension $n \le 3$ and for the class of simplicial projective toric varieties. Finally, we provide an algorithmic framework to verify the conjecture for arbitrary rational fans.",0,arxiv,Matematik,CC-BY/arXiv,The Intersection Cohomology of a Fan and the Hodge Conjecture for Toric Varieties
"In the standard random graph process, edges are added to an initially empty graph one by one uniformly at random. A classic result by Ajtai, KomlÃ³s, and SzemerÃ©di, and independently by BollobÃ¡s, states that in the standard random graph process, with high probability, the graph becomes Hamiltonian exactly when its minimum degree becomes $2$; this is known as a \emph{hitting time} result. Johansson extended this result by showing the following: For a graph $G$ with $Î´(G) \geq (1/2+\varepsilon)n$, in the random graph process constrained to the host graph $G$, the hitting times for minimum degree $2$ and Hamiltonicity still coincide with high probability.   In this paper, we extend Johansson's result to Berge Hamilton cycles in hypergraphs. We prove that if an $r$-uniform hypergraph $H$ satisfies either $Î´_1(H) \geq (\frac{1}{2^{r-1}} + \varepsilon)\binom{n-1}{r-1}$ or $Î´_2(H) \geq \varepsilon n^{r-2}$, then in the random process generated by the edges of $H$, the time at which the hypergraph reaches minimum degree $2$ coincides with the time at which it contains a Berge Hamilton cycle with high probability. This generalizes the work of Bal, Berkowitz, Devlin, and Schacht, who established the result for the case where $H$ is a complete $r$-uniform hypergraph.",0,arxiv,Matematik,CC-BY/arXiv,Berge Hamilton cycles in a random sparsification of dense hypergraphs
"One of the generalizations of multiple zeta values is the $q$-version, and in the case of finite sums, they may be expressed explicitly in polynomial form. Several results have been found when the powers of the factors in the denominator are equal and when they are small. In this paper, we give explicit formulas for the case when the powers are unequal and are small.",0,arxiv,Matematik,CC-BY/arXiv,Some explicit values of a $q$-multiple zeta function whose denominator power is not uniform
"We study the totally nonnegative part of the Peterson variety in arbitrary Lie type and establish its connection to the strongly dominant weight polytope. In particular, we prove that the totally nonnegative part of the Peterson variety is a regular CW-complex, which is homeomorphic to a cube as a cell-decomposed space. This confirms a conjecture of Rietsch for all Lie types.",0,arxiv,Matematik,CC-BY/arXiv,Totally nonnegative Peterson variety and strongly dominant weight polytope
"This paper introduces the concept of generalized interlacing families of polynomials, which extends the classical interlacing polynomial method to handle polynomials of varying degrees. We establish a fundamental property for these families, proving the existence of a polynomial with a desired degree whose smallest root is greater than or equal to the smallest root of the expected polynomial. Applying this framework to the generalized CUR matrix approximation problem, we derive a theoretical upper bound on the spectral norm of a residual matrix, expressed in terms of the largest root of the expected polynomial. We then explore two important special cases: the classical CUR matrix decompositions and the row subset selection problem. For classical CUR matrix decompositions, we derive an explicit upper bound for the largest root of the expected polynomial. This yields a tighter spectral norm error bound for the residual matrix compared to many existing results. Furthermore, we present a deterministic polynomial-time algorithm for solving the classical CUR problem under certain matrix conditions. For the row subset selection problem, we establish the first known spectral norm error bound. This paper extends the applicability of interlacing families and deepens the theoretical foundations of CUR matrix decompositions and related approximation problems.",0,arxiv,Matematik,CC-BY/arXiv,Generalized Interlacing Families: New Error Bounds for CUR Matrix Decompositions
"We demonstrate three properties conjectured to hold for a certain function by Levin (2025) in a study of the blimpy graphical shape of the number of bit strings with a given score under an interesting scoring system. The properties include discrete convexity, a simple formula for the greatest argument at which the function is negative, and a positive expectation under a certain probability function. A new set of inequalities which imply the latter is presented and proved under some monotonicity assumptions.",0,arxiv,Matematik,CC-BY/arXiv,"Proof of a combinatorial conjecture posed in ""The Blimpy Shape of Heady-s and Taily-s Bit Strings"""
"We study the modular representation theory of rank $3$ association schemes arising from partial geometries with parameters $(s,t,Î±)$. First, we obtain an explicit closed formula for the Frame number of the point scheme in terms of the number of points $v$ and the parameter $s+t+1-Î±$, and use it to characterize the primes $p$ for which the adjacency algebra over $\mathbb{F}_p$ is not semisimple. We then give a complete case-by-case description of the Jacobson radical of this algebra in four arithmetic situations and determine the generic $p$-ranks of the adjacency matrices.   As a step toward understanding the modular representation theory of coherent configurations of type $[3,2;3]$ associated with strongly regular designs, we analyze the relationship between the modular structure of the point scheme and that of the design algebra. For the generalized quadrangle $\mathrm{GQ}(2,2)$ we obtain partial results on the structure of the $2$-modular adjacency algebra $\mathbb{F}_2 \mathfrak{X}$, and we explain the representation-theoretic difficulties that prevent a complete determination of its Wedderburn decomposition and Gabriel quiver, which remains open and is formulated as Problem~6.8.",0,arxiv,Matematik,CC-BY/arXiv,Frame Numbers and Jacobson Radicals for Partial Geometries and Related Coherent Configurations
"The Connes-Kreimer Hopf algebra of rooted trees is an operated Hopf algebra whose coproduct satisfies the classical Hochschild 1-cocycle condition. In this paper, we extend the setting from rooted trees to the space $H_{\rm RT}(X,Î©)$ of $(X,Î©)$-rooted trees, in which internal vertices are decorated by a set $Î©$ and leafs are decorated by $X \cup Î©$. We introduce a new coalgebra structure on $H_{\rm RT}(X,Î©)$ whose coproduct satisfies a weighted Hochschild 1-cocycle condition involving multiple operators, thereby generalizing the classical condition. A combinatorial interpretation of this coproduct is also provided. We then endow $H_{\rm RT}(X,Î©)$ with a Hopf algebra structure. Finally, we define weighted $Î©$-cocycle Hopf algebras, characterized by a Hochschild 1-cocycle condition with weights, and show that $H_{\rm RT}(X,Î©)$ is the free object in the category of $Î©$-cocycle Hopf algebras.",0,arxiv,Matematik,CC-BY/arXiv,Generalized Connes-Kreimer Hopf algebras on decorated rooted forests by weighted cocycles
"We derive Rogers--Ramanujan type partition identities at the fundamental weight $Î›_0$ for the exceptional affine types $G_2^{(1)}$, $D_4^{(3)}$, $F_4^{(1)}$, $E_6^{(2)}$, $E_6^{(1)}$, $E_7^{(1)}$ and $E_8^{(1)}$. Our starting point is the Dousse--Konan reformulation of the $(\mathrm{KMN})^2$ crystal character formula, applied to the level-one perfect crystal $B=B(Î¸)\sqcup B(0)$ of Benkart--Frenkel--Kang--Lee with ground element $Ï†\in B(0)$. This realizes the normalized character $e^{-Î›_0}\mathrm{ch} L(Î›_0)$ as generating functions of grounded $B$-colored partitions governed locally by the crystal energy. After principal specialization, we obtain a colored partition model subject to explicit difference, congruence, and initial conditions. On the product side, under the same specialization, the Weyl--Kac character formula yields an explicit Euler-type product, equivalently the generating function for partitions with parts in a concrete allowed set. Comparing the two specializations gives coefficientwise equalities of generating functions. A key computational feature is that the difference matrix can be produced from the crystal data without explicitly computing the energy function. For each type we tabulate the congruence data, forbidden initial parts, and the full difference matrix, and we provide reproducible coefficient checks.",0,arxiv,Matematik,CC-BY/arXiv,Rogers-Ramanujan type identities at $Î›_0$ from perfect crystals of exceptional quantum affine algebras
"We study the Pascal determinantal arrays $\PD_k$, whose entries $\PD_k(i,j)$ are the $k\times k$ minors of the lower-triangular Pascal matrix $P=( \binom{a}{b} )_{a,b\ge 0}$.   We prove an exact factorization of the row-wise log-concavity operator:   \[   \LC(\PD_k)=\PD_{k-1}\Had\PD_{k+1},   \]   where $\LC(a)_j=a_j^2-a_{j-1}a_{j+1}$ and $\Had$ denotes the Hadamard (entrywise) product.   This identity is established by an elementary manipulation of the Desnanot--Jacobi (Dodgson) identity in two adjacent positions.   We further prove a general inequality asserting that the log-concavity operator is submultiplicative under Hadamard products of log-concave arrays:   $\LC(A\Had X)\ge\LC(A)\Had\LC(X)$.   Combining the factorization with this inequality yields a uniform algebraic proof that every row of every array $\PD_k$ ($k\ge 1$) is infinitely log-concave, extending the celebrated theorem of McNamara and Sagan from Pascal's triangle ($\PD_1$) to the entire determinantal hierarchy.   Applications include the log-convexity of $\{\PD_k(i,j)\}_{k\ge 0}$ in the determinantal order $k$ and a family of determinantal Hadamard inequalities.",0,arxiv,Matematik,CC-BY/arXiv,A Factorization of the Log-Concavity Operator for Pascal Determinantal Arrays and Their Infinite Row-Wise Log-Concavity
"A weak deletion sequence is a sequence $(G_1,\ldots,G_n)$ of graphs so that for each $i\in[n-1]$ either $G_i$ is isomorphic to a subgraph of $G_{i+1}$, or vice versa: $G_{i+1}$ is isomorphic to a subgraph of $G_i$. We prove that determining the simultaneous planar embeddability of weak deletion sequences of $2$-connected graphs is NP-hard.",0,arxiv,Matematik,CC-BY/arXiv,Hardness of Planarity for Weak Temporal Sequences of 2-Connected Graphs
"A graph is $P_t$-free if it contains no induced subgraph isomorphic to a $t$-vertex path. A graph is not bipartite if and only if it contains an induced subgraph isomorphic to a $k$-vertex cycle, where $k$ is odd. We focus on the 3-coloring problem for $P_t$-free graphs that have only one prescribed induced odd cycle length. For any integer $t$ and any odd integer $k$, let $\mathcal{G}_{t,k}$ be the class of graphs that are $P_{t}$-free and all their induced odd cycles must be $C_k$. In this paper, we present a polynomial-time algorithm that solves the 3-coloring problem for any graph in $\mathcal{G}_{10,7}$.",0,arxiv,Matematik,CC-BY/arXiv,3-Coloring $P_t$-Free Graphs With Only One Prescribed Induced Odd Cycle Length
"We introduce a new family of polynomials, crystal skeleton polynomials, to better understand enumeration of standard Young tableaux, quasi-Yamanouchi tableaux and interactions with Gessel's expansion of a Schur function, quasi-crystals and crystal skeletons as Maas-GariÃ©py introduced in 2023. After developing calculus of those polynomials, we organize thoughts on major index, charge, depth, inversions with RSK correspondence and a bivariate factorial. Also, we revisit the theorem on internal zeros of fake degree polynomials by Billey--Konvalinka--Swanson (2020). These results altogether improve Gessel's expansion.",0,arxiv,Matematik,CC-BY/arXiv,"Crystal skeleton polynomials with major index, charge and depth"
"We analyze a two-player game in which players take turns avoiding the selection of certain points within a convex geometry. The objective is to prevent the convex closure of all chosen points from encompassing a predefined set. The first player forced into a move that results in the inclusion of this set loses the game. We redevelop a theoretical framework for these avoidance games and determine their nim numbers, including cases involving vertex geometries of trees, edge geometries of trees, and scenarios where the predefined set consists of extreme points.",0,arxiv,Matematik,CC-BY/arXiv,Impartial Avoidance Games on Convex Geometries
"Let $m$ be a positive integer and let $G$ be a graph. The zero-sum Ramsey number $R(G,\mathbb{Z}_m)$ is the least integer $N$ (if it exists) such that for every edge-coloring $Ï‡\, : \, E(K_N) \, \rightarrow \, \mathbb{Z}_m$ one can find a copy of $G$ in $K_N$ such that $\sum_{e \, \in \, E(G)}{Ï‡(e)} \, = \, 0$.   In this paper, we show that, for every prime $p$, $$R(F,\mathbb{Z}_p)\leq n+9p-12$$ for every forest $F$ in $n\geq 3p^2-12p+11$ vertices with $p\mid e(F)$.",0,arxiv,Matematik,CC-BY/arXiv,A linear upper bound on the zero-sum Ramsey number of forests in $\mathbb{Z}_p$
"The mim-width of a graph is a powerful structural parameter that, when bounded by a constant, allows several hard problems to be polynomial-time solvable - with a recent meta-theorem encompassing a large class of problems [SODA2023]. Since its introduction, several variants such as sim-width and omim-width were developed, along with a linear version of these parameters. It was recently shown that mim-width and all these variants all paraNP-hard, a consequence of the NP-hardness of distinguishing between graphs of linear mim-width at most 1211 and graphs of sim-width at least 1216 [ICALP2025]. The complexity of recognizing graphs of small width, particularly those close to $1$, remained open, despite their especially attractive algorithmic applications.   In this work, we show that the width recognition problems remain NP-hard even on small widths. Specifically, after introducing the novel parameter Omim-width sandwiched between omim-width and mim-width, we show that: (1) deciding whether a graph has sim-width = 1, omim-width = 1, or Omin-width = 1 is NP-hard, and the same is true for their linear variants; (2) the problems of deciding whether mim-width $\leq$ 2 or linear mim-width $\leq$ 2 are both NP-hard. Interestingly, our reductions are relatively simple and are from the Unrooted Quartet Consistency problem, which is of great interest in computational biology but is not commonly used (if ever) in the theory of algorithms.",0,arxiv,Matematik,CC-BY/arXiv,On the hardness of recognizing graphs of small mim-width and its variants
"Generalizing a variety of earlier problems on stable contracts in two-sided markets, Alkan and Gale introduced in 2003 a general stability model on a bipartite graph $G=(V,E)$ in which the vertices are interpreted as ``agents'', and the edges as possible ``contract'' between pairs of ``agents''. The edges are endowed with nonnegative capacities $b$ giving upper bounds on ``contract intensities'', and the preferencies of each ``agent'' $v\in V$ depend on a \emph{choice function} (CFs) that acts on the set of ``contracts'' involving $v$, obeying three well motivated axioms of \it{consistence}, \it{substitutability} and \it{cardinal monotonicity}. In their model, the capacities and choice functions can take reals or discrete values and, extending well-known earlier results on particular cases, they proved that systems of \it{stable} contracts always exist and, moreover, their set $\cal S$ constitutes a distributive lattice under a natural comparison relation $\prec$.   In this paper, we study Alkan--Gale's model when all capacities and choice functions take integer values. We characterize the set of rotations -- augmenting cycles linking neighboring stable assignments in the lattice $(\cal S,\prec)$, and construct a weighted poset in which the lattice the closed functions is isomorphic to $(\cal S,\prec)$, thus obtaining an explicit representation for the latter. We show that in general the size of the poset is at most $b^{\rm max}|E|$, where $b^{\rm max}$ is the maximal capacity, and the poset can be constructed in pseudo polynomial time. Then we explain that by imposing an additional condition on CFs, the size of the poset becomes polynomial in $|V|$, and the total time reduces to a polynomial in $|V|,\log b^{\rm max}$.",0,arxiv,Matematik,CC-BY/arXiv,A poset representation for stable contracts in a two-sided market generated by integer choice functions
"The boundary of the Milnor fiber associated with a complex line arrangement is a three dimensional plumbed manifold, and it is a combinatorial invariant. We prove the reverse implication, which was conjectured NÃ©methi and SzilÃ¡rd. That is, this boundary of the Milnor fiber determines the combinatorics of the arrangement. Furthermore, we give an explicit method which constructs the poset associated with the arrangement, given a plumbing graph in normal form for the boundary.",0,arxiv,Matematik,CC-BY/arXiv,The Milnor fiber boundary of an arrangement determines its combinatorics
"Seaweed algebras are a class of Lie algebras that are naturally characterized by a pair of compositions, which in turn are represented visually as planar graphs called meanders. These meanders provide a straightforward method for computing the index of the associated algebra. The goal of this paper is to enumerate those seaweed algebras with a fixed index and whose associated compositions have restricted part sizes. In particular, we enumerate those with composition part sizes from so-called acyclic sets. We also establish a bijection between sets of indecomposable seaweed algebras with meanders with certain restricted part sizes and sets of permutations with restricted displacements. In certain cases, the index of the algebra can be determined by a simple statistic on the permutation.",0,arxiv,Matematik,CC-BY/arXiv,Seaweed algebras with restricted part sizes
"We study the fully packed loop-$O(n)$ model on planar triangulations. This model is also bijectively equivalent to the Fortuin--Kasteleyn model of planar maps with parameter $q\in (0,4)$ at its self-dual point. These have been traditionally studied using either techniques from analytic combinatorics (based in particular on the gasket decomposition of Borot, Bouttier and Guitter arXiv:1106.0153) or probabilistic arguments (based on Sheffield's hamburger-cheeseburger bijection arXiv:1108.2241). In this paper we establish a dictionary relating quantities of interest in both approaches. This has several consequences. First, we derive an exact expression for the partition function of the fully packed loop-$O(n)$ model on triangulations, as a function of the outer boundary length. This confirms predictions by Gaudin and Kostov. In particular, this model exhibits critical behaviour, in the sense that the partition function exhibits a power-law decay characteristic of the critical regime at this self-dual point. This can be thought of as the planar map analogue of Nienhuis' predictions for the critical point of the loop-$O(n)$ model on the hexagonal lattice. Finally, we derive precise asymptotics for geometric features of the FK model of planar maps when $0 < q <4$, such as the exact tail behaviour of the perimeters of clusters and loops. This sharpens previous results of arXiv:1502.00450 and arXiv:1502.00546. A key step is to use the above dictionary and the probabilistic results to justify rigorously an ansatz commonly assumed in the analytic combinatorics literature.",0,arxiv,Matematik,CC-BY/arXiv,Critical behaviour of the fully packed loop-$O(n)$ model on planar triangulations
"Ricci curvature and its associated flow offer powerful geometric methods for analyzing complex networks. While existing research heavily focuses on applications for undirected graphs such as community detection and core extraction, there have been relatively less attention on directed graphs.   In this paper, we introduce a definition of Ricci curvature and an accompanying curvature flow for directed graphs. Crucially, for strongly connected directed graphs, this flow admits a unique global solution. We then apply this flow to detect strongly connected subgraphs from weakly connected directed graphs. (A weakly connected graph is connected overall but not necessarily strongly connected). Unlike prior work requiring graphs to be strongly connected, our method loosens this requirement. We transform a weakly connected graph into a strongly connected one by adding edges with very large artificial weights. This modification does not compromise our core subgraph detection. Due to their extreme weight, these added edges are automatically discarded during the final iteration of the Ricci curvature flow.   For core evaluation, our approach consistently surpasses traditional methods, achieving better results on at least two out of three key metrics. The implementation code is publicly available at https://github.com/12tangze12/Finding-core-subgraphs-on-directed-graphs.",0,arxiv,Matematik,CC-BY/arXiv,Finding core subgraphs of directed graphs via discrete Ricci curvature flow
We study monotone paths in ErdÅ‘s-RÃ©nyi random graphs on numbered vertices. Benjamini & Tzalik established a phase transition at $p = \frac{\log n}{n}$ for this model. We refine the critical value to $p = \frac{\log n - \log \log n }{n}$ and identify the critical window of order $Î˜(1/n)$.,0,arxiv,Matematik,CC-BY/arXiv,A phase transition in ErdÅ‘s-Barak random graphs
"The celebrated theorem of Kechris, Pestov and TodorÄeviÄ‡ connecting structural Ramsey theory with topological dynamics has as a consequence that the FraÃ¯ssÃ© limit of a Ramsey class of non-trivial finite relational structures has a reduct which is a total order; this implies an earlier result of NeÅ¡etÅ™il, according to which the structures in such a class are rigid (have trivial automorphism groups). In this paper, we give an alternative proof of this fact. If $\mathcal{C}$ is a FraÃ¯ssÃ© class of rigid structures over a finite relational language, then either the FraÃ¯ssÃ© limit of $\mathcal{C}$ has a reduct which is a total order, or there is an explicit failure of the Ramsey property involving a pair $(A,B)$ of structures in $\mathcal{C}$ with $|A|=2$.",0,arxiv,Matematik,CC-BY/arXiv,A footnote to the KPT theorem in structural Ramsey theory
"We give a complete classification of edge-to-edge tilings of the sphere by regular polygons under a unified framework. Without assuming convexity of the tiles or polyhedrality of the underlying graph, our proof is independent of the Johnson-Zalgaller classification of solids with regular faces (1967), which took over 200 pages. We apply a blend of trigonometric, algebraic and combinatorial tools of independent interest.",0,arxiv,Matematik,CC-BY/arXiv,Tiling the Sphere with Regular Polygons
"We introduce and study several affine (=annular in this paper) versions of the classical diagram algebras such as Temperley-Lieb, partition, Brauer, Motzkin, rook Brauer, rook, planar partition, and planar rook algebras. We give generators and relation presentation for them and their associated categories, study their representation theory, and the asymptotic behavior of tensor products of their representations in the monoid case.",0,arxiv,Matematik,CC-BY/arXiv,"Affine diagram categories, algebras and monoids"
"Let $F$ and $G$ be simple finite oriented graphs (without symmetric arcs). A graph $G$ is called $F$-irregular if any two distinct vertices in $G$ belong to a different number of subgraphs of $G$ isomorphic to $F$. In this paper, we investigate the problem of the existence of $\overrightarrow{C_n}$-irregular graphs, where $\overrightarrow{C_n}$ is an oriented circle of order $n$ (a strongly connected oriented graph that is formed from a simple undirected cycle $C_n$ on $n$ vertices by orienting each of its edges).   For every integer $n \ge 3$, we prove that there exists an infinite family of $\overrightarrow{C_n}$-irregular graphs. In addition, we show that the order of a non-trivial $\overrightarrow{C_3}$-irregular graph can be any integer not less than $10$ and nothing else. We also construct $\overrightarrow{C_4}$-irregular graphs of any order starting from $7$ and prove that there is no non-trivial $\overrightarrow{C_4}$-irregular graph of order less than $7$.",0,arxiv,Matematik,CC-BY/arXiv,On $\overrightarrow{C_{n}}$-irregular oriented graphs
"A graph $G = (V, E)$ is word-representable if there exists a word $w$ over the alphabet $V$ such that, for any two distinct vertices $x, y \in V$, $xy \in E$ if and only if $x$ and $y$ alternate in $w$. Two letters $x$ and $y$ are said to alternate in $w$ if, after removing all other letters from $w$, the resulting word is of the form $xyxy\dots$ or $yxyx\dots$ (of even or odd length). For a given set $R = \{r_1, r_2, \dots, r_k\}$ of jump elements, an undirected circulant graph $C_n(R)$ on $n$ vertices has vertex set $\{0, 1, \dots, n-1\}$ and edge set $ E = \left\{ \{i,j\} \;\middle|\; |i - j| \bmod n \in \{r_1, r_2, \dots, r_k\} \right\}, $ where $0 < r_1 < r_2 < \dots < r_k < \frac{n}{2}$. Recently, Kitaev and Pyatkin proved that every 4-regular circulant graph is word-representable. Srinivasan and Hariharasubramanian further investigated circulant graphs and obtained bounds on the representation number for $k$-regular circulant graphs with $2 \le k \le 4$. In addition to these positive results, their work also presents examples of non-word-representable circulant graphs. In this work, we study word-representability and the representation number of 5-regular circulant graphs via techniques from elementary number theory and group theory, as well as graph coloring, graph factorization and morphisms.",0,arxiv,Matematik,CC-BY/arXiv,On the Word-Representability of 5-Regular Circulant Graphs
We will construct solvable lattice models whose partition functions are Demazure characters. We will construct a crystal structure on the states of the model and prove that the states of the closed model form a Demazure crystal.,0,arxiv,Matematik,CC-BY/arXiv,Closed Colored Models and Demazure Crystals
"Given positive integers $k$ and $n$, we present methods to construct all groups of order at most $n$ that contain a Cayley set of size $k$, and to enumerate the Cayley sets of order $k$ in a given group, up to the action of the automorphism group. We use these methods to generate complete lists of pairwise nonisomorphic 3-valent Cayley graphs with at most 5000 vertices and 4-valent Cayley graphs with at most 1025 vertices.",0,arxiv,Matematik,CC-BY/arXiv,A census of Cayley graphs
"The $t$-connected ideal of a graph $G$ is generated by all connected induced subgraphs of $G$ with $t$ vertices. When $t = 2$, this coincides with the usual edge ideal of the graph. Following the work of Faridi et al., we give a classification of the graphs whose $t$-connected ideals are minimally resolved by their Scarf complex. We also consider the $t$-path ideal of a graph $G$ which is the ideal generated by all paths of length $t$ in $G$. In this case, we are able to give a classification of the same type for paths of length $t = 4$.",0,arxiv,Matematik,CC-BY/arXiv,Scarf complexes of connected and path ideals
"The Shannon capacity of graphs, introduced by Shannon in 1956 to model zero-error communication, asks for determining the rate of growth of independent sets in strong powers of graphs. Much is still unknown about this parameter, for instance whether it is computable. Recent work has established a dual characterization of the Shannon capacity in terms of the asymptotic spectrum of graphs. A core step in this duality theory is to shift focus from Shannon capacity itself to studying the asymptotic relations between graphs, that is, the asymptotic cohomomorphisms. Towards understanding the structure of Shannon capacity, we study the ""combinatorial complexity"" of asymptotic cohomomorphism. As our main result, we prove that the asymptotic cohomomorphism order is universal for all countable preorders. That is, we prove that any countable preorder can be order-embedded into the asymptotic cohomomorphism order (i.e. appears as a suborder). Previously this was only known for (non-asymptotic) cohomomorphism. Our proof is based on techniques from asymptotic spectrum duality and convex structure of the asymptotic spectrum of graphs. Our approach in fact leads to a new proof of the universality of (non-asymptotic) cohomomorphism.",0,arxiv,Matematik,CC-BY/arXiv,Universality of asymptotic graph homomorphism
"The $k$-core of a graph is its largest subgraph with minimum degree at least $k$, a fundamental concept for uncovering hierarchical structures. In this paper, we establish a connection between the $k$-core and the high-order spectra of graphs, a concept originally introduced by CvetkoviÄ‡, Doob, and Sachs. Specifically, we consider the high-order spectra defined via the $k$-adjacency tensor. Within this framework, we prove that a graph admits a non-empty $k$-core if and only if the spectral radius of the $k$-adjacency tensor is greater than or equal to $1$. Moreover, when the $k$-core exists, vertices corresponding to positive entries in the Perron vector of the $k$-adjacency tensor belong to the $k$-core. We thus define the $k$-order eigenvector centrality via the Perron vector, which provides both membership identification and a measure of relative influence within the $k$-core. Numerical experiments confirm our theoretical findings and illustrate the properties of this centrality measure in some real-world networks.",0,arxiv,Matematik,CC-BY/arXiv,The $k$-core of a graph and its high-order spectra
"A polytope is called indecomposable if it cannot be expressed (non-trivially) as a Minkowski sum of other polytopes. Since the concept was introduced by Gale in 1954, several increasingly strong criteria have been developed to characterize indecomposability. Our first contribution is a new indecomposability criterion that unifies and generalizes most of the previous techniques. The key new ingredient of our method is the introduction of the graph of (implicit) edge dependencies, which has broader applications in the study of deformation cones of polytopes, beyond indecomposability. One of our main applications is providing new indecomposable deformed permutahedra that are not matroid polytopes. In 1970, Edmonds posed the problem of characterizing the extreme rays of the submodular cone, that is, indecomposable deformed permutahedra. Matroid polytopes from connected matroids give one such family of polytopes. We provide a new infinite disjoint family by taking certain graphical zonotopes and deeply truncating 1 or 2 specific vertices. In this way, we construct $2 \lfloor\frac{n-1}{2}\rfloor$ new indecomposable deformations of the $n$-permutahedron in $\mathbb{R}^n$. We also showcase other applications of our tools. For example, we use them to refute a conjecture by Smilansky (1987) stating that an indecomposable polytope needs to have few vertices with respect to its number of facets. We provide bounds on the dimension of deformation cones and characterize certain of their rays, we introduce parallelogramic Minkowski sums whose deformation cone can be written as a product of deformation cones, and we construct indecomposable polytopes via truncations and stackings.",0,arxiv,Matematik,CC-BY/arXiv,Indecomposability and beyond via the graph of edge dependencies
"The Matsushita fundamental groups of a graph $X$, denoted $Ï€_1^r(X)$, are certain discrete versions of the fundamental group for topological spaces. For $r=2$, these groups have a nice combinatorial description, due to Sankar. In this paper we prove two results about $Ï€_1^2$. First, we prove a Seifert-van Kampen-type theorem. Similar results have previously been obtained by Barcelo, et al. (and strengthened by Kapulkin and Mavinkurve) for a different notion of discrete fundamental group. Second, we prove that an arbitrary group $G$ can be realized as $Ï€_1^2(X)$ for some graph $X$. Our construction works equally well for the aforementioned alternate discrete fundamental group, and our second result thus generalizes a theorem of Kapulkin and Mavinkurve which applies only to finitely presented groups $G$.",0,arxiv,Matematik,CC-BY/arXiv,On Matsushita $Ï€_1^2$ discrete fundamental groups
"We give a bijective correspondence between the number of nilpotent matrices over a Boolean semiring and the number of directed acyclic graphs on ordered vertices. We then enumerate pairs of maps between two finite sets whose composites are eventually constant by forming a bijection that relates a pair of such maps with a spanning tree in a complete bipartite graph, and an edge of said tree. This generalizes the main principle of A. Joyal's proof of Cayley's formula. Finally, we generalize T. Leinster's work by considering a pair of finite-dimensional vector spaces and show a bijectivity between a nilpotent pair of maps and a balanced vector with the hom spaces between them. This leads us to an elegant formula for the number of nilpotent pairs.",0,arxiv,Matematik,CC-BY/arXiv,Eventually constant maps for two sets and nilpotent pairs
"We compute the $cd$-index $Î¨_{cd}$ of matroid base polytopes $\mathscr{P}(M)$ for a large family of matroids $M$. The $cd$-index is a polynomial in two non-commutative variables that compactly encodes the count of face flags $\mathcal{F} = \{Ïƒ_1 \subset \dots \subset Ïƒ_s \}$ with prescribed $\dim Ïƒ_i = d_i$. This comprises the $f$-vector of $\mathscr{P}(M)$, which recently Ferroni and SchrÃ¶ter treated as an almost-valuative invariant; i.e. a valuative part plus an error term. We initiate a similar program for $Î¨_{cd}(\mathscr{P}(M))$ and show that for an elementary split matroid $M$ the error term in the computation of $Î¨_{cd}(\mathscr{P}(M))$ surprisingly depends only on modular pairs of cyclic flats.   This allows us to implement computations requiring only the counts $Î»(r,h)$ and $Î¼(Î±,Î²,a,b)$ of cyclic flats and modular pairs of cyclic flats, respectively, that fulfill some rank and cardinality conditions. We illustrate the methods with sparse paving matroids.",0,arxiv,Matematik,CC-BY/arXiv,The $cd$-index of base polytopes for connected split matroids
"We show that the separating systole of high genus triangulations is of logarithmic order (in the size of the triangulation). Our methods also allow us to show an enumerative result, i.e. the convergence of the ""genus ratio"" for high genus triangulations. This complements the convergence of the ""size ratio"" that was proven in previous work with Budzinski.",0,arxiv,Matematik,CC-BY/arXiv,The separating systole and the genus ratio of high genus triangulations
"This paper discusses what the dimension data of irreducible representations of a finite group looks like in some specific cases, including unipotent and reductive groups over finite fields.   The essence of our investigation is whether the dimension data of irreducible representations of a finite group can be ``geometerized'', to become equal to the cardinality of certain orbit spaces.   The first part of this paper deals with nilpotent and reductive groups over finite fields, whereas the second part deals with the symmetric group $S_n$. The main conclusion that we want to bring out to contrast these two cases is that for reductive groups over finite fields, the dimension data is concentrated (in a statistical sense) in a neighborhood of the maximal dimension, whereas for the symmetric group, it is spread out.",0,arxiv,Matematik,CC-BY/arXiv,Dimension statistics of representations of finite groups
"The legendary Mario and Luigi show us that whether you slap in the crossings as early as a warp pipe can shoot you or as late as the very last bend, the water system in Yoshi Hill comes out exactly the same!",0,arxiv,Matematik,CC-BY/arXiv,Plumbing bijections
"The chromatic threshold $Î´_Ï‡(H)$ of a graph $H$ is the infimum of $d>0$ such that the chromatic number of every $n$-vertex $H$-free graph with minimum degree at least $dn$ is bounded in terms of $H$ and $d$. A breakthrough result of Allen, BÃ¶ttcher, Griffiths, Kohayakawa, and Morris determined $Î´_Ï‡(H)$ for every graph $H$; in particular, if $Ï‡(H)=r\ge 3$, then $Î´_Ï‡(H) \in\{\frac{r-3}{r-2},~\frac{2 r-5}{2 r-3},~\frac{r-2}{r-1}\}$.   In this paper we investigate the trade-off between minimum degree and edge density in the critical window around the chromatic threshold. For a fixed graph $H$ with $Ï‡(H)=r$, allowing a constant deficit below $Î´_Ï‡(H)$, we prove sharp (up to lower-order terms) upper bounds on the edge density of $n$-vertex $H$-free graphs whose chromatic number diverges. Equivalently, within this degree regime we show that a suitable global bound on the number of edges forces the chromatic number to remain bounded. Our results thus quantify how global edge density can compensate for a deficit in the local minimum-degree condition near $Î´_Ï‡(H)$; more specifically, we obtain explicit bounds in two of the three possible cases arising in the trichotomy of $Î´_Ï‡(H)$. Our extremal constructions -- based on ErdÅ‘s graphs and blowups of Borsuk--Hajnal graphs -- show that these bounds are best possible up to $o(n^2)$ terms.",0,arxiv,Matematik,CC-BY/arXiv,Edge density and minimum degree thresholds for $H$-free graphs with unbounded chromatic number
The Clifford defect is a rational number associated to the Weierstrass semigroup at a given point of an algebraic curve. It describes the error-correcting capability of the so-called Modified Algorithm for decoding the corresponding one-point codes defined at the point. This defect also finds applications in other contexts involving one-point codes. We study the Clifford defect of some numerical semigroups arising from curves and give explicit formulas for them.,0,arxiv,Matematik,CC-BY/arXiv,The Clifford defect of a numerical semigroup
"A well-known consequence of Schur's theorem is that for $r\in \mathbb{N}$, if $n$ is sufficiently large, then any $r$-colouring of $[n]$ results in monochromatic $a,b,c\in [n]$ such that $ab=c$. In this paper we are interested in the threshold at which the binomial random set $[n]_p$ almost surely inherits this Ramsey-type property. In particular for $r=2$ colours, we show that this threshold lies between $n^{-1/9-o(1)}$ and $n^{-1/11}$. Whilst analogous questions for solutions to (sets of) linear equations are now well understood, our work suggests that both the behaviour of the thresholds and the proof methods needed to determine them differ substantially in the non-linear setting.",0,arxiv,Matematik,CC-BY/arXiv,Monochromatic products in random integer sets
"We prove that on a very general principally polarized abelian 6-fold, the smallest multiple of the minimal curve class which can be represented by an algebraic cycle is 6.",0,arxiv,Matematik,CC-BY/arXiv,Optimality of the Prym-Tyurin construction for $\mathcal{A}_6$
"This paper investigates limit points of the deformed Laplacian matrix, which merges the Laplacian and signless Laplacian matrices of a graph through a quadractic one-parameter family of matrices. First, we show that any value greater or equal to 1 is a deformed Laplacian limit point (for different values of the parameter $s$) using a simple family of trees. Second, we define $(T_k)_{k \in \mathbb{N}}$ the Shearer's sequence of caterpillars for $Î»>1$ and we present a convergence criterion based on Shearer's approach. Our main result is that for any fixed value $Î»_0>1$ there exists a unique value $0<s^* <\sqrt{Î»_0} -1$ such that, and for any $s \in (0,s^*)$ the interval $[Î»_0, \; +\infty)$ is entirely formed by $s$-deformed Laplacian limit points (for the same value of $s$). Finally, we provide some numerical data exploring the limit properties.",0,arxiv,Matematik,CC-BY/arXiv,Distribution of deformed Laplacian limit points
"Let $C_{s,t}$ be the complete bipartite geometric graph, with $s$ and $t$ vertices on two distinct parallel lines respectively, and all $s t$ straight-line edges drawn between them. In this paper, we show that every complete bipartite simple topological graph, with parts of size $2(k-1)^4 + 1$ and $2^{k^{5k}}$, contains a topological subgraph weakly isomorphic to $C_{k,k}$. As a corollary, every $n$-vertex simple topological graph not containing a plane path of length $k$ has at most $O_k(n^{2 - 8/k^4})$ edges. When $k = 3$, we obtain a stronger bound by showing that every $n$-vertex simple topological graph not containing a plane path of length 3 has at most $O(n^{4/3})$ edges. We also prove that $x$-monotone simple topological graphs not containing a plane path of length 3 have at most a linear number of edges.",0,arxiv,Matematik,CC-BY/arXiv,Unavoidable patterns and plane paths in dense topological graphs
"An ErdÃ¶s matrix $E$ is a bistochastic matrix whose sum of squares of entries (Frobenius norm squared) equals its maxtrace (maximum of all the $Ïƒ$-traces for permutations $Ïƒ$'s). We characterize all ErdÃ¶s $E$ by the patterns of their zero entries; showing that each such skeleton has at most one $E$. We present an algorithm to find all $n\times n$ ErdÃ¶s matrices, which finds them up to $n\leqslant 5$ quickly and also size $n=6$. We further show some presently known RCDS matrices to be ErdÃ¶s.",0,arxiv,Matematik,CC-BY/arXiv,Characterization of ErdÃ¶s matrices by their zero entries
"This study presents a new formula for umbral operators which provides three key insights. First, it clarifies a connection between umbral calculus and iteration theory. Second, it paves the way for a definition of fractional exponents of umbral operators. And lastly, its proof synthesizes a multitude of existing operational calculus results that demonstrates a new level of effectiveness in the field. We demonstrate its application through a new and natural extension of the Laguerre polynomials.",0,arxiv,Matematik,CC-BY/arXiv,On formulas and fractional exponents for umbral operators
"The metric dimension of a graph is the minimum number of landmark vertices required so that every vertex can be uniquely identified by its distances to the landmarks. This parameter captures the fundamental tradeoff between compact information encoding and unambiguous identification in networked systems. In this work, we determine exact value for the metric dimension of the Cartesian product $K_{1,m} \square K_{1,n}$, also known as hub-and-spoke grids, across all values of $m$ and $n$. In addition, we present a constructive linear-time algorithm that builds a minimum resolving set, providing both theoretical guarantees and practical feasibility. We complement our results with visualization of parameter regimes that illustrate the design space. The findings establish design rules for minimizing landmark sensors and support applications in graph-based localization, monitoring networks, and intelligent information systems. Our results extend the theory of metric dimension and contribute efficient methods of direct relevance to information science and computational graph theory.",0,arxiv,Matematik,CC-BY/arXiv,Metric dimension of Cartesian product of stars
"We show that the notion of MAT-freeness for hyperplane arrangements depends on the underlying field. In particular, MAT-freeness is not combinatorial.",0,arxiv,Matematik,CC-BY/arXiv,MAT-Freeness is not combinatorial
In this work we take a step towards characterising strongly flip-flat classes of graphs. Strong flip-flatness appears to be the analogue of uniform almost-wideness in the setting of dense classes of graphs. We prove that strongly flip-flat classes of graphs that are weakly sparse are indeed uniformly almost-wide.,0,arxiv,Matematik,CC-BY/arXiv,Weakly-sparse and strongly flip-flat classes of graphs are uniformly almost-wide
"In this paper, we generalize the special subset of the Markov-Lagrange spectrum (and the Markov spectrum) called the discrete Markov spectrum. The discrete Markov spectrum is defined in terms of the Markov numbers, which arise as positive integer solutions to the Markov equation $x^2 + y^2 + z^2 = 3xyz.$ Using the tool called snake graphs, originating from cluster algebra theory, we first reconstruct proofs of its properties in a combinatorial framework and then extend it to the generalized setting. We then introduce the generalized discrete Markov spectrum, defined analogously via the generalized Markov numbers, which arise as positive integer solutions to the generalized Markov equation $x^2 + y^2 + z^2 + k_1 yz + k_2 zx + k_3 xy = (3 + k_1 + k_2 + k_3) xyz.$ We prove that this generalized spectrum is contained in the Markov-Lagrange spectrum and thus the Markov spectrum.",0,arxiv,Matematik,CC-BY/arXiv,Generalized discrete Markov spectra
"For a positive integer $k$, the \emph{ total $k$-cut complex} of a graph $G$, denoted as $Î”_k^t(G)$, is the simplicial complex whose facets are $Ïƒ\subseteq V(G)$ such that $|Ïƒ| = |V(G)|-k$ and the induced subgraph $G[V(G) \setminus Ïƒ]$ does not contain any edge. These complexes were introduced by Bayer et al.\ in \cite{Bayer2024TotalCutcomplex} in connection with commutative algebra. In the same paper, they studied the homotopy types of these complexes for various families of graphs, including cycle graphs $C_n$, squared cycle graphs $C_n^2$, and Cartesian products of complete graphs and path graphs $K_m \square P_2$ and $K_2 \square P_n$. In this article, we extend the work of Bayer et al.\ for these families of graphs.  We focus on the complexes $Î”_2^t(G)$ and determine the homotopy types of these complexes for three classes of graphs: (i) $p$-th powers of cycle graphs $C_n^p$ (ii) $K_m \square P_n$ and (iii) $K_m \square C_n$. Using discrete Morse theory, we show that these complexes are homotopy equivalent to wedges of spheres. We also give the number and dimension of spheres appearing in the homotopy type. Our result on powers of cycle graphs $C_n^p$ proves a conjecture of Shen et al.\ about the homotopy type of the complexes $Î”_2^t(C_n^p)$.",0,arxiv,Matematik,CC-BY/arXiv,Total $2$-cut complexes of powers of cycle graphs and Cartesian products of certain graphs
"We introduce a new family of symmetric polynomials $\mathfrak{G}^{(\mathbf{u},\mathbf{v})}_Î»$ arising from exactly solvable lattice models associated with the quantised loop algebra $\mathcal{U}_{q}(\mathfrak{sl}_{2}[z^\pm])$. The polynomials $\mathfrak{G}^{(\mathbf{u},\mathbf{v})}_Î»$ unify $q$-Whittaker polynomials, inhomogeneous $q$-Whittaker polynomials, Grothendieck polynomials and their duals. Using Yang--Baxter equation, we derive Cauchy identities and combinatorial formulas for the transition coefficients.",0,arxiv,Matematik,CC-BY/arXiv,Inhomogeneous $q$-Whittaker Polynomials I: Duality and Expansions
"We settle the Polynomial Freiman--Ruzsa (PFR/Marton) conjecture for the integers and for cyclic groups. More precisely, we show that if $A$ is a finite subset of $\mathbb{Z}$ or $\mathbb{Z}/N\mathbb{Z}$ with $|A+A| \le K|A|$, then there is a subgroup $H$ of index at most $K^{O(1)}$ such that $A$ is contained in at most $K^{O(1)}$ cosets of $H$. The proof is based on a new spectral stability dichotomy for the $L^4$ Fourier mass of $\mathbf{1}_A$: either this mass is concentrated on a span of size $K^{O(1)}$, or, after passing to a quotient of codimension $K^{O(1)}$, the doubling constant of the image of $A$ decreases by a definite power of $K$. Using Freiman modeling we transfer this dichotomy to cyclic groups, obtain polynomial Bogolyubov-type bounds, and deduce Marton's conjecture in $\mathbb{Z}$ and $\mathbb{Z}/N\mathbb{Z}$. As a corollary, we also recover and extend the finite-field formulation of Marton's conjecture: in odd characteristic we obtain a direct spectral proof, and together with the characteristic-2 result of Green, Gowers, Manners, and Tao this yields a complete resolution of the conjecture for all finite fields. For context beyond finite fields, we recall their theorem for abelian groups of bounded exponent.",0,arxiv,Matematik,CC-BY/arXiv,The Polynomial Freiman-Ruzsa (Marton) Conjecture in Integers and Finite Fields via Spectral Stability
"Stein (2020) conjectured that for any positive integer $k$, every oriented graph of minimum semi-degree greater than $k/2$ contains every oriented path of length $k$. This conjecture is true for directed paths by a result from Jackson (JGT, 1981). In this paper, we establish the validity of Stein's conjecture specifically for any oriented path with two blocks, where, a block of an oriented path $P$ refers to a maximal directed subpath within $P$.",0,arxiv,Matematik,CC-BY/arXiv,Paths with two blocks in oriented graphs of large minimum semi-degree
"Given a function $p : V(G)\to \mathbb N$ and an integer $k\ge 0$, define $p_k(G)$ as the number of vertices with $p(v)\ge k$. We say that $p_k(G)$ is bounded for all $\HH$-free graphs if there exists a constant $c=c(\HH)$ such that $p_k(G)<c$ for all such graphs $G$. Here, a graph $G$ is said to be $\HH$-free if it contains no member of $\HH$ as an induced subgraph. When $p$ represents the degree of a vertex, Ramsey's theorem implies that $p_0(G)$ is bounded for every $\{K_n, E_n\}$-free graphs, where $K_n$ and $E_n$ denote the complete graph and the edgeless graph on $n$ vertices, respectively. The connected version of Ramsey's theorem says that $p_0(G)$ is bounded for all $\{K_n, P_n, K_{1,n}\}$-free connected graphs, where $P_n$ and $K_{1,n}$ are the $n$-vertex path and the star with $n$ leaves. In this paper, we extend the Ramsey's theorem to $p_2(G)$ where $p$ denotes the degree, the local independent number, the local component number, and sharp degree, that is, we characterize the forbidden family of graphs $\HH$ such that $p_2(G)$ is bounded for all (connected) $\HH$-free graphs. Moreover, we also characterize the forbidden family of graphs $\HH$ for which there is a constant $c=c(\HH)$ such that $p_c(G)$ is bounded for all $\HH$-free graphs.",0,arxiv,Matematik,CC-BY/arXiv,Unavoidable induced subgraphs forced by graphs with many vertices of prescribed properties
"For given graphs $G_{1}, G_{2}$ and $G$, let $G\rightarrow (G_{1}, G_{2})$ denote that each red-blue-coloring of $E(G)$ yields a red copy of $G_{1}$ or a blue copy of $G_{2}$. Arag{Ã£}o, Marciano and Mendon{\c c}a [L. Arag{Ã£}o, J. Pedro Marciano and W. Mendon{\c c}a, Degree conditions for Ramsey goodness of paths, {\it European Journal of Combinatorics}, {\bf 124} (2025), 104082] proved the following. Let $G$ be a graph on $N\geq (n- 1)(m- 1)+ 1$ vertices. If $Î´(G)\geq N- \lceil n/2\rceil$, then $G\rightarrow (P_{n}, K_{m})$, where $P_{n}$ is a tree on $n$ vertices. In this note, we generalize $P_{n}$ to any tree $T_{n}$ with $n$ vertices, and improve the lower bound of $Î´(G)$. We further improve the lower bound when $T_{n}\neq K_{1, n- 1}$, which partially confirms their conjecture.",0,arxiv,Matematik,CC-BY/arXiv,A note on degree conditions for Ramsey goodness of trees
"In this paper, we find necessary and sufficient conditions for the Law of Large Numbers of averaged empirical measures of $N$-particle ensembles, in terms of the asymptotics of their Bessel generating functions, in the fixed temperature regime. This settles an open problem posed by Benaych-Georges, Cuenca and Gorin. For one direction, we use the moment method through Dunkl operators, and for the other we employ a special case of the formula of Chapuy--Dolega for the generating function of infinite constellations. As applications, we prove that the LLN for $Î¸$-sums and $Î¸$-corners of random matrices are given by the free convolution and free projection, respectively, regardless of the value of inverse temperature parameter $Î¸$. We also prove the LLN for a time-slice of the $Î¸$-Dyson Brownian motion.",0,arxiv,Matematik,CC-BY/arXiv,Law of Large Numbers for continuous $N$-particle ensembles at fixed temperature
"A combinatorial neural code is a subset of the power set $2^{[n]}$ on $[n]=\{1,\dots, n\}$, in which each $1\leq i\leq n$ represents a neuron and each element (codeword) represents the co-firing event of some neurons. Consider a space $X\subseteq\mathbb{R}^d$, simulating an animal's environment, and a collection $\mathcal{U}=\{U_1,\dots,U_n\}$ of open subsets of $X$. Each $U_i\subseteq X$ simulates a place field which is a specific region where a place cell $i$ is active. Then, the code of $\mathcal{U}$ in $X$ is defined as $\text{code}(\mathcal{U},X)=\left\{Ïƒ\subseteq[n]\bigg|\bigcap_{i\inÏƒ} U_i\setminus\bigcup_{j\notinÏƒ}U_j\neq\varnothing\right\}$. If a neural code $\mathcal{C}=\text{code}(\mathcal{U},X)$ for some $X$ and $\mathcal{U}$, we say $\mathcal{C}$ has a realization of open subsets of some space $X$. Although every combinatorial neural code obviously has a realization by some open subsets, determining whether it has a realization by some open convex subsets remains unsolved. Many studies attempted to tackle this decision problem, but only partial results were achieved. In fact, a previous study showed that the decision problem of convex neural codes is NP-hard. Furthermore, the authors of this study conjectured that every convex neural code can be realized as a minor of a neural code arising from a representable oriented matroid, which can lead to an equivalence between convex and polytope convex neural codes. Even though this conjecture has been confirmed in dimension two, its validity in higher dimensions is still unknown. To advance the investigation of this conjecture, we provide a complete characterization of the covering relations within the poset $\mathbf{P_{Code}}$ of neural codes.",0,arxiv,Matematik,CC-BY/arXiv,Covering Relations in the Poset of Combinatorial Neural Codes
"Given integers $m\le c$ and an exact $c$-coloring of the edges of a complete countably infinite graph (i.e. a coloring that uses exactly $c$ colors), must there be an infinite subgraph that is exactly $m$-colored? Using the Infinite Ramsey Theorem It is easy to show that the statement is true if $m=1,2$ or $c$. Erickson conjectured that it is false in all other cases. Stacey and Weidl proved that for each $m\ge 3$ there is some large enough $C(m)$ such that the conjecture is true for all pairs $(c,m)$ with $c>C(m)$. The main aim of this paper is to show that for all large enough $m$ the conjecture holds for all $c>m$. This reduces the number of cases needed to fully verify the conjecture to a finite number.",0,arxiv,Matematik,CC-BY/arXiv,Exactly Colored Complete Subgraphs of Infinite Graphs
"We consider a random geometric graph process where random points $(X_i)_{i \ge 1}$ are embedded consecutively in the $d$-dimensional unit torus $\mathbb{T}^d$, and every two points at distance at most $r$ form an edge. As $r\to 0$, we confirm that well-known hitting time results for $k$-connectivity (with $k\ge 1$ fixed) and Hamiltonicity in the ErdÅ‘s-RÃ©nyi graph process also hold for the considered geometric analogue. Moreover, we exhibit a sort of probabilistic monotonicity for each of these properties.   We also study a geometric analogue of the power of choice where, at each step, an agent is given two random points sampled independently and uniformly from $\mathbb{T}^d$ and must add exactly one of them to the already constructed point set. When the agent is allowed to make their choice with the knowledge of the entire sequence of random points (offline 2-choice), we show that they can construct a connected graph at the first time $t$ when none of the first $t$ pairs of proposed points contains two isolated vertices in the graph induced by $(X_i)_{i=1}^{2t}$, and maintain connectivity thereafter by following a simple algorithm. We also derive analogous results for $k$-connectivity and Hamiltonicity. This shows that each of the said properties can be attained two times faster (time-wise) and with four times fewer points in the offline 2-choice process compared to the 1-choice process.   In the online version where the agent only knows the process until the current time step, we show that $k$-connectivity and Hamiltonicity cannot be significantly accelerated (time-wise) but may be realised on two times fewer points compared to the 1-choice analogue.",0,arxiv,Matematik,CC-BY/arXiv,"Sharp thresholds, hitting times and the power of choice for random geometric graphs"
"We investigate the additive theory of the set $S = \{1^c, 2^c, \dots, N^c\}$ when $c$ is a real number. In the language of additive combinatorics, we determine the asymptotic behaviour of the additive energy of $S$. When $c$ is rational, this is either known, or follows from existing results, and our contribution is a resolution of the irrational case. We deduce that for all $c \not \in \{0, 1, 2\}$, the cardinality of the sumset $S + S$ asymptotically attains its natural upper bound $N(N + 1)/2$, as $N \to \infty$. We show that there are infinitely many, effectively computable numbers $c$ such that the set $\{p^c : \textrm{$p$ prime}\}$ is additively dissociated (actually linearly independent over $\mathbb{Q}$), and we provide an effective procedure to compute the digits of such $c$.",0,arxiv,Matematik,CC-BY/arXiv,Additive relations in irrational powers
"We introduce a new broadly unifying family of combinatorial objects, which we call permutation flows, associated to an acyclic directed graph $G$ together with a framing $F$. This new family is combinatorially rich and contains as special cases various families of combinatorial objects that are frequently studied in the literature, as is the case of permutations, circular permutations, multipermutations, Stirling permutations, Catalan objects and their generalizations. When permutation flows are decorated with compatible shuffles, they also include the combinatorics of parking functions and their generalizations.   This model is geometrically rich. We show that permutation flow shuffles define a family of unimodular triangulations of the flow polytope $F_G(a)$ on $G$ with an integer balanced netflow vector a where only the last entry is negative. As an application we provide a new proof of the Lidskii volume formula of Baldoni and Vergne for this family of polytopes and a reformulation of the same formula where every term is explained by the nature of the combinatorial objects involved. Permutation flow triangulations extend the Danilov, Karzanov, and Koshevoy triangulations that were defined for the case where a=e_0-e_n. We provide a formula for the h^*-polynomial of the flow polytope as the descent enumerating polynomial of permutation flows.   The model comes with an order structure induced by intuitive operators on permutation flows which we call the weak order. This order includes as special cases the weak order on permutations, the Tamari lattice, order ideals in Young's lattice, and their generalizations, among others. It was conjectured in 2020 by the three authors, together with Benedetti, Harris, and Morales, that this poset is in general a lattice. This conjecture has been recently established with independent proofs by Bell and Ceballos, and by Berggren and Serhiyenko.",0,arxiv,Matematik,CC-BY/arXiv,Permutation Flows I: Triangulations of Flow Polytopes (Research Announcement)
"In 2014, Keevash proved the existence of $(n,q,r)$-Steiner systems (equivalently $K_q^r$-decompositions of $K_n^r$) for all large enough $n$ satisfying the necessary divisibility conditions. In 2021, Glock, KÃ¼hn, and Osthus proposed a generalization of this result. Namely they conjectured a hypergraph version of Nash-Williams' Conjecture positing that if a $K_q^r$-divisible $r$-graph $G$ on $n$ vertices has minimum $(r-1)$-degree (denoted $Î´(G)$ hereafter) at least $\left(1-Î˜_r\left(\frac{1}{q^{r-1}}\right)\right) \cdot n$, then $G$ admits a $K_q^r$-decomposition.   The best known progress on this conjecture dates to the second proof of the Existence Conjecture by Glock, KÃ¼hn, Lo, and Osthus wherein they showed that $Î´(G)\ge \left(1-\frac{c}{q^{2r}}\right)\cdot n$ suffices for large enough $n$, where $c$ is a constant depending on $r$ but not $q$. As for the fractional relaxation, the best known bound is due to Delcourt, Lesgourgues, and the second author, who proved that $Î´(G)\ge \left(1-\frac{c}{q^{r-1 + o(1)}}\right)\cdot n$ guarantees a $K_q^r$-fractional decomposition.   We prove that for every integer $r\ge 2$, there exists a real $c>0$ such that if a $K_q^r$-divisible $r$-graph $G$ satisfies $Î´(G)\ge \max\left\{ Î´_{K_q^r}^* + \varepsilon,~~1 -\frac{c}{\binom{q}{r-1}} \right\} \cdot n$, then $G$ admits a $K_q^r$-decomposition for all large enough $n$, where $Î´_{K_q^r}^*$ denotes the fractional $K_q^r$-decomposition threshold. Combined with the fractional result above, this proves that $\left(1-\frac{c}{q^{r-1 + o(1)}}\right)\cdot n$ suffices for the Hypergraph Nash-Williams' Conjecture, approximately confirming the correct order of $q$. Our proof uses the newly developed method of refined absorption; we also develop a non-uniform TurÃ¡n theory to prove the existence of many embeddings of absorbers which may be of independent interest.",0,arxiv,Matematik,CC-BY/arXiv,On the Hypergraph Nash-Williams' Conjecture
"We find a layered permutation $w\in S_n$ whose Schubert polynomial $\mathfrak S_w(x_1, \dots, x_n)$ has support of size asymptotically at least $n!/4^n$. This gives precise asymptotics for the growth rate of $Î²(n):= \max_{w\in S_n}|\mathrm{supp}(\mathfrak S_w)|$. We find a different layered permutation $w\in S_n$ whose Grothendieck polynomial has support of size asymptotically at least $n!/e^{\sqrt{2n} \cdot \ln(n)}$ and obtain more precise asymptotics for the growth rate of $Î²^{\mathfrak G}(n):=\max_{w\in S_n}|\mathrm{supp}(\mathfrak G_w)|$.",0,arxiv,Matematik,CC-BY/arXiv,Asymptotically maximal Schubitopes
"The $k$-Markov numbers, introduced by Gyoda and Matsushita, are those which appear in positive integral solutions to $x^2 + y^2 + z^2 + k(xy + xz + yz) = (3+3k)xyz$. When $k =0$, this recovers the ordinary Markov numbers. A long-standing question in the theory of Markov numbers is Frobenius's unicity conjecture, concerning whether every Markov number is the maximum in a unique solution triple. Aigner gave a series of weaker, related conjectures which were confirmed to be true by Lee, Li, Rabideau, and Schiffler using techniques from the theory of cluster algebras. We show here that $k$-Markov numbers also satisfy Aigner's conjectures.",0,arxiv,Matematik,CC-BY/arXiv,Orderings of k-Markov Numbers
"Universal cover in $\mathbb{E}^{n}$ is a measurable set that contains a congruent copy of any set of diameter 1. Lebesgue's universal covering problem, posed in 1914, asks for the convex set of smallest area that serves as a universal cover in the plane ($n=2$).   A simple universal cover in $\mathbb{E}^n$ is provided by the classical theorem of Jung, which states that any set of diameter 1 in an $n$-dimensional Euclidean space is contained in a ball $J_n$ of radius $\sqrt{\tfrac{n}{2n+2}}$; in other words, $J_n$ is a universal cover in $\mathbb{E}^n$.   We show that in high dimensions, Jung's ball $J_n$ is asymptotically optimal with respect to the volume, namely, for any universal cover $U \subset \mathbb{E}^n$, $$ {\rm Vol}(U) \ge (1-o(1))^n{\rm Vol}(J_n). $$",0,arxiv,Matematik,CC-BY/arXiv,On asymptotic Lebesgue's universal covering problem
"We study the geometric properties of graphs with non-negative Ollivier-Ricci curvature, a discrete analogue of non-negative Ricci curvature in Riemannian geometry. We prove that for each $d<\infty$ there exists a constant $C_d$ such that if $G=(V,E)$ is a finite graph with non-negative Ollivier-Ricci curvature and with degrees bounded by $d$ then the average log-volume growth and random walk displacement satisfy \[   \frac{1}{|V|} \sum_{x\in V} \log \#B(x,r) \leq \exp\left[C_d \sqrt{\log r}\right] = r^{o(1)} \] and \[   \frac{1}{|V|} \sum_{x\in V} \mathbf{E}_x [d(X_0,X_n)^2] \leq n \exp\left[C_d \sqrt{\log n}\right] = n^{1+o(1)} \] for every $n,r\geq 2$. This significantly strengthens a result of Salez (GAFA 2022), who proved that the average displacement of the random walk is $o(n)$ and deduced that non-negatively curved graphs of bounded degree cannot be expanders. Our results also apply to infinite transitive graphs and, more generally, to bounded-degree unimodular random rooted graphs of non-negative Ollivier-Ricci curvature.",0,arxiv,Matematik,CC-BY/arXiv,Bounded-degree graphs of non-negative Ollivier-Ricci curvature have subexponential growth and diffusive random walk
"Given a tree $T$, its 3-coloring graph $\mathcal{C}_3(T)$ has as vertices the proper 3-colorings of $T$, with edges joining colorings that differ at exactly one vertex. We call the diameter of $\mathcal{C}_3(T)$ the 3-coloring diameter of $T$. We introduce the notion of balanced labelings of $T$ and show that the 3-coloring diameter equals the maximum $L_1$-norm of a balanced labeling. Using this equivalence, we determine the maximum and minimum values of the 3-coloring diameter over all trees on $n$ vertices and characterize the extremal trees.",0,arxiv,Matematik,CC-BY/arXiv,Extremal diameters of 3-coloring graphs of trees
"The Quantum Max-$d$-Cut ($d$-QMC) problem is a special instance of a $2$-local Hamiltonian problem, representing the quantum analog of the classical Max-$d$-Cut problem. The $d$-QMC problem seeks the largest eigenvalue of a Hamiltonian defined on a graph with $n$ vertices, where edges correspond to swap operators acting on $(\mathbb{C}^d)^{\otimes n}$. In recent years, progress has been made by investigating the algebraic structure of the $d$-QMC Hamiltonian. Building on this approach, this article solves the $d$-QMC problem for complete tripartite graphs for small local dimensions, $d \le 3$.",0,arxiv,Matematik,CC-BY/arXiv,Quantum Max Cut for complete tripartite graphs
"We prove that for any sequence of binary alphabets $\mathcal{A}_1,\mathcal{A}_2,\dots$, there exists a cube-free word $c_1c_2\dots$ so that $c_1\in\mathcal{A}_1,c_2\in\mathcal{A}_2,\dots$. In particular, for every $n$, there are at least $1.35^n$ cube-free words in $\mathcal{A}_1\times\mathcal{A}_2\times\dots\times \mathcal{A}_n$. We also prove that if the list of alphabets is computable then one of these words is computable and its $n$th letter can be computed in time polynomial in $n$.",0,arxiv,Matematik,CC-BY/arXiv,There exist infinite cube-free words over any sequence of binary alphabets
"The Strong Ramsey game $\mathcal{R}(B,G)$ is a two player game with players $P_1$ and $P_2$, where $B$ and $G$ are $k$-uniform hypergraphs for some $k \geq 2$. $G$ is always finite, while $B$ may be infinite. $P_1$ and $P_2$ alternately color uncolored edges $e \in B$ in their respective color and $P_1$ begins. Whoever completes a monochromatic copy of $G$ in their own color first, wins the game. If no one claims a monochromatic copy of $G$ in a finite number of moves, the game is declared a draw. For a $t \in \mathbb{N}$, let $\hat{K}_{2,t}$ denote the $K_{2,t}$ together with the edge connecting the two vertices in the partition class of size 2. The purpose of this paper is to give a winning strategy for $P_1$ in the game $\mathcal{R}(K_{\aleph_0}, \hat{K}_{2,3})$.",0,arxiv,Matematik,CC-BY/arXiv,"$\mathcal{R}(K_{\aleph_0}, \hat{K}_{2,3})$ is a win for Player 1"
"This paper investigates spectral properties of the deformed Laplacian matrix, which merges the Laplacian and signless Laplacian matrices of a graph through a one-parameter family of matrices. We present general results on the eigenvalues of these matrices for simple undirected graphs. Additionally, we analyze the spectrum of the deformed Laplacian in the specific cases of trees and H-join graphs. For trees, we derive strong results on the localization of eigenvalues, while for H-join graphs, we explicitly compute the spectrum of the deformed Laplacian.",0,arxiv,Matematik,CC-BY/arXiv,Spectral properties of the deformed Laplacian matrix of trees and H-join graphs
We show that the KÃ¤hler-Einstein metrics on the four families of examples of symmetric toric Fano manifolds presented by Batyrev and Selivanova cannot be realized as metrics induced by immersions into projective spaces equipped with Fubini-Study metrics. We obtain a similar conclusion for the non-symmetric examples discovered by Nill and Paffenholz. A consequence is that a centrally symmetric toric Fano manifold admits a KÃ¤hler-Einstein metric induced by a projective immersion if and only if it is a product of projective lines. These results provide evidence for a broader conjecture characterizing which KÃ¤hler-Einstein metrics can be induced by projective immersions.,0,arxiv,Matematik,CC-BY/arXiv,KÃ¤hler-Einstein toric submanifolds of the projective space
"In this paper, we study when a real matrix Schubert variety is stationary with respect to the first variation. We first show that a necessary condition for its open dense regular part to be a minimal submanifold is that the corresponding partial permutation is vexillary. Among vexillary partial permutations, we establish minimality by a geometric argument when the Rothe diagram is of Grassmannian type and has at most two connected components. We further obtain, as a corollary, the minimality of those varieties that decompose as products of this type. These varieties include all determinantal varieties as well as some new minimal cones.",0,arxiv,Matematik,CC-BY/arXiv,On stationary real matrix Schubert varieties
"Tom Leinster gave a bijective correspondence between the set of operators on a finite-dimensional vector space $V$ and the set of pairs consisting of a nilpotent operator and a vector in $V$. Over a finite field this bijection implies that the probability that an operator be nilpotent is the reciprocal of the number of vectors in $V$. We generalize this correspondence to pairs of operators between pairs of vector spaces and determine the probability that a random pair of operators be nilpotent. We also determine the set-theoretical counterpart of this construction and compute the number of eventually constant pairs of maps between two finite sets, closely related to the number of spanning trees in a complete bipartite graph.",0,arxiv,Matematik,CC-BY/arXiv,Pairs of eventually constant maps and nilpotent pairs
"We introduce a parameterized family of invariants for $\ell$-uniform hypergraphs. To each $\mathbb{K}$-linear transformation $T:\mathbb{K}^{\ell}\to \mathbb{K}^r$ we associate a function $\mathrm{Sig}(-,T)$ that maps $\ell$-uniform hypergraphs to $\mathbb{K}$-vector spaces. Given an $\ell$-uniform hypergraph $\mathcal{H}=(V,E)$, we use $\mathrm{Sig}(\mathcal{H},T)$ to define an equivalence relation $\equiv_T$ on $V$ called $T$-fusion, which determines a quotient hypergraph $\mathfrak{F}(\mathcal{H},T)$ called the $T$-frame of $\mathcal{H}$. We show that the map $U:\mathbb{K}^{\ell}\to \mathbb{K}$, where $U(Î»)=Î»(1)+\cdots+Î»(\ell)$, is universal in that $\mathrm{Sig}(\mathcal{H},T)$ embeds in $\mathrm{Sig}(\mathcal{H},U)$, and $U$-fusion refines $T$-fusion for any $T:\mathbb{K}^{\ell}\to\mathbb{K}^r$. We further show that $\mathfrak{F}(\mathfrak{F}(\mathcal{H},U),U)=\mathfrak{F}(\mathcal{H},U)$ for any $\ell$-uniform hypergraph $\mathcal{H}$, so $\mathfrak{F}(-,U)$ is a closure function on the set of $\ell$-uniform hypergraphs. We explore the properties of this one-time simplification of a hypergraph.",0,arxiv,Matematik,CC-BY/arXiv,New linear invariants of hypergraphs
"We characterize the shifted simple graphs and the $3$-uniform shifted hypergraphs whose inverse image under exterior shifting is the set of bases of a matroid: those are exactly the hypergraphs whose hyperedges form an initial lex-segment. There are several examples of known matroids arising in this way: the simplicial matroid, the hyperconnectivity matroid and the area-rigidity matroid. For $k\ge 4$, we provide a similar characterization for shifted $k$-uniform hypergraphs satisfying an additional combinatorial condition.   For symmetric shifting, we prove an analogous characterization for shifted simple graphs, where the classical generic rigidity matroid is an example of a matroid arising in this way.",0,arxiv,Matematik,CC-BY/arXiv,Matroids arising from algebraic shifting
"The distribution of prime constellations, such as Twin Primes ($p, p+2$), is traditionally analyzed via probabilistic models or analytic sieve theory. While heuristic predictions are accurate, rigorous proofs are obstructed by the ""Parity Barrier"", which prevents classical sieves from distinguishing primes from semi-primes in the asymptotic limit. In this work, we present a structural proof of existence based on deterministic signal processing. We treat the sequence of integers as a signal generated by a rigid Diophantine basis ($N=2n+3m$) and define a fundamental certification window $\mathcal{W} = [P, m_0^2)$ derived from the basis limit $m_0$. We demonstrate that the non-existence of constellations (the ""Null Hypothesis"") constitutes a low-entropy signal state, a ""Prime Desert"", that requires infinite spectral resolution to maintain over a quadratic window. Since the sieving basis is finite ($p \le m_0$), the system is band-limited and structurally incapable of synthesizing the destructive interference required to sustain a zero count. By invoking the Chinese Remainder Theorem and analyzing the detailed correlation structure of residue classes, we prove that positive and negative correlations between sieved positions cancel at leading order, constraining the variance of the signal to scale linearly with the mean ($O(Î¼)$) rather than the quadratic scaling ($Î©(Î¼^2)$) required to support a Prime Desert. This Variance Gap implies that the signal must strictly oscillate around its mean, rendering the existence of prime constellations a mandatory consequence of the system's finite spectral bandwidth.",0,arxiv,Matematik,CC-BY/arXiv,Structural Existence of Prime Constellations: Asymptotic Spectral Stability in Finite Sieve Windows
"For a finite set $V\subset \mathbb{R}^n$, a set $T\subset \mathbb{R}^n$ is called $V$-closed if $t \in T$ and $v\in V$ imply that either $t+v\in T$ or $t-v \in T$. The set $P(V):=\{\sum_{v \in W} v: W \subset V\}$ is clearly $V$-closed and so are its translates. We show, assuming $V$ contains no parallel vectors, that if $T$ is closed and $V$-closed, and $x \in T$ is an extreme point of $\operatorname{cl} \operatorname{conv} T$, then there is a translate of $P(V)$ containing $x$ and contained in $\operatorname{conv} T$. This result is used to determine the value of a special balancing game. A byproduct is that when $m\ge 2$ and is not a power of 2, then the $m$-sets of a $2m$-set can be coloured Red and Blue so that complementary $m$-sets have distinct colours and every point of the $2m$-set is contained in the same number of Red and Blue sets.",0,arxiv,Matematik,CC-BY/arXiv,Balancing games on unbounded sets
"This paper is motivated by basic complexity and probability questions about permanents of random matrices over finite fields, and in particular, about properties separating the permanent and the determinant. Fix $q = p^m$ some power of an odd prime, and let $k \leq n$ both be growing. For a uniformly random $n \times k$ matrix $A$ over $\mathbb{F}_q$, we study the probability that all $k \times k$ submatrices of $A$ have zero permanent; namely that $A$ does not have full ""permanental rank"". When $k = n$, this is simply the probability that a random square matrix over $\mathbb{F}_q$ has zero permanent, which we do not understand. We believe that the probability in this case is $\frac{1}{q} + o(1)$, which would be in contrast to the case of the determinant, where the answer is $\frac{1}{q} + Î©_q(1)$. Our main result is that when $k$ is $O(\sqrt{n})$, the probability that a random $n \times k$ matrix does not have full permanental rank is essentially the same as the probability that the matrix has a $0$ column, namely $(1 +o(1)) \frac{k}{q^n}$. In contrast, for determinantal (standard) rank the analogous probability is $Î˜(\frac{q^k}{q^n})$. At the core of our result are some basic linear algebraic properties of the permanent that distinguish it from the determinant.",0,arxiv,Matematik,CC-BY/arXiv,Permanental rank versus determinantal rank of random matrices over finite fields
We consider Whitehead's integral formula and propose an algorithm for computing the Hopf invariant for simplicial mappings.,0,arxiv,Matematik,CC-BY/arXiv,Computing the Hopf invariant
"We introduce a multi-parameter family of random edge weights on the Aztec diamond graph, given by certain Gamma variables, and prove several results about the corresponding random dimer measures.   Firstly, we show there is no phase transition at the level of the free energy. This provides rigorous backing for the physics predictions of Zeng-Leath-Hwa and later works that dimer models with random weights are in the glassy `super-rough' phase at all temperatures with no phase transition.   Secondly, we show that the random dimer covers themselves enjoy exact distributional equalities of certain marginals with path locations in new `hybrid' integrable polymers. These reduce to the stationary log-Gamma, strict-weak, and Beta polymer in random environment in certain cases, allowing transfer of known results from integrable polymers to dimers with random weights. As an example application, we prove that the turning points at the boundaries of the Aztec diamond exhibit fluctuations of order $n^{2/3}$, in contrast to the $n^{1/2}$ fluctuations for deterministic weights.   Underlying all these is a key integrability property of the weights: they are the unique family for which independence is preserved under the shuffling algorithm.",0,arxiv,Matematik,CC-BY/arXiv,The Gamma-disordered Aztec diamond
"We investigate a graph-theoretic problem motivated by questions in quantum computing concerning the propagation of information in quantum circuits. A graph $G$ is said to be a bounded extension of its subgraph $L$ if they share the same vertex set, and the graph distance $d_L(u, v)$ is uniformly bounded for edges $uv\in G$. Given vertices $u, v$ in $G$ and an integer $k$, the geodesic slice $S(u, v, k)$ denotes the subset of vertices $w$ lying on a geodesic in $G$ between $u$ and $v$ with $d_G(u, w) = k$. We say that $G$ has bounded geodesic slices if $|S(u, v, k)|$ is uniformly bounded over all $u, v, k$. We call a graph $L$ geodesically directable if it has a bounded extension $G$ with bounded geodesic slices.   Contrary to previous expectations, we prove that $\mathbb{Z}^2$ is geodesically directable. Physically, this provides a setting in which one could devise exactly-solvable chaotic local quantum circuits with non-trivial correlation patterns on 2D Euclidean lattices. In fact, we show that any bounded extension of $\mathbb{Z}^2$ is geodesically directable. This further implies that all two-dimensional regular tilings are geodesically directable.",0,arxiv,Matematik,CC-BY/arXiv,Combinatorial foundations for solvable chaotic local Euclidean quantum circuits in two dimensions
"We consider Eulerian cycles without transversal selfintersections in $4$-valent planar graphs. We prove that any cycle of this type in the graph of an ideal right-angled hyperbolic $3$-polytope corresponds to a hyperbolic link such that its complement consists of $4$-copies of this polytope glued according to its checkerboard coloring. Moreover, this link consists of trivially embedded circles bijectively corresponding to the vertices of the polytope. For such cycles we prove that the $3$-antiprism $A(3)$ (octahedron) has exactly $2$ combinatorially different cycles, the $4$-antiprism $A(4)$ has exactly $7$ combinatorially different cycles, and these cycles correspond to $7$ cycles (perhaps combinatorially equivalent) on any polytope different from antiprisms, and any antiprism $A(k)$ has at least $2$ combinatorially different cycles. The $2$-fold branched covering space corresponding to our link is a small cover over some simple $3$-polytope. This small cover is defined by a Hamiltonian cycle on it. We show that any Hamiltonian cycle on a compact right-angled hyperbolic $3$-polytope arises in this way, while for a Hamiltonian cycle on a right-angled hyperbolic $3$-polytope of finite volume the necessary and sufficient condition is that at each ideal vertex it does not go straight. We introduce a transformation of a Eulerian cycle along conjugated vertices allowing to build new cycles from a given one.",0,arxiv,Matematik,CC-BY/arXiv,On hyperbolic links associated to Eulerian cycles on ideal right-angled hyperbolic $3$-polytopes
"With a view towards studying the multitemporal wave equation on affine buildings recently introduced by Anker-RÃ©my-Trojan [arXiv:2312.06860], we systematically develop the basic properties of the discrete wave equation on $\mathbb{Z}$ and use this to explain existing results about the wave equation on regular graphs. Furthermore, we explicitly compute the incoming and outgoing translation representations and the scattering operator, in the sense of Lax-Phillips, for regular and biregular trees. Finally, we use the wave equation on biregular graphs to extend a result of Brooks-Lindenstrauss about delocalization of eigenfunctions on regular graphs to the setting of biregular graphs.",0,arxiv,Matematik,CC-BY/arXiv,The discrete wave equation with applications to scattering theory and quantum chaos
"The container methods are powerful tools to bound the number of independent sets of graphs and hypergraphs, and they have been extremely influential in the area of extremal and probabilistic combinatorics. We will focus on more specialized graph container methods due to Sapozhenko (1987) that deal with sets in expander graphs. Entropy, first introduced by Shannon (1948) in the area of information theory, is a measure of the expected amount of information contained in a random variable. Entropy has seen lots of fascinating applications in a wide range of enumeration problems. In this survey article, we will discuss recent developments that exploit a combination of the two methods on enumerating graph homomorphisms.",0,arxiv,Matematik,CC-BY/arXiv,Asymptotic enumeration via graph containers and entropy
"For a simple graph $G$, let $n$ denote its number of vertices, and let $N(G,K_t)$ denote the number of copies of $K_t$ in $G$. Zykov's theorem (1949) asserts that for any $K_{r+1}$-free graph and $t \ge 2$, \[ N(G,K_t) \le {r \choose t}\left(\frac{n}{r}\right)^t \] We generalize Zykov's bound within a vertex-based localization framework.   For each vertex $v \in V(G)$, let $c(v)$ denote the order of the largest clique containing $v$. In this paper, we show that \[ N(G,K_t) \le n^{t-1} \sum_{v \in V(G)} \frac{1}{c(v)^t} {c(v) \choose t} \] We further show that equality holds if and only if $G$ is a regular complete multipartite graph. \newline Note that if we impose the condition that, $G$ is $K_{r+1}$-free, then $c(v) \leq r$ for all $v \in V(G)$. Thus, plugging $c(v) = r$ for all $v \in V(G)$, we retrieve Zykov's bound.",0,arxiv,Matematik,CC-BY/arXiv,Generalized Zykov's Theorem
"A digraph $G$ is \emph{$k$-geodetic} if for any pair $u,v \in V(G)$ there is at most one $u,v$-walk of length not exceeding $k$. The order of a $k$-geodetic digraph with minimum out-degree $d$ is bounded below by the directed Moore bound $M(d,k) = 1 + d + d^2+ \cdots +d^k$. It is known that the Moore bound cannot be achieved for $d,k \geq 2$. A $k$-geodetic digraph with minimum degree $d$ and order one greater than the Moore bound has \emph{excess one}. In this paper we prove a conjecture that no excess one digraphs exist for $d,k \geq 2$, thus complementing the result of Bannai and Ito on the non-existence of undirected graphs with excess one.",0,arxiv,Matematik,CC-BY/arXiv,There are no excess one digraphs
"We present a simple $q$-ary family of single-error-correcting, double-error-detecting (SEC--DED) linear codes whose parity checks are tied directly to the base-$p$ ($q=p$ prime) digits of the coordinate index. For blocklength $n=p^r$ the construction uses only $r+1$ parity checks -- \emph{near-Hamming} overhead -- and admits an index-based decoder that runs in a single pass with constant-time location and magnitude recovery from the syndromes. Based on the prototype, we develop two extensions: Code A1, which removes specific redundant trits to achieve higher information rate and support variable-length encoding; and Code A2, which incorporates two group-sum checks together with a 3-wise XOR linear independence condition on index subsets, yielding a ternary distance-4 (SEC--TED) variant. Furthermore, we demonstrate how the framework generalizes via $n$-wise XOR linearly independent sets to construct codes with distance $d = n + 1$, notably recovering the ternary Golay code for $n = 5$ -- showing both structural generality and a serendipitous link to optimal classical codes.   Our contribution is not optimality but \emph{implementational simplicity} and an \emph{array-friendly} structure: the checks are digitwise and global sums, the mapping from syndromes to error location is explicit, and the SEC--TED upgrade is modular. We position the scheme against classical $q$-ary Hamming and SPC/product-code baselines and provide a small comparison of parity overhead, decoding work, and two-error behavior.",0,arxiv,Matematik,CC-BY/arXiv,Digit-Indexed q-ary SEC-DED Codes with Near-Hamming Overhead
"The study of the size of subsets in a semigroup have shown that many of these subsets have strong combinatorial properties and contribute richly to the algebraic structure of the Stone-Cech compactification of a discrete semigroup. N. Hindman and D. Strauss have proved that if u, v $\in \mathbb{N}$, M is a u \times v matrix satisfying restrictions that vary with the notion of largeness and if $Î¨$ is a notion of large sets in $\mathbb{N}$ then $\{\vec{x} \in \mathbb{N}^v: M\vec{x} \in Î¨^u\}$ is large set in $\mathbb{N}^v$. In this article, we investigate the above result for various notions of largeness near zero in $\mathbb{R}^+$.",0,arxiv,Matematik,CC-BY/arXiv,Preservation of notion of large sets near zero over reals
"Let $G$ be a simple graph on $n$ vertices, and let $J_G$ denotes the corresponding binomial edge ideal in $S=\mathbb{K}[x_1,\ldots,x_n,y_1,\ldots,y_n]$, where $\mathbb{K}$ is a field. We show that if a vertex satisfies a certain degree condition, then some Hilbert coefficients remain unchanged upon its removal, thereby providing a reduction technique for computing Hilbert coefficients. As an application, for any $i\geq 0$ and a pair $(r,s)$ with $r\geq 2, s\in \mathbb{Z}$, we show that there always exists a graph $G$ such that $\mathrm{reg}(S/J_G)=r$ and $e_i(S/J_G)=s$, where $\mathrm{reg}(S/J_G)$ and $e_i(R/J_G)$ denote the Castelnuovo-Mumford regularity and the $i$-th Hilbert coefficient of $S/J_G$, respectively. In particular, this demonstrates that there is no inherent relationship between the regularity and the Hilbert coefficients for the class of binomial edge ideals.",0,arxiv,Matematik,CC-BY/arXiv,Hilbert Coefficients and Regularity of Binomial Edge Ideals
"Novikov algebras provide a simple but powerful algebraic axiomatization of important features of classical diferential calculus. We study their structure properties, modeling their relationships with commutative algebras with a derivation, featuring the role of their Lie and pre-Lie structures and analyzing the structure of their enveloping algebras. We focus on the combinatorial analysis of the PoincarÃ©-Birkhoff-Witt Theorem (classical and pre-Lie), the pre-Lie exponential and logarithm. The topic is important for applications of the theory and has been treated intensively for pre-Lie algebras. However, specific formulas can be obtained in the Novikov case. We analyze their structure, as well as featuring various remarkable properties. Related statistical phenomena on trees, tableaux and permutations are investigated in this context.",0,arxiv,Matematik,CC-BY/arXiv,Lie and pre-Lie theory of Novikov algebras
"We show that the elements of the Kazhdan--Lusztig basis of the spherical Hecke algebra of type $G_2$ have an atomic decomposition. As a by-product, we obtain a new algorithm to compute generalized Kostka--Foulkes polynomials in type $G_2$.",0,arxiv,Matematik,CC-BY/arXiv,Atomic decomposition for an affine Weyl group of type $G_2$
"The polynomial $\sum_{Ï€\in W}q^{maj(Ï€)}$ of major index over a classical Weyl group $W$ with a generating set $S$ is called the Mahonian polynomial over $W$, and also the polynomial $\sum_{Ï€\in W}(-1)^{l(Ï€)}q^{maj(Ï€)}$ of major index together with sign over the group $W$ is called the signed Mahonian polynomial over the group $W$, where $l$ is the length function on $W$ defined in terms of the generating set $S$. We concern with the signed Mahonian polynomial $$\sum_{Ï€\in D_{n}^{(c)}}(-1)^{L(Ï€)}q^{fmaj(Ï€)}$$ on the set $D_{n}^{(c)}$ of colored derangements in the group $G_{c,n}$ of colored permutations, where $L$ denotes the length function defined by means of a complex root system described by Bremke and Malle in $G_{c,n}$ and $fmaj$ defined by Adin and Roichman in $G_{c,n}$ represents the \textit{flag-major index}, which is a Mahonian statistic. As an application of the formula for signed Mahonian polynomials on the set of colored derangements, we will derive a formula to count colored derangements of even length in $G_{c,n}$ when $c$ is an even number. Finally, we conclude by providing a formula for the difference between the number of derangements of even and odd lengths in $G_{c,n}$ when $c$ is even.",0,arxiv,Matematik,CC-BY/arXiv,Signed Mahonian Polynomials on Colored Derangements
"Fox's conjecture (1962) states that the sequence of absolute values of the coefficients of the Alexander polynomial of alternating links is trapezoidal. While the conjecture remains open in general, a number of special cases have been settled, some quite recently: Fox's conjecture was shown to hold for special alternating links by Hafner, MÃ©szÃ¡ros, and Vidinas (2023) and for certain diagrammatic Murasugi sums of special alternating links by Azarpendar, JuhÃ¡sz, and KÃ¡lmÃ¡n (2024). In this paper, we give an alternative proof of Azarpendar, JuhÃ¡sz, and KÃ¡lmÃ¡n's aforementioned beautiful result via a dimer model for the Alexander polynomial. In doing so, we not only obtain a significantly shorter proof of Azarpendar, JuhÃ¡sz, and KÃ¡lmÃ¡n's result than the original, but we also obtain several theorems of independent interest regarding the Alexander polynomial, which are readily visible from the dimer point of view.",0,arxiv,Matematik,CC-BY/arXiv,A dimer view on Fox's trapezoidal conjecture
"We study the free boundary $q$-Whittaker and Hall--Littlewood processes, two probability measures on sequences of partitions. We prove that a certain observable of the free boundary $q$-Whittaker process exhibits a $(q,t)$ symmetry after a random shift, generalizing a previous result of Imamura, Mucciconi, and Sasamoto, and an extension of that result due to the first author. Our proof is completely different, and as part of our proof, we find contour integral formulas for the free boundary $q$-Whittaker process. We also show a matching between certain observables in the free boundary Hall--Littlewood process and a quasi-open six vertex model, and explain how work of Finn and Vanicat gives an evaluation of a bounded sum over skew Hall--Littlewood functions as a rectangular Koornwinder polynomial.",0,arxiv,Matematik,CC-BY/arXiv,Free boundary q-Whittaker and Hall-Littlewood processes
"We determine dominance regions associated to cluster algebras of affine type. In the most interesting cases, the dominance region is a line segment, which we describe explicitly. Motivations for this work include a project to determine all pointed bases for cluster algebras of affine type and a separate project to determine all theta functions in the affine case. The proofs draw on known results from the doubled Cambrian fan and almost-positive roots models, as well as a new tool that we develop: a detailed description of neighboring seeds of affine type (seeds that are, in some sense, as close as possible to the boundary of the g-vector fan).",0,arxiv,Matematik,CC-BY/arXiv,Dominance regions for affine cluster algebras
"Previous work of the second author and Wolf showed that given a set $A\subseteq \mathbb{F}_p^n$ of bounded $\textrm{VC}_2$-dimension, there is a high rank quadratic factor $\mathcal{B}$ of bounded complexity such that $A$ is approximately equal to a union of atoms of $\mathcal{B}$. That proof yielded bounds of tower type on the linear and quadratic complexities. It was later shown by the same authors that the quadratic complexity can be improved to logarithmic, however that proof provided no improvement on the linear component. In this paper we prove that the bound on the linear complexity can be improved to a triple exponential in the case of linear rank functions, and a quadruple exponential for polynomial rank functions of higher degree.   Our strategy is based on the one developed by Gishboliner, Wigderson, and Shapira to prove the analogous result in the hypergraph setting. Step 1 is to prove a``cylinder"" version of the quadratic arithmetic regularity lemma, which says that given a set $A\subseteq G=\mathbb{F}_p^n$, there is a partition of $G$ into atoms of (possibly distinct) quadratic factors of high rank and bounded complexity, so that most atoms in the partition are uniform with respect to the set $A$, in the sense of a certain local $U^3$ norm. Step 2 is to show that if $A$ has bounded $\textrm{VC}_2$-dimension, then it has density near $0$ or $1$ on all atoms which are uniform in the sense of Step 1. Step 1 relies on a recent local version of the $U^3$ inverse theorem due to Prendiville, and is necessarily phrased in terms of a local $U^3$ norm implicit in that paper. On the other hand, Step 2 relies on a counting lemma for a different local $U^3$ due to Terry and Wolf, which we prove here is approximately the same as the local $U^3$ norm used in Step 1.",0,arxiv,Matematik,CC-BY/arXiv,On the linear complexity of subsets of $\mathbb{F}_p^n$ bounded $\textrm{VC}_2$-dimension
"Let $Î±_1, \cdots, Î±_d$ be real numbers, and let $S$ be the set of integers $s$ so that $||Î±_i s||_{\mathbb{R}/\mathbb{Z}}>Î´$ for some $i$ and some fixed $Î´>0$. We prove $S$ is not \enquote{$2$-large}, i.e. there is a $2$-coloring of $\mathbb{N}$ that avoids arbitrarily long arithmetic progressions with common differences in $S$.",0,arxiv,Matematik,CC-BY/arXiv,$2$-large sets are sets of Bohr recurrence
"Networks are often modeled using graphs, and within this setting we introduce the notion of $k$-fault-tolerant mutual visibility. Informally, a set of vertices $X \subseteq V(G)$ in a graph $G$ is a $k$-fault-tolerant mutual-visibility set ($k$-ftmv set) if any two vertices in $X$ are connected by a bundle of $k+1$ shortest paths such that: ($i$) each shortest path contains no other vertex of $X$, and ($ii$) these paths are internally disjoint. The cardinality of a largest $k$-ftmv set is denoted by $\mathrm{f}Î¼^{k}(G)$. The classical notion of mutual visibility corresponds to the case $k = 0$.   This generalized concept is motivated by applications in communication networks, where agents located at vertices must communicate both efficiently (i.e., via shortest paths) and confidentially (i.e., without messages passing through the location of any other agent). The original notion of mutual visibility may fail in unreliable networks, where vertices or links can become unavailable.   Several properties of $k$-ftmv sets are established, including a natural relationship between $\mathrm{f}Î¼^{k}(G)$ and $Ï‰(G)$, as well as a characterization of graphs for which $\mathrm{f}Î¼^{k}(G)$ is large. It is shown that computing $\mathrm{f}Î¼^{k}(G)$ is NP-hard for any positive integer $k$, whether $k$ is fixed or not. Exact formulae for $\mathrm{f}Î¼^{k}(G)$ are derived for several specific graph topologies, including grid-like networks such as cylinders and tori, and for diameter-two networks defined by Hamming graphs and by the direct product of complete graphs.",0,arxiv,Matematik,CC-BY/arXiv,Fault-tolerant mutual-visibility: complexity and solutions for grid-like networks
We prove that the lonely runner conjecture holds for nine runners. Our proof is based on a couple of improvements of the method we used to prove the conjecture for eight runners.,0,arxiv,Matematik,CC-BY/arXiv,The lonely runner conjecture holds for nine runners
"Let $t$ be a positive real number. A graph is called \emph{$t$-tough} if the removal of any vertex set $S$ that disconnects the graph leaves at most $|S|/t$ components. The toughness of a graph is the largest $t$ for which the graph is $t$-tough. A graph is minimally $t$-tough if the toughness of the graph is $t$, and the deletion of any edge from the graph decreases the toughness. Series--parallel graphs are graphs with two distinguished vertices called   terminals, formed recursively by two simple composition operations, series   and parallel joins. They can be used to model series and parallel electric circuits.   We characterize the minimally $t$-tough series-parallel graphs for all $t\ge 1/2$. It is clear that there is no minimally $t$-tough series-parallel graph if $t>1$. We show that for $1\ge t >1/2$, most of the series-parallel graphs with toughness $t$ are minimally $t$-tough, but most of the series-parallel graphs with toughness $1/2$ are not minimally $1/2$-tough.",0,arxiv,Matematik,CC-BY/arXiv,Minimally tough series-parallel graphs with toughness at least $1/2$
"In the first paper of the Graph Minors series [JCTB '83], Robertson and Seymour proved the Forest Minor theorem: the $H$-minor-free graphs have bounded pathwidth if and only if $H$ is a forest. In recent years, considerable effort has been devoted to understanding the unavoidable induced substructures of graphs with large pathwidth or large treewidth. In this paper, we give an induced counterpart of the Forest Minor theorem: for any $t \geqslant 2$, the $K_{t,t}$-subgraph-free $H$-induced-minor-free graphs have bounded pathwidth if and only if $H$ belongs to a class $\mathcal F$ of forests, which we describe as the induced minors of two (very similar) infinite parameterized families. This constitutes a significant step toward classifying the graphs $H$ for which every weakly sparse $H$-induced-minor-free class has bounded treewidth. Our work builds on the theory of constellations developed in the Induced Subgraphs and Tree Decompositions series.",0,arxiv,Matematik,CC-BY/arXiv,Excluding a Forest Induced Minor
"We study the representation theory of the infinite type A Hecke algebra over a non-Archimedean field in the case where the parameter is a pseudo-uniformizer. Specifically, we consider a family of representations, called almost-symmetric, which satisfy additional topological and algebraic constraints. We give a full classification of the irreducible almost-symmetric representations. These turn out to be indexed by integer partitions arising as topological completions of specific direct limits of Hecke-Specht modules. We give detailed analysis of these representations and construct functionals analogous to finite Hecke algebra traces.",0,arxiv,Matematik,CC-BY/arXiv,Non-archimedean Infinite Hecke Algebra
"The chromatic number of the finite projective space $\mathrm{PG}(n-1,q)$, denoted $Ï‡_q(n)$, is the minimum number of colors needed to color its points so that no line is monochromatic. We establish a recursive upper bound $Ï‡_q(n) \leq Ï‡_q(d) + Ï‡_q(n - d)$ for all $1 \leq d < n$ and use it to prove new upper bounds on $Ï‡_q(n)$ for all $q$. For $q = 2$, we further refine this recursion and prove that \[ Ï‡_2(n) \le \lfloor 2n/3 \rfloor + 1 \] for all $n \ge 2$, and that this bound is tight for all $n \le 7$. In particular, this recovers all previously known cases for $n \le 6$ and resolves the first open case $n = 7$. On the lower-bound side, using a connection with multicolor Ramsey numbers for triangles, we note that \[ Ï‡_2(n) \ge (1 - o(1))\,\frac{n}{\log n}. \]   We also consider $Ï‡_q(t;n)$, the minimum number of colors needed to color the points of $\mathrm{PG}(n-1,q)$ with no monochromatic $(t - 1)$-dimensional subspace, and establish an equivalence between $Ï‡_q(t;n)$ and the multicolor vector-space Ramsey numbers $R_q(t;k)$. Using this equivalence together with our upper bounds on $Ï‡_q(t;n)$, we improve, for every fixed $t$, the best known lower bounds on $R_q(t;k)$ from $Î©_q(\log k)$ to $Î©(k)$.",0,arxiv,Matematik,CC-BY/arXiv,The chromatic number of finite projective spaces
"In this paper, we consider the bounds for the largest eigenvalue and the sum of the $k$ largest Laplacian eigenvalues of signed graphs. Firstly, we give an upper bound on the largest eigenvalue of the adjacency matrix of a signed graph and characterize the extremal graphs that attain this bound. Secondly, we prove that a non-bipartite signed graph $Î“$ of order $n$ and size $m$ contains a balanced triangle if $Î»_{1}(Î“)\ge \sqrt{m-1}$, $Î»_{1}(Î“) \ge |Î»_{n}(Î“)|$ and $Î“\not \sim (C_{5}\cup (n-5)K_{1},+)$, where $Î»_{1}(Î“)$ is the largest eigenvalue of the adjacency matrix of $Î“$. Thirdly, we confirm a conjecture proposed in [Linear Multilinear Algebra 51 (1) (2003) 21--30] that: if $Î“$ is a connected signed graph, then   $$   \sum_{i=1}^{k}Î¼_{i}(Î“) >\sum_{i=1}^{k}d_{i}(Î“)~~(1\le k\le n-1),   $$   where $Î¼_{1}(Î“)\geÎ¼_{2}(Î“)\ge\cdots \ge Î¼_{n}(Î“)$ are Laplacian eigenvalues of $Î“$, and $d_{1}(Î“)\ge d_{2}(Î“)\ge \dots \ge d_{n}(Î“)$ are vertex degrees of $Î“$. Finally, we give a lower bound for the sum of the $k$ largest Laplacian eigenvalues of a connected signed graph.",0,arxiv,Matematik,CC-BY/arXiv,Bounds for the largest eigenvalue and sum of Laplacian eigenvalues of signed graphs
"A digraph $D$ is an oriented graph if $D$ does not have a pair of opposite arcs. The degree of a vertex $v$ of $D$ is the sum of the in-degree and out-degree of $v.$ Let $fvs(D)$ be the minimum number of vertices whose deletion from $D$ makes it acyclic. Let $D$ be a digraph with $n$ vertices and maximum degree $Î”$. We prove the following bounds. If $D$ is an oriented graph, then $fvs(D)\leq \frac{3n}{7}$ when $Î”\le 4$ and $fvs(D)\leq \frac{n}{2}$ when $Î”\le 5$. If $D$ is a connected digraph, $Î”\le 4$ and $D$ is not obtained from an odd undirected cycle by replacing every edge with the pair of opposite arcs with the same endvertices, then $fvs(D)\leq \frac{n}{2}$. If $D$ is an arbitrary digraph with $Î”\le 5$ then $fvs(D)\leq \frac{2n}{3}.$ Note that all the above bounds are tight.",0,arxiv,Matematik,CC-BY/arXiv,Feedback vertex sets of digraphs with bounded maximum degree
"For $0 \leq Î±< 1$, the $Î±$-spectral radius of a graph $G$ is defined as   the largest eigenvalue of $A_Î±(G)=Î±D(G)+(1-Î±)A(G)$, where $D(G)$ and $A(G)$ are the diagonal matrix of degrees and adjacency matrix of $G$, respectively.   A graph is called color-critical if it contains an edge whose deletion reduces its chromatic number.   The celebrated ErdÅ‘s-Stone-Simonovits theorem asserts that $ \mathrm{ex}(n,\mathcal{F})=\left(1-\frac{1}{Ï‡(\mathcal{F})-1}+o(1)\right)\frac{n^2}{2},$   where $Ï‡(\mathcal{F})$ is the chromatic number of $\mathcal{F}$.   Nikiforov and Zheng et al. established the adjacency spectral and signless Laplacian spectral versions of this theorem, respectively.   In this paper, we present the $Î±$-spectral version of this theorem, which unifies the aforementioned results. Furthermore, we characterize the $Î±$-spectral extremal graphs for color-critical graphs, thereby extending the existing results on adjacency spectral and signless Laplacian spectral extremal graphs for such graphs.",0,arxiv,Matematik,CC-BY/arXiv,The $Î±$-spectral TurÃ¡n type problems for graphs
"The graph parameter treedepth is minor-monotone; hence, the class of graphs with treedepth at most $k$ is minor-closed. By the Graph Minor Theorem, such a class is characterized by a finite set of forbidden minors. A conjecture of DvoÅ™Ã¡k, Giannopoulou, and Thilikos states that every such forbidden minor has at most $2^k$ vertices. We present an algorithm that, given $n, k \in \mathbb{N}$, computes the set of forbidden minors, forbidden subgraphs, and forbidden induced subgraphs on at most $n$ vertices, for the class of graphs of treedepth at most $k$. Applying this algorithm to $k = 4$ and $n = 16$, we enumerate 1546 forbidden minors, 1718 forbidden subgraphs, and 12204 forbidden induced subgraphs. Assuming the above conjecture holds, these sets constitute the complete obstruction sets for graphs of treedepth at most 4.",0,arxiv,Matematik,CC-BY/arXiv,Computing Treedepth Obstructions
"Given a graph or multigraph $G$, let $Ï‡'_{trans}(G)$ denote the minimum integer $n$ such that any proper $Ï‡'(G)$--edge coloring of $G$ can be transformed into any other proper $Ï‡'(G)$--edge coloring of $G$ by a series of transformations such that each of the intermediate colorings is a proper $Ï‡'(G)$--edge coloring of $G$ and each of the transformations involves at most $n$ color classes of the previous coloring. We call $Ï‡'_{trans}(G)$ the {\it edge chromatic transformation index of $G$}.   In this paper we show that if $G$ is a graph with maximum degree at least $4$, where every block is either a bipartite graph, a series-parallel graph, a chordless graph, a wheel graph or a planar graph of girth at least $7$, then $Ï‡'_{trans}(G)\leq 4$. This bound is sharp for series-parallel and wheel graphs. We also show that $Ï‡'_{trans}(G)\leq 8$ for all planar graphs $G$, $Ï‡'_{trans}(G)\leq 5$ if $G$ is a Halin graph and $Ï‡'_{trans}(G)=2$ if $G$ is a regular bipartite planar multigraph. Finally, we consider the analogous problem for vertex colorings, and show that for any $k\geq 3$ there is an infinite class $\cal G$$(k)$ of graphs with chromatic number $k$ such that for every $G\in \cal G$$(k)$ any two proper $k$-vertex colorings of $G$   can be transformed to each other only by a transformation, involving all $k$ color classes.",0,arxiv,Matematik,CC-BY/arXiv,The edge chromatic transformation index of graphs
"In this paper, we prove the upper bound conjecture proposed by Saeedi Madani \& Kiani on the Castelnuovo-Mumford regularity of generalized binomial edge ideals. We give a combinatorial upper bound of regularity for generalized binomial edge ideals, which is better than the bound claimed in that conjecture. Also, we show that the bound is tight by providing an infinite class of graphs.",0,arxiv,Matematik,CC-BY/arXiv,On a regularity-conjecture of generalized binomial edge ideals
"We study the probabilistic zero forcing process, a probabilistic variant of the classical zero forcing process. We show that for every connected graph $G$ on $n$ vertices, there exists an initial set consisting of a single vertex such that the expected propagation time is $n/2 + O(1)$. This result is tight and confirms a conjecture posed by Narayanan and Sun. Additionally, we show tight bounds on the probabilistic throttling number, which captures the trade-off between the size of the initial set and the speed of propagation. Namely, we show that for every connected graph $G$ on $n$ vertices, there exists an initial set consisting of $O(\sqrt{n})$ vertices such that the expected propagation time is $O(\sqrt{n})$. This improves upon previous results by Geneson and Hogben, and confirms another conjecture by Narayanan and Sun.",0,arxiv,Matematik,CC-BY/arXiv,Tight bounds for expected propagation time of probabilistic zero forcing
"Let $G$ be a graph, and let $v$ and $e$ be a vertex and an edge of $G$, respectively. Define $c(v)$ (resp. $c(e)$) to be the order of the largest clique in $G$ containing $v$ (resp. $e$). Denote the adjacency eigenvalues of $G$ by $Î»_1 \ge \cdots \ge Î»_n$. We study localized refinements of spectral TurÃ¡n-type theorems by replacing global parameters such as the clique number $Ï‰(G)$, size $m$ and order $n$ of $G$ with local quantities $c(v)$ and $c(e)$.   Motivated by a conjecture of Elphick, Linz and Wocjan (2024), we first propose a vertex-localized strengthening of Wilf's inequality: \[ \sqrt{s^{+}(G)} \le \sum_{v\in V(G)}\left(1-\frac{1}{c(v)}\right), \] where $s^+(G) = \sum_{Î»_i > 0}Î»_i^2$. Inspired by the BollobÃ¡s-Nikiforov conjecture (2007) on the first two eigenvalues, we then introduce an edge-localized analogue: \[Î»_1^2(G) + Î»_2^2(G) \le \sum_{e\in E(G)} 2\left(1-\frac{1}{c(e)}\right).\] As evidence of their validity, we verify the above conjectures for diamond-free graphs and random graphs. We also propose strengthening of the spectral versions of the ErdÅ‘s, Stone and Simonovits Theorem by replacing the spectral radius with $\sqrt{s^{+}(G)}$ and establish it for all $F$-free graphs with $Ï‡(F)=3$. A key ingredient in our proofs is a general upper bound relating $\sqrt{s^{+}(G)}$ to the triangle count $t(G)$. Finally, we prove a localized version of Nikiforov's walk inequality and conjecture a stronger localized version. These results contribute to the broader program of localizing spectral extremal inequalities.",0,arxiv,Matematik,CC-BY/arXiv,Localization of spectral TurÃ¡n-type theorems
"For a real number $c > 4$, we prove that every graph $G$ with $Î±(G) \leq 2$ and $|V(G)| \geq ct$ has a matching $M$ with $|M| = t$ such that the number of non-adjacent pairs of edges in $M$ is at most: \begin{equation*}   \left( \frac{1}{c\left(c-1\right)^2} + O_c\left(t^{-1/3} \right) \right) \binom{t}{2}. \end{equation*} This is related to an open problem of Seymour (2016) about Hadwiger's Conjecture, who asked if there is a constant $\varepsilon > 0$ such that every graph $G$ with $Î±(G) \leq 2$ has $\text{had}(G) \geq (\frac{1}{3} + \varepsilon) |V(G)|$.",0,arxiv,Matematik,CC-BY/arXiv,Dense Matchings of Linear Size in Graphs with Independence Number 2
"We study the asymptotic distribution of integers sharing the same rooted-tree structure that encodes their complete prime factorization tower. For each tree we derive an explicit density formula depending only on a pair $(m,k)$, the density signature of the tree, up to a suitable multiplicative scalar factor and introduce the corresponding tree zeta function, which generalizes the prime zeta function. Classical results such as the prime number theorem and later work by Landau appear as special cases.",0,arxiv,Matematik,CC-BY/arXiv,Tree asymptotic densities in number theory
"We study the homology group of the Milnor fiber boundary of a hyperplane arrangement in $\mathbb{C}^{3}$. By the work of NÃ©methi--SzilÃ¡rd, the homeomorphism type of the Milnor fiber boundary is combinatorially determined, and an explicit formula for the first Betti number is known. However, the torsion part of the first homology group is poorly understood. In this paper, under some conditions, we prove that the number of even-order torsion summands of the first homology group is greater than or equal to the Euler characteristic of the projectivized complement.",0,arxiv,Matematik,CC-BY/arXiv,Even torsions in the homology group of the Milnor fiber boundary of hyperplane arrangements in $\mathbb{C}^3$
"We derive a formula for the number of lattice points in type B generalized permutohedra, providing a concise alternative to the formula obtained recently by Eur, Fink, Larson, and Spink as a result from a study of delta-matroids. Our approach builds upon the existing framework and techniques introduced by Postnikov in his work on type A generalized permutohedra, a family of polytopes interconnected with many mathematical concepts such as matroids and Weyl groups. In particular, we express the number of lattice points in type B generalized permutohedra in terms of Postnikov's notion of G-draconian sequences, from which their Ehrhart polynomials and volume formula follow as consequences.",0,arxiv,Matematik,CC-BY/arXiv,Counting Lattice Points in Generalized Permutohedra From A to B
"The determination of the quantum chromatic number of graphs has attracted considerable attention recently. However, there are few families of graphs whose quantum chromatic numbers are determined. A notable exception is the family of orthogonality graphs, whose quantum chromatic numbers are fully determined. In this paper, we extend these results by determining the exact quantum chromatic number of several subgraphs of the orthogonality graphs. Using the technique of combinatorial designs, we also determine the quantum chromatic number of the distance-2 Hamming graph, whose edges consist of binary vectors of Hamming distance 2, for infinitely many length.",0,arxiv,Matematik,CC-BY/arXiv,Quantum Chromatic Number of Subgraphs of Orthogonality Graphs and the Distance-2 Hamming Graph
"In this paper we deal with a subclass of chordal graphs, which are simultaneously strictly chordal and interval, the strictly interval graphs. We present a new characterization of the class that leads to a simple linear recognition algorithm. Next we introduce a new subclass of strictly interval graphs, the $SI$-core graphs, that are non-split and non-cograph graphs and show that several elements of the new class are Laplacian integral.",0,arxiv,Matematik,CC-BY/arXiv,Structural and Spectral Properties of Strictly Interval Graphs
"For positive integers $n,d$, let $Î»(n,d)$ be the minimal number of vertices of a triangulation of $n$-sphere which admits a degree $d$ simplicial map to the boundary of $(n+1)$-simplex. We show that $\lim_{d\to\infty}\frac{Î»(n,d)}d=0$ for any $n\ge3$, disproving O. Musin's conjecture. Using similar idea, for any $C$ we construct a triangulation of $\mathbb{S}^n$, $n\ge3$, for which $\frac{f_j}{f_i}>C$, for any $0\le i<j\le n$ such that $i<\lfloor\frac{n-1}2\rfloor$. All triangulations we obtain are isomorphic to boundaries of convex polytopes in $\mathbb{R}^{n+1}$.",0,arxiv,Matematik,CC-BY/arXiv,On vertex-minimal simplicial maps to the sphere
"Cyclic structures are fundamental topological features in graphs, playing critical roles in network robustness, information flow, community structure, and various dynamic processes. Algorithmic tools that can efficiently probe and analyze these cyclic topologies are increasingly vital for tasks in graph mining, network optimization, bioinformatics, and social network analysis. A core primitive for quantitative analysis of cycles is finding the Minimum Weight Cycle (MWC), representing the shortest cyclic path in a weighted graph. However, computing the MWC efficiently remains a challenge, particularly compared to shortest path computations. This paper introduces a novel deterministic algorithm for finding the MWC in general weighted graphs. Our approach adapts the structure of Dijkstra's algorithm by introducing and minimizing a \textit{composite distance} metric, effectively translating the global cycle search into an iterative node-centric optimization. We provide a rigorous proof of correctness based on loop invariants. We detail two mechanisms for accelerating the search: a provable node discarding technique based on intermediate results, and a highly effective graph pruning heuristic. This heuristic dynamically restricts the search to relevant subgraphs, leveraging the principle of locality often present in complex networks to achieve significant empirical speedups, while periodic resets ensure global optimality is maintained. The efficiency of the proposed MWC algorithm enables its use as a core component in more complex analyses focused on cyclic properties. We illustrate this through a detailed application case study: accelerating the computation of the Loop Modulus, a measure of cycle richness used in advanced network characterization. Our algorithm dramatically reduces the runtime of the iterative constraint-finding bottleneck in this computation.",0,arxiv,Matematik,CC-BY/arXiv,A Fast Algorithm for Finding Minimum Weight Cycles in Mining Cyclic Graph Topologies
"In this work, the grammar of Hao \[ G=\{\, u\rightarrow u^{b_1+b_2+1} v^{a_1+a_2},\quad v\rightarrow u^{b_2}v^{a_2+1} \,\}, \] together with the correspondence between grammars and Combinatorial Differential Equations, is employed to obtain an interpretation of any triangular array of the form \[ T(n,k)=(a_2 n + a_1 k + a_0)\,T(n-1,k) + (b_2 n + b_1 k + b_0)\,T(n-1,k-1). \] Explicit formulas and structural properties are then derived through Analytic Differential Equations. In particular, the $r$-Whitney--Eulerian numbers and the cases where $b_2n+b_1k+b_0=1$ are obtained explicitly. Applications include new interpretation formulas for the $r$-Eulerian numbers with generating function for a special case.   Keywords: triangular recurrence, formal grammar, Combinatorial operators, differential equations,$r$-Eulerian, combinatorial interpretation, $r$-Whitney--Eulerian.",0,arxiv,Matematik,CC-BY/arXiv,Triangular Arrays using context-free grammar
"Motivated by earlier work of P.~A.~MacMahon and recent contributions of T.~Amdeberhan, G.~E.~Andrews, K.~Ono, A.~Singh, and R.~Tauraso on higher-order partition enumerants, we study a class of $q$-series arising from nested divisor structures. In particular, we consider the $q$-series \[ V_k(q) = \sum_{1 \le n_1 \le n_2 \le \cdots \le n_k} \frac{q^{\,n_1+n_2+\cdots+n_k}} {(1-q^{n_1})^2(1-q^{n_2})^2\cdots(1-q^{n_k})^2}, \] introduced recently as MacMahon-type generating functions. We further define a new MacMahon-type series \[ W_k(q) = \sum_{1 \le n_1 \le n_2 \le \cdots \le n_k} \frac{q^{\,2(n_1+n_2+\cdots+n_k)-k}} {(1-q^{2n_1-1})^2(1-q^{2n_2-1})^2\cdots(1-q^{2n_k-1})^2}, \] and establish families of identities, generating function relations, and hypergeometric representations for the truncated forms of $V_k(q)$ and $W_k(q)$. Connections with overpartition pairs and bipartitions with distinct odd parts arise naturally in this context.",0,arxiv,Matematik,CC-BY/arXiv,MacMahon-type $q$-series
"We extend Raimi's classical partition theorem to the continuous setting of the circle and $n$-dimensional torus. Building on recent work of HegyvÃ¡ri, Pach, and Pham in finite groups, we prove that there exist measurable partitions of the $n$-dimensional torus $\mathbb{T}^n$ with the property that for any finite measurable cover, some translated part of the cover has positive measure intersection with every partition element. Our proof adapts combinatorial arguments from the finite setting using measure-theoretic techniques and slicing arguments in product spaces.",0,arxiv,Matematik,CC-BY/arXiv,Raimi's theorem for the $n$-dimensional torus
"In this paper, we give the relationship between spectral radius and local structures of graphs and hypergraphs. Our work shows that certain local subgraphs (subhypergraphs) must occur when the spectral radius ratio is large. We also give spectral bounds on the local vector chromatic number in terms of tensor eigenvalues of graphs.",0,arxiv,Matematik,CC-BY/arXiv,Spectral characterizations of local structures of graphs and hypergraphs
"The positive part $U_q^+$ of the quantized enveloping algebra $U_q(\widehat{\mathfrak{sl}}_2)$ has a reflection equation presentation of Freidel-Maillet type, due to Baseilhac 2021. This presentation involves a K-matrix of dimension $2 \times 2$. Under an embedding of $U_q^+$ into a $q$-shuffle algebra due to Rosso 1995, this K-matrix can be written in closed form using a PBW basis for $U_q^+$ due to Terwilliger 2019. This PBW basis, together with two PBW bases due to Damiani 1993 and Beck 1994, can be obtain from a uniform approach by Ruan 2025. Following a natural fusion technique, we will construct fused K-matrices of arbitary meaningful dimension in closed form using the uniform approach. We will also show that any pair of these fused K-matrices satisfy Freidel-Maillet type equations.",0,arxiv,Matematik,CC-BY/arXiv,Freidel-Maillet type equations on fused K-matrices over the positive part of $U_q(\widehat{\mathfrak{sl}}_2)$
"In the Maker-Breaker resolving game, two players named Resolver and Spoiler alternately select unplayed vertices of a given graph $G$. The aim of Resolver is to select all the vertices of some resolving set of $G$, while Spoiler aims to select at least one vertex from every resolving set of $G$. In this paper, this game is investigated on the lexicographic product of graphs. It is proved that if Spoiler has a winning strategy on a graph $H$ no matter who starts the game, or if the first player has a winning strategy on $H$, then Spoiler always has a winning strategy on $G\circ H$. Special attention is paid to lexicographic products in which the second factor is either complete, or a path, or a cycle. For instance, in $G\circ P_{2\ell}$ and in $G\circ C_{2\ell}$, Resolver always wins, while in $G\circ P_{2\ell+1}$ and in $G\circ C_{2\ell+1}$ the same conclusion holds provided $G$ is free from false twins. On the other hand, Spoiler always wins on $G\circ P_5$. In most of the cases, the corresponding Maker-Breaker resolving number is also determined.",0,arxiv,Matematik,CC-BY/arXiv,Maker-Breaker resolving game played on lexicographic products of graphs
"Minimal prime graphs are connected graphs on at least two vertices whose complements satisfy the following conditions: triangle-freeness, 3- colorability, and edge-maximality with respect to the latter two properties. These graphs are prime graphs (or Gruenberg-Kegel graphs) of finite solv- able groups with the maximum number of Frobenius actions among their Sylow subgroups, and as such minimal prime graph complements have been shown to be highly structured, including, for instance, the presence of induced 5-cycles. It is also known that the minimum degree of minimal prime graph complements is 2. In this note, we show that the existence of a degree 2 vertex in a minimal prime graph complement determines its whole structure: it is simply a 5-cycle with three vertices, exactly two of which are adjacent to each other, being duplicated finitely often. In particular, such graphs belong to a class of graphs known as reseminant.",0,arxiv,Matematik,CC-BY/arXiv,Degree 2 vertices in minimal prime graph complements
"Given a $k$-colouring of a graph $G$ and two of the colours, a $Kempe$ $chain$ is a connected component of the subgraph of $G$ induced by the vertices coloured with one of these two colours. A $Kempe$ $swap$ changes one colouring into another by interchanging the colours of the vertices in a Kempe chain. Two colourings are $Kempe$ $equivalent$ if each can be obtained from the other by a series of Kempe swaps; the set of Kempe equivalent colourings is called a $Kempe$ $class$. For a graph $G$, let $Ï‡(G)$ denote its chromatic number and let $\mathcal{C}_{k}(G)$ denote the set of all $k$-colourings of $G$. We say $G$ is $Kempe$ $connected$ if for all $k\ge Ï‡(G)$, $\mathcal{C}_{k}(G)$ forms a Kempe class. For a graph $H$, graph $G$ is called $H$-$free$ if no induced subgraph of $G$ is isomorphic to $H$.   We prove that every $H$-free graph is Kempe connected if and only if $H$ is an induced subgraph of the path on four vertices, $P_4$.   The graph 2$K_2$ consists of four vertices and two edges which are not adjacent. We prove that for all $p\ge 0$, there is a $k$-colourable 2$K_2$-free graph $G$ such that $\mathcal{C}_{k+p}(G)$ does not form a Kempe class.",0,arxiv,Matematik,CC-BY/arXiv,Kempe changes in $H$-free graphs
"In 2023, Defant introduced toric promotion as a cyclic analogue of SchÃ¼tzenberger's well known promotion operator. Toric promotion is defined by a choice of simple graph $G$ and acts on the labeling of $G$ by a series of involutions. Defant described the orbit length of toric promotion on trees and showed that it does not depend on the initial labeling; we prove an analogous result for complete graphs. A natural question is how toric promotion behaves under certain graph operations. In the main results of this article, we analyze the orbits of toric promotion under the bridge sum graph operation, which joins two graphs by adding an edge between a vertex of each graph. We show that the orbit length of toric promotion on any graph constructed via a bridge sum of a tree or a complete graph with a simple graph does not depend on the restriction of the initial labeling to the tree or complete subgraph. Additionally, we describe the orbit lengths of toric promotion on the bridge sums of two complete graphs and the bridge sums of a tree with a complete graph, and show that they do not depend on the initial labeling. Finally, we describe the orbit length of toric promotion on the corona product of a complete graph with any tree, and show that it does not depend on the initial labeling.",0,arxiv,Matematik,CC-BY/arXiv,Orbits of toric promotion on bridge sums
"Building on prior work that established Matrix Quasi-tree Theorems for special embedded graphs, in this paper, we develop a comprehensive theory applicable to all embedded graphs. We introduce symbolic skew-adjacency matrices and reduction maps as key innovations, and prove that a specific polynomial derived from these matrices encodes all spanning quasi-trees of a bouquet. This result provides a complete analogue of the Matrix Tree Theorem for topological graph theory, with applications to quasi-tree enumeration in both orientable and non-orientable embedded graphs.",0,arxiv,Matematik,CC-BY/arXiv,Matrix Quasi-tree Theorem
"Let $G$ be a connected graph having more than two vertices and let $d_i$ denote the degree of vertex $v_i$ in $G$. Let $E(G)$ represent the edge set of $G$. Then, the augmented Sombor (ASO) index of $G$ is defined as $ASO(G) = \sum_{v_i v_j \in E(G)} \sqrt{(d_i + d_j - 2)^{-1}(d_i^2 + d_j^2)}.$ It is known that the cycle graph $C_n$ uniquely minimizes the ASO index in the class of all $n$-order unicyclic graphs. In this paper, we prove that the unique $n$-order unicyclic graph of maximum degree $n-1$ maximizes the ASO index in the aforementioned unicyclic graph class. We also prove that $ASO(G-v_iv_j)<ASO(G)$ whenever neither of the graphs $G-v_iv_j$ and $G$ contains any isolated edge. Utilizing this edge-deletion property, we characterize the unique graph maximizing the ASO index among all fixed-order connected graphs with a specified vertex connectivity (or edge connectivity).",0,arxiv,Matematik,CC-BY/arXiv,On the Augmented Sombor Index of Graphs
"Let $G$ be a connected graph with vertex set $\{v_1, v_2, \ldots, v_\mathbf{n}\}$. As a variant of the classical distance matrix, the \emph{exponential distance matrix} was introduced independently by Yan and Yeh, and by Bapat et al. For a nonzero indeterminate $q$, the exponential distance matrix $\mathscr{F} = (\mathscr{F}_{ij})_{\mathbf{n} \times \mathbf{n}}$ of $G$ is defined by $\mathscr{F}_{ij} = q^{d_{ij}},$ where $d_{ij}$ denotes the distance between vertices $v_i$ and $v_j$ in $G$. A connected graph is said to be a \emph{bi-block graph} if each of its blocks is a complete bipartite graph, possibly of varying bipartition sizes. In this paper, we obtain explicit expressions for the determinant, inverse, and cofactor sum of the exponential distance matrix of bi-block graphs. As a consequence, some known results concerning the exponential distance matrix and the $q$-Laplacian matrix are generalized.",0,arxiv,Matematik,CC-BY/arXiv,The exponential distance matrix of bi-block graphs
"Let $R$ be a commutative ring with identity and $G$ a graph. An extending generalized spline on $G$ is a vertex labeling $f \in \prod_{v} M_v$, where for each edge $e=uv$ there exists an $R$-module $M_{uv}$ together with homomorphisms $ \varphi_u : M_u \to M_{uv}$ and $ \varphi_v : M_v \to M_{uv}$ such that $\varphi_u(f_u)=\varphi_v(f_v).$ Extending generalized splines are further generalizations for generalized splines. They can also be considered as generalized splines over modules.   In this paper, we prove that some of the results for splines can be extended to generalized splines over modules $M_v=m_v\mathbb Z$ at each vertex $v$ and we define a method of a graph reduction based on graph operations on vertices and edges to produce an explicit $\mathbb{Z}$-module basis for generalized splines over modules. This corresponds to a sequence of surjective homomorphisms between the associated spline modules so that the space of splines decomposes as a direct sum of certain submodules.",0,arxiv,Matematik,CC-BY/arXiv,Generalized Splines over $\mathbb{Z}$-Modules on Arbitrary Graphs
"We investigate the algebra generated by the operators $x$ and $\mathrm{I} = \int_0^x$, which satisfy the commutation relation \[ [\mathrm{I},x] = \mathrm{I}x - x\mathrm{I} = - \mathrm{I}^2. \] We develop a combinatorial framework for the normal ordering of words in this algebra and show that any word can be written in the form \[ w = \sum_{i,j} c(i,j) \, x^i \mathrm{I}^j, \] where the coefficients $c(i,j)$ are signed integers. Focusing on powers of the operator $(x\mathrm{I})^n$, we demonstrate that the corresponding coefficients coincide with the classical Bessel numbers (OEIS A001498). We further extend this analysis to powers of the generalized operators $(x^Î»\mathrm{I}^Î´)^n$ and, finally, provide an explicit normal-ordered expression for an arbitrary word.",0,arxiv,Matematik,CC-BY/arXiv,Normal Ordering in the Algebra Generated by $x$ and $\mathrm{I}$ and a Combinatorial Generalization of Bessel Numbers
"We provide a complete and explicit characterization of the exposed extreme rays of the cone of sums of nonnegative circuit (SONC) polynomials. The criterion we derive is purely combinatorial and depends only on the existence of certain circuits within the ground set and on the nature of the corresponding extreme ray. Our constructive proofs also yield explicit exposing functionals, offering a basis for algorithmic detection of exposed rays in SONC-based optimization.",0,arxiv,Matematik,CC-BY/arXiv,Exposed extreme rays of the SONC cone
"We show that a certain class of affine hyperplane arrangements are $K(Ï€,1)$ by endowing their Falk complexes with an injective metric. This gives new examples of infinite $K(Ï€,1)$ arrangements in dimension $n>2$.",0,arxiv,Matematik,CC-BY/arXiv,"A new class of affine $K(Ï€,1)$ arrangements"
"We consider the Anderson model on the finite grid $G = \mathbb Z/L_1\mathbb Z\times\cdots\times\mathbb Z/L_d\mathbb Z$, defined by the random Hamiltonian $H_t=Î”+tV$, where $Î”$ is the discrete Laplacian and $V=\mathrm{diag}(\{Ï‰_{x}\}_{x\in G})$ is a random onsite potential with $Ï‰_x\simÎ¼$ i.i.d. We ask the natural question of when $H_t$ has simple eigenvalues and non-vanishing eigenvectors. We prove that, when $Î¼$ is a continuous probability distribution, $H_t$ has this property for all but finitely many $t$ values with probability $1$. However, when $Î¼$ is a Bernoulli distribution, the conditions fail with positive probability, for which we give a lower bound. We also calculate the exact probability of these conditions being met in the Bernoulli case when $d = 1$ and $L = L_1$ is prime.",0,arxiv,Matematik,CC-BY/arXiv,Simple Eigenvalues and Non-vanishing Eigenvectors of the Anderson Model
"We use a new $q$-exponential operator based on the $q^{\pm1}$-derivative $\D_{q^{\pm1}}$ of order 1 to derive summation formulas for bilateral basic hypergeometric series ${}_{0}Ïˆ_{1}$, ${}_{1}Ïˆ_{1}$, ${}_{1}Ïˆ_{2}$, and ${}_{2}Ïˆ_{2}$. In addition, we provide summation formulas for bilateral series whose terms are basic hypergeometric functions.",0,arxiv,Matematik,CC-BY/arXiv,A $q$-Exponential Operator Based on the Derivative of Order 1 and Summation of Bilateral Basic Hypergeometric Series
"Given a finite poset $P$, its zeta matrix $\mathbf Z$ encode fundamental incidence-theoretic information about the order structure. In this paper we introduce and study the \emph{order-complement matrix} $\overline{\mathbf Z} = \mathbf J - \mathbf Z$, where $\mathbf J$ is the all-ones matrix. We prove a closed formula for its characteristic polynomial and for its determinant, showing that $\det(\overline{\mathbf Z}) = (-1)^n \tildeÏ‡(P)$, where $n = |P|$ and $\tildeÏ‡(P)$ is the reduced Euler characteristic of $P$. This provides a new, unexpectedly simple linear-algebraic expression for the Euler characteristic of a poset, complementing existing determinant formulas for matrices derived from incidence relations.",0,arxiv,Matematik,CC-BY/arXiv,A formula for the Euler characteristic of a poset through the determinant of the order-complement matrix
"We study the nullspace of the adjacency matrix of split graphs, whose vertex set can be partitioned into a clique and an independent set. We introduce the clique-kernel, a subspace that decides whether clique vertices lie in the support of a kernel eigenvector, and we prove that its dimension is at most one. This yields the formula $null(Sp) = null(R) + \dim(\mathrm{Cker}(Sp))$, which fully describes the nullity of a split graph in terms of the biadjacency submatrix $R$. We also analyze unbalanced split graphs through the concept of swing vertices and characterize the structure of their kernel supports. Furthermore, we study the behavior of the nullspace under Tyshkevich composition and derive a closed formula for the determinant. These results provide a unified algebraic framework for understanding when a split graph is singular and how its combinatorial structure determines its nullspace.",0,arxiv,Matematik,CC-BY/arXiv,On the nullspace of split graphs
"We introduce the totally nonnegative Lagrangian Grassmannian $\rm{LG}_{\geq 0}^R (n,2n)$, a new subset of the totally nonnegative Grassmannian consisting of subspaces isotropic with respect to a certain bilinear form $R$. We describe its cell structure and show that each cell admits a representation by a rotationally symmetric (not necessarily reduced) plabic graph. Along the way, we develop new techniques for working with non-reduced plabic graphs.",0,arxiv,Matematik,CC-BY/arXiv,Rotationally symmetric plabic graphs and the Lagrangian Grassmannian
"We introduce the concept of quantum polymorphisms to the complexity theory of non-local games. We use this notion to give a full characterisation of the existence of commutativity gadgets for relational structures, introduced by Ji as a method for achieving quantum soundness of classical CSP reductions. Prior to our work, a classification was only known in the Boolean case [Culf--Mastel, STOC'25]. As an application of our framework, we prove that the entangled CSP parameterised by odd cycles is undecidable. Furthermore, we establish a quantum version of Galois connection for entangled CSPs in the case of non-oracular quantum homomorphisms.",0,arxiv,Matematik,CC-BY/arXiv,Quantum Polymorphisms and Commutativity Gadgets
"A graph $Î“$ is said to be universal for a class of graphs $\mathcal{H}$ if $Î“$ contains a copy of every $H \in \mathcal{H}$ as a subgraph. The number of edges required for a host graph $Î“$ to be universal for the class of $D$-degenerate graphs on $n$ vertices has been shown to be $O(n^{2-1/D}(\log n)^{2/D}(\log\log n)^{5})$. We generalise this result to $r$-uniform hypergraphs, showing the following. Given $D, r \ge 2$ and $n$ sufficiently large, there exists a constant $C = C(D, r)$ such that there exists a graph with at most \[Cn^{r-1/D}(\log n)^{2/D}(\log\log n)^{2r+1}\] edges which is universal for the class of $D$-degenerate $r$-uniform hypergraphs on $n$ vertices. This is tight up to the polylogarithmic term.",0,arxiv,Matematik,CC-BY/arXiv,Bounds for Hypergraph Universality
"In this work, we delve into the study of the 2-switch-degree of a graph $G$, which is nothing more than the degree of $G$ as a vertex of the realization graph $\mathcal{G}(s)$ associated with the degree sequence $s$ of $G$. We explore the characteristics of active and inactive vertices, the basic properties of the degree, explicit formulas for its computation, and its behavior in specific families of graphs, such as trees and unicyclic graphs.",0,arxiv,Matematik,CC-BY/arXiv,The 2-switch-degree of a graph
"The $\textit{$m$-deck}$ of an $n$-vertex graph is the multiset of unlabeled induced subgraphs with $m$ vertices. Caterpillars are trees in which all nonleaf vertices lie on a single path. We prove for $n\ge48$ that any $n$-vertex caterpillar is reconstructible (up to isomorphism) from its $m$-deck when $m>n/2$. The result is sharp, since for $n\ge6$ there are two $n$-vertex caterpillars having the same $\lfloor n/2 \rfloor$-deck. Our result proves the special case for caterpillars of a 1990 conjecture by NÃ½dl about trees.",0,arxiv,Matematik,CC-BY/arXiv,Caterpillars with $n$ vertices are reconstructible from subgraphs with at most $n/2+1$ vertices
"Let $p$ be an odd prime, Jianqiang Zhao has established a curious congruence, which is $$   \sum_{i+j+k=p \atop i,j,k > 0} \frac{1}{ijk} \equiv -2B_{p-3}\pmod p , $$ where $B_{n}$ denotes the $n$-th Bernoulli number. In this paper, we will generalize this kind of sums and prove a family of similar congruences modulo prime powers $p^r$.",0,arxiv,Matematik,CC-BY/arXiv,On a kind of generalized multi-harmonic sums
"This paper is devoted to the structure of the complete asymptotic expansion of the probability that a large combinatorial object is irreducible or consists of a given number of irreducible parts, where irreducibility is understood in terms of combinatorial construction SEQ, labeled or unlabeled. We show that for rapidly growing (i.e. gargantuan) combinatorial classes, the coefficients that appear in this expansion are integers and can be interpreted as linear combinations of the counting sequences of three closely related combinatorial classes. We apply this general asymptotic result to labeled and unlabeled (multi-)tournaments, as well as to (multi-)permutations and (multi-)matchings. We also explore the limits of our approach with respect to other combinatorial constructions.",0,arxiv,Matematik,CC-BY/arXiv,Asymptotic probability of irreducibles II: sequence
"We reduce the $p^2$ block all-one matrices in the generalized block Laplacian spectrum of graphs to $p$ block all-one matrices in the generalized block diagonal Lapalcian spectrum of graphs introduced by Wang and the second author (\textit{Adv. Appl. Math.} 173B (2026)). In this case the matrices are all real symmetric, and hence the spectrum is real, which does not hold for the generalized block Laplacian spectrum. We also investigate the analogue by Hermitian adjacency matrix of digraphs.",0,arxiv,Matematik,CC-BY/arXiv,Generalized block diagonal Laplacian spectrum of graphs
"We investigate the Gerver-Ramsey collinearity problem of determining the maximum number of points in a north-east lattice path without $k$ collinear points. Using a satisfiability solver, up to isomorphism we enumerate all north-east lattice paths avoiding $k$ collinear points for $k \leq 6$. We also find a north-east lattice path avoiding $k = 7$ collinear points with 327 steps, improving on the previous best length of 260 steps found by Shallit.",0,arxiv,Matematik,CC-BY/arXiv,North-East Lattice Paths Avoiding $k$ Collinear Points via Satisfiability
"In 2019, Trombetti and Zhou introduced a new family of $\mathbb{F}_{q^n}$-linear Maximum Rank Distance (MRD) codes over $\mathbb{F}_{q^{2n}}$. For such codes we propose a new syndrome-based decoding algorithm. It is well known that a syndrome-based decoding approach relies heavily on a parity-check matrix of a linear code. Nonetheless, Trombetti-Zhou codes are not linear over the entire field $\mathbb{F}_{q^{2n}}$, but only over its subfield $\mathbb{F}_{q^{n}}$. Due to this lack of linearity, we introduce the notions of $\mathbb{F}_{q^{n}}$-generator matrix and $\mathbb{F}_{q^{n}}$-parity-check matrix for a generic $\mathbb{F}_{q^{n}}$-linear rank-metric code over $\mathbb{F}_{q^{rn}}$ in analogy with the roles that generator and parity-check matrices play in the context of linear codes. Accordingly, we present an $\mathbb{F}_{q^n}$-generator matrix and $\mathbb{F}_{q^n}$-parity-check matrix for Trombetti-Zhou codes as evaluation codes over an $\mathbb{F}_q$-basis of $\mathbb{F}_{q^{2n}}$. This relies on the choice of a particular basis called \emph{trace almost dual basis}. Subsequently, denoting by $d$ the minimum distance of the code, we show that if the rank weight $t$ of the error vector is strictly smaller than $\frac{d-1}{2}$, the syndrome-based decoding of Trombetti-Zhou codes can be converted to the decoding of Gabidulin codes of dimension one larger. On the other hand, when $t=\frac{d-1}{2}$, we reduce the decoding to determining the rank of a certain matrix. The complexity of the proposed decoding for Trombetti-Zhou codes is also discussed.",0,arxiv,Matematik,CC-BY/arXiv,Decoding Trombetti-Zhou codes: a new syndrome-based decoding approach
"Determining the complexity of colouring ($4K_1, C_4$)-free graph is a long open problem. Recently Penev showed that there is a polynomial-time algorithm to colour a ($4K_1, C_4, C_6$)-free graph. In this paper, we will prove that if $G$ is a ($4K_1, C_4, P_6$)-free graph that contains a $C_6$, then $G$ has bounded clique-width. To this purpose, we use a new method to bound the clique-width, that is of independent interest. As a consequence, there is a polynomial-time algorithm to colour ($4K_1, C_4, P_6$)-free graphs.",0,arxiv,Matematik,CC-BY/arXiv,"On the structure of ($4K_1$, $C_4$, $P_6$)-free graphs"
"We investigate the generalized Latin square graph $Î“(S)$ for a finite semigroup $S$, extending the combinatorial concept of Latin rectangle graphs. By analyzing factorization multiplicities $(N_S)$ and cancellation failure $(N_R,N_C)$, we derive the degree formula $Â°(v) = 2n - 3 + Q(i,j)$. We prove that graph regularity is equivalent to $Q(i,j)$ being constant -- a condition termed \emph{compensated factorization} -- which often arises from congruence-induced symmetries. A non-cancellative semigroup of order 4 generating a 9-regular graph is constructed. Finally, computational enumeration up to order $n = 6$ (using the \textsf{smallsemi} package for GAP) confirms that regularity is a rare property which decreases significantly as the order increases.",0,arxiv,Matematik,CC-BY/arXiv,Generalized Latin Square Graphs of Semigroups: A Counting Framework for Regularity
"An ordered graph is a graph enhanced with a linear order on the vertex set. An ordered graph is a core if it does not have an order-preserving homomorphism to a proper subgraph. We say that $H$ is the core of $G$ if (i) $H$ is a core, (ii) $H$ is a subgraph of $G$, and (iii) $G$ admits an order-preserving homomorphism to $H$. We study complexity aspects of several problems related to the cores of ordered graphs. Interestingly, they exhibit a different behavior than their unordered counterparts. We show that the retraction problem, i.e., deciding whether a given graph admits an ordered-preserving homomorphism to its specific subgraph, can be solved in polynomial time. On the other hand, it is \NP-hard to decide whether a given ordered graph is a core. In fact, we show that it is even \NP-hard to distinguish graphs $G$ whose core is largest possible (i.e., if $G$ is a core) from those, whose core is the smallest possible, i.e., its size is equal to the ordered chromatic number of $G$. The problem is even \wone-hard with respect to the latter parameter.",0,arxiv,Matematik,CC-BY/arXiv,On Computational Aspects of Cores of Ordered Graphs
"Ordered matchings, defined as graphs with linearly ordered vertices, where each vertex is connected to exactly one edge, play a crucial role in the area of ordered graphs and their homomorphisms. Therefore, we consider related problems from the complexity point of view and determine their corresponding computational and parameterized complexities. We show that the subgraph of ordered matchings problem is NP-complete and we prove that the problem of finding ordered homomorphisms between ordered matchings is NP-complete as well, implying NP-completeness of more generic problems. In parameterized complexity setting, we consider a natural choice of parameter - a number of vertices of the image ordered graph. We show that in contrast to the complexity context, finding homomorphisms if the image ordered graph is an ordered matching, this problem parameterized by the number of vertices of the image ordered graph is FPT, which is known to be W[1]-hard for the general problem. We also determine that the problem of core for ordered matchings is solvable in polynomial time which is again in contrast to the NP-completeness of the general problem. We provide several algorithms and generalize some of these problems into ordered graphs with colored edges.",0,arxiv,Matematik,CC-BY/arXiv,On Computational Aspects of Ordered Matching Problems
"We examine ordered graphs, defined as graphs with linearly ordered vertices, from the perspective of homomorphisms (and colorings) and their complexities. We demonstrate the corresponding computational and parameterized complexities, along with algorithms associated with related problems. These questions are interesting, and we show that numerous problems lead to various complexities. The reduction from homomorphisms of unordered structures to homomorphisms of ordered graphs is proved, achieved with the use of ordered bipartite graphs. We then determine the NP-completeness of the problem of finding ordered homomorphisms of ordered graphs and the XP and W[1]-hard nature of this problem parameterized by the number of vertices of the image ordered graph. Classes of ordered graphs for which this problem can be solved in polynomial time are also presented.",0,arxiv,Matematik,CC-BY/arXiv,Complexity Aspects of Homomorphisms of Ordered Graphs
"Continuing the investigation of real Calabi-Yau hypersurfaces in toric varieties obtained by patchworking, we present a new theorem concerning the computation of their first Betti number using mirror symmetry. Although the proof of this result will appear elsewhere, we focus here on its consequences and applications to the topology of real Calabi-Yau hypersurfaces.",0,arxiv,Matematik,CC-BY/arXiv,First Betti number of real Calabi-Yau hypersurfaces: examples
"We prove that one variable equations in the lamplighter group $\MZ_2\wr \MZ$ are decidable and describe an algorithm for solving such equations. The algorithm has super-exponential time complexity in the worst case. We also show that, for most equations, decidability can be determined in nearly quadratic time; that is, the problem admits a nearly quadratic-time solution in the generic case.",0,arxiv,Matematik,CC-BY/arXiv,One variable equations over the lamplighter group
"Twinned chain polytopes form a broad class of non-centrally symmetric reflexive polytopes and exhibit intriguing structures. In the present paper, we show that the number of facets of $d$-dimensional twinned chain polytopes is at most $6^{d/2}$. In case $d$ is even, the equality holds if and only if the polytope is isomorphic to a free sum of $d/2$ copies of del Pezzo polygons. This result contributes a partial answer to Nill's conjecture: the number of facets of a $d$-dimensional reflexive polytope is at most $6^{d/2}$.",0,arxiv,Matematik,CC-BY/arXiv,Facet numbers of non-centrally symmetric reflexive polytopes arising from posets
"In this paper, we study rooted products of graphs from the perspective of combinatorial commutative algebra. For edge ideals, we introduce the 2-Cohen-Macaulayness with respect to a vertex and use it to investigate when edge ideals of rooted products of graphs are Cohen-Macaulay. Moreover, we completely determine when attaching a graph on at most six vertices to a given graph as rooted products, yields a Cohen-Macaulay edge ideal. Also, we define mulit-clique corona graphs as a generalization of clique-corona graphs and multi-whisker graphs. We prove that multi-clique corona graphs are vertex decomposable and hence sequentially Cohen-Macaulay. Also, we give formulas for the projective dimension and the Castelnuovo-Mumford regularity.",0,arxiv,Matematik,CC-BY/arXiv,Algebraic study on rooted products of graphs and multi-clique corona graphs
"A spanning subgraph $F$ of a graph $G$ is defined as an even factor of $G$, if the degree $d_F(v)=2k, k\in\mathbb{N}^+$ for every vertex $v\in V(G)$. This note establishes a sufficient condition to ensure that a connected graph $G$ of even order with the minimum degree $Î´$ contains an even factor based on the signless Laplacian spectral radius.",0,arxiv,Matematik,CC-BY/arXiv,Signless Laplacian spectral conditions for even factors in graphs
"While Kronecker coefficients $g(Î»,Î¼,Î½)$ with bounded rows are polynomial-time computable via lattice-point methods, no explicit closed-form formulas have been obtained for genuinely three-row cases in the 87 years since Murnaghan's foundational work. This paper provides such formulas for the first time and identifies a universal structural boundary at parameter value 5 where elementary combinatorial patterns collapse.   We analyze two independent families of genuinely three-row coefficients and establish that for $k \leq 4$, the formulas exhibit elementary structure: oscillation bounds follow the triangular-Hogben pattern, and polynomial expressions factor completely over $\mathbb{Z}$. At the critical threshold $k=5$, this structure collapses: the triangular pattern fails, and algebraic obstructions -- irreducible quadratic factors with negative discriminant -- emerge.   We develop integer forcing, a proof technique exploiting the tension between continuous asymptotics and discrete integrality. As concrete results, we prove that $g((n,n,1)^3) = 2 - (n \mod 2)$ for all $n \geq 3$ -- the first explicit formula for a genuinely three-row Kronecker coefficient -- derive five explicit polynomial formulas for staircase-hook coefficients, and verify Saxl's conjecture for 132 three-row partitions.",0,arxiv,Matematik,CC-BY/arXiv,Algebraic Obstructions and the Collapse of Elementary Structure in the Kronecker Problem
"Let $n$ be an integer greater than $1$. We consider a group presented as $G(p_1,p_2,\dots,p_n)=\langle x_1,x_2,\dots, x_n \mid x_1^{p_1} =x_2^{p_2}=\cdots =x_n^{p_n} \rangle$, with integers $p_1,p_2,\dots,p_n$ satisfying $2 \leq p_1 \leq p_2 \leq \cdots \leq p_n$. This group is an amalgamated free product of infinite cyclic groups and is geometrically realized as the fundamental group of a Seifert fiber space over the 2-dimensional disk with $n$ cone points whose associated cone angles are $\frac{2Ï€}{p_1},\frac{2Ï€}{p_2},\dots,\frac{2Ï€}{p_n}$. In this paper, we present a formula for the spherical growth series of the group $G(p_1,\dots,p_n)$ with respect to the generating set $\{x_1,\dots,x_n,x_1^{-1},\dots,x_n^{-1}\}$. We show that from this formula, a rational function expression for the spherical growth series of $G(p_1,\dots,p_n)$ can be derived in concrete form for given $p_1,\dots,p_n$. In fact, we wrote an elementary computer program based on this formula that yields an explicit form of a single rational fraction expression for the spherical growth series of $G(p_1,\dots,p_n)$. We present such expressions for several tuples $(p_1,\dots,p_n)$. In 1999, C. P. Gill obtained a similar formula for the same group in the case $n=2$ and showed that there exists a rational function expression for the spherical growth series of $G(p_1,\dots,p_n)$ for $n \geq 2$.",0,arxiv,Matematik,CC-BY/arXiv,The spherical growth series of amalgamated free products of infinite cyclic groups
"In 2019, Andrews investigated integer partitions in which all parts of a given parity are smaller than those of the opposite parity and introduced eight partition functions based on the parity of the smaller parts and parts of a given parity appearing at most once or an unlimited number of times. Recently, Bringmann, Craig and Nazaroglu studied the asymptotic behavior of the eight partition functions proved several inequalities for sufficiently large $n$. At the end of their paper, they asked for combinatorial proofs of those inequalities. In this paper, we prove that an inequality on partitions separated by parity holds for $n\geq 373$ by a combinatorial method. This answers a question posed by Bringmann, Craig and Nazaroglu.",0,arxiv,Matematik,CC-BY/arXiv,Combinatorial proof of an inequality on some partitions separated by parity
"Polyomino ideals, defined as the ideals generated by the inner $2$-minors of a polyomino, are a class of binomial ideals whose algebraic properties are closely related to the combinatorial structure of the underlying polyomino. We provide a unified account of recent advances on two central themes: the characterization of prime polyomino ideals and the emerging connection between the Hilbert-PoincarÃ© series and Gorensteinness of $K[\mathcal{P}]$ with the classical rook theory. Some further related properties, as radicality, primary decomposition, and levelness are discussed, and a \textit{Macaulay2} package, namely \texttt{PolyominoIdeals}, is also presented.",0,arxiv,Matematik,CC-BY/arXiv,Recent Advances in the Theory of Polyomino Ideals
"Let ${\mathcal C}(Î©)$ be the linear code arising from a projective system $Î©$ of $\mathrm{PG}(V).$ Consider the point-line geometry $Î“=({\mathcal P},{\mathcal L})$ and a projective embedding $\varepsilon\colon Î“\rightarrow \mathrm{PG}(V)$ of $Î“.$ We show that the projective code obtained by taking as projective system $Î©:=\varepsilon(\mathcal{P})$ is minimal if the graph induced on the set $Î“\setminus\varepsilon^{-1}(H)$ by the collinearity graph of $Î“$ is connected for any hyperplane $H$ of $\mathrm{PG}(V)$. As an application, Grassmann codes, Segre codes, polar Grassmann codes of orthogonal, symplectic, hermitian type and codes arising from the point-hyperplane geometry of a projective space are minimal codes.",0,arxiv,Matematik,CC-BY/arXiv,On minimal codes arising from projective embeddings of point-line geometries
"We consider a graph representation in the plane, called the transparent rectangle visibility graph (TRVG), where each vertex is represented by a rectangle in the plane with sides parallel to the plane axes, in a way that any two vertices are adjacent if and only if a vertical or horizontal line can be drawn from the interior of one rectangle to the other. Expanding upon previously done work by Juntarapomdach and Kittipassorn, we show that $K_{3,3,3}$ is not a TRVG, and classify complete $k$-partite TRVGs. We also prove that the complement of $C^2_n$ is not a TRVG whenever $n \geq 15$, and that every $k$-partite TRVG with $n$ vertices has at most $2(k-1)n-k(k-1)$ edges. Furthermore, we introduce a novel representation, the intersecting transparent rectangle visibility graph (ITRVG), and show that there exists a graph that is an ITRVG but not a TRVG.",0,arxiv,Matematik,CC-BY/arXiv,Multipartite and Structural Results on Transparent Rectangle Visibility Graphs
"We investigate the minimal free resolutions of closed neighborhood ideals of graphs within the framework of Barile-Macchia (BM) resolutions. We show that for any tree $T$, the closed neighborhood ideal $NI(T)$ is bridge-friendly, and hence its BM resolution is minimal. The combinatorial structure of trees further allows us to construct a maximal critical cell of size $Î±(T)$, leading to the equality $\mathrm{pdim}(R/NI(T)) = Î±(T)$, where $Î±(T)$ denotes the independence number of $T$ and $\mathrm{pdim}$ is the projective dimension. Using Betti splitting techniques, we also obtain explicit formulas for the graded Betti numbers of $NI(P_n)$, where $P_n$ is the path graph on $n$ vertices. Finally, we make some observations on the bridge-friendly condition of the closed neighborhood ideals of chordal and bipartite graphs.",0,arxiv,Matematik,CC-BY/arXiv,Barile-Macchia Resolutions and the closed neighborhood ideal
"We study the generic fibre of the Hadamard product of linear spaces via matroid theory and tropical geometry. To do so, we introduce the flip product, a numerical invariant associated to a pair of matroids defined via the stable intersection of their (flipped) Bergman fans. Our first main result is that the cardinality of a generic fibre for the Hadamard product of linear spaces is exactly the flip product of their matroids. We also provide a recursive algorithm for computing the flip product of any pair of matroids. As an application of our techniques, we extend the notion of realisation numbers from rigidity theory to rotational-symmetric and periodic realisation numbers and we provide combinatorial algorithms to compute them. Finally, we show a number of existing matroid invariants are specialisations of the flip product, including the beta invariant.",0,arxiv,Matematik,CC-BY/arXiv,Counting fibres of the Hadamard product using Bergman fans
"A temporal graph $G$ is a sequence $(G_t)_{t \in I}$ of graphs on the same vertex set of size $n$. The \emph{temporal exploration problem} asks for the length of the shortest sequence of vertices that starts at a given vertex, visits every vertex, and at each time step $t$ either stays at the current vertex or moves to an adjacent vertex in $G_t$. Bounds on the length of a shortest temporal exploration have been investigated extensively. Perhaps the most fundamental case is when each graph $G_t$ is connected and has bounded maximum degree. In this setting, Erlebach, Kammer, Luo, Sajenko, and Spooner [ICALP 2019] showed that there exists an exploration of $G$ in $\mathcal{O}(n^{7/4})$ time steps. We significantly improve this bound by showing that $\mathcal{O}(n^{3/2} \sqrt{\log n})$ time steps suffice.   In fact, we deduce this result from a much more general statement. Let the \emph{average temporal maximum degree} $D$ of $G$ be the average of $\max_{t \in I} d_{G_t}(v)$ over all vertices $v \in V(G)$, where $d_{G_t}(v)$ denotes the degree of $v$ in $G_t$. If each graph $G_t$ is connected, we show that there exists an exploration of $G$ in $\mathcal{O}(n^{3/2} \sqrt{D \log n})$ time steps. In particular, this gives the first subquadratic upper bound when the underlying graph has bounded average degree. As a special case, this also improves the previous best bounds when the underlying graph is planar or has bounded treewidth and provides a unified approach for all of these settings. Our bound is subquadratic already when $D=o(n/\log n)$.",0,arxiv,Matematik,CC-BY/arXiv,Improved exploration of temporal graphs
"Given an oriented graph $D$, the inversion of a subset $X$ of vertices consists in reversing the orientation of all arcs with both endpoints in $X$. When the subset $X$ is of size $p$ (resp. at most $p$), this operation is called an $(=p)$-inversion (resp. $(\leq p)$-inversion). Then, an oriented graph is $(=p)$-invertible if it can be made acyclic by a sequence of $p$-inversions. We observe that, for $n=|V(D)|$, deciding whether $D$ is $(=n-1)$-invertible is equivalent to deciding whether $D$ is acyclically pushable, and thus NP-complete. In all other cases, when $p \neq n-1$, we construct a polynomial-time algorithm to decide $(=p)$-invertibility.   We then consider the $(= p)$-inversion number, $\text{inv}^{= p}(D)$ (resp. $(\leq p)$-inversion number, $\text{inv}^{\leq p}(D)$), defined as the minimum number of $(=p)$-inversions (resp. $(\leq p)$-inversions) rendering $D$ acyclic. We show that every $(=p)$-invertible digraph $D$ satisfies $\text{inv}^{= p}(D) \leq |A(D)|$ for every integer $p\geq 2$. When $p$ is even, we bound $\text{inv}^{= p}$ by a (linear) function of the feedback arc set number, and rule out the existence of any bounding function for odd $p$.   Finally, we study the complexity of deciding whether the $(= p)$-inversion number, or the $(\leq p)$-inversion number, of a given oriented graph is at most a given integer $k$. For any fixed positive integer $p \geq 2$, when $k$ is part of the input, we show that both problems are NP-hard even in tournaments. In general oriented graphs, we prove $W[1]$-hardness for both problems when parameterized by $p$, even for $k=1$. In contrast, we exhibit polynomial kernels in $p + k$ for both problems in tournaments.",0,arxiv,Matematik,CC-BY/arXiv,Making an oriented graph acyclic using inversions of bounded or prescribed size
"We propose definitions of the common bases complex, the poset of decompositions, and the poset of partial decompositions for arbitrary spherical buildings. We show that the poset of decompositions is Cohen-Macaulay, and that the poset of partial decompositions is spherical and homotopy equivalent to the common bases complex. To prove these results, we rely on the concepts of opposition, Levi spheres, and convexity in buildings.   In particular, our results extend the already known constructions for the linear case (vector spaces) to arbitrary buildings. As a byproduct, we see that the poset of ordered partial decompositions carries the square of the Steinberg representation.",0,arxiv,Matematik,CC-BY/arXiv,Posets of decompositions in spherical buildings
"An edge subset \( S \subseteq E(G) \) is called a 3-restricted edge-cut if \( G - S \) is disconnected and each component of \( G - S \) contains at least three vertices. The 3-restricted edge-connectivity of a graph \( G \), denoted by \( Î»_3(G) \), is defined as the minimum cardinality among all 3-restricted edge-cuts if there are at least one; otherwise, \( Î»_3(G) = +\infty \). It is proved that $Î»_3(G)\leqÎ¾_3(G)$ if $G$ has a 3-restricted edge-cut, where $Î¾_3(G) = \min \left\{ |[X, V(G) \setminus X]_G|:|X| = 3 \text{ and } G[X] \text{ is connected} \right\}.$ If \( Î»_3(G) = Î¾_3(G) \), then \( G \) is said to be maximally 3-restricted edge-connected. The direct product of two graphs $G$ and $H$, denoted by $G \times H$, is defined as the graph with vertex set \( V(G \times H) = V(G) \times V(H) \), where two vertices \( (u_1, v_1) \) and \( (u_2, v_2) \) are adjacent in \( G \times H \) if and only if \( u_1u_2 \in E(G) \) and \( v_1v_2 \in E(H) \). In this paper, we determine, for a regular connected graph \( G\), the 3-restricted edge-connectivity of \( G \times C_n \), \( G \times K_n \) and \( G \times T_n \), where \( C_n \), \( K_n \) and \( T_n \) are the cycle, the complete graph and the total graph with \( n \) vertices, respectively. As corollaries, we establish sufficient conditions for the direct product graphs \( G \times C_n \), \( G \times K_n \) and \( G \times T_n \) to be maximally 3-restricted edge-connected.",0,arxiv,Matematik,CC-BY/arXiv,The 3-restricted edge-connectivity of the direct product graphs
"Given a connected graph $G=(V,E)$ and a $k$-set $S\subseteq V(G)$, the $Steiner$ $distance$ $d_{G}(S)$ of $S$ is defined as the size of a minimum tree including $S$ in $G$. The $Steiner$ $k$-$eccentricity$ of a vertex $v$ in $G$ is the maximum value of $d_G(S)$ over all $S\subseteq V(G)$ with $|S|=k$ and $v\in S$. The minimum Steiner $k$-eccentricity over all vertices, denoted by $Sr_k(G)$, is called the $Steiner$ $k$-$radius$ of $G$ and the maximum Steiner $k$-eccentricity over all vertices, denoted by $Sd_k(G)$, is its $Steiner$ $k$-$diameter$. The $Steiner$ $(k,k^{\prime})$-$eccentricity$ of a $k^{\prime}$-subset $S^{\prime}$ of $V(G)$, which is an extension of the Steiner $k$-eccentricity of a vertex $v$, is defined as the maximum Steiner distance over all $k$-subsets of $V(G)$ containing $S^{\prime}$. The minimum Steiner $(k,k^{\prime})$-eccentricity among all $k^{\prime}$-subsets of $V(G)$, denoted by $Sr_{k,k^{\prime}}(G)$, is called the $Steiner$ $(k,k^{\prime})$-$radius$ of $G$. In 1989, Chartrand, Oellermann, Tian and Zou showed that for any $k\geq3$, $Sd_k(T)\leq \frac{k}{k-1}Sr_k(T)$ for any tree $T$. In this paper, we generalize this result and show that $Sd_k(T)\leq \frac{k}{k-k^{\prime}}Sr_{k,k^{\prime}}(T)$ for any $k\geq3$, $k>k^{\prime}\geq1$. Furthermore, for $k^{\prime}=2$ and $k^{\prime}=3$, we obtain a tight upper bound of the Steiner $k$-diameter by the Steiner $(k,k^{\prime})$-radius for all trees.",0,arxiv,Matematik,CC-BY/arXiv,"On the Steiner $k$-diameter and Steiner ($k,k^{\prime}$)-radius of trees"
"The Lonely Runner Conjecture of Wills and Cusick states that if $k+1$ runners start running at distinct constant speeds around a unit-length circular track, then for each runner there is a time when he/she is at least $1/(k+1)$ away from all other runners. Rosenfeld recently obtained a computer-assisted proof of the conjecture for $8$ runners. By refining his approach with a sieve, we obtain proofs (also computer-assisted) for $9$ and $10$ runners.",0,arxiv,Matematik,CC-BY/arXiv,Nine and ten lonely runners
"A connected nontrivial graph $G$ is {\it matching covered} if every edge of $G$ is contained in some perfect matching of $G$. A matching covered graph $G$ is {\it minimal} if $G-e$ is not matching covered for each edge $e$ of $G$. A graph is said to be {\it factor-critical} if $G-v$ has a perfect matching for every $v\in V(G)$. A factor-critical graph $G$ is said to be {\it minimal factor-critical} if $G-e$ is not factor-critical graph for each edge $e\in E(G)$. In this paper, by employing ear decomposition and edge-exchange techniques, the greatest spectral radii of minimal matching covered bipartite graphs and minimal factor-critical graphs are determined, and the corresponding extremal graphs are characterized.",0,arxiv,Matematik,CC-BY/arXiv,The spectral radii and extremal graphs of two types of minimal graphs
"Chung and Graham [J. London Math. Soc., 1983] claimed to prove that there exists an $n$-vertex graph $G$ with $\frac{5}{2}n \log_2 n + O(n)$ edges that contains every $n$-vertex tree as a subgraph. Frati, Hoffmann and TÃ³th [Combin. Probab. Comput., 2023] discovered an error in the proof. By adding more edges to $G$ the error can be corrected, bringing the number of edges in $G$ to $\frac{7}{2}n \log_2 n + O(n). $   We make the first improvement to Chung and Graham's bound in over four decades by showing that there exists an $n$-vertex graph with $\frac{14}{5}n \log_2 n + O(n) $ edges that contains every $n$-vertex tree as a subgraph.   Furthermore, we generalise this bound for treewidth-$k$ graphs by showing that there exists a graph with $O(kn\log n)$ edges that contains every $n$-vertex treewidth-$k$ graph as a subgraph. This is best possible in the sense that $Î©(kn\log{n})$ edges are required.",0,arxiv,Matematik,CC-BY/arXiv,On Universal Graphs for Trees and Tree-Like Graphs
"We explore the novel connection between rook placements on collections of cells, also known as pruned chessboards, and the algebraic properties of ideals generated by $2$-minors. We design an algorithm to compute the switching rook polynomial of a collection of cells and show that it coincides with the $h$-polynomial of the associated coordinate ring for all collections up to rank 10 and polyominoes up to rank 12. Motivated by this evidence, we conjecture that the correspondence holds in general, and we prove it for certain convex collections of cells by algebraic tools.",0,arxiv,Matematik,CC-BY/arXiv,Switching rook polynomial of collections of cells
"We investigate a pursuit-evasion game on an undirected graph in which a robber, moving at a fixed constant speed, attempts to evade a team of cops who are blind to the robber's location and can quickly travel between any pair of vertices in the graph. The blind cop-width is the minimum number of cops needed to catch the robber on a given graph. We link it with other known graph parameters defined in terms of pursuit-evasion games, and show a new lower bound with respect to treewidth. The proof introduces the notion of balanced minors, where all branch sets of a minor model have equal size.",0,arxiv,Matematik,CC-BY/arXiv,Blind cop-width and balanced minors of graphs
"We prove that several natural graph classes have tree-decompositions with minimum width such that each bag has bounded treewidth. For example, every planar graph has a tree-decomposition with minimum width such that each bag has treewidth at most 3. This treewidth bound is best possible. More generally, every graph of Euler genus $g$ has a tree-decomposition with minimum width such that each bag has treewidth in $O(g)$. This treewidth bound is best possible. Most generally, every $K_p$-minor-free graph has a tree-decomposition with minimum width such that each bag has treewidth at most some polynomial function $f(p)$.   In such results, the assumption of an excluded minor is justified, since we show that analogous results do not hold for the class of 1-planar graphs, which is one of the simplest non-minor-closed monotone classes. In fact, we show that 1-planar graphs do not have tree-decompositions with width within an additive constant of optimal, and with bags of bounded treewidth. On the other hand, we show that 1-planar $n$-vertex graphs have tree-decompositions with width $O(\sqrt{n})$ (which is the asymptotically tight bound) and with bounded treewidth bags. Moreover, this result holds in the more general setting of bounded layered treewidth, where the union of a bounded number of bags has bounded treewidth.",0,arxiv,Matematik,CC-BY/arXiv,Optimal Tree-Decompositions with Bags of Bounded Treewidth
"We introduce a fun problem that can be considered as a variant of the classic birthday problem, the Bottleneck Birthday Problem (BBP). It is stated as: what is the maximum number of people we have to choose so that no day of the year has more than $r \geq 1$ birthdays incident on it with probability at least 1/2? We provide a survey of techniques used in the literature on occupancy and load balancing problems to derive recurrence relations for exact computation of the probability and the number of people, keeping probability fixed at a threshold. Further, we show that restricted Stirling numbers of the second kind can be used to derive an additional recurrence, in a novel way. We provide numerical results from an implementation of the recurrences.",0,arxiv,Matematik,CC-BY/arXiv,The Bottleneck Birthday Problem
"We provide an involution proof of a Catalan-tangent number identity arising from the study of peak algebra that was found by Aliniaeifard and Li. In the course, we find a new combinatorial identity for the tangent numbers $T_{2n+1}$: $$ \sum_{k=0}^{n}(-1)^{k}{2n+1\choose 2k}2^{2n-2k}T_{2k+1}=(-1)^nT_{2n+1}. $$ Moreover, we derive two different $q$-analogs of the above identity from the combinatorial perspective.",0,arxiv,Matematik,CC-BY/arXiv,An involution for a Catalan-tangent number identity
"An $r$-rooted digraph is a flame if for each non-root vertex $v$, there is a set of edge-disjoint directed paths from $r$ to $v$ that covers all ingoing edges of $v$. The study of flames was initiated by LovÃ¡sz, who showed that in a finite rooted digraph, the edge-minimal subgraphs that preserve all local edge-connectivities from the root are always flames. It is known that the edge sets of the flame subgraphs of any finite rooted digraph form a greedoid. SzeszlÃ©r showed recently that if the digraph is acyclic, then the bases of this greedoid are the bases of a matroid. We show that a suitable formulation of SzeszlÃ©r's theorem is valid for infinite digraphs under the additional assumption that there are no backward-infinite directed paths (which assumption is indeed essential). We also prove that the ''correct'' infinite generalisation of LovÃ¡sz's theorem also holds for this class of digraphs.",0,arxiv,Matematik,CC-BY/arXiv,Large flames in rooted acyclic digraphs without backward-infinite paths
"We revisit the framework of interactive proofs for distribution testing, first introduced by Chiesa and Gur (ITCS 2018), which has recently experienced a surge in interest, accompanied by notable progress (e.g., Herman and Rothblum, STOC 2022, FOCS 2023; Herman, RANDOM~2024). In this model, a data-poor verifier determines whether a probability distribution has a property of interest by interacting with an all-powerful, data-rich but untrusted prover bent on convincing them that it has the property. While prior work gave sample-, time-, and communication-efficient protocols for testing and estimating a range of distribution properties, they all suffer from an inherent issue: for most interesting properties of distributions over a domain of size $N$, the verifier must draw at least $Î©(\sqrt{N})$ samples of its own. While sublinear in $N$, this is still prohibitive for large domains encountered in practice.   In this work, we circumvent this limitation by augmenting the verifier with the ability to perform an exponentially smaller number of more powerful (but reasonable) \emph{pairwise conditional} queries, effectively enabling them to perform ``local comparison checks'' of the prover's claims. We systematically investigate the landscape of interactive proofs in this new setting, giving polylogarithmic query and sample protocols for (tolerantly) testing all \emph{label-invariant} properties, thus demonstrating exponential savings without compromising on communication, for this large and fundamental class of testing tasks.",0,arxiv,Matematik,CC-BY/arXiv,Interactive Proofs For Distribution Testing With Conditional Oracles
"Motzkin and Taussky (and independently, Gerstenhaber) proved that the unital algebra generated by a pair of commuting $d\times d$ matrices over a field has dimension at most $d$. Since then, it has remained an open problem to determine whether the analogous statement is true for triples of matrices which pairwise commute. We answer this question for combinatorially-motivated classes of such triples.",0,arxiv,Matematik,CC-BY/arXiv,On combinatorial algebras generated by three commuting matrices
"In this paper, we prove that the zero-divisor graph $Î“(P)$ of a Boolean poset $P$ is both well-covered and Cohen--Macaulay. Furthermore, for a poset $\mathbf{P} = \prod_{i=1}^{n} P_i$ $(n \ge 3)$, where each $P_i$ is a finite bounded poset satisfying $Z(P_i) = \{0\}$ for all $i$, and $\le |P_1| \le |P_2| \le \cdots \le |P_n|, $ we show that the zero-divisor graph $Î“(\mathbf{P})$ is Cohen--Macaulay if and only if $\mathbf{P}$ is a Boolean lattice.",0,arxiv,Matematik,CC-BY/arXiv,Cohen-Macauleyness of the Zero-Divisor Graph of a Boolean Poset
"Fix a strong rectangulation pattern $P$ of size $L$. We show that the growth constant of the class of strong rectangulations avoiding $P$ is strictly smaller than $Î›=27/2$, the growth constant for all strong rectangulations. More precisely, forbidding any such $P$ yields a pattern-uniform exponential drop of at least $Î›- 1/Î›^{3L-1}$. Consequently, the proportion of $P$-avoiding rectangulations among all rectangulations tends to zero as $n\to \infty$. This is the first result on the uniform drop of exponential growth for pattern-avoiding rectangulations. The proof utilizes the standard correspondence with leftmost history quadrant walks, along with a pattern-insertion scheme that controls the radius of convergence of the associated generating functions, thereby establishing the first uniform exponential upper bound for rectangulation classes defined by geometric avoidance.",0,arxiv,Matematik,CC-BY/arXiv,Rectangulations avoiding a pattern
"We show that MisÃ¨re Partizan Arc Kayles is PSPACE-complete on planar graphs via a reduction from Bounded Two-Player Constraint Logic. Furthermore, we show how to embed our gadgets onto the square and triangular grids. In order to clearly explain these results, we get into the details of Bounded Two-Player Constraint Logic and find three PSPACE-complete variants of that as well.",0,arxiv,Matematik,CC-BY/arXiv,"MisÃ¨re Partizan Arc Kayles is PSPACE-complete, even on Planar Graphs"
"Classical spectral graph theory characterizes graphs with logarithmic mixing time. In this work, we present a combinatorial characterization of graphs with constant mixing time. The combinatorial characterization is based on the small-set bipartite density condition, which is weaker than having near-optimal spectral radius and is stronger than having near-optimal small-set vertex expansion.",0,arxiv,Matematik,CC-BY/arXiv,A Combinatorial Characterization of Constant Mixing Time
"Historically, proofs of $\mathrm{BPI}$ in models without choice have relied on a contradiction framework that was introduced by Halpern. We introduce the filter extension property for permutation models and symmetric extensions, which formalizes the naÃ¯ve approach to extend arbitrary filters to ultrafilters by repeatedly extending filters by minimal increments. We use this framework to give the first direct proof of $\mathrm{BPI}$ in the generalized Cohen model $N(I,Q)$ -- a model that adds a Dedekind-finite set of mutually $Q$-generic filters over a ground model $M\vDash\mathrm{ZFC}$. In the case that the index set $I$ is large, we adapt Harrington's proof of the Halpern-LÃ¤uchli theorem to prove the result. We then extend the results from Karagila and Schlicht to show that $I$ can be assumed to be large without loss of generality. The approach given by Harrington's proof is essentially dynamical, and we show that this technique can be used in permutation models to reprove a direction of Blass' theorem: that a dynamical condition called the Ramsey property is sufficient for $\mathrm{BPI}$ to hold in a permutation model. We then introduce a dynamical generalization of the Ramsey property called the virtual Ramsey property, which abstracts core features of our adaptation of Harrington's proof, and we prove that the virtual Ramsey property is sufficient for $\mathrm{BPI}$ to hold in a symmetric extension.",0,arxiv,Matematik,CC-BY/arXiv,On BPI in Symmetric Extensions Part 1
"We show that there is a set which is not a set of multiple recurrence despite being a set of recurrence for nil-Bohr sets. This answers Huang, Shao, and Ye's \enquote{higher-order} version of Katznelson's Question on Bohr recurrence and topological recurrence in the negative. Equivalently, we construct a set $S$ so that there is a finite coloring of $\mathbb{N}$ without three-term arithmetic progressions with common differences in $S$, but so that $S$ lacks the usual polynomial obstacles to arithmetic progressions.",0,arxiv,Matematik,CC-BY/arXiv,New Obstacles to Multiple Recurrence
"We show a nearly optimal lower bound on the length of linear relaxed locally decodable codes (RLDCs). Specifically, we prove that any $q$-query linear RLDC $C\colon \{0,1\}^k \to \{0,1\}^n$ must satisfy $n = k^{1+Î©(1/q)}$. This bound closely matches the known upper bound of $n = k^{1+O(1/q)}$ by Ben-Sasson, Goldreich, Harsha, Sudan, and Vadhan (STOC 2004).   Our proof introduces the notion of robust daisies, which are relaxed sunflowers with pseudorandom structure, and leverages a new spread lemma to extract dense robust daisies from arbitrary distributions.",0,arxiv,Matematik,CC-BY/arXiv,Nearly Tight Lower Bounds for Relaxed Locally Decodable Codes via Robust Daisies
"We give a simple, short and self-contained presentation of Bourgain's discretised projection theorem from 2010, which is a fundamental tool in many recent breakthroughs in geometric measure theory, harmonic analysis, and homogeneous dynamics. Our main innovation is a short elementary argument that shows that a discretised subset of $\R$ satisfying a weak ``two-ends'' spacing condition is expanded by a polynomial to a set of positive Lebesgue measure.",0,arxiv,Matematik,CC-BY/arXiv,Simple proofs of discretised projection theorems
"Given integers $n\ge s\ge 2$, let $e(n,s)$ stand for the maximum size of a family of subsets of an $n$-element set that contains no $s$ pairwise disjoint members. The study of this quantity goes back to the 1960s, when Kleitman determined $e(sm-1,s)$ and $e(sm,s)$ for all integer $m,s\ge 1$. The question of determining $e(n,s)$ is closely connected to its uniform counterpart, the subject of the famous ErdÅ‘s Matching Conjecture.   The problem of determining $e(n,s)$ has proven to be very hard and, in spite of some progress during these years, even a general conjecture concerning the value of $e(n,s)$ is missing. In this paper, we completely solve the problem for $n\le 3s$. In this regime, the average size of a set in an $s$-matching is at most $3$, and it is a delicate interplay between the `missing' $2$- and $3$-element sets that plays a key role here. Four types of extremal families appear in the characterization. Our result sheds light on how the extremal function $e(n,s)$ may behave in general.",0,arxiv,Matematik,CC-BY/arXiv,A complete solution of the ErdÅ‘s-Kleitman matching problem for $n\le 3s$
"Reiher, RÃ¶dl, Sales, and Schacht initiated the study of relative TurÃ¡n densities of ordered graphs and showed that it is more subtle and interesting than the unordered case. For an ordered graph $F$, its relative TurÃ¡n density, $Ï_{<}(F)$, is the greatest $Î±$ such that every ordered graph $G$ has an $F$-free subgraph with at least $Î±e(G)$ edges.   This paper contains two main results about relative TurÃ¡n densities. First, we find a family of host graphs that is optimal for all $F$. Second, we characterise the ordered graphs with zero relative TurÃ¡n density: precisely those with no monotone path of length two.",0,arxiv,Matematik,CC-BY/arXiv,Relative TurÃ¡n densities for ordered graphs: all and nothing
"We solve Ealy's conjecture from 1977 by showing that for each odd prime $p$, a finite generalized quadrangle each point of which admits a central symmetry of order $p$, is either a classical symplectic quadrangle in dimension $3$, or a Hermitian quadrangle in dimension $3$ or $4$. As a byproduct, we vastly generalize the aforementioned result by determining the finite generalized quadrangles whose every point admits at least one nontrivial central symmetry.",0,arxiv,Matematik,CC-BY/arXiv,Ealy's conjecture in odd characteristic
"This work presents conjectures about eigenvalues of matrices associated with $k$-path graphs, the algebraic connectivity, defined as the second smallest eigenvalue of the Laplacian matrix, and the $Î±$-index, as the largest eigenvalue of the $A_Î±$-matrix. For this purpose, a process based in Pereira et al., is presented to generate lists of $k$-path graphs containing all non-isomorphic 2-paths, 3-paths, and 4-paths of order $n$, for $6 \leq n \leq 26, 8 \leq n \leq 19$, and $10 \leq n \leq 18$, respectively. Using these lists, exhaustive searches for extremal graphs of fixed order for the mentioned eigenvalues were performed. Based on the empirical results, conjectures are suggested about the structure of extremal $k$-path graphs for these eigenvalues.",0,arxiv,Matematik,CC-BY/arXiv,$k$-path graphs: experiments and conjectures about algebraic connectivity and $Î±$-index
"We study the relations under Weihrauch reducibility of the well-ordering preservation principle for the operator $X \mapsto X^Ï‰$ and the Ordered Ramsey Theorem. Both principles are known to be equivalent to $Î£^0_2$-induction in Reverse Mathematics.   We show that the Ordered Ramsey Theorem is Weihrauch-equivalent to the parallel product of the well-ordering preservation principle for the operator $X \mapsto X^Ï‰$ and the Eventually Constant Tail principle.   By previous work from Pauly, Pradic and SoldÃ , the Ordered Ramsey Theorem is known to be Weihrauch-equivalent to the parallel product of the Eventually Constant Tail principle and the parallelization of the jump of the Limited Principle of Omniscience. We show that the latter pinciple and the well-ordering preservation principle for $X \mapsto X^Ï‰$ are Weihrauch-incomparable.",0,arxiv,Matematik,CC-BY/arXiv,Weihrauch reducibility between Ramsey-type theorems and well-ordering principles at the level of $Î£^0_2$-induction: A pilot study
"We establish the first scaling limit for FK($q$)-weighted planar maps in the critical case $q=4$, resolving a problem that has remained open since Sheffield's seminal work arXiv:1108.2241. In that work, Sheffield proved a scaling limit for $q<4$ via the celebrated hamburger-cheeseburger bijection, which initiated the peanosphere (mating-of-trees) approach to Liouville quantum gravity. We prove that, at criticality, the associated burger count $\mathcal{S}$ and discrepancy $\mathcal{D}$ satisfy \[ \left(\frac{\mathcal{S}_{\lfloor nt \rfloor}}{\sqrt{n}}, \frac{\log(n)}{{2Ï€}\sqrt{n}} \mathcal{D}_{\lfloor nt \rfloor}\right)_{t\in\mathbb{R}} \stackrel{\text{d}}{\longrightarrow} (B^1_t, B^2_{t})_{t\in\mathbb{R}}, \] where $B^1$ and $B^2$ are independent two-sided Brownian motions. To the best of our knowledge, no conjecture for the correct discrepancy scaling factor had previously been formulated. Matching the limiting process with the critical mating of trees arXiv:2109.00275, we establish the first rigorous planar map convergence towards CLE$_4$ and critical ($Î³=2$) Liouville quantum gravity, in the peanosphere sense. Our proof is based on a novel approach that reveals the exactly solvable nature of the model through a correspondence with the (bicoloured) fully packed loop-$O(2)$ model on triangulations, and yields critical geometric exponents matching the predictions of conformal field theory.",0,arxiv,Matematik,CC-BY/arXiv,Scaling limits of critical FK-decorated random planar maps with $q=4$
"Hughes introduced the projective planes that bear his name in 1957 and they have since been studied extensively. However, until now, no polynomial representation of a planar ternary ring that represents them has been determined. In this paper, we rectify this omission by determining a reduced PTR polynomial for any Hughes plane defined over a regular nearfield. The polynomials obtained provide a new surprising connection: both the Catalan numbers and generalized Catalan numbers occur among the coefficients, depending on the representation. Since every PTR polynomial has connections with several classes of permutation polynomials, we obtain three new infinite classes of permutation polynomials as a consequence of our main result, and these, too, involve the Catalan numbers. The differential uniformity of new permutation polynomials is also determined.",0,arxiv,Matematik,CC-BY/arXiv,A PTR polynomial for the Hughes planes and a new class of permutation polynomials involving Catalan numbers
"We introduce the self-projecting Grassmannian, an irreducible subvariety of the Grassmannian parametrizing linear subspaces that satisfy a generalized self-duality condition. We study its relation to classical moduli spaces, such as the moduli spaces of pointed curves of genus $g$, as well as to other natural subvarieties of the Grassmannian. We further translate the self-projectivity condition in the combinatorial language of matroids, introducing self-projecting matroids, and we computationally investigate their realization spaces inside the self-projecting Grassmannian.",0,arxiv,Matematik,CC-BY/arXiv,The Self-Projecting Grassmannian
"Graph is considered neutral if its assortativity coefficient $r$ is equal to zero. In this paper, we address an outstanding conjecture, i.e., whether is there a neutral graph on $n$ vertices. First, we show that for $n\geq7$, there is at least one neutral tree, which suggests that we find a representative of any order neutral graph. Additionally, we obtain that given $n\geq13$, there exist at least one neutral non-tree graph.",0,arxiv,Matematik,CC-BY/arXiv,On existence of neutral graph
"We construct explicit families of graphs whose eigenvalues are asymptotically distributed according to Wigner's semicircle law; in other words, that are spectrally indistinguishable from random graphs. However, in other respects they are strikingly dissimilar from random graphs; for example, they are $K_{2,3}$-free graphs with almost the maximum possible edge density.",0,arxiv,Matematik,CC-BY/arXiv,Spectrally indistinguishable pseudorandom graphs
"We consider configurations of lines in 3-space with incidences prescribed by a graph. This defines a subvariety in a product of Grassmannians. Leveraging a connection with rigidity theory in the plane, for any graph, we determine the dimension of the incidence variety and characterize when it is irreducible or a complete intersection. We study its multidegree and the family of Schubert problems it encodes. Our spanning-tree coordinates enable efficient symbolic computations. We also provide numerical irreducible decompositions for incidence varieties with up to eight lines. These constructions with lines play a key role in the Landau analysis of scattering amplitudes in particle physics.",0,arxiv,Matematik,CC-BY/arXiv,Varieties of Lines in 3-Space
"Given a language, which in this article is a set of strings of some fixed length, we study the problem of producing its elements by a procedure in which each position has its own local rule. We introduce a way of measuring how much communication is needed between positions. The communication structure is captured by a simplicial complex whose vertices are the positions and the simplices are the communication channels between positions. The main problem is then to identify the simplicial complexes that can be used to generate a given language. We develop the theory and apply it to a number of languages.",0,arxiv,Matematik,CC-BY/arXiv,Local generation of languages
"A secure coalition in a graph $G$ consists of two disjoint vertex sets $V_1$ and $V_2$, neither of which is a secure dominating set, but whose union $V_1 \cup V_2$ forms a secure dominating set. A secure coalition partition ($sec$-partition) of $G$ is a vertex partition $Ï€= \{V_1, V_2, \dots, V_k\}$ where each set $V_i$ is either a secure dominating set consisting of a single vertex of degree $n-1$, or a set that is not a secure dominating set but forms a secure coalition with some other set $V_j \in Ï€$. The maximum cardinality of a secure coalition partition of $G$ is called the secure coalition number of $G$, denoted $SEC(G)$. For every $sec$-partition $Ï€$ of a graph $G$, we associate a graph called the secure coalition graph of $G$ with respect to $Ï€$, denoted $SCG(G,Ï€)$, where the vertices of $SCG(G,Ï€)$ correspond to the sets $V_1, V_2, \dots, V_k$ of $Ï€$, and two vertices are adjacent in $SCG(G,Ï€)$ if and only if their corresponding sets in $Ï€$ form a secure coalition in $G$. In this study, we prove that every graph admits a $sec$-partition. Further, we characterize the graphs $G$ with $SEC(G) \in \{1,2,n\}$ and all trees $T$ with $SEC(T) = n-1$. Finally, we show that every graph $G$ without isolated vertices is a secure coalition graph.",0,arxiv,Matematik,CC-BY/arXiv,Secure coalitions in graphs
"For integers $k,g,d$, a $(k;g,d)$-cage (or simply girth-diameter cage) is a smallest $k$-regular graph of girth $g$ and diameter $d$ (if it exists). The order of a $(k;g,d)$-cage is denoted by $n(k;g,d)$. We determine asymptotic lower and upper bounds for the ratio between the order and the diameter of girth-diameter cages as the diameter goes to infinity. We also prove that this ratio can be computed in constant time for fixed $k$ and $g$.   We theoretically determine the exact values $n(3;g,d)$, and count the number of corresponding girth-diameter cages, for $g \in \{4,5\}$. Moreover, we design and implement an exhaustive graph generation algorithm and use it to determine the exact order of several open cases and obtain -- often exhaustive -- sets of the corresponding girth-diameter cages. The largest case we generated and settled with our algorithm is a $(3;7,35)$-cage of order 136.",0,arxiv,Matematik,CC-BY/arXiv,On the order-diameter ratio of girth-diameter cages
"Let $G$ be graph with vertex set $V(G)$ and order $n$. A coalition in a graph $G$ consists of two disjoint sets of vertices $V_1$ and $V_2$, neither of which is a dominating set but whose union $V_1 \cup V_2$ is a dominating set. A coalition partition, abbreviated $c$-partition, in a graph $G$ is a vertex partition $Ï€=\left\{V_1 , V_2,\dots, V_k\right\}$ such that every set $V_i$ of $Ï€$ is either a singleton dominating set, or is not a dominating set but forms a coalition with another set $V_j$ in $Ï€$. The sets $V_i$ and $V_j$ are coalition partners in $G$. The coalition number $C(G)$ equals the maximum order $k$ of a $c$-partition of $G$. For any graph $G$ with a $c$-partition $Ï€=\left\{V_1,V_2,\dots,V_k\right\}$, the coalition graph $CG(G,Ï€)$ of $G$ is a graph with vertex set $V_1,V_2,\dots, V_k$, corresponding one-to-one with the set $Ï€$, and two vertices $V_i$ and $V_j$ are adjacent in $CG(G,Ï€)$ if and only if the sets $V_i$ and $V_j$ are coalition partners in $Ï€$. In [4], authors proved that for every graph $G$ there exist a graph $H$ and $c$-partition $Ï€$ such that $CG(H,Ï€)\cong G$, and raised the question: Does there exist a graph $H^*$ of smaller order $n^*$ and size $m^*$ with a $c$-partition $Ï€^*$ such that $CG(H^*,Ï€^*)\cong G$?. In this paper, we constructed a graph $H^*$ of small order and size and a $c$- partition $Ï€^*$ such that $CG(H^*,Ï€^*)\cong G$. Recently, Haynes et al.[5] defined the coalition count $c(G)$ of a graph $G$ as the maximum number of different coalition in any $c$-partition of $G$. We characterize all graphs $G$ with $c(G)=1$. Further, imposing some suitable conditions on coalition number, we study the properties of coalition count of graph.",0,arxiv,Matematik,CC-BY/arXiv,On Coalition Graphs and Coalition Count of Graphs
"Recently, Alon and Frankl (JCTB, 2024) determined the maximum number of edges in $K_{\ell+1}$-free $n$-vertex graphs with bounded matching number. For integers $\ell\ge r \ge 2$, the family $\mathcal{K}_{\ell+1}^{r}$ consists of all $r$-graphs $F$ with at most $\binom{\ell+1}{2}$ edges such that, for some $(\ell+1)$-set $K$, every pair $\{x,y\} \subseteq K$ is covered by an edge in $F$. In this paper, we study the maximum number of edges in $\mathcal{K}_{\ell+1}^r$-free $r$-uniform hypergraphs that have the matching number at most $s$, that is, $\mathrm{ex}_r(n, \{\mathcal{K}_{\ell+1}^r, M^r_{s+1}\})$, and obtain the exact value for sufficiently large $n$, along with the corresponding extremal hypergraph. This result can be viewed as a hypergraph extension of the work of Alon and Frankl. In addition, for the $3$-uniform Fano plane $\mathbb{F}$, we determine the exact value of $\mathrm{ex}_3(n, \{\mathbb{F}, M^3_{s+1}\})$, and characterize the corresponding extremal hypergraph.",0,arxiv,Matematik,CC-BY/arXiv,A hypergraph analogue of Alon-Frankl Theorem
"Hyperuniformity, which is a type of long-range order that is characterized by the suppression of long-range density fluctuations in comparison to the fluctuations in standard disordered systems, has emerged as a powerful concept to aid in the understanding of diverse natural and engineered phenomena. In the present paper, we harness hyperuniform point patterns to generate a class of disordered, spatially embedded networks that are distinct from both perfectly ordered lattices and uniformly random geometric graphs. We refer to these networks as \emph{hyperuniform-point-pattern-induced (HuPPI) networks}, and we compare them to their counterpart \emph{Poisson-point-pattern-induced (PoPPI) networks}. By computing the local geometric and transport properties of HuPPI networks, we demonstrate how hyperuniformity imparts advantages in both transport efficiency and robustness. Specifically, we show that HuPPI networks have systematically smaller total effective resistances, slightly faster random-walk mixing times, and fewer extreme-curvature edges than PoPPI networks. Counterintuitively, we also find that HuPPI networks simultaneously have more negative mean Ollivier--Ricci curvatures and smaller global resistances than PoPPI networks, indicating that edges with moderately negative curvatures need not create severe bottlenecks to transport. We also demonstrate that the network-generation method strongly influences these properties and in particular that it often overshadows differences that arise from underlying point patterns. These results collectively demonstrate potential advantages of hyperuniformity in network design and motivate further theoretical and experimental exploration of HuPPI networks.",0,arxiv,Matematik,CC-BY/arXiv,Local Geometric and Transport Properties of Networks that are Generated from Hyperuniform Point Patterns
"T.-W. Chao and H.-H. H. Yu showed in 2023 that a graph with $R$ red, $G$ green, and $B$ blue edges has at most $\sqrt{2 RGB}$ rainbow triangles. They proved this bound using the entropy method. We give a computer-free flag-algebra proof of this bound, and we also convert our proof into a classical counting proof. The ideas in our proof lead to an even shorter entropy proof. We also show uniqueness of the extremal construction.   Additionally, we prove a similar result that gives a sharp upper bound on the number of properly $3$-edge-colored $K_4$'s in graphs with $R$ red, $G$ green and $B$ blue edges.",0,arxiv,Matematik,CC-BY/arXiv,Density of rainbow triangles and properly colored $K_4$'s
"Evolutionary program synthesis systems such as AlphaEvolve, OpenEvolve, and ShinkaEvolve offer a new approach to AI-assisted mathematical discovery. These systems utilize teams of large language models (LLMs) to generate candidate solutions to a problem as human readable code. These candidate solutions are then 'evolved' with the goal of improving them beyond what an LLM can produce in a single shot. While existing mathematical applications have mostly focused on problems of establishing bounds (e.g., sphere packing), the program synthesis approach is well suited to any problem where the solution takes the form of an explicit construction. With this in mind, in this paper we explore the use of OpenEvolve for combinatorial bijection discovery. We describe the results of applying OpenEvolve to three bijection construction problems involving Dyck paths, two of which are known and one of which is open. We find that while systems like OpenEvolve show promise as a valuable tool for combinatorialists, the problem of finding novel, research-level bijections remains a challenging task for current frontier systems, reinforcing the need for human mathematicians in the loop. We describe some lessons learned for others in the field interested in exploring the use of these systems.",0,arxiv,Matematik,CC-BY/arXiv,"Even with AI, Bijection Discovery is Still Hard: The Opportunities and Challenges of OpenEvolve for Novel Bijection Construction"
"A combinatorial game is a two-player game without hidden information or chance elements. The main object of combinatorial game theory is to obtain the outcome, which player has a winning strategy, of a given combinatorial game. Positions of many well-known combinatorial games are naturally decomposed into a disjunctive sum of multiple components and can be analyzed independently for each component. Therefore, the study of disjunctive sums is a major topic in combinatorial game theory. Combinatorial games in which both players have the same set of possible moves for every position are called impartial games. In the normal-play convention, it is known that the outcome of a disjunctive sum of impartial games can be obtained by computing the Grundy number of each term. The theory of impartial games is generalized in various forms. This paper proposes another generalization of impartial games to a new framework, impartial games with activeness: each game is assigned a status of either ``active'' or ``inactive''; the status may change by moves; a disjunctive sum of games ends immediately, not only when no further moves can be made, but also when all terms become inactive. We formally introduce impartial games with activeness and investigate their fundamental properties.",0,arxiv,Matematik,CC-BY/arXiv,Impartial Games with Activeness
"In a previous work, BÃ³na and Pantone studied permutations that avoided all but one pattern of length $k$ that began with a length $k-1$ increasing subsequence. We draw the connection between that idea and distant patterns, first discussed heavily in a work by Dimitrov, and study similar permutation classes, where the index not part of the increasing subsequence can vary. We find a large class of Wilf-Equivalences between $k+1$ classes of $k$ patterns of length $k+1$, and outline several classes of unbalanced Wilf-Equivalences related to the first class. Using this, we are also find new bounds on the exponential growth rate on all monotone distant patterns with a single gap constraint.",0,arxiv,Matematik,CC-BY/arXiv,Permutations Almost Avoiding Monotone Distant Patterns
"We study the torus-equivariant homology $H_*^T(\mathrm{Gr}_G)$ of the affine Grassmannian $\mathrm{Gr}_G$, where $G=\mathrm{Sp}_{2n}(\mathbb{C})$ is the symplectic group. This homology admits a natural ring structure and a Schubert basis, giving rise to a well-defined Schubert calculus. We realize $H_*^T(\mathrm{Gr}_G)$ in terms of symmetric functions. Our first main result introduces a new family of symmetric functions, called the \emph{dual affine Schur $P$-functions}, which represent the Schubert classes. These functions are defined through the action of the affine nil-Hecke algebra, and specialize, in the stable limit as $n\to \infty$, to the dual factorial $P$-functions of Nakagawa and Naruse. Our second main result gives a precise comparison between this symmetric function model and the geometric construction of $H_*^T (\mathrm{Gr}_G)$ due to Ginzburg and Peterson, which identifies it with a coordinate ring of a centralizer family in the Langlands dual group.",0,arxiv,Matematik,CC-BY/arXiv,Equivariant homology of the symplectic affine Grassmannian and dual affine Schur $P$-functions
"We investigate the expected number of calls required to achieve Bingo in a generalized (n,m)-Bingo game, where each n x n card is filled by sampling n numbers from m possible values per column. Using the inclusion-exclusion principle, we derive exact formulas for the probability distribution and the expected game length. Our main theoretical result proves that the expected number of calls is a linear function of m.",0,arxiv,Matematik,CC-BY/arXiv,On the Expected Duration of a Generalized Bingo Game
"Given the collection of all $m\times n$ rectangular grids which have a fixed number $1\leq r\leq mn$ of blocked cells, we explicitly describe a proper subset of the collection which is guaranteed to contain at least one grid from each equivalence class under symmetry, eliminating the majority of redundant grids. We analyze the extent to which redundant grids remain in the reduced set, and give general cases in which our methods exactly produce a complete set of canonical representatives for the equivalence classes. As an application of our results, we specify collections of polyomino tiling problems and find all solvable grids in each collection.",0,arxiv,Matematik,CC-BY/arXiv,Toward a Canonical Representation of Blocked Rectangular Grids with an Application to Finite Tiling Problems
"The family of $(k, \ell)$-sparse graphs, introduced by Lorea, plays a central role in combinatorial optimization and has a wide range of applications, particularly in rigidity theory. A key algorithmic challenge is to compute a maximum-weight $(k, \ell)$-sparse subgraph of a given edge-weighted graph. Although prior approaches have long provided an $O(nm)$-time solution, a previously proposed $O(n^2 + m)$ method was based on an incorrect analysis, leaving open whether this bound is achievable.   We answer this question affirmatively by presenting the first $O(n^2 + m)$-time algorithm for computing a maximum-weight $(k, \ell)$-sparse subgraph, which combines an efficient data structure with a refined analysis. This quadratic-time algorithm enables faster solutions to key problems in rigidity theory, including computing minimum-weight redundantly rigid and globally rigid subgraphs. Further applications include enumerating non-crossing minimally rigid frameworks and recognizing kinematic joints. Our implementation of the proposed algorithm is publicly available online.",0,arxiv,Matematik,CC-BY/arXiv,"Quadratic-Time Algorithm for the Maximum-Weight $(k, \ell)$-Sparse Subgraph Problem"
"In this paper, we study some new factorizations of period-doubling sequences over a $k$-letter alphabet, where $k\geq 2$. First, we define the combinatorial and arithmetic properties of these sequences. Then, we define the kernel words of period-doubling sequences and demonstrate how to factorize a binary sequence using its kernel words. Next, we define gap sequences for period-doubling sequences and explore their relationship with kernel words. Lastly, we present a factorization of period-doubling sequences for $k\geq 3$ based on kernel words and gap sequences.",0,arxiv,Matematik,CC-BY/arXiv,A new factorization of the generalized period-doubling sequences through kernel words and gaps sequences
"The famous pancake theorem states that for every finite set $X$ in the plane, there exist two orthogonal lines that divide $X$ into four equal parts. We propose an algorithm whose running time is linear in the number of points in $X$ and prove that this complexity is optimal. We also consider generalizations of the pancake theorem and show that orthogonal hyperplanes can be found in polynomial time.",0,arxiv,Matematik,CC-BY/arXiv,Orthogonal partitions into four parts
"In recent work by Botkin, Dawsey, Hemmer, Just and the present author, a deterministic model of prime number distribution is developed based on properties of integer partitions that gives almost exact estimates for $Ï€(n)$, the number of primes less than or equal to positive integer $n$, up to $n=10{,}000$. In this follow-up paper, the author summarizes the ideas behind this partition-theoretic model of primes and formulates a computational model that is practically exact in its estimates of $Ï€(n)$ up to $n=100,000$.",0,arxiv,Matematik,CC-BY/arXiv,"Partition-theoretic model of prime distribution, II"
"We introduce and study the locus $\mathbb{M}_{g,d}^\textrm{nd}$ of genus $g$ tropical plane curves of gonality $d$ inside the moduli space $\mathbb{M}^{\textrm{nd}}_{g}$ of tropical plane curves of genus $g$. Each such tropical curve arises from a Newton polygon, and we conjecture that the gonality of the tropical curve is equal to an easily computed parameter of this polygon called the expected gonality, closely related to the lattice width of the polygon. Let $\mathbb{M}_{g,{\underline{d}}}^\textrm{nd}$ denote the locus of tropical curves whose associated Newton polygon has expected gonality $d$. We prove that for fixed $d$ and sufficiently large genus $g$, the dimensions of these two loci agree: \[ \\dim\left(\mathbb{M}_{g,d}^\textrm{nd}\right) =\dim\left(\mathbb{M}_{g,{\underline{d}}}^\textrm{nd}\right). \] Our results provide evidence that, in sufficiently high genus compared to expected gonality, the gonality of a tropical curve is determined by the expected gonality of the Newton polygon from which it arises.",0,arxiv,Matematik,CC-BY/arXiv,The $d$-gonal locus in the moduli space of tropical plane curves
"We develop a circular-street argument, in the style of Pollak, to obtain a new proof that there are $C_n = \frac{1}{n+1}\binom{2n}{n}$ weakly increasing parking functions of length $n \geq 1$, where $C_n$ is the $n$th Catalan number.",0,arxiv,Matematik,CC-BY/arXiv,A Pollak Proof for the Number of Weakly Increasing Parking Functions
"We make four contributions to the theory of optimal subspace packings and equi-isoclinic subspaces: (1) a new lower bound for block coherence, (2) an exact count of equi-isoclinic subspaces of even dimension $r$ in $\mathbb{R}^{2r+1}$ with parameter $Î±\neq \tfrac{1}{2}$, (3) a new upper bound for the number of $r$-dimensional equi-isoclinic subspaces in $\mathbb{R}^d$ or $\mathbb{C}^d$, and (4) a proof that when $d=2r$, a further refinement of this bound is attained for every $r$ in the complex case and every $r=2^k$ in the real case. For each of these contributions, the proof ultimately relies on a dimension count.",0,arxiv,Matematik,CC-BY/arXiv,Dimension-counting bounds for equi-isoclinic subspaces
"In 2001, Robertson and Schaal found the 2-color off-diagonal generalized Schur numbers: for two positive integers $k$ and $l$, they determined the smallest positive integer $S = S(k, l)$ such that for any coloring of the integers from 1 to $S$ using red and blue, there must be a red solution to the equation $x_1 + x_2 + \dots + x_k = x_0$ or a blue solution to the equation $x_1 + x_2 + \dots + x_l = x_0$. We extend this result to find the continuous version: for two positive integers $k$ and $l$, we find the smallest real number $S = S_\mathbb{R} (k, l)$ such that for any coloring of the real numbers from 1 to $S$ using red and blue, there must be a red solution to the equation $x_1 + x_2 + \dots + x_k = x_0$ or a blue solution to the equation $x_1 + x_2 + \dots + x_l = x_0$.",0,arxiv,Matematik,CC-BY/arXiv,Off-Diagonal Continuous Rado Numbers $x_1 + x_2 + \dots + x_k = x_0$
"In his PhD Thesis, E.R. Scheinerman conjectured that planar graphs are intersection graphs of line segments in the plane. This conjecture was proved with two different approaches by J. Chalopin and the author, and by the author, L. Isenmann, and C. Pennarun. In the case of 3-colorable planar graphs E.R. Scheinerman conjectured that it is possible to restrict the set of slopes used by the segments to only 3 slopes. Here we prove this conjecture by using an approach introduced by S. Felsner to deal with contact representations of planar graphs with homothetic triangles.",0,arxiv,Matematik,CC-BY/arXiv,3-colorable planar graphs have an intersection segment representation using 3 slopes
"Topological indices are graph-theoretic descriptors that play a crucial role in mathematical chemistry, capturing the structural characteristics of molecules and enabling the prediction of their physicochemical properties. A widely studied category of topological indices, known as degree-based topological indices, are calculated as the sum of the weights of a graph's edges, where each edge weight is determined by a formula that depends solely on the degrees of its endpoints.   This work focuses exclusively on chemical graphs in which no vertex has a degree greater than 3, a model for conjugated systems. Within a polyhedral framework, each chemical graph is mapped to a point in a three-dimensional space, enabling extremal values of any degree-based topological index to be determined through linear optimization over the corresponding polyhedron. Analysis within this framework reveals that extremality is limited to a small subset of chemical graph families, implying that certain chemical graphs can never attain extremality for any degree-based topological index.   The main objective of this paper is to present ChemicHull, an online tool we have developed to determine and display extremal chemical graphs for arbitrary degree-based topological indices. To illustrate the power of this tool, we easily recover established results, emphasizing its effectiveness for chemically significant graph classes such as chemical trees and unicyclic chemical graphs. This tool also enabled the identification of a counterexample to a previously published extremal result concerning the RandiÄ‡ index.",0,arxiv,Matematik,CC-BY/arXiv,ChemicHull: an online tool for determining extremal chemical graphs of maximum degree at most 3 for any degree-based topological indices
"The dichromatic number $\vecÏ‡(D)$ of a digraph $D=(V,A)$ is the minimum number of sets in a partition $V_1,\ldots{},V_k$ of $V$ into $k$ subsets so that the induced subdigraph $D[V_i]$ is acyclic for each $i\in [k]$. This is a generalization of the chromatic number for undirected graphs as a graph has chromatic number at most $k$ if and only if the complete biorientation of $G$ (replace each edge by a directed 2-cycle) has dichromatic number at most $k$. In this paper we introduce the acyclic dichromatic number $\vecÏ‡_{\rm a}(D)$ of a digraph $D$ as the minimum number of sets in a partition $V_1,\ldots{},V_k$ of $V$ so that the induced subdigraph $D[V_i]$ is acyclic for each $i\in [k]$ and each of the bipartite induced subdigraphs $D[V_i,V_j]$ is acyclic for each $1\leq i<j\leq k$. This parameter, which resembles the definition of acyclic chromatic number for undirected graphs, has apparently not been studied before.   We derive a number of results which display the difference between the dichromatic number and the acyclic dichromatic number, in particular, there are digraphs $D$ with arbitrarily large $\vecÏ‡_{\rm a}(D)-\vecÏ‡(D)$, even among tournaments with dichromatic number 2 and bipartite tournaments (where the dichromatic number is always 2). We prove several complexity results, including that deciding whether $\vecÏ‡_{\rm a}(D)\leq 2$ is NP-complete already for bipartite digraphs, while it is polynomial for tournaments (contrary to the case for dichromatic number). We also generalize the concept of heroes of a tournament to acyclic heroes of tournaments.",0,arxiv,Matematik,CC-BY/arXiv,Acyclic dichromatic number of oriented graphs
"A graph $G$ is called $H$-saturated if $G$ contains no copy of $H$, but $G+e$ contains a copy of $H$ for any edge $e\in E(\overline{G})$. The saturation number of $H$ is the minimum number of edges in an $H$-saturated graph of order $n$, denoted by $sat(n,H)$. In this paper, we investigate $sat(n,K_{2}\vee P_{k})$, where $k\geq 3$. Let $a_k$ be an integer, defined as follows: $a_k=k$ for $3\leq k\leq 5$; $a_k=3\cdot 2^{t-1}-2$ for $k=2t\geq 6$; and $a_k=2^{t+1}-2$ for $k=2t+1\geq 7$. We show that $sat(n, K_{2}\vee P_{k})=2n-3+sat(n-2,P_{k})$ for $n\geq a_k+2$ and $k\geq 3$, characterize the $K_{2}\vee P_{k}$-saturated graphs with $sat(n,K_{2}\vee P_{k})$ edges, the $K_{1}\vee P_{k}$-saturated graphs with $sat(n,K_{1}\vee P_{k})$ edges for $3\leq k\leq5$ and the $P_{k}$-saturated graphs with $sat(n, P_{k})$ edges for $3\leq k\leq4$. Furthermore, we propose some questions for further research.",0,arxiv,Matematik,CC-BY/arXiv,Saturation numbers of $K_{2}\vee P_{k}$
"A wiring diagram is a labeled directed graph that represents an abstract concept such as a temporal process. In this article, we introduce the notion of a quasi-skeleton wiring diagram graph, and prove that quasi-skeleton wiring diagram graphs correspond to Hasse diagrams. Using this result, we designed algorithms that extract wiring diagrams from sequential data. We used our algorithms in analyzing the behavior of an autonomous agent playing a computer game, and the algorithms correctly identified the winning strategies. We compared the performance of our main algorithm with two other algorithms based on standard clustering techniques (DBSCAN and agglomerative hierarchical), including when some of the data was perturbed. Overall, this article brings together techniques in category theory, graph theory, clustering, reinforcement learning, and data engineering.",0,arxiv,Matematik,CC-BY/arXiv,From data to concepts via wiring diagrams
"In this paper, we introduce the framework of a generalized design, which represents any linear operator as a finite sum of local linear maps attached to finitely many points, thereby abstracting the core of design theory without employing integration. We then construct such a design on the space of sections of the tautological bundle over the complex projective line. By using the irreducible decomposition of this space as an SU(2)-representation, we show that the projection onto its lowest-dimensional summand can be realized as a finite sum of these local maps. Our construction relies on invariant theory for the binary icosahedral group and an analysis of fixed-point subspaces in symmetric tensor representations.",0,arxiv,Matematik,CC-BY/arXiv,Designs on the Tautological bundle
"We prove a new version of Hall's Harem Theorem, where the final matching is realized by a unary function with additional conditions on behavior of cycles. The present paper can be considered as a helpful companion of the paper of the author: arXiv:2105.06304, where a computable version of Hall's Harem Theorem with controlled sizes of cycles is proved. These two versions of Hall's Harem Theorem are independent: none of them follows from the other one.",0,arxiv,Matematik,CC-BY/arXiv,Hall's Harem Theorem with controlled sizes of cycles
"In this paper, we continue the study of linear sets with complementary weights. We find criteria to determine the set of points of any fixed weight and use this to present particular linear sets with few points of weight more than one. We also present a product-type construction for linear sets of complementary type arising from any linear set, allowing us to control the weight distribution of the obtained linear set. Finally, we use this construction to create linear sets with many different weights, along with point sets of even type with many distinct intersection numbers.",0,arxiv,Matematik,CC-BY/arXiv,On the weight distribution of linear sets with complementary weights and related constructions
"A plank is the part of space between two parallel planes. The following open problem, posed 45 years ago, can be viwed as the converse of Tarski's plank problem (Bang's theorem): Is it true that if the total width of a collection of planks is sufficiently large, then the planks can be individually translated to cover a unit ball $B$?   A translative covering of $B$ by planks is said to be non-dissective if the planks can be added one by one, in some order, such that the uncovered part remains connected at each step, and is empty at the end. Improving a classical result of Groemer, we show that every set of $C/Îµ^{7/4}$ planks of width $Îµ$ admits a non-dissective translative covering of $B$, provided $C$ is large enough. Our proof yields a low-complexity algorithm. We also establish the first nontrivial lower bound of $c/Îµ^{4/3}$ for this quantity.",0,arxiv,Matematik,CC-BY/arXiv,Non-dissective coverings by planks
"In a study, published in \emph{Nature}, researchers from DeepMind and mathematicians demonstrated a general framework using machine learning to make conjectures in pure mathematics. Their work uses neural networks and attribution techniques to guide human intuition towards making provable conjectures. Here, we build upon this framework to develop a method for identifying sufficient conditions that imply a given mathematical statement. Our approach trains neural networks with a custom loss function that prioritizes high precision. Then uses attribution techniques and exploratory data analysis to make conjectures. As a demonstration, we apply this process to Stanley's problem of $e$-positivity of graphs--a problem that has been at the center of algebraic combinatorics for the past three decades. Guided by AI, we rediscover that one sufficient condition for a graph to be $e$-positive is that it is co-triangle-free, and that the number of claws is the most important factor for $e$-positivity. Based on the most important factors in Saliency Map analysis of neural networks, we suggest that the classification of $e$-positive graphs is more related to continuous graph invariants rather than the discrete ones. Furthermore, using neural networks and exploratory data analysis, we show that the claw-free and claw-contractible-free graphs with $10$ and $11$ vertices are $e$-positive, resolving a conjecture by Dahlberg, Foley, and van Willigenburg.",0,arxiv,Matematik,CC-BY/arXiv,How to Use Deep Learning to Identify Sufficient Conditions: A Case Study on Stanley's $e$-Positivity
"We introduce a spoke-arc decomposition of non-crossing annular pair partitions $NC_2(p,q)$ that records spoke type and orientation, isolates spoke-level contributions, and factorizes the dependence on the ellipticity parameter $Î³$ into a spoke factor and arc weights. This yields closed-form descriptions of the limiting covariance of Gaussian elliptic matrices. As a corollary, we show that an independent family of Gaussian elliptic random matrices is asymptotically second-order free.",0,arxiv,Matematik,CC-BY/arXiv,Global Fluctuations of Gaussian Elliptic Matrices
"We determine whether each known generating set of arbitrary oriented Reidemeister moves is minimal. We then provide a complete classification of minimal generating sets that include a coherent Reidemeister move of type II. We also classify all minimal generating sets that include a braid-type Reidemeister move of type III. Beyond these two cases, we identify 16 possible candidates for minimal generating sets. Among them, we prove that 12 are indeed minimal, whereas the minimality and even the generating property of the remaining 4 sets remains unsolved (Remark 5.1).",0,arxiv,Matematik,CC-BY/arXiv,Minimal Generating sets of Reidemeister moves
"We prove that for the preorder induced by a function f: V -> V, the family of all order ideals is average-rare, that is, its normalized degree sum (nds) is nonpositive. As a base case in our reduction, we establish the same result for functional partial orders (or rooted forests). We also propose a conjecture related to Frankl's Conjecture. All proofs have been formally verified in the proof assistant Lean 4.",0,arxiv,Matematik,CC-BY/arXiv,Average-Rare Order Ideals in Functional Preorders
"Whereas Steiner systems $S(2,k,v)$ with block length $k \le 5$ have large amount of examples and the existence is established for all admissible $v$, for $k\ge 6$ only few examples are known even for decided cases. In this paper the existence of $S(2,9,369)$ is established and some new examples for other admissible pairs $(k,v)$ are given. In particular, lots of new unitals of order $6$ (or $S(2,7,217)$) together with $S(2,7,175)$, $S(2,7,259)$, $S(2,8,120)$, $S(2,8,504)$, $S(2,9,513)$ are presented. Found examples suggest two conjectures on infinite series of designs.",0,arxiv,Matematik,CC-BY/arXiv,"Existence of $S(2,9,369)$, new unitals of order $6$ and other Steiner systems with block length $\ge 7$"
"The crank-mex theorem states that the number of integer partitions of $n$ with nonnegative crank equals the number with odd minimal excludant (mex). Andrews and M. Newman recently refined that result in terms of the number of parts greater than one. Here, we establish and expand a complementary result connecting the partitions with even mex, having fixed points, with negative crank, and with positive crank, all refined in terms of number of parts greater than one. We provide both analytic and combinatorial proofs.",0,arxiv,Matematik,CC-BY/arXiv,Extending Andrews and Newman's refinement of the crank-mex theorem
"Given a word $w$, what is the maximum possible number of appearances of $w$ reading contiguously along any of the directions in $\{-1, 0, 1\}^d \setminus \{\mathbf{0}\}$ in a large $d$-dimensional grid (as in a word search)? Patchell and Spiro first posed a version of this question, which Alon and Kravitz completely answered for a large class of ""well-behaved"" words, including those with no repeated letters. We study the general case, which exhibits greater variety and is often more complicated (even for $d=1$). We also discuss some connections to other problems in combinatorics, including the storied $n$-queens problem.",0,arxiv,Matematik,CC-BY/arXiv,Words with Repeated Letters in a Grid
"Sufficient conditions for a simple graph to be characterized up to isomorphism given its spectrum and the spectrum of its complement graph are known due to Wang and Xu. This note establishes a related sufficient condition in the presence of loops: if the walk matrix has square-free determinant, then the graph is characterized by its generalized spectrum. The proof includes a general result about symmetric integral matrices.",0,arxiv,Matematik,CC-BY/arXiv,A sufficient condition for generalized spectral characterization of graphs with loops
"In this article, we explore the combinatorics of balanced collections. A collection of subsets of the set $[n] = \{1, \dots, n\}$ is called \emph{balanced} if the relative interior of the convex hull of the corresponding characteristic vectors intersects the main diagonal of the $n$-dimensional cube, and it is called \emph{minimal} if it contains no proper balanced subcollections. In particular, we establish both upper and lower bounds for the number of minimal balanced collections. Specifically, we prove that if $B_n$ denotes the number of minimal balanced collections, then $\frac{0.288}{n!} \, 2^{(n-1)^2} < B_n < \frac{120}{n!} \, 2^{n^2 - n}$.",0,arxiv,Matematik,CC-BY/arXiv,Combinatorics of Minimal Balanced Collections
"We prove that Anderson's conjecture on symmetric sequencings and Bailey's conjecture on 2-sequencings hold for sufficiently large groups. In addition, we discuss extensions of partial harmonious sequences and partial R-sequencings. Several further results on double sequencings are presented, both in the context of abelian groups and for sufficiently large non-abelian groups.",0,arxiv,Matematik,CC-BY/arXiv,Symmetric sequencings and other combinatorial properties of large groups
"For every positive integer $n$, we find a complete classification for planar graphs according to the collection of numbers of common neighbours for every $n$-tuple of distinct vertices. Our results expand the literature on planar graphical degree sequences, that have recently been the object of renewed attention. Here we completely settle the version with no multiplicities of the vast problem of planar graphical $n$-degree sequences.",0,arxiv,Matematik,CC-BY/arXiv,Common neighbours in planar graphs
"Since their introduction, torsion theories have played a key role in the study of abelian and pointed categories. In representation theory, torsion theories and lattices of torsion classes of mod$ A$, for $A$ a finite-dimensional algebra, have been widely studied. The more recent definition of pretorsion theories, that can be given for any category, has expanded the theory, giving many more instances of ``non-pointed torsion theories'' in unexpected settings. In this work, we introduce and study the lattice $\mathcal{L}_t(A)$ of pretorsion classes of mod$ A$. These lattices are in close connection with the lattices tors$ A$ of torsion classes of mod$ A$. We fully describe the completely join-irreducible elements of $\mathcal{L}_t(A)$. Moreover, we characterise and give a full classification of when $\mathcal{L}_t(A)$ is distributive and further describe when it can be identified with the \emph{distributive closure} of tors$ A$. Finally, we show how the lattices of pretorsion classes, together with their duals, can be used to build pretorsion theories in mod$ A$.",0,arxiv,Matematik,CC-BY/arXiv,Lattices of pretorsion classes
"Let $Î“$ denote a distance-regular graph with vertex set $X$ and diameter $D \geq 3$. Fix a vertex $x \in X$. Let the field $\mathbb{F}$ be either $\mathbb{R}$ or $\mathbb{C}$. Let $\operatorname{Mat}_X(\mathbb{F})$ denote the $\mathbb{F}$-algebra of matrices whose rows and columns are indexed by $X$ and all entries in $\mathbb{F}$. The Terwilliger algebra $T^\mathbb{F} = T^\mathbb{F}(x)$ is the subalgebra of $\operatorname{Mat}_X(\mathbb{F})$ generated by the adjacency matrix $A$ of $Î“$ and the dual primitive idempotents $\{E_i^*\}_{i=0}^D$ of $Î“$ with respect to $x$. Let $\{E_i\}_{i=0}^D$ denote the primitive idempotents of $A$. Assume that the ordering $\{E_i\}_{i=0}^D$ is $Q$-polynomial. Let $W$ denote an irreducible $T^\mathbb{F}$-module. We say that $W$ is sharp over $\mathbb{F}$ whenever $\dim (E_r^* W) = 1$, where $r$ is the endpoint of $W$. It is known, by Nomura and Terwilliger (2008), that every irreducible $T^\mathbb{C}$-module is sharp. In this paper, we prove that every irreducible $T^\mathbb{R}$-module is sharp. Once this is established, we obtain four additional results: (i) if $W$ is an irreducible $T^\mathbb{R}$-module, then its complexification $W^\mathbb{C}= W \otimes_{\mathbb{R}} \mathbb{C}$ is an irreducible $T^\mathbb{C}$-module; (ii) two irreducible $T^\mathbb{R}$-modules $W_1$ and $W_2$ are isomorphic if and only if their complexifications $W_1^\mathbb{C}$ and $W_2^\mathbb{C}$ are isomorphic as $T^\mathbb{C}$-modules; (iii) if $\bigoplus_{i=1}^h \operatorname{Mat}_{n_i}(\mathbb{C})$ is the Wedderburn decomposition of $T^\mathbb{C}$, then $\bigoplus_{i=1}^h \operatorname{Mat}_{n_i}(\mathbb{R})$ is the Wedderburn decomposition of $T^\mathbb{R}$; (iv) each of the subalgebras $E_1^* T E_1^*$, $E_1 T E_1$, $E_D^* T E_D^*$, and $E_D T E_D$ is commutative and every element of these algebras is a symmetric matrix.",0,arxiv,Matematik,CC-BY/arXiv,Every $Q$-polynomial distance-regular graph is sharp over $\mathbb{R}$
"We address a question posed by Fessler-Jensen-Kelsey-Owen regarding graphs whose second gonality is greater than the first by exactly 1. We answer the question affirmatively under a stronger condition, thereby characterising the entire gonality sequence for a large family of graphs. We prove a structure theorem for the graphs satisfying the condition, and show that they are all obtained via an inductive process by gluing together complete and banana graphs under certain rules.",0,arxiv,Matematik,CC-BY/arXiv,On gonality-tight graphs
"The Eulerian numbers form a triangular array with many interesting properties. The numbers arise from various combinatorial and probabilistic interpretations, and have been studied in a variety of mathematical contexts. In this article we examine two distinct alternating sign formulas for the Eulerian numbers and show how they can be proved using a sign-reversing involution technique described by Benjamin and Quinn known as the ``D.I.E.'' method. Each of these arguments lends itself to a broad generalization, shedding light on different parts of mathematics.",0,arxiv,Matematik,CC-BY/arXiv,The Eulerian numbers can D.I.E
"In this paper, we study the multigraded Betti numbers of Veronese embeddings of projective spaces. Due to Hochster's formula, we interpret these multigraded Betti numbers in terms of the homology of certain simplicial complexes. By analyzing these simplicial complexes and applying Forman's discrete Morse theory, we derive vanishing and non-vanishing results for these multigraded Betti numbers.",0,arxiv,Matematik,CC-BY/arXiv,Multigraded Betti numbers of Veronese embeddings
"In this paper, we prove similar results for odd and even cycle lengths. Let $L_o(G)$ denote the set of odd cycle lengths of $G$ and $\ell_o(G)$ denote the longest odd cycle length. In 1992, GyÃ¡rfÃ¡s proved that $Ï‡(G)\leq 2|L_o(G)|+2$, and if $w(G)\leq 2|L_o(G)|+1$, then $Ï‡(G)\leq 2|L_o(G)|+1$. We first prove that if $G$ is a 2-connected non-bipartite graph with $Î´(G)\geq 2k$, then $|L_o(G)|\geq k$. Moreover, if $|L_o(G)|=k$, then $2|L_o(G)|+1=\ell_o(G)$, and either $K_{2k+1}\subseteq G$ or $Ï‡(G)\leq 2k$. Applying this result, we prove that if $w(G)\leq 2|L_o(G)|$, then $Ï‡(G)\leq 2|L_o(G)|$ for $|L_o(G)|\geq 2$, improving the result of GyÃ¡rfÃ¡s. We also construct a class of graphs with $w(G)=2|L_o(G)|-1$ but $Ï‡(G)=2|L_o(G)|$ for every $|L_o(G)|\geq 2$. Using our result, we give a short proof of a similar result of $Ï‡(G)$ and $\ell_o(G)$ proved by Kenkre and Vishwanathan.   Our second part is about even cycle lengths. Let $L_e(G)$ denote the set of even cycle lengths of $G$ and $\ell_e(G)$ denote the longest even cycle length. In 2004, MihÃ³k and Schiermeyer proved that $Ï‡(G)\leq 2|L_e(G)|+3$, and if $w(G)\leq 2|L_e(G)|+2$, then $Ï‡(G)\leq 2|L_e(G)|+2$. We first prove that if $G$ is a 2-connected graph with $Î´(G)\geq 2k+1$, then $|L_e(G)|\geq k$. Moreover, if $|L_e(G)|=k$, then $2|L_e(G)|+2=\ell_e(G)$, and either $K_{2k+2}\subseteq G$ or $Ï‡(G)\leq 2k+1$. Applying this result, we prove that if $w(G)\leq 2|L_e(G)|+1$, then $Ï‡(G)\leq 2|L_e(G)|+1$ for $|L_e(G)|\geq 2$, improving the result of MihÃ³k and Schiermeyer. We also construct a class of graphs with $w(G)=2|L_e(G)|$ but $Ï‡(G)=2|L_e(G)|+1$ for every $|L_e(G)|\geq 2$. Our result can deduce a similar result of $Ï‡(G)$ and $\ell_e(G)$.   The above results also improve some results of consecutive odd or even cycle lengths.",0,arxiv,Matematik,CC-BY/arXiv,"Odd and even cycle lengths, minimum degree and chromatic number in graphs"
"The celebrated Nash-Williams and Tutte's theorem states that a graph $G=(V, E)$ contains $k$ edge disjoint spanning trees if and only if $Î½_{f}(G) \geq k$, where $$Î½_{f}(G):=\min_{|\mathcal{\mathcal{P}}|>1, \text{$\mathcal{P}$ is a partition of $V(G)$}}\frac{|E( \mathcal{P})|}{|\mathcal{P}|-1}.$$ Inspired by the NDT theorem as structural explanations for the fractional part of Nash-Williams' forest decomposition theorem, Fang and Yang extended Nash-Williams and Tutte's theorem and proved that if $Î½_{f}(G) > k+ \frac{d-1}{d}$, then $G$ contains $k$ edge disjoint spanning trees and another forest $F$ with $ |E(F)|> \frac{d-1}{d} (|V(G)|-1)|$, and if $F$ is not a spanning tree, then $F$ has a component with at least $d$ edges. In this paper, we give a digraphic version of their result; however, the mixed graphic version remains open.",0,arxiv,Matematik,CC-BY/arXiv,Packing spanning arborescences with extra large one
"Amdeberhan and Merca recently studied arithmetic properties of the sequence $a(n)$, the reciprocal of the crank parity function, which counts the number of integer partitions of weight $n$ whose even parts are monochromatic and whose odd parts may appear in one of three colors (OEIS A298311). A key result of their work was the congruence $a(7n + 2) \equiv 0 \pmod{7}$ for all $n \geq 0$. We prove new congruences for the reciprocal crank parity function modulo powers of $7$.",0,arxiv,Matematik,CC-BY/arXiv,Congruences Modulo Powers of 7 for the Reciprocal Crank Parity Function
"Autocatalysis is an important feature of metabolic networks, contributing crucially to the self-maintenance of organisms. Autocatalytic subsystems of chemical reaction networks (CRNs) are characterized in terms of algebraic conditions on submatrices of the stoichiometric matrix. Here, we derive sufficient conditions for subgraphs supporting irreducible autocatalytic systems in the bipartite KÃ¶nig representation of the CRN. On this basis, we develop an efficient algorithm to enumerate autocatalytic subnetworks and, as a special case, autocatalytic cores, i.e., minimal autocatalytic subnetworks, in full-size metabolic networks. The same algorithmic approach can also be used to determine autocatalytic cores only. As a showcase application, we provide a complete analysis of autocatalysis in the core metabolism of E. coli and enumerate irreducible autocatalytic subsystems of limited size in full-fledged metabolic networks of E. coli, human erythrocytes, and Methanosarcina barkeri (Archea). The mathematical and algorithmic results are accompanied by software enabling the routine analysis of autocatalysis in large CRNs.",0,arxiv,Matematik,CC-BY/arXiv,Enumeration of Autocatalytic Subsystems in Large Chemical Reaction Networks
"Kamyczura introduced the notion of a majority additive $k$-coloring of a graph $G$ as a function $c: V(G) \to \{1,2,\ldots,k\}$ such that $$\left|\left\{u \in N_G(v):\sum_{w \in N_G(u)} c(w) = s \right\}\right|\leq \max\left\{1,\frac{d_G(v)}{2}\right\}$$ for every vertex $v$ of $G$ and every positive integer $s$. We show that every graph $G$ of maximum degree $Î”$ admitting a majority additive coloring has a majority additive $\mathcal{O}\left(Î”^2\right)$-coloring. Under additional restrictions we improve this to sublinear in $Î”$. We show that determining whether a majority additive $k$-coloring exists for a given graph is NP-complete for all $k\geq 2$.",0,arxiv,Matematik,CC-BY/arXiv,Majority additive coloring and the maximum degree
"We give a Hilton-Milner Theorem for the $r$-independent sets in the graph that is the union of copies of $K_k$. That is, we determine the maximum intersecting families of $r$-independent sets in this graph, subject to the condition that the sets in a family do not all share a common element. As a by-product, we also find a tight upper bound for the sum of sizes of a pair of cross intersecting families made up of the same objects.   We apply our theorem to find the largest intersecting family of $r$-independent sets in a family of graphs called ``depth-two claws"". This confirms the Holroyd--Talbot conjecture for depth-two claws, extending previous results on these graphs (which covered cases where $r$ was relatively small compared to the number of vertices) to all possible values of $r$.",0,arxiv,Matematik,CC-BY/arXiv,Hilton-Milner Theorem for the $r$-independent sets in a union of cliques
"We investigate the multisigns of Hamiltonian circles in the multisigned complete graph \(Î£_n := (K_n, Ïƒ, \mathbb{F}_2^m)\). The \emph{multisign} of a circle \(C\) is defined as the sum \[ Ïƒ(C) := \sum_{e \in E(C)} Ïƒ(e). \] For a fixed \(m\) and sufficiently large \(n\), we show that the set of multisigns of Hamiltonian circles \[ \{Ïƒ(H) : H \text{ is a Hamiltonian circle of } Î£_n)\} \] forms either a subspace, an affine subspace, or the entire space \(\mathbb{F}_2^m\), except in certain exceptional cases.",0,arxiv,Matematik,CC-BY/arXiv,Realization and classification of Hamiltonian-circle multisigns
"The ultimate independence ratio of a graph $G$ is defined as $\mathscr{I}(G) = \lim_{k\rightarrow\infty } \frac{Î±(G^{\Box k})}{|V(G)|^k},$ where $Î±(G^{\Box k})$ is the independence number of the Cartesian product of $k$ copies of $G$. For all graphs $G$, Hahn, Hell, and Poljak (1995) proved that $\frac{1}{Ï‡(G)} \leq \mathscr{I}(G) \leq \frac{1}{Ï‰(G)}$ where $Ï‡(G)$ is the chromatic number, and $Ï‰(G)$ is the clique number of $G$. So all graphs $G$ with $Ï‡(G) = Ï‰(G)$ satisfy $\mathscr{I}(G) = \frac{1}{Ï‡(G)} = \frac{1}{Ï‰(G)}$. A construction of Zhu demonstrates that there exists a graph $G$ with $\frac{1}{Ï‡(G)} < \mathscr{I}(G) < \frac{1}{Ï‰(G)}$, so neither equality holds in general. In response, Hahn, Hell, and Poljak conjectured that all wheel graphs $W_n$ satisfy $\mathscr{I}(W_n) = \frac{1}{Ï‡(W_n)}$.   For even wheels $W_{2t}$ this follows from the fact $Ï‡(W_{2t}) = Ï‰(W_{2t}) = 3$. Odd wheels of length at least $5$ present a more challenging case, since $Ï‡(W_{2t+1}) = 4$ and $Ï‰(W_{2t+1}) = 3$. First, we prove that odd wheels of length at least $7$ satisfy $\mathscr{I}(W_{2t+1})\leq \frac{4t^2+6t}{3(2t+2)^2}<\frac{1}{3}$, which provides the best upper bound for large odd wheels. Next, we prove that $\mathscr{I}(W_5) \leq \frac{1019}{3888}$, improving an upper bound of Hahn, Hell, and Poljak that $\mathscr{I}(W_5) \leq \frac{11}{41}$. Our proofs combine counting arguments, recursive bounds on $Î±(W^{\Box k}_{2t+1})$, and computer-assisted calculation in the $W_5$ case.",0,arxiv,Matematik,CC-BY/arXiv,Improved Bounds for the Ultimate Independence Ratio of Odd Wheels
"Two directions in algorithms and complexity involve: (1) classifying which optimization problems can be solved in polynomial time, and (2) understanding which computational problems are hard to solve \emph{on average} in addition to the worst case. For many average-case problems, there does not currently exist strong evidence via reductions that they are hard. However, we can still attempt to predict their polynomial time tractability by proving lower bounds against restricted classes of algorithms.   Geometric approaches to predicting tractability typically study the \emph{optimization landscape}. For optimization problems with random objectives or constraints, ideas originating in statistical physics suggest we should study the \emph{overlap} between approximately-optimal solutions. Formally, properties of \emph{Gibbs measures} and the \emph{Franz--Parisi potential} imply lower bounds against natural local search algorithms, such as Langevin dynamics. A related theory, the \emph{Overlap Gap Property (OGP)}, proves rigorous lower bounds against classes of algorithms which are stable functions of their input.   A remarkable recent work of Li and Schramm showed that the shortest path problem in random graphs admits lower bounds against a class of stable algorithms, via the OGP. Yet this problem is polynomial time tractable. We further investigate this. We find that both the OGP and the Franz--Parisi potential predict that: (1) local search will fail in the optimization landscape of shortest paths, but (2) local search should succeed in the optimization landscape for shortest path \emph{trees}, which is true. Using the Franz--Parisi potential, we explain an analogy with results from combinatorial optimization -- submodular minimization is tractable via local search on the LovÃ¡sz extension, even though ``naive'' local search over sets or the multilinear extension provably fails.",0,arxiv,Matematik,CC-BY/arXiv,"Overlap Analysis of the Shortest Path Problem: Local Search, Landscapes, and Franz--Parisi Potential"
"We establish several independent results concerning extremal, left modular, congruence uniform, and semidistributive lattices. An equivalent characterization of left modular lattices is obtained in terms of edge-labellings, together with necessary and sufficient conditions on the doubling steps in the construction of congruence normal lattices that ensure left modularity or extremality. We prove that a congruence uniform lattice is shellable if and only if it is extremal. We answer a question of Barnard by constructing a counterexample showing that an induced subcomplex of a canonical join complex need not itself be such a complex. Finally, we show that the order dimension of a semidistributive extremal lattice equals the chromatic number of the complement of its Galois graph, generalizing a theorem of Dilworth for distributive lattices. As an application, we determine the dimensions of generalizations of the Hochschild lattice, of the parabolic Tamari lattice, and of some lattices of torsion classes.",0,arxiv,Matematik,CC-BY/arXiv,Extremality in semidistributive lattices
"We develop a unified framework for Berezin integrals over Grassmann variables that establishes master identities for exponential quadratic fermionic forms and linear fermionic forms coupled to both bosonic and fermionic sources. The construction is rigorous for both real and complex fermions in arbitrary dimensions and remains well-defined even when the underlying matrices are singular. Our main mathematical results appear in two master theorems. Theorem 12 provides a comprehensive identity for Berezin integrals over Grassmann variables for real fermions with mixed bosonic-fermionic sources, applicable to any antisymmetric matrix. Its complex analogue, Theorem 13, yields corresponding determinant-based representations. Together, they serve as generating functionals for a wide range of combinatorial and physical models. Key applications include the dimer, monomer-dimer, matching, and almost-matching problems. We revisit the Kasteleyn theorem for planar dimers using Berezin integrals. We construct monomer-dimer systems through the \textit{Monobisyzexant (Mbsz)} function, which generalizes the Hafnian to incorporate monomer contributions and admits a Pfaffian-sum representation for planar graphs (Theorem 5); and practical techniques for handling singular matrices via unitary block decomposition (Theorem 6) and spectral analysis. We further present explicit mappings between Hafnians and Pfaffians and their submatrix generalizations (Hafnianinhos and Pfaffianinhos); an alternative source-ordered Berezin integral representation for spanning trees and forests using complex bosonic sources that regularizes the Laplacian zero mode (Theorem 10). Overall, this work offers a flexible toolkit for the theoretical analysis and computational implementation of graph-based models and lattice field theories using Berezin integrals over Grassmann variables .",0,arxiv,Matematik,CC-BY/arXiv,A Generalized Grassmann-Pfaffian Framework for Monomer-Dimer and Spanning Trees
We investigate the geode and some of its generalizations from the point of view on noncommutative symmetric functions.,0,arxiv,Matematik,CC-BY/arXiv,The noncommutative geode
"In this work, the weighted paths to interpret any triangular array of the form   \[   T(n,k)=(a_2 n + a_1 k + a_0)\,T(n-1,k)   + (b_2 n + b_1 k + b_0)\,T(n-1,k-1),   \]   allowing a structural analysis of the coefficients $\big(T(n,k)\big)_{n,k\in \mathbb{N}}$.   This leads to explicit expressions for general $T(n,k)$, with simpler formulas arising in the cases $a_2=0$ or $b_2=0$, as well as in the fully general case.   Applications include explicit formulas for the $r$-Eulerian numbers and the marked $r$-Eulerian numbers ones. We will write also the case where $b_{n,k}=1$, as a matrix of passage.   \textbf{Keywords:} triangular recurrence, weighted paths, $r$-Eulerian numbers, combinatorial interpretation.",0,arxiv,Matematik,CC-BY/arXiv,Explicit Formulas and Combinatorial Interpretation of Triangular Arrays
"For a fixed dimension $k\ge 1$, let us consider the randomly growing simplical complex on the vertex set $\{1,2,\dots,n\}$ defined as follows: We start with the empty complex, and for each $k+1$-element subset $Ïƒ$ of $\{1,2,\dots,n\}$, we add $Ïƒ$ and all of its subsets to the complex at some random time $t_Ïƒ$, where $(t_Ïƒ)$ are i.i.d. uniform random elements of $[0,n]$. As the complex evolves, new $k-1$-dimensional cycles are born and then at a later time they die, that is, they get filled in. The notion of persistence diagrams, which is a standard tool in topological data analysis, provides a way to record these birth and death times. In this paper, we understand the asymptotic behavior of the persistence diagrams of the above defined randomly evolving complexes as $n$ goes to infinity.   As the single time marginals of the above process are variants of the Linial-Meshulam complex, our results can be viewed as extensions of the results of Linial and Peled on the Betti numbers of the Linial-Meshulam complex.   Our proof relies on the notion of local weak convergence of graphs and a generalization of the results of Bordenave, Lelarge and Salez on the rank of sparse random matrices.",0,arxiv,Matematik,CC-BY/arXiv,The persistent homology of the Linial-Meshulam process
"The family of generalized Paley graphs of prime power order $q$ and degree $(q-1)/k$ is studied. It is shown that the automorphism group of a graph in this family is a subgroup of ${\mathrm{AÎ“L}}(1,q)$ whenever $q$ is sufficiently large relative to $k$. Furthermore, under the same conditions, the Weisfeiler-Leman dimension of these graphs is proved to be at most $5$. In particular, the same bound holds for the Van Lint-Schrijver graphs.",0,arxiv,Matematik,CC-BY/arXiv,The automorphism groups and identification of some Generalized Paley Graphs
"Let $G$ be a finite group. A finite collection of elements from $G$, where the order is disregarded and repetitions are allowed, is said to be a product-one sequence if its elements can be ordered such that their product in $G$ equals the identity element of $G$. Then, the Gao's constant $\mathsf E (G)$ of $G$ is the smallest integer $\ell$ such that every sequence of length at least $\ell$ has a product-one subsequence of length $|G|$. For a positive integer $n$, we denote by $C_n$ a cyclic group of order $n$. Let $G = C_n \rtimes_s C_2$ with $s^2\equiv 1\pmod n$ be a metacyclic group. The direct and inverse problems of $\mathsf E (G)$ were settled recently, except for the case that $G=C_{3n_2}\rtimes_s C_2$ with $n_2\neq 1$, $\gcd(n_2,6)=1$, $s\equiv -1 \pmod 3$, and $s\equiv 1\pmod {n_2}$. In this paper, we complete the remaining case and hence for all metacyclic groups of the form $G=C_n \rtimes C_2$, the Gao's constant and the associated inverse problem are now fully settled (see Theorem 1.2).",0,arxiv,Matematik,CC-BY/arXiv,On zero-sum problems over metacyclic groups $C_n \rtimes_s C_2$
"EÄŸecioÄŸlu and Remmel provide a combinatorial proof (using special rim hook tableaux) that the product of the Kostka matrix $K$ and its inverse $K^{-1}$ equals the identity matrix $I$. They then pose the problem of proving the reverse identity $K^{-1}K =I$ combinatorially. Sagan and Lee prove a special case of this identity using overlapping special rim hook tableaux. Loehr and Mendes provide a full proof using bijective matrix algebra that relies on the EÄŸecioÄŸlu--Remmel map. In this article, we solve the problem in full generality independent of the EÄŸecioÄŸlu--Remmel bijection. To do this, we start by proving NSym versions of both Kostka matrix identities using sign-reversing involutions involving the tunnel hook coverings recently introduced by the first and third authors. Then we modify our sign-reversing involutions to reduce to Sym. Finally, we show that our bijection is different than the Loehr and Mendes result by constructing an injective map between special rim tableaux and the symmetric group $S_n.$",0,arxiv,Matematik,CC-BY/arXiv,A new proof of an EÄŸecioÄŸlu--Remmel inverse Kostka matrix problem via a Garsia--Milne involution involving Sym and NSym
"We investigate machine learning approaches to approximating the \emph{domination number} of graphs, the minimum size of a dominating set. Exact computation of this parameter is NP-hard, restricting classical methods to small instances. We compare two neural paradigms: Convolutional Neural Networks (CNNs), which operate on adjacency matrix representations, and Graph Neural Networks (GNNs), which learn directly from graph structure through message passing. Across 2,000 random graphs with up to 64 vertices, GNNs achieve markedly higher accuracy ($R^2=0.987$, MAE $=0.372$) than CNNs ($R^2=0.955$, MAE $=0.500$). Both models offer substantial speedups over exact solvers, with GNNs delivering more than $200\times$ acceleration while retaining near-perfect fidelity. Our results position GNNs as a practical surrogate for combinatorial graph invariants, with implications for scalable graph optimization and mathematical discovery.",0,arxiv,Matematik,CC-BY/arXiv,Graph Neural Networks vs Convolutional Neural Networks for Graph Domination Number Prediction
"We define a set of restricted Reidemeister moves and show that if $K$ is obtained from $K_0\,\#\,K_1$ using those moves, then the crossing number of $K$ is at least $c(K_0)+c(K_1)$. We also explore topological interpretations of this result.",0,arxiv,Matematik,CC-BY/arXiv,Additivity of Crossing Number via Restricted Reidemeister Moves
"Frameproof codes are a class of secure codes introduced by Boneh and Shaw in the context of digital fingerprinting, and have been widely studied from a combinatorial point of view. In this paper, we study a quantitative extension of frameproof codes and hypergraphs, referred to as {\it quantitative frameproof codes and hypergraphs}. We give asymptotically optimal bounds on the maximum sizes of these structures and determine their exact sizes for a broad range of parameters. In particular, we introduce a generalized version of the ErdÅ‘s matching number in our proof and derive relevant estimates for it.",0,arxiv,Matematik,CC-BY/arXiv,Quantitative Frameproof Codes and Hypergraphs
"We show that the graph of a bent function is a Salem set in an appropriate sense. We also establish a simple result that quantifies redundancies in the difference operators of a function, which applies to bent functions over fields of odd characteristic via their equivalence to perfect non-linear functions in that setting. We end by demonstrating, by entirely elementary means, that the distance between two distinct planar functions must be at least two.",0,arxiv,Matematik,CC-BY/arXiv,Some observations on bent and planar functions
"We examine the graded automorphism groups of quantum affine spaces and classify these groups for spaces of dimension 7 or less. Using permutation actions on partitions, we investigate cases when the group decomposes as a product of graded automorphism groups of smaller dimensional spaces, and we describe the groups arising from the Kronecker tensor product of independent quantum parameter matrices.",0,arxiv,Matematik,CC-BY/arXiv,Stabilizing Automorphisms of Quantum Affine Space
We show that the categories of directed and undirected reflexive graphs carry exactly two (up to isomorphism) biclosed monoidal structures.,0,arxiv,Matematik,CC-BY/arXiv,Biclosed monoidal structures on the categories of digraphs and graphs
"In the space of equioriented type $A$ quiver representations, we define subvarieties called ""open quiver loci"" by placing strict rank conditions on the maps within representations. The closures of these subvarieties are the quiver loci, whose equivariant cohomology classes are the quiver polynomials of Buch and Fulton. We present one geometric formula and two combinatorial formulas that compute equivariant Chern--Schwartz--MacPherson (CSM) classes of open quiver loci; these classes refine the data of the quiver polynomials. The second combinatorial formula is in terms of ""chained generic pipe dreams,"" which modify the pipe dreams of Bergeron and Billey to more strongly resemble the lacing diagrams of Abeasis and Del Fra. We also present two new formulas for quiver polynomials; these are streamlined versions of known formulas due to Knutson, Miller, and Shimozono, in the sense that they contain fewer terms.",0,arxiv,Matematik,CC-BY/arXiv,Three formulas for CSM classes of open quiver loci
"Given a graph $G$ with vertex set $V$, $f : V \rightarrow \{0, 1, 2\}$ is a \emph{Roman $\{2\}$-dominating function} (or \emph{italian dominating function}) of $G$ if for every vertex $v\in V$ with $f(v) =0$, either there exists a vertex $u$ adjacent to $v$ with $f(u) = 2$, or two distinct vertices $x,\; y$ both adjacent to $v$ with $f(x)=f(y)=1$. The decision problem associated with Roman $\{2\}$-domination is NP-complete even for bipartite graphs (Chellali et al., 2016).   In this work we initiate the study of Roman $\{2\}$-domination on graph classes with a limited number of 4-paths. We base our study on a modular decomposition analysis. In particular, we study Roman $\{2\}$-domination under some operations in graphs such as join, union, complementation, addition of pendant vertices and addition of twin vertices. We then obtain the Roman $\{2\}$-domination number of spiders, well-labelled spiders and certain prime split graphs that are crucial in the modular decomposition of partner-limited graphs. In all, we provide linear-time algorithms to compute the Roman $\{2\}$-domination number of cographs, $P_4$-sparse graphs, $P_4$-tidy graphs and partner-limited graphs. Finally, we derive the NP-completeness of Roman $\{2\}$-domination on $P_4$-laden graphs.",0,arxiv,Matematik,CC-BY/arXiv,"Roman $\{2\}$-domination on Graphs with ""few"" 4-paths"
"Talagrand conjectured that if a family of sets $\mathcal{F}$ over $X = \{ 1,2,\cdots, N \}$ is of large measure, then constant times of unions of sets in $\mathcal{F}$ will cover a large portion of the power set of $X$. This conjecture is a central open problem at the intersection of combinatorics and probability theory, and was described by Talagrand as a personal favorite. This paper provides a proof confirming this conjecture.",0,arxiv,Matematik,CC-BY/arXiv,A Proof of Talagrand's Creating Large Sets Conjecture
"We study the relationship between three combinatorial objects -- a taffy pulling machine, the Calkin-Wilf tree of all fractions, and Conway's rational tangles. After introducing these objects, we develop a taffy analogue for Conway's characterization of rational tangles, and we give a direct geometric connection between rational tangles and taffy pulls.",0,arxiv,Matematik,CC-BY/arXiv,"Taffy, Trees, and Tangles"
"Let $n$ be a nonnegative integer, and $f(n)$ the number of unlabeled finite topologies on $n$ points. We prove that $f(n+m) \geq f(n) f(m)$ both for the labeled and unlabeled cases. Moreover, we prove a similar inequality for labeled and unlabeled $T_0$ topologies.",0,arxiv,Matematik,CC-BY/arXiv,A super-multiplicative inequality for the number of finite unlabeled arbitrary and $T_0$ topologies
"Alon, Krivelevich, and Sudakov conjectured in 1999 that every $F$-free graph of maximum degree at most $Î”$ has chromatic number $O(Î”/ \log Î”)$. This was previously known only for almost bipartite graphs, that is, for subgraphs of $K_{1,t,t}$ (verified by Alon, Krivelevich, and Sudakov themselves), while most recent results were concerned with improving the leading constant factor in the case where $F$ is almost bipartite. We prove this conjecture for all $3$-colorable graphs $F$, i.e. subgraphs of $K_{t,t,t}$, representing the first progress toward the conjecture since it was posed.   A closely related conjecture of Ajtai, ErdÅ‘s, KomlÃ³s, and SzemerÃ©di from 1981 asserts that for every graph $F$, every $n$-vertex $F$-free graph of average degree $d$ contains an independent set of size $Î©(n \log d / d)$. We prove this conjecture in a strong form for all 3-colorable graphs $F$. More precisely, we show that every $n$-vertex $K_{t,t,t}$-free graph of average degree $d$ contains an independent set of size at least $(1 - o(1)) n \log d / d$, matching Shearer's celebrated bound for triangle-free graphs (the case $t = 1$) and thereby yielding a substantial strengthening of it. Our proof combines a new variant of the RÃ¶dl nibble method for constructing independent sets with a TurÃ¡n-type result on $K_{t,t,t}$-free graphs.",0,arxiv,Matematik,CC-BY/arXiv,"Independent sets and colorings of $K_{t,t,t}$-free graphs"
"In this note, we prove that every graph obtained from a bipartite graph by iteratively splitting vertices into two adjacent twins has the de Bruijn-ErdÅ‘s property.",0,arxiv,Matematik,CC-BY/arXiv,Splitting vertices of bipartite graphs preserves de Bruijn-ErdÅ‘s property
"In 1977, Duke and ErdÅ‘s asked the following general question: What is the largest size of a family $\mathtt{F} \subset \binom{[n]}{k}$ that does not contain a sunflower with $s$ petals and core of size exactly $t - 1$? This problem is closely related to the famous ErdÅ‘s--Rado sunflower problem of determining the size $Ï†(s,t)$ of the largest $t$-uniform family with no $s$-sunflower. In this paper, we answer this question exactly for $t=2$, odd $s$ and $k\ge 5$, provided $n$ is large enough. Previously, the only know exact extremal result on this problem was due to Chung and Frankl from 1987.   One of the important ingredients for the proof that we obtained is a stability result for the Duke--ErdÅ‘s problem, which was previously not known, mostly due to our lack of understanding of the behaviour of $Ï†(s,t)$.   For large $k$ and $n$ we in fact manage to reduce the Duke--ErdÅ‘s problem to an ErdÅ‘s--Rado-like problem which depends on $t$ and $s$ only. In particular, we get a good understanding of the structure of extremal families for the Duke--ErdÅ‘s problem in terms of the ErdÅ‘s--Rado problem. Previously, a much looser variant of this connection (only in terms of the sizes, rather than the structure, of respective extremal families) was established in a seminal work of Frankl and FÃ¼redi from 1987.",0,arxiv,Matematik,CC-BY/arXiv,Exact results and the structure of extremal families for the Duke--ErdÅ‘s forbidden sunflower problem
"The well-known quadrangle criterion states that a latin square is istopic to the Cayley table of a group if and only if all quadrangles spanned by the same triple of symbols coincide in the fourth symbol. Gowers and Long (2020) reformulated it in the following way: the Cayley tables of the most associative quasigroups have the maximum number of octahedra.   In the present paper, we state the multidimensional quadrangle condition for $d$-dimensional latin hypercubes in terms of the reconstruction of subcubes of order $2$ from a bundle of $d+1$ entries and in terms of the maximal number of cuboctahedra. In particular, we show that the must associative $d$-ary quasigroups have Cayley tables such that every $2$-dimensional plane is isotopic to a latin square principally isotopic to the Cayley table of a group. We also estimate the number of cuboctahedra in latin squares and hypercubes from below and provide some computational results.",0,arxiv,Matematik,CC-BY/arXiv,Multidimensional quadrangle condition and cuboctahedra in latin hypercubes
"We conjecture affine or Hall-Littlewood analogues of the dual Jacobi-Trudi formulas for orthogonal and symplectic Schur functions indexed by rectangular partitions of maximal height. These conjectures are then used to derive $t$-analogues of many known Rogers-Ramanujan identities for the characters of standard modules of affine Lie algebras. This includes $t$-analogues of the classical Rogers-Ramanujan identities, (some of) the Andrews-Gordon identities and the $\mathrm{C}_n^{(1)}$, $\mathrm{A}_{2n}^{(2)}$ and $\mathrm{D}_{n+2}^{(2)}$ GOW identities. We also prove an affine analogue of the dual Jacobi-Trudi formula for Schur functions indexed by rectangular partitions of arbitrary height.",0,arxiv,Matematik,CC-BY/arXiv,"Affine Jacobi-Trudi formulas and $q,t$-Rogers-Ramanujan identities"
"Let $G$ be a connected graph, and let $b$ and $k$ be two positive integers with $b\equiv1$ (mod 2). A $[1,b]$-odd factor of $G$ is a spanning subgraph $F$ of $G$ with $d_F(v)\equiv1$ (mod 2) and $1\leq d_F(v)\leq b$ for every $v\in V(G)$. A graph $G$ is called $k$-critical with respect to $[1,b]$-odd factor if $G-X$ contains a $[1,b]$-odd factor for every $X\subseteq V(G)$ with $|X|=k$. Let $\mathcal{D}(G)$ denote the distance matrix of $G$. The largest eigenvalue of $\mathcal{D}(G)$, denoted by $Î¼(G)$, is called the distance spectral radius of $G$. In this paper, we prove an upper bound for $Î¼(G)$ in a connected graph $G$ which guarantees $G$ to be $k$-critical with respect to $[1,b]$-odd factor.",0,arxiv,Matematik,CC-BY/arXiv,"Distance spectral radius for a graph to be k-critical with respect to [1,b]-odd factor"
"We study the TurÃ¡n numbers of $3$-graphs avoiding $3$-graphs $F$ and $M_{s+1}^3$, a matching of size $s+1$. We disprove a conjecture of Gerbner, Tompkins, and Zhou [European Journal of Combinatorics, 2025, 127:104155] on $\ex(n,\{F,M^3_{s+1}\})$ for $3$-graph $F$ with $Ï‡(F)=2$ by constructing infinitely many counterexamples. For this family, we determine the asymptotic TurÃ¡n number via edge-colored TurÃ¡n problem. In addition, for the $3$-graph $F_{3,2}$ with edge set $\{123,145,245,345\}$, we determine the exact value of $\ex(n,\{F_{3,2}, M_{s+1}^3\})$ for every integers $s$ and all $n \ge 12s^2$.",0,arxiv,Matematik,CC-BY/arXiv,Triple systems with bounded matching number: some constructions and exact TurÃ¡n number
"We show that the infinite product defined by \[ P(z) = -\prod_{n=1}^{\infty} (Î¦_n(z))^{-1/n}, \] where \( Î¦_n(z) \) is the \( n \)-th cyclotomic polynomial, is constant inside the unit disk. The proof translates a result of Ramanujan on Ramanujan sums, equivalent to the prime number theorem, to the setting of infinite products. We also show that similar identities proved by Ramanujan lead to additional results on infinite cyclotomic products.",0,arxiv,Matematik,CC-BY/arXiv,Constancy of an Infinite Cyclotomic Product via Ramanujan Sums
"The cubic pancake graphs are Cayley graphs over the symmetric group $\mathrm{Sym}_n$ generated by three prefix reversals. There is the following open problem: characterize all the sets of three prefix reversals that generate $\mathrm{Sym}_n$. We present a partial answer to this problem, in particular, we characterize all generating sets of three elements that contain at least one of the prefix reversals $r_2, r_3, r_{n-2}$, and $r_{n-1}$. We also give some computational results relating to the diameter and the girth of some cubic pancake graphs.",0,arxiv,Matematik,CC-BY/arXiv,Generating the symmetric group by three prefix reversals
"A multigraph $G = (V, E)$ is $(k, \ell)$-sparse if every subset $X \subseteq V$ induces at most $\max\{k|X| - \ell, 0\}$ edges. Finding a maximum-size $(k, \ell)$-sparse subgraph is a classical problem in rigidity theory and combinatorial optimization, with known polynomial-time algorithms. This paper presents a highly efficient and flexible implementation of an augmenting path method, enhanced with a range of powerful practical heuristics that significantly reduce running time while preserving optimality. These heuristics $\unicode{x2013}$ including edge-ordering, node-ordering, two-phase strategies, and pseudoforest-based initialization $\unicode{x2013}$ steer the algorithm toward accepting more edges early in the execution and avoiding costly augmentations. A comprehensive experimental evaluation on both synthetic and real-world graphs demonstrates that our implementation outperforms existing tools by several orders of magnitude. We also propose an asymptotically faster algorithm for extracting an inclusion-wise maximal $(k,2k)$-sparse subgraph with the sparsity condition required only for node sets of size at least three, which is particularly relevant to 3D rigidity when $k = 3$. We provide a carefully engineered implementation, which is publicly available online and is proposed for inclusion in the LEMON graph library.",0,arxiv,Matematik,CC-BY/arXiv,"Efficient Algorithms and Implementations for Extracting Maximum-Size $(k,\ell)$-Sparse Subgraphs"
"In this paper, we determine the $r$-colour size Ramsey number of the path $P_k$, up to constants. In particular, for every fixed $r \geq 2$ and $k \geq 100\log r$, we have   \[ \widehat{R}_r(P_k)=Î˜((r^2 \log r) \, k).\] Perhaps surprisingly, we do this by improving the lower bound on $\widehat{R}_r(P_k)$.",0,arxiv,Matematik,CC-BY/arXiv,The multicolour size Ramsey number of a path
"The lonely runner conjecture of Wills and Cusick asserts that if $n$ runners with distinct constant speeds run around a a circular unit length track, starting at a common time and place, then each runner will at some time be separated by a distance of at least $\frac{1}{n}$ from all other runners. A weaker lower bound of $\frac{1}{2n-2}$ follows from the so-called trivial union bound, and subsequent work upgraded this to bounds of the form $\frac{1}{2n}+\frac{c}{n^2}$ for various constants $c>0$. Tao strengthened this to $\frac{1}{2n}+\frac{(\log n)^{1-o(1)}}{n^2}$. In this paper, we obtain a polynomial improvement of the form $$\frac{1}{2n}+\frac{1}{n^{5/3+o(1)}}.$$",0,arxiv,Matematik,CC-BY/arXiv,Riesz products and the Lonely Runner Conjecture: A wider gap of loneliness
"In a companion paper, a canonical bijection was established between strong formal subdivisions of lower Eulerian posets and triples consisting of a lower Eulerian poset, a corresponding rank function, and a non-minimal element such that the join with any other element exists. The main goal of this paper is to relate the local $h$-polynomials of a strong formal subdivision to the Kazhdan-Lusztig-Stanley (KLS) invariants associated to its corresponding lower Eulerian poset under this bijection. As an application, we show that Braden and MacPherson's relative $g$-polynomials are alternative encodings of corresponding local $h$-polynomials. We also further develop equivariant KLS theory and give equivariant generalizations of our main results, as well as an application to equivariant Ehrhart theory.",0,arxiv,Matematik,CC-BY/arXiv,Subdivisions of lower Eulerian posets and KLS theory
"This paper investigates the conditions under which a given circular (synchronizing) DFA is \emph{simple} (sometimes referred to as \emph{primitive}) and when it is \emph{irreducible}. Our notion of irreducibility slightly differs from the classical one, since we are considering our monoid representations to be over $\mathbb{C}$ instead of $\mathbb{Q}$; nevertheless, several well-known results remain valid-for instance, the fact that every irreducible automaton is necessarily simple. We provide a complete characterization of simplicity in the circular case by means of the \emph{weak contracting property}. Furthermore, we establish necessary and sufficient conditions for a circular \emph{contracting automaton} (a stronger condition than the weakly contracting one) to be irreducible, and we present examples illustrating our results.",0,arxiv,Matematik,CC-BY/arXiv,Simplicity and irreducibility in circular automata
"There is a natural notion of a subdivision of a lower Eulerian poset called a strong formal subdivision, which abstracts the notion of a polyhedral subdivision of a polytope, or a proper, surjective morphism of fans. We show that there is a canonical bijection between strong formal subdivisions and triples consisting of a lower Eulerian poset, a corresponding rank function, and a non-minimal element such that the join with any other element exists. The bijection uses the non-Hausdorff mapping cylinder construction introduced by Barmak and Minian. A corresponding bijection for $CW$-posets is given, as well as an application to computing the $cd$-index of an Eulerian poset. A companion paper explores applications to Kazhdan-Lusztig-Stanley theory.",0,arxiv,Matematik,CC-BY/arXiv,Subdivisions of lower Eulerian posets
"We introduce and study a new restricted family of overpartitions, called block-separated overpartitions, in which no two consecutive distinct part-size blocks may both be overlined. Using a two-state transfer-matrix automaton, we derive a closed matrix-product expression for the ordinary generating function, establish an Euler-type factorization, and obtain an explicit normalized recurrence suitable for computation of arbitrary coefficients. We further prove that the possible overlining patterns on the distinct blocks are counted by Fibonacci numbers, giving natural bijections with independent sets on paths, pattern-avoiding binary words, and Fibonacci tilings.",0,arxiv,Matematik,CC-BY/arXiv,Block-Separated Overpartitions and Their Fibonacci-Type Structure
"We survey the known group properties that a sequence of finite groups or group actions needs to satisfy to admit subsets of bounded cardinality producing expander Cayley or Schreier graphs. We prove that an infinite amenable group and solvable groups of bounded derived length do not produce expander Schreier graphs, generalizing with easier proofs results of Lubotzky and Weiss for Cayley graphs. In particular, the poor expansion properties of a group action cannot in general be detected by looking at the abelian sections or at the representations above the stabilizer of a point.",0,arxiv,Matematik,CC-BY/arXiv,Groups that produce expander graphs
"We study the existence of equilateral polygons in planar integer lattices. Maehara showed that it's sufficient to work with rectangular lattices $Î›(m) = L[(1,0),(0,\sqrt{m})]$ with $m \equiv 3 \pmod{4}$. Building on results of Maehara and of Iino and Sakiyama, we show that for every such $m$ there exists $N$ such that for all $n \geq N$, the lattice $Î›(m)$ contains an equilateral $n$-gon. This extends previous classifications of equilateral polygons in planar lattices.",0,arxiv,Matematik,CC-BY/arXiv,Equilateral n-gons in planar integer lattices
"We prove formulas for generalized rank deviations for overpartitions. These formulas are in terms of Appell-Lerch series and sums of quotients of theta functions and extend work of Lovejoy and the second author. As an application, we compute a dissection.",0,arxiv,Matematik,CC-BY/arXiv,Generalized rank deviations for overpartitions
"Zero forcing in a graph refers to the evolution of vertex states under repeated application of a color change rule. Typically the states are chosen to be blue and white, and a forcing set is an initial set of blue vertices such that all of the vertices are blue at the end of the process. In this context, the propagation time of a set in a graph is the number of iterations of the color change rule required to have all vertices blue, performing independent color changes simultaneously. Different minimal forcing sets need not have the same propagation time, and we study the realizability of specific integers as propagation times of minimal forcing sets in graphs for two of the most well-studied color change rules (standard and positive semidefinite). Particular attention is paid to the case where all minimal forcing sets have the same propagation time, and we term this phenomenon fixed propagation time. For each of the two variants, we present a general form of graphs all of which have fixed propagation time equal to one. We conjecture that these are the only such graphs and prove the conjectures for joins of graphs. Families of graphs with longer fixed propagation time for standard forcing are exhibited, and it is shown that such graphs do not exist for positive semidefinite forcing.",0,arxiv,Matematik,CC-BY/arXiv,Zero forcing propagation time intervals and graphs with fixed propagation time
"The bipartite-hole-number of a graph $G$, denoted by $\widetildeÎ±(G)$, is the minimum number $k$ such that there exist positive integers $s$ and $t$ with $s+t=k+1$ with the property that for any two disjoint sets $A,B\subseteq V(G)$ with $|A|=s$ and $|B|=t$, there is an edge between $A$ and $B$. In this paper, we first prove that any $2$-connected graph $G$ satisfying $d_G(x)+d_G(y)\ge 2\widetildeÎ±(G)-2$ for every pair of non-adjacent vertices $x,y$ is hamiltonian except for a special family of graphs, thereby extending results of Li and Liu (2025), and Ellingham, Huang and Wei (2025). We then establish a stability version of a theorem by McDiarmid and Yolov (2017): every graph whose minimum degree is at least its bipartite-hole-number minus one is hamiltonian except for a special family of graphs.",0,arxiv,Matematik,CC-BY/arXiv,Extending two results on hamiltonian graphs involving the bipartite-hole-number
"The multiple exchange property for matroid bases states that for any bases $A$ and $B$ of a matroid and any subset $X\subseteq A\setminus B$, there exists a subset $Y\subseteq B\setminus A$ such that both $A-X+Y$ and $B+X-Y$ are bases. This classical result has not only found applications in matroid theory, but also in the analysis and design of various algorithms. In our work, we prove a common generalization of this and other known basis exchange properties by showing that for any subsets $X \subseteq A \setminus B$ and $Y \subseteq B \setminus A$, there exist subsets $U \subseteq A \setminus B$ and $V \subseteq B \setminus A$ such that $X\subseteq U$, $Y\subseteq V$, $A-U+V$ and $B+U-V$ are bases, and $|U|=|V|$ is at most the rank of $X+Y$. As an application, we prove the robustness of the local search algorithm for finding maximum weight matroid bases. For matroids representable over a field of characteristic zero, we further generalize our exchange property to include the very recent Equitability Theorem (SODA 2026), by establishing a far-reaching generalization of the Grassmann-PlÃ¼cker identity.",0,arxiv,Matematik,CC-BY/arXiv,Generalizing the Multiple Exchange Property for Matroid Bases
"We study the factorization of Schubert polynomials into elementary symmetric polynomials. We conjecture that this occurs when the permutation corresponding to the Schubert polynomial does not contain the patterns $1432$, $1423$, $4132$, and $3142$. We prove one direction of this and provide progress towards the second direction, including obstructions arising from permutations with a rectangular array of crosses in their bottom pipe dream. This characterization helps us identify new ties between elementary symmetric polynomials and Schubert polynomials. It contributes to the broader understanding of pattern avoidance phenomena in algebraic combinatorics.",0,arxiv,Matematik,CC-BY/arXiv,Schubert Polynomials and Elementary Symmetric Products
"We use recent advances in the theory of Furstenberg sets to prove new incidence results of SzemerÃ©di--Trotter strength for $Î´$-discretized structures with Cartesian product flavor. We use these results to make progress on a number of problems that include energy estimates and Fourier decay of fractal measures supported on curves, as well as various sum-product-like results governed by fractal dimension.",0,arxiv,Matematik,CC-BY/arXiv,Incidence estimates for quasi-product sets and applications
"Let $K$ be a field of characteristic zero, let $I \subset S = K[x_1,\dots,x_n]$ be a homogeneous ideal, and let $\partial(I)$ be its gradient ideal. We study the relationship between $\mathrm{reg}\,I$ and $\mathrm{reg}\,\partial(I)$. While earlier work by BusÃ©, Dimca, Schenck, and Sticlaru showed these regularities are generally incomparable for hypersurface ideals, we prove they remain incomparable even for monomial ideals with linear resolution, answering a question of J. Herzog. In fact, for any integers $a \in \mathbb{Z}$ and $b \ge - 1$, we construct monomial ideals $I$ and $J$ such that $\mathrm{reg}\,I - \mathrm{reg}\,\partial(I) = a$, $\mathrm{reg}\,\partial(J) - \mathrm{reg}\,J = b$ and $J$ has linear resolution. We introduce monomial ideals with differential linear resolution as those monomial ideals whose all iterated gradient ideals have linear resolution. We prove that polymatroidal ideals, equigenerated (strongly) stable ideals, powers of edge ideals with linear resolution, complementary edge ideals with linear resolution, and certain equigenerated squarefree monomial ideals with many generators satisfy this property.",0,arxiv,Matematik,CC-BY/arXiv,On the gradient of a monomial ideal
"We establish an integral representation for the Dirichlet generating function of the coefficients of Euler's pentagonal number theorem. The Bromwich-type integral enables analytic continuation to the entire complex plane, filling a gap in the literature and providing a new framework for studying the sequence's analytic structure. Furthermore, we derive the asymptotic behavior as the variable tends to negative infinity, and give integral representations for the Euler function $Ï†(q)$ and the Dedekind eta function $Î·(Ï„)$. Moreover, we obtain an explicit formula for the Dirichlet generating function at each positive integer, expressed as a finite sum.",0,arxiv,Matematik,CC-BY/arXiv,A Dirichlet Generating Function for the Coefficients of Euler's Pentagonal Number Theorem
"We prove that with high probability $G(n,p)$ with $p \geq n^{-4/11 + o(1)}$ admits a fractional triangle decomposition (FTD), i.e., a nonnegative weighting of its triangles such that for each edge, the total weight of the triangles containing it equals one. This improves on the state of the art, due to Delcourt, Kelly, and Postle, that $p \geq n^{-1/3+o(1)}$ suffices.   The proof is algorithmic: Given $G \sim G(n,p)$, we first construct an approximate FTD by taking a uniform weighting of the triangles. We then use specialized gadgets to iteratively shift weights and obtain successively better approximations of an FTD.",0,arxiv,Matematik,CC-BY/arXiv,On fractional triangle decompositions of random graphs
"In the first chapter of their classic book ""Concrete Mathematics"", Graham, Knuth, and Patashnik consider the maximum number of pieces that can be obtained from a pancake by making n cuts with a knife blade that is straight, or bent into a V, or bent twice into a Z. We extend their work by considering knives, or ""cookie-cutters"", of even more exotic shapes, including a k-armed V, a chain of k connected line segments, a long-legged version of one of the letters A, E, L, M, T, W, or X, a convex polygon, a circle, a figure 8, a pentagram, a hexagram, or a lollipop. In many cases a counting argument combined with Euler's formula produces an explicit expression for the maximum number of pieces. ``Strict'' versions of the letters A and T are also considered, for which we have only conjectural formulas.",0,arxiv,Matematik,CC-BY/arXiv,Cutting a Pancake with an Exotic Knife
"We present constructions and bounds for additive codes over a finite field in terms of their geometric counterpart, i.e.\ projective systems. It is known that the maximum number of $(l-1)$-spaces in $\operatorname{PG}(2,q)$, such that no hyperplane contains three, is given by $q^l+1$ if $q$ is odd. Those geometric objects are called generalized ovals. We show that cardinality $q^l+2$ is possible if we decrease the dimension a bit. We completely determine the minimum possible lengths of additive codes over $\mathbb{F}_9$ of dimension $2.5$ and give improved constructions for other small parameters. As an application, we consider multispreads in $\operatorname{PG}(4,q)$, in particular, completing the characterization of parameters of $\mathbb{F}_4$-linear $64$-ary one-weight codes.",0,arxiv,Matematik,CC-BY/arXiv,"Generalized ovals, 2.5-dimensional additive codes, and multispreads"
"In addition to general considerations, the present work includes the enumeration of the equivalence-classes of p-polygons with p vertices for p bigger than 3 with certain symmetry properties: 1. We count the equivalence-classes of p-polygons with p symmetry axes, the so called regular polygons. 2. We count the equivalence-classes of p-polygons with exactly one axis of symmetry. 3. We count the equivalence-classes of p-polygons with no axis of symmetry, the so called asymmetrical p-polygons. For p = 5 and p = 7 we show in all three cases a set of representatifs of the equivalenceclasses.",0,arxiv,Matematik,CC-BY/arXiv,Symmetries of p-polygons
"Let $n\geqÎ½$, let $T$ be an $n$-vertex tree with bipartition class sizes $t_1\geq t_2$, and let $S$ be a $Î½$-vertex tree with bipartition class sizes $Ï„_1\geqÏ„_2$. Using four natural constructions, we show that the Ramsey number $R(T,S)$ is lower bounded by $\underline{R}(T,S)=\max\{n+Ï„_2,Î½+\min\{t_2,Î½\},\min\{2t_1,2Î½\},2Ï„_1\}-1$.   Our main result shows that there exists a constant $c>0$, such that for all sufficiently large integers $n\geqÎ½$, if (i) $Î”(T)\leq cn/\log n$ and $Î”(S)\leq cÎ½/\logÎ½$, (ii) $Ï„_2\geq t_2$, and (iii) $Î½\geq t_1$, then $R(T,S)=\underline{R}(T,S)$. In particular, this determines the exact Ramsey numbers for a large family of pairs of trees. We also provide examples showing that $R(T,S)$ can exceed $\underline{R}(T,S)$ if any one of the three assumptions (i), (ii), and (iii) is removed.",0,arxiv,Matematik,CC-BY/arXiv,Asymmetric Ramsey numbers of trees
"Let $G$ be a group. The directed endomorphism graph, $\dend(G)$ of $G$ is a directed graph with vertex set $G$ and there is a directed edge from the vertex $a$ to the vertex $b$ if $a \neq b$ and there exists an endomorphism on $G$ mapping $a$ to $b$. The endomorphism graph, $\uend(G)$ is the corresponding undirected simple graph. The automorphism graph of $G$ is similarly defined for automorphisms: it is a disjoint union of complete graphs on the orbits of $\Aut(G)$. The endomorphism digraph is a special case of a digraph associated with a transformation monoid, and we begin by introducing this. We have explored graph theoretic properties like size, planarity, girth etc. and tried finding out for which types of groups these graphs are complete, diconnected, trees, bipartite and so on, as well as computing these graphs for some special groups. We conclude with examples showing that things are not always simple.",0,arxiv,Matematik,CC-BY/arXiv,Endomorphism and automorphism graphs of finite groups
"Around 1991, J.M. and P.B. Borwein established a cubic analogue of Jacobi's fundamental identity for theta functions. Their identity serves as the foundation for the subsequent development by B.C. Berndt, S. Bhargava, and F.G. Garvan of Ramanujan's theory of elliptic functions to the cubic base. In 2013, D. Schultz discovered an identity for theta series in three variables which generalizes the Borweins' identity. In this article, we revisit Schultz's identity and present two distinct approaches to its derivation. Our investigation not only provides new proofs but also yields several identities of a similar type. Furthermore, this study enables us to construct new two-variable generalizations of Jacobi's original classical identity.",0,arxiv,Matematik,CC-BY/arXiv,On Schultz's generalization of Borweins' cubic identity
"Let $\binom{[n]}{k}$ denote the collection of all $k$-subsets of the standard $n$-set $[n]=\{1,2,\ldots,n\}$. Let $n>2k$ and let $\mathcal{F}\subset \binom{[n]}{k}$ be an {\it intersecting} $k$-graph, i.e., $F\cap F'\neq \emptyset$ for all $F,F'\in \mathcal{F}$. The number of edges $F\in \mathcal{F}$ containing $x\in [n]$ is called the {\it degree} of $x$. Assume that $d_1\geq d_2\geq \ldots\geq d_n$ are the degrees of $\mathcal{F}$ in decreasing order. An important result of Huang and Zhao states that for $n>2k$ the minimum degree $d_n$ is at most $\binom{n-2}{k-2}$. For $n\geq 6k-9$ we strengthen this result by showing $d_{2k+1}\leq \binom{n-2}{k-2}$. As to the second and third largest degrees we prove the best possible bound $d_3\leq d_2\leq \binom{n-2}{k-2}+\binom{n-3}{k-2}$ for $n>2k$. Several more best possible results of a similar nature are established.",0,arxiv,Matematik,CC-BY/arXiv,On the largest degrees in intersecting hypergraphs
"A graph is called odd if all of its vertex degrees are odd. A long-standing conjecture asked whether there exists a positive constant $c$ such that every $n$-vertex graph without isolated vertices contains an odd induced subgraph on at least $cn$ vertices. In 2022, Ferber and Krivelevich resolved this conjecture affirmatively with $c=10^{-4}$. A natural question is to determine the largest possible constant $c$. In 1994, Caro remarked that if $2/7$ is a valid value for $c$, then it is the largest possible one. To the best of our knowledge, the bound $c\ge 2/7$ has not been improved. Previous research has established tight bounds for specific graph classes -- for instance, $c = 2/5$ for graphs with maximum degree at most $3$ and without isolated vertices. In this paper, we prove that $c=2/7$ is the tight bound for graphs with maximum degree at most $4$ and without isolated vertices. Our result provides some support for $2/7$ being the largest value of $c$.",0,arxiv,Matematik,CC-BY/arXiv,Odd Induced Subgraphs in Graphs of Maximum Degree Four
"A classical result of Nosal asserts that every $m$-edge graph with spectral radius $Î»(G)> \sqrt{m}$ contains a triangle. A celebrated extension of Nikiforov [35] states that if $G$ is an $m$-edge graph with $Î»(G)> \sqrt{(1- {1}/{r})2m}$, then $G$ contains a clique $K_{r+1}$. This result implies the TurÃ¡n theorem and Wilf theorem, and offers a new perspective on the existence of substructures. The edge-spectral conditions are versatile for enforcing substructures, as they can be applied to any sparse graph regardless of its edge density. In this paper, we prove that for any color-critical graph $F$ with chromatic number $Ï‡(F)=r+1\ge 4$, if $m$ is sufficiently large and $G$ is an $F$-free graph with $m$ edges, then $Î»(G)\le \sqrt{(1- {1}/{r})2m}$, with equality if and only if $G$ is a regular complete $r$-partite graph. This settles an open problem proposed by Yu and Li [52] and also gives spectral bounds for graphs forbidding books and wheels.   Secondly, we establish an asymptotic formula and structural characterization when we forbid an almost-bipartite graph $F$, where $F$ is called almost-bipartite if it can be made bipartite by removing at most one edge. As applications, we determine the unique $m$-edge spectral extremal graph for every integer $m$ when avoiding certain substructures, including complete bipartite graphs plus an edge, cycles plus an edge, and theta graphs, etc. Our results resolve an open problem proposed by Li, Zhao and Zou [24], as well as two conjectures posed by Liu and Li [31]. The arguments in our proofs are based on the edge-spectral stability method recently established by the authors. In addition, we develop some new spectral techniques, including the stability result for the Perron--Frobenius eigenvector.",0,arxiv,Matematik,CC-BY/arXiv,Edge-spectral TurÃ¡n theorems for color-critical graphs with applications
"In a 2022, Bartoli, Cossidente, Marino, and Pavese proved that in the projective space ${\rm PG}(3,q^3)$, one can find three $\mathbb F_q$-subgeometries such that the union of their point sets is a strong blocking set. This proves the existence of linear minimal codes with parameters $[3(q^2+1)(q+1),4]_{q^3}$ for every prime power $q$. We give a short proof of this result for odd values of $q > 9$, using the theory of small blocking sets in projective planes.",0,arxiv,Matematik,CC-BY/arXiv,A note on short minimal codes from subgeometries
"Let $W_{\mathrm{aff}}$ be an extended affine Weyl group and $\mathbf{H}$ and $J$ be the corresponding affine and asymptotic Hecke algebras with standard bases $\{T_x\}$ and $\{t_w\}$, respectively. Viewing $J$ as a subalgebra of the $\mathbf{q}^{-\frac{1}{2}}$-adic completion of $\mathbf{H}$, we give formulas for the coefficient of $T_x$ in $t_w$ for various $x$ and $w$ in the lowest two-sided cell, in terms of generalized exponents of the Langlands dual group, under a hypothesis on the left cell containing $w$. In particular our results hold for the canonical left cell. For such $w$ we also define a seemingly new positive basis for the corresponding subring of $J$. For $\mathrm{GL}_n$, we give partial results for some other cells.",0,arxiv,Matematik,CC-BY/arXiv,A positivity property in the based ring of the lowest two-sided cell
"The goal of this note is to connect some interesting results in the literature on algebraic graph theory and finite geometry. In 1999, Weng gave an almost complete classification of classical distance-regular graphs of negative type with diameter at least 4. He proved that these graphs are either dual polar graphs of Hermitian polar spaces, Hermitian forms graphs, or fall into a last category. It was recently proved by Yian et al. that the latter category does not exist when the diameter equals 3, which by Weng's results proves that they do not exist for bigger diameter. Using a result of Vanhove, this proves that certain hemisystems in Hermitian polar spaces cannot exist.",0,arxiv,Matematik,CC-BY/arXiv,A note on the classification of classical distance-regular graphs of negative type and the non-existence of hemisystems
"The frustration index of a signed graph is defined as the minimum number of negative edges among all switching-equivalent signatures. This can be regarded as a generalization of the classical \textsc{Max-Cut} problem in graphs, as the \textsc{Max-Cut} problem is equivalent to determining the frustration index of signed graphs with all edges being negative signs. In this paper, we prove that the frustration index of an $n$-vertex signed connected simple subcubic graph, other than $(K_4, -)$, is at most $\frac{3n + 2}{8}$, and we characterize the family of signed graphs for which this bound is attained. This bound can be further improved to $\frac{n}{3}$ for signed $2$-edge-connected simple subcubic graphs, with the exceptional signed graphs being characterized. As a corollary, every signed $2$-edge-connected simple cubic graph on at least $10$ vertices and with $m$ edges has its frustration index at most $\frac{2}{9}m$, where the upper bound is tight as it is achieved by an infinite family of signed cubic graphs.",0,arxiv,Matematik,CC-BY/arXiv,Frustration indices of signed subcubic graphs
"Let $G$ be a graph of genus $g$ with boundary $Î´Î©$. For $g=0$, Lin and Zhao [J. Lond. Math. Soc. 112 (2025), Paper No. e70238] proved an upper bound for the first (non-trivial) Steklov eigenvalue of $(G, Î´Î©)$, and they posed the problem of determining a corresponding bound for graphs of genus $g>0$. In this paper, we prove an $O\left(\frac{g}{|Î´Î©|}\right)$ bound for a bounded-degree graph of positive genus $g$. Our result can be regarded as a discrete analogue of Kokarev's bound [Adv. Math. 258 (2014), 191-239], up to a constant factor.",0,arxiv,Matematik,CC-BY/arXiv,The first Steklov eigenvalue bound for graphs of positive genus
"We investigate string graphs through the lens of graph product structure theory, which describes complicated graphs as subgraphs of strong products of simpler building blocks. A graph $G$ is called a string graph if its vertices can be represented by a collection $\mathcal{C}$ of continuous curves (called a string representation of $G$) in a surface so that two vertices are adjacent in $G$ if and only if the corresponding curves in $\mathcal{C}$ cross. We prove that every string graph with bounded maximum degree in a fixed surface is isomorphic to a subgraph of the strong product of a graph with bounded treewidth and a path. This extends recent product structure theorems for string graphs. Applications of this result are presented. This product structure theorem ceases to be true if the `bounded maximum degree' assumption is relaxed to `bounded degeneracy'. For string graphs in the plane, we give an alternative proof of this result. Specifically, we show that every string graph in the plane has a `localised' string representation where the number of crossing points on the curve representing a vertex $u$ is bounded by a function of the degree of $u$.   Our proof of the product structure theorem also leads to a result about the treewidth of outerstring graphs, which qualitatively extends a result of Fox and Pach [Eur. J. Comb. 2012] about outerstring graphs with bounded maximum degree. We extend our result to outerstring graphs defined in arbitrary surfaces.",0,arxiv,Matematik,CC-BY/arXiv,String Graphs: Product Structure and Localised Representations
"The dimension of Kakeya sets can be bounded using sum-difference exponents $\SD(R;s)$ for various sets of rational slopes $R$ and output slope $s$; the arithmetic Kakeya conjecture, which implies the Kakeya conjecture in all dimensions, asserts that the infimum of such exponents is $1$. The best upper bound on this infimum currently is $1.67513\dots$. In this note, inspired by numerical explorations from the tool \texttt{AlphaEvolve}, we study the regime where the cardinality of the set of slopes $R$ is bounded. In this regime, we establish that these exponents converge to $2$ at a rate controlled by the \emph{rational complexity} of $s$ relative to $R$, which measures how efficiently $s$ can be expressed as a rational combination of slopes in $R$.",0,arxiv,Matematik,CC-BY/arXiv,"Sum-difference exponents for boundedly many slopes, and rational complexity"
"Richardson tableaux are a remarkable subfamily of standard Young tableaux introduced by Karp and Precup in order to index the irreducible components of Springer fibers equal to Richardson varieties. We show that the set of insertion tableaux of noncrossing partial matchings on $\{1,2,,\ldots, n\}$ by applying the Robinson--Schensted algorithm coincides with the set of Richardson tableaux of size $n$. This leads to a natural one-to-one correspondence between the set of Richardson tableaux of size $n$ and the set of Motzkin paths with $n$ steps, in response to a problem proposed by Karp and Precup. As consequences, we recover some known and establish new properties for Richardson tableaux. Especially, we relate the $q$-counting of Richardson tableaux to $q$-Catalan numbers.",0,arxiv,Matematik,CC-BY/arXiv,Richardson tableaux and noncrossing partial matchings
"A drawing of a graph is 1-planar if each edge participates in at most one crossing and adjacent edges do not cross. Up to symmetry, each crossing in a 1-planar drawing belongs to one out of six possible crossing types, where a type characterizes the subgraph induced by the four vertices of the crossing edges. Each of the 63 possible nonempty subsets $\mathcal{S}$ of crossing types gives a recognition problem: does a given graph admit an $\mathcal{S}$-restricted drawing, that is, a 1-planar drawing where the crossing type of each crossing is in $\mathcal{S}$?   We show that there is a set $\mathcal{S}_{\rm bad}$ with three crossing types and the following properties: If $\mathcal{S}$ contains no crossing type from $\mathcal{S}_{\rm bad}$, then the recognition of graphs that admit an $\mathcal{S}$-restricted drawing is fixed-parameter tractable with respect to the treewidth of the input graph. If $\mathcal{S}$ contains any crossing type from $\mathcal{S}_{\rm bad}$, then it is NP-hard to decide whether a graph has an $\mathcal{S}$-restricted drawing, even when considering graphs of constant pathwidth.   We also extend this characterization of crossing types to 1-planar straight-line drawings and show the same complexity behaviour parameterized by treewidth.",0,arxiv,Matematik,CC-BY/arXiv,A Dichotomy for 1-Planarity with Restricted Crossing Types Parameterized by Treewidth
"For any finite poset we define a generating polynomial counting upsets, downsets, and their intersection. We investigate the behaviour of this polynomial with respect to poset operations, show that it distinguishes series-parallel posets, and comment on connections to the causal set approach to quantum gravity.",0,arxiv,Matematik,CC-BY/arXiv,The Generalized Interval Polynomial of a Poset
"A (not necessarily proper) vertex coloring of a graph $G$ with color classes $V_1$, $V_2$, $\dots$, $V_k$, is said to be a {\it Fair And Tolerant vertex coloring of $G$ with $k$ colors}, whenever $V_1$, $V_2$, $\dots$, $V_k$ are nonempty and there exist two real numbers $Î±$ and $Î²$ such that $Î±\in [0,1]$ and $Î²\in [0,1]$ and the following condition holds for each arbitrary vertex $v$ and every arbitrary color class $V_i$: $$ \bigl| V_i \cap N (v) \bigr| = \begin{cases}   Î±Â°(v) & \mbox{ if } \ \ v \notin V_i   Î²Â°(v) & \mbox{ if } \ \ v \in V_i . \end{cases} $$ The {\it FAT chromatic number} of $G$, denoted by $Ï‡^{\rm FAT} (G)$, is defined as the maximum positive integer $k$ for which $G$ admits a Fair And Tolerant vertex coloring with $k$ colors. The concept of the FAT chromatic number of graphs was introduced and studied by Beers and Mulas, where they asked for the existence of a function $f \colon \mathbb{N} \to \mathbb{R}$ in such a way that the inequality $Ï‡^{\rm FAT} (G) \ \leq \ f \big( Ï‡(G) \big)$ holds for all graphs $G$. Another similar interesting question concerns the existence of some function $g \colon \mathbb{N} \to \mathbb{R}$ such that the inequality $Ï‡(G) \ \leq \ g \left( Ï‡^{\rm FAT} (G) \right)$ holds for every graph $G$. In this paper, we establish that both questions admit negative resolutions.",0,arxiv,Matematik,CC-BY/arXiv,On Fair and Tolerant Colorings of Graphs
"The Ramsey number for the pair of graphs $\mathbb{K}_{1,n}$ (star) versus $W_{m}$ (wheel) has been extensively studied. In contrast, the Ramsey number of $\mathbb{K}_{2,n}$ versus wheel is not yet explored due to the structural complexity of $\mathbb{K}_{2,n}$. In this article, we have established an exact value of $\mathbb{K}_{2,n}$ versus $W_{m}$ for large $n$ and $m$. In particular, we have proved \begin{equation*} R(\mathbb{K}_{2,n}, W_{m})=3n+4 \end{equation*} if $n$ and $m$ are sufficiently large integers where $n\geq4m$ and $m$ is an odd integer. This also proves the $W_{m}$-goodness of $\mathbb{K}_{2,n}$. We have used a probabilistic method composed of structural analysis in our proof.",0,arxiv,Matematik,CC-BY/arXiv,An exact Ramsey number of large bipartite graphs versus odd wheel
"We introduce an optimal transport based approach for comparing undirected graphs with non-negative edge weights and general vertex labels, and we study connections between the resulting linear program and the graph isomorphism problem. Our approach is based on the notion of a joining of two graphs $G$ and $H$, which is a product graph that preserves their marginal structure. Given $G$ and $H$ and a vertex-based cost function $c$, the optimal graph joining (OGJ) problem finds a joining of $G$ and $H$ minimizing degree weighted cost. The OGJ problem can be written as a linear program with a convex polyhedral solution set. We establish several basic properties of the OGJ problem, and present theoretical results connecting the OGJ problem to the graph isomorphism problem. In particular, we examine a variety of conditions on graph families that are sufficient to ensure that for every pair of graphs $G$ and $H$ in the family (i) $G$ and $H$ are isomorphic if and only if their optimal joining cost is zero, and (ii) if $G$ and $H$ are isomorphic, the the extreme points of the solution set of the OGJ problem are deterministic joinings corresponding to the isomorphisms from $G$ to $H$.",0,arxiv,Matematik,CC-BY/arXiv,Optimal graph joining with applications to isomorphism detection and identification
"We study rank-2 cluster scattering diagrams through moduli spaces of quiver representations and a recently developed combinatorial framework of tight gradings. Combining quiver-theoretic and combinatorial methods, we prove and extend a collection of conjectures posed by Elgin--Reading--Stella concerning the structural and enumerative properties of the wall-function coefficients. The tight grading perspective also provides a new proof of the Weyl group symmetry of the scattering diagram.",0,arxiv,Matematik,CC-BY/arXiv,Cluster scattering diagrams via quiver moduli and tight gradings
"We present a proof of the sufficiency of Rado's condition for the partition regularity of linear Diophantine equations that avoids any use of van der Waerden's theorem. The proof is based on fundamental properties that are common knowledge in combinatorics of numbers and is entirely elementary, with the sole exception of a standard application of the compactness principle.",0,arxiv,Matematik,CC-BY/arXiv,A Van der Waerden-free proof of Rado's theorem
"In this paper, we study approximate Hadamard matrices, that is, well-conditioned $n\times n$ matrices with all entries in $\{\pm1\}$. We show that the smallest-possible condition number goes to $1$ as $n\to\infty$, and we identify some explicit infinite families of approximate Hadamard matrices.",0,arxiv,Matematik,CC-BY/arXiv,Asymptotically optimal approximate Hadamard matrices
Consider an infinite sequence $(x_k)_{k=1}^{\infty}$ on the unit circle $\mathbb{S}^1$. We may interpret the first $n$ elements $(x_k)_{k=1}^{n}$ as places where the `circular stick' $\mathbb{S}^1$ is broken into a total of $n+1$ pieces. It is clear that they cannot all be the same length all the time. de Bruijn and ErdÅ‘s (1949) show that the ratio of the largest to the smallest has to be arbitrarily close to 2 infinitely many times which is sharp. They also consider the problem of balancing the length of $r$ consecutive intervals and prove $$ \frac{\max \mbox{length of}~r~\mbox{consecutive intervals}}{\min \mbox{length of}~r~\mbox{consecutive intervals}} \geq 1 + \frac{1}{r}.$$ We prove that this ratio can be as small as $1 + c \log{r}/ r$. This is done by means of refined discrepancy estimates for the van der Corput sequence over very short intervals and proves a conjecture of Brethouwer.,0,arxiv,Matematik,CC-BY/arXiv,Balanced Stick Breaking
"Let $A_k(n)$ denote the set of $k$-distinct partitions of $n$, and let $B_k(n)$ be the set of $k$-regular partitions of $n$. Glaisher showed that $\# A_k(n) = \# B_k(n)$. For $k=2$, this equality yields the celebrated Euler's partition theorem. In this paper, we present a new partition set $E_k(n)$, which is equinumerous to $B_k(n)$.",0,arxiv,Matematik,CC-BY/arXiv,Generalizations of Euler's Theorem to $k$-regular partitions
"In this paper we give group-theoretical conditions on the maximal parabolic subgroups of a coset geometry for it to be a chiral hypertope, bypassing the need to construct the incidence graph of the coset geometry to determine whether or not it is a chiral hypertope. This result permits to study much larger coset geometries with computers and gives hope on proving theoretical results about coset geometries that are chiral hypertopes.",0,arxiv,Matematik,CC-BY/arXiv,Testing chirality on hypertopes
"An open problem in convex geometry asks whether two simplices $A,B\subseteq\mathbb{R}^d$, both containing the origin in their convex hulls, admit a polynomial-length sequence of vertex exchanges transforming $A$ into $B$ while maintaining the origin in the convex hull throughout. We propose a matroidal generalization of the problem to oriented matroids, concerning exchange sequences between bases under sign constraints on elements appearing in certain fundamental circuits. We formulate a conjecture on the minimum length of such a sequence, and prove it for oriented graphic matroids of directed graphs. We also study connections between our conjecture and several long-standing open problems on exchange sequences between pairs of bases in unoriented matroids.",0,arxiv,Matematik,CC-BY/arXiv,A note on embracing exchange sequences in oriented matroids
"We pursue the study of edge-irregulators of graphs, which were recently introduced in [Fioravantes et al. Parametrised Distance to Local Irregularity. IPEC, 2024]. That is, we are interested in the parameter Ie(G), which, for a given graph G, denotes the smallest k >= 0 such that G can be made locally irregular (i.e., with no two adjacent vertices having the same degree) by deleting k edges. We exhibit notable properties of interest of the parameter Ie, in general and for particular classes of graphs, together with parameterized algorithms for several natural graph parameters.   Despite the computational hardness previously exhibited by this problem (NP-hard, W[1]-hard w.r.t. feedback vertex number, W[1]-hard w.r.t. solution size), we present two FPT algorithms, the first w.r.t. the solution size plus Delta and the second w.r.t. the vertex cover number of the input graph.   Finally, we take important steps towards better understanding the behaviour of this problem in dense graphs. This is crucial when considering some of the parameters whose behaviour is still uncharted in regards to this problem (e.g., neighbourhood diversity, distance to clique). In particular, we identify a subfamily of complete graphs for which we are able to provide the exact value of Ie(G). These investigations lead us to propose a conjecture that Ie(G) should always be at most m/3 + c, where $m$ is the number of edges of the graph $G$ and $c$ is some constant. This conjecture is verified for various families of graphs, including trees.",0,arxiv,Matematik,CC-BY/arXiv,Graph Irregularity via Edge Deletions
"The HVN is a graph formed by removing two edges incident to the same vertex from the complete graph $K_5$. In this paper, we prove that every ($P_2\cup P_4$, HVN)-free graph $G$ satisfies $Ï‡(G)\leq\lceil\frac{4}{3}Ï‰(G)\rceil$ when $Ï‰(G)\ge4$, where $Ï‡(G)$ and $Ï‰(G)$ denote the chromatic number and clique number of $G$, respectively. Furthermore, this bound is optimal for every $Ï‰(G)\ge4$. Constructions demonstrating the optimality of the bound are provided. Our work unifies several previously known results on $Ï‡$-binding functions for several graph classes.",0,arxiv,Matematik,CC-BY/arXiv,"Optimal chromatic bound for ($P_2\cup P_4$, HVN)-free graphs"
"Quasi-strongly regular graphs form a significant generalization of strongly regular graphs. We study the eigenvalues of a family of such graphs, $Î“_H(G)$, constructed from a finite group $G$ and a subgroup $H$. Our main results include a sufficient condition for $Î“_H(G)$ to be integral and an explicit computation of its entire spectrum when $H$ is normal, revealing that the spectrum in this case depends only on $|G|$ and the index $[G:H]$.",0,arxiv,Matematik,CC-BY/arXiv,An integral family of quasi-strongly regular Cayley graphs
"In this paper, we study variants of weight enumerators of linear codes over $\mathbb{F}_q$. We generalize the concept of average complete joint weight enumerators of two linear codes over $\mathbb{F}_q$. We also give its MacWilliams type identities. Then we establish a monomial analogue of Yoshida's theorem for this average complete joint weight enumerators. Finally, we present the generalized representation for average of $g$-fold complete joint weight enumerators for $\mathbb{F}_q$-linear codes and establish a monomial matrix analogue of Yoshida's theorem for average $g$-fold complete joint weight enumerators.",0,arxiv,Matematik,CC-BY/arXiv,Monimial Matrix Analogue of Yoshida's theorem
"In this paper, we consider $\mathcal{L}\mathcal{R}$-ending partisan rulesets as a branch of combinatorial game theory. In these rulesets, the sets of options of both players are the same. However, there are two kinds of terminal positions. If the game ends in one of the terminal positions, then a player wins and if the game ends in the other terminal position, the other player wins. We introduce notations for positions in $\mathcal{L}\mathcal{R}$-ending partisan rulesets and show their algebraic structures. We also introduce some examples of $\mathcal{L}\mathcal{R}$-partisan rulesets and show how our results can be used for analyzing the rulesets.",0,arxiv,Matematik,CC-BY/arXiv,$\mathcal{L}\mathcal{R}$-Ending partisan rulesets
"We investigate depoissonization, the problem of recovering asymptotics of sequence coefficients from their exponential generating function. Classical approaches rely on complex-analytic growth conditions, but here we develop real-variable methods that avoid such assumptions. We also address the inverse problem, deriving asymptotic expansions of the generating function itself in terms of its coefficients, thereby extending Ramanujan's original expansion. Taken together, these results offer a unified and elementary framework for depoissonization and its reverse, with applications to analytic combinatorics and probability.",0,arxiv,Matematik,CC-BY/arXiv,An Elementary Approach to Depoissonization
"A characterization is completed for finite groups acting arc-transitively on maps with square-free Euler characteristic, associated with infinite families of regular maps of square-free Euler characteristic presented. This is based on a classification of finite groups of which each Sylow subgroup has a cyclic or dihedral subgroup of prime index.",0,arxiv,Matematik,CC-BY/arXiv,Finite groups and arc-transitive maps of square-free Euler characteristic
"Given a signed bipartite graph $(B, Ï€)$ of negative girth $2k$, we present a necessary and sufficient condition for it to have the following property: each signed bipartite graph $(G, Ïƒ)$ whose negative girth is at least $2k$ and whose underlying graph has treewidth at most $t$ admits a homomorphism to $(B, Ï€)$.   Applying the result on the signed projective cube $SPC(2k-1)$, we conclude that every signed bipartite graph of negative girth at least $2k$ whose underlying graph is a partial 3-tree admits a homomorphism to $SPC(2k-1)$. For planar partial 3-trees, applying duality we conclude that if $G$ is a planar $2k$-regular multigraph whose dual has treewidth at most 3 and such that every edge-cut $(X, V\backslash X)$, where $|X|$ is odd, has size at least $2k$, then $G$ is $2k$-edge-colorable. This supports a conjecture of Seymour which, in full generality, largely extends Tait's reformulation of the four-color theorem, claiming that the fractional edge-chromatic number of a planar multigraph determines its edge-chromatic number.   Finally, noting the contrast between fractional isomorphism and quantum isomorphism, where the former admits a polynomial time algorithm while the latter is proved to be undecidable, and observing the similarities of these notions to the subject of our study, we ask if there is an algorithm to decide if an input signed graph $\widehat{B}$ has the following property: if a signed planar graph $\widehat{G}$ does not map to $\widehat{B}$, it would be because a cycle in $\widehat{G}$ does not map to $\widehat{B}$. In other words, minimal planar graphs that do not map to $\widehat{B}$ are signed cycles.",0,arxiv,Matematik,CC-BY/arXiv,Bounding signed bipartite partial t-trees and application to edge-coloring
"A family $\mathcal{G}$ of sets is a copy of a poset $(P,\leqslant)$ if $(\mathcal{G},\subseteq)$ is isomorphic to $(P,\leqslant)$. The forbidden subposet problem asks for determining $La^*(n,P)$, the maximum size of a family $\mathcal{F}\subseteq 2^{[n]}$ that does not contain any copy of $P$. We study the rainbow version of this problem: what is the maximum size $La_R^*(n,P)$ of a family $\mathcal{F}=\cup_{i=1}^mA^i$ such that all $A^i$ are antichains and there is no copy of $P$ with all sets coming from distinct $A^i$ or equivalently $\mathcal{F}$ admits a proper coloring (sets $F\subset F'$ must receive different colors) with no rainbow copy of $P$.   A poset $(Q,\leqslant')$ rainbow forces $(P,\leqslant)$ if any proper coloring $c$ of $Q$ ($q\leqslant' q'$ or $q'\leqslant' q$ implies $c(q)\neq c(q')$) admits a rainbow copy of $P$. We establish connection between the $La^*$ and the $La^*_R$ functions via poset rainbow forcing, determine the asymptotics of $La_R^*(n,T)$ for all tree posets and obtain further exact or asymptotic results for antichains and complete bipartite posets.",0,arxiv,Matematik,CC-BY/arXiv,Rainbow TurÃ¡n problems for forbidden subposets
"We show that for $2\le d\le 4$, every finite geometric simplicial complex $Î”$ in $\mathbb{R}^d$ with vertices on the moment curve can be extended to a triangulation $T$ of the cyclic polytope $C$ where $Î”, T$ and $C$ all have the same vertex set. Further, for $d\ge 5$ we construct for every $n\ge d+3$ complexes $Î”$ on $n$ vertices for which no such triangulations $T$ exist.   Our result for $d=4$ has the following novel algebraic application, due to a correspondence by Oppermann and Thomas (JEMS, 2012): every maximal rigid object in $\mathcal{O}_{A_n^{2}}$ is cluster tilting, where $\mathcal{O}_{A_n^Î´}$ denotes a higher dimensional cluster category introduced by Oppermann and Thomas for $A_n^Î´$, where $A_n^Î´$ denotes a higher Auslander algebra of linearly oriented type $A$.",0,arxiv,Matematik,CC-BY/arXiv,On an extension problem on the moment curve
"This paper is devoted to the study of $2$-designs with $Î»\ge (r,Î»)^2$ admitting a flag-transitive automorphism group $G$. The group $G$ has been shown to be point-primitive of either almost simple or affine type. In this paper, we classify the $2$-designs with $Î»\geq (r,Î»)^2>1$ admitting a flag-transitive almost simple automorphism group with socle $\mathrm{PSL}_n(q)$ or $\mathrm{PSU}_n(q)$ for $n \geq 3$.",0,arxiv,Matematik,CC-BY/arXiv,"Flag-transitive $2$-$(v,k,Î»)$ designs with $Î»\ge (r,Î»)^2$"
"Subgraph counting is a fundamental task that underpins several network analysis methodologies, including community detection and graph two-sample tests. Counting subgraphs is a computationally intensive problem. Substantial research has focused on developing efficient algorithms and strategies to make it feasible for larger unweighted graphs. Implementing those algorithms can be a significant hurdle for data professionals or researchers with limited expertise in algorithmic principles and programming. Furthermore, many real-world networks are weighted. Computing the number of weighted subgraphs in weighted networks presents a computational challenge, as no efficient algorithm exists for the worst-case scenario. In this paper, we derive explicit formulas for counting small edge-weighted subgraphs using the weighted adjacency matrix. These formulas are applicable to unweighted networks, offering a simple and highly practical analytical tool for researchers across various scientific domains. In addition, we introduce a generalized methodology for calculating arbitrary weighted subgraphs.",0,arxiv,Matematik,CC-BY/arXiv,On the number of small edge-weighted subgraphs
"Let $F_l$ be the fan graph obtained by joining a vertex with a path on $l-1$ vertices. Yu, Li and Peng [Discrete Math. 346 (2023)] conjectured that if the number of edges of $G$ is $m$ and the spectral radius $Î»(G)>\frac{k-1+\sqrt{4m-k^2+1}}{2}$, then $G$ contains a $F_{2k+1}$ and $F_{2k+2}$, unless $G=K_{k}\vee (\frac{m}{k}-\frac{k-1}{2})K_1$. The case $k\geq 3$ of the above conjecture has been confirmed by Li, Zhao and Zou [J. Graph theory 110 (2025)]. Zhang and Wang [Discrete Math. 347 (2024)], Yu, Li and Peng [Discrete Math. 348 (2025)], Gao and Li [Discrete Math. 349 (2026)] confirmed the case $k=2$. However, the extremal graphs for the case $k=2$ only exist when $m$ is odd. The case with $m$ even has not been determined. In this paper, we characterize the extremal graph for $F_6$ and even $m\ge 3000$.",0,arxiv,Matematik,CC-BY/arXiv,Spectral extremal graphs for $F_6$-free graphs with even size
"The rook polynomial is a generating function that enumerates the number of ways to place rooks, with no two in the same row or column, on a collection of cells regarded as a pruned chessboard. In combinatorial commutative algebra, special attention is devoted to its variant, the switching rook polynomial, which is conjectured to coincide with the $h$-polynomial of the $K$-algebra associated with the given collection of cells. In this context, palindromicity plays a crucial role, as it reflects the algebraic property of Gorensteinness. In this paper, we introduce a new combinatorial property, called domino-stability, and we prove that the switching rook polynomial of a collection of cells $\mathcal{P}$ is palindromic if and only if $\mathcal{P}$ is domino-stable. Building upon this result, we derive new insights into the characterization of Gorenstein $K$-algebras arising from polyominoes or, more generally, from collections of cells.",0,arxiv,Matematik,CC-BY/arXiv,Switching Rook Polynomials of Collections of Cells: Palindromicity and Domino-Stability
"We consider the $k$-nested sum of integer powers, $F(n,m,k)$, defined as repeated partial sums of the classical Faulhaber polynomials. We provide an explicit recurrence relation relating $F(n,m,k)$ to sums of lower power $m-1$ and higher nesting level $k+1$. This identity is derived from a core algebraic relation on the binomial coefficients that form the kernel of the nested sum's representation. We discuss the relevance to the 2010 paper by S.~Butler and P.~Karasik, ``A Note on Nested Sums'' (JIS, Vol.~13, Article~10.4.4), which studies nested sums of powers of integers that generalize Faulhaber-type sums. We also discuss the equivalence to a related recurrence previously established in the context of hypersums of powers of integers by J.~L.~Cereceda.",0,arxiv,Matematik,CC-BY/arXiv,Recurrence Relations for k-Fold Nested Power Sums
"A spline is an assignment of polynomials to the vertices of a graph, where the difference of two polynomials along an edge must belong to the ideal labeling that edge. We consider a ring of splines $\mathcal{M}_{H}$ constructed on a graph whose vertices are the Weyl group $\mathfrak{W}_n$ of signed permutations, and whose edges and edge-ideals are defined using an order ideal $H$ of positive roots. These splines are a module over the polynomial ring in two ways, and a $\mathfrak{W}_n$-module by the dot action. These structures on $\mathcal{M}_{H}$ give rise to the graded left and right dot action representations of $\mathfrak{W}_n$. The left representation is the type B/C generalization of the type A dot action for regular semisimple Hessenberg varieties (and thus, chromatic quasisymmetric functions), and the right representation is the same for corresponding manifolds of isospectral matrices (and thus, unicellular LLT polynomials). This paper gives explicit module generators for the degree-one graded piece of $\mathcal{M}_{H}$ and computes the degree-one piece of the both dot action representations for all $H$ using the combinatorial data of $H$.",0,arxiv,Matematik,CC-BY/arXiv,Signed permutations and degree-one dot action representations for types B and C
"We introduce and develop the theory of UMEL-shellable posets. These are posets equipped with an edge-lexicographical labeling satisfying certain uniformity and monotonicity properties. This framework encompasses classical families of combinatorial geometries, including uniform matroids, projective and affine geometries, braid matroids of type A and B, and all Dowling geometries. It also comprises all rank-uniform supersolvable lattices, and therefore also all rank-uniform distributive lattices. Our main result establishes real-rootedness phenomena for the Chow polynomials, the augmented Chow polynomials, and the chain polynomials associated with those posets, thus making simultaneous progress towards conjectures by Ferroni--SchrÃ¶ter, Huh--Stevens, and Athanasiadis--Kalampogia-Evangelinou. In the special case of lattices of flats of matroids, the (augmented) Chow polynomials coincide with the Hilbert--PoincarÃ© series of the Chow ring associated to the smooth and generally noncompact toric varieties of the (augmented) Bergman fan of the matroid, whereas the chain polynomial encodes the Hilbert--PoincarÃ© series of the Stanley--Reisner ring of the Bergman complex of the matroid. Therefore, these real-rootedness results are tightly linked to the study of these algebro-geometric structures in matroid theory.",0,arxiv,Matematik,CC-BY/arXiv,Chow polynomials of rank-uniform labeled posets
"We provide counterexamples to several conjectures concerning strongly maximal and strongly minimal structures in infinite graphs and hypergraphs. In particular, we construct 3-uniform hypergraphs without strongly maximal matchings and without strongly minimal covers, and from our construction for covers we build a graph with no strongly minimal colouring. We also consider several refinements of these problems.   Our results resolve conjectures and questions of Aharoni; Aharoni and Berger; Aharoni, Berger, Georgakopoulos, and SprÃ¼ssel; Aharoni and Korman; and Tardos.",0,arxiv,Matematik,CC-BY/arXiv,Counterexamples to conjectures on strong maximality and minimality
"We study the rational homology of the Deligne-Mumford compactification $\overline{\mathcal M}_{g,n}$ of the moduli space of stable curves via a family of Morse functions, the $\text{sys}_T$ functions, which encode geometric information about short geodesics on hyperbolic surfaces. Exploiting the Morse-theoretic properties of $\text{sys}_T$, including the existence of an index gap and the behavior of critical points near boundary strata, we prove that in low degrees the homology of $\overline{\mathcal M}_{g,n}$ is supported entirely on the boundary $\partial \mathcal M_{g,n}$.   Furthermore, we establish finite generation and stability phenomena for the rational homology across all genera and numbers of marked points. Using stable graphs and explicit attaching maps, we show that for each fixed degree $k$, a finite set of critical points generates all $k$-th homology classes via attaching stable graphs of stratum dimension $0$. This result recovers previously known stability in the genus direction, such as Tosteson's theorem, and provides a concrete, geometric construction of homology generators. Our approach unifies combinatorial, geometric, and Morse-theoretic techniques to give a comprehensive picture of the low-degree and stable homology of $\overline{\mathcal M}_{g,n}$.",0,arxiv,Matematik,CC-BY/arXiv,Stability phenomena in Deligne-Mumford compactifications via Morse theory
"Given a graph $G$ and a non-negative integer $d$ let $Î±_d(G)$ be the order of a largest induced $d$-degenerate subgraph of $G$. We prove that for any pair of non-negative integers $k>d$, if $G$ is a $k$-degenerate graph, then $Î±_d(G) \geq \max\{ \frac{(d+1)n}{k+d+1}, n - Î±_{k-d-1}(G)\}$. For $k$-degenerate graphs this improves a more general lower bound of Alon, Kahn, and Seymour. By modifying our argument we obtain improved lower bound on $Î±_d(G)$ for graphs of bounded genus. This extends earlier work on degenerate subgraphs of planar graphs.",0,arxiv,Matematik,CC-BY/arXiv,A Note on Large Degenerate Induced Subgraphs in Sparse Graphs
"We introduce and study a generalization of the Narayana numbers $N_d(n,k) = \frac{1}{n+1} \binom{n+1}{k+1} \binom{ n + (n-k)(d-2)+1}{k}$ for integers $d \geq 2$ and $n,k \geq 0$. This two-parameter array extends the classical Narayana numbers ($d=2$) and yields a $d$-ary analogue of the Catalan numbers $C_d(n) = \sum_{k=0}^n N_d(n,k)$. We give nine combinatorial interpretations of $N_d(n,k)$ that unify and generalize known combinatorial interpretations of the Narayana numbers and $C_3(n)$ in the literature. In particular, we show that $N_d(n,k)$ counts a natural class of operator monomials over a $d$-ary associative algebra, thereby extending a result of Bremner and Elgendy for the binary case. We also construct explicit bijections between these monomials and several families of classic combinatorial objects, including SchrÃ¶der paths, Dyck paths, rooted ordered trees, and $231$-avoiding permutations.",0,arxiv,Matematik,CC-BY/arXiv,A new generalization of the Narayana numbers inspired by linear operators on associative $d$-ary algebras
"The cyclotomic matrix is commonly used to arrange cyclotomic numbers in a convenient format. A natural question is whether the structure of the matrix can reflect properties of these numbers. In this article, we examine cyclotomic numbers through their associated cyclotomic matrix and reveal an algebraic structure by relating it to a basis element of a Schur ring. This viewpoint leads to structural identities and reinterpretations of classical results. As an application, we investigate the power difference set problem and establish conditions expressed through cyclotomic matrices, including spectral and determinant characterizations.",0,arxiv,Matematik,CC-BY/arXiv,Cyclotomic Matrices and Power Difference Sets
"Although false for general graphs, this note gives an elementary proof of the bunkbed conjecture for any acyclic graph. The argument is short and self-contained, and may be of educational interest.",0,arxiv,Matematik,CC-BY/arXiv,An elementary proof of the bunkbed conjecture for forests
"We investigate commutators of free variables of the form \( i[x, s] \), where \( s \) is a semicircular element. We show that although \( s \) and \( i[x, s] \) are not free, their sum nevertheless satisfies the free additive convolution identity \[ Î¼_{s + i[x, s]} = Î¼_s \boxplus Î¼_{i[x, s]}. \] Furthermore, we prove that the polynomial \( x + i[x, s] \) is freely infinitely divisible whenever \( x \) itself is freely infinitely divisible.",0,arxiv,Matematik,CC-BY/arXiv,On some properties of free commutators with semicircular variables
"Let $n>c_1\ge c_2$ and $Î£$ be positive integers with $n\cdot c_1\ge Î£\ge n\cdot c_2.$ Let $\mD=\dds{n}Î£{c_1}{c_2}$ denote the set of all degree sequences of length $n$ with the even sum $Î£$ and satisfying $c_1\ge d_i\ge c_2.$ We show that if all degree sequences in $\mD$ are graphic, then $\mD$ is $3n^{13}$-stable. (The concept of $P$-stability was introduced by Jerrum and Sinclair in 1990.) In particular, this implies that the switch Markov-chain mixes rapidly on all such degree sequences.   In this paper we also study the inverse direction. We show the following: if all graphic sequences of a degree sequence region satisfy the $p(n)$-stability condition then the overwhelming majority of the sequences in the region is graphic. This answers affirmatively a question raised in the paper \DOI{10.1016/j.aam.2024.102805}.",0,arxiv,Matematik,CC-BY/arXiv,Any fully graphic region of degree sequences can be sampled rapidly
In this article we prove that the adjoint polynomial of arbitrary convex polytopes is up to scaling uniquely determined by vanishing to the right order on the polytopes residual arrangement. This answers a problem posed by Kohn and Ranestad and generalizes their main theorem to non-simple polytopes. We furthermore prove that the adjoint polynomial is already characterized by vanishing to the right order on a zero-dimensional subset of the residual arrangement.,0,arxiv,Matematik,CC-BY/arXiv,Geometry of Adjoint Hypersurfaces for Polytopes
"A set of Pauli stings is well characterized by the graph that encodes its commutatitivity structure, i.e., by its frustration graph. This graph provides a natural interface between graph theory and quantum information, which we explore in this work. We investigate all aspects of this interface for a special class of graphs that bears tight connections between the groundstate structures of a spin systems and topological structure of a graph. We call this class $\hbar$-perfect, as it extends the class of perfect and $h$-perfect graphs.   Having an $\hbar$-perfect graph opens up several applications: we find efficient schemes for entanglement detection, a connection to the complexity of shadow tomography, tight uncertainty relations and a construction for computing good lower on bounds ground state energies. Conversely this also induces quantum algorithms for computing the independence number. Albeit those algorithms do not immediately promise an advantage in runtime, we show that an approximate Hamilton encoding of the independence number can be achieved with an amount of qubits that typically scales logarithmically in the number of vertices. We also we also determine the behavior of $\hbar$-perfectness under basic graph operations and evaluate their prevalence among all graphs.",0,arxiv,Matematik,CC-BY/arXiv,"Simultaneous variances of Pauli strings, weighted independence numbers, and a new kind of perfection of graphs"
"We prove that every graph of rankwidth at least $72r$ contains an induced subgraph whose minimum balanced cutrank is at least $r$, which implies a vertex subset where every balanced separation has $\mathbb{F}_2$-cutrank at least $r$. This implies a novel relation between rankwidth and a well-linkedness measure, defined entirely by balanced vertex cuts. As a byproduct, our result supports the notion of rank-expansion as a suitable candidate for measuring expansion in dense graphs.",0,arxiv,Matematik,CC-BY/arXiv,Rankwidth of Graphs with Balanced Separations: Expansion for Dense Graphs
"Tropicalisation (with trivial coefficients) is a process that turns a polynomial equation into a combinatorial predicate on subsets of the set of variables. We show that for each minuscule representation of a simple reductive group, there is a set of quadratic equations cutting out the orbit of the highest weight vector whose tropicalisation characterises the set of Coxeter matroids for that representation which satisfy the strong exchange property.",0,arxiv,Matematik,CC-BY/arXiv,Quadratic exchange equations for Coxeter matroids
"We introduce a new class of simplicial complexes, called \emph{$t$-Young complexes}, arising from a Young diagram and a positive integer~$t$. We show that every $t$-Young complex is either contractible or homotopy equivalent to a wedge of spheres. A complete characterization of their vertex-decomposability is provided, and in several cases, we establish explicit formulas for their homotopy types. Interestingly, $t$-Young complexes naturally appear as the Alexander dual complexes of squarefree powers of $t$-path ideals of path graphs, as well as of certain ideals generated by subsets of their minimal generators. As an application, we derive formulas for the projective dimension and Krull dimension of these squarefree powers.",0,arxiv,Matematik,CC-BY/arXiv,$t$-Young complexes and squarefree powers of $t$-path ideals
"In this paper, we study the higher Steklov eigenvalues of graphs on surfaces. We obtain the upper bound of higher Steklov eigenvalues of a finite graph $G$ with boundary $B$ and genus $g$ by using metrical deformation via probability flows. This result can be regarded as a discrete analogue of Karpukhin's bound in spectral geometry.",0,arxiv,Matematik,CC-BY/arXiv,Higher Steklov eigenvalues of graphs on surfaces
"We investigate paths in the hexagonal circle packing and enumerate them with respect to width, height, number of steps, area, and kissing number. Functional equations and the kernel method yield closed bivariate generating functions together with coefficient formulas and asymptotics. We establish bijections with skew Dyck paths, constrained Motzkin paths, and peakless Motzkin paths, and show that several of the associated counting arrays are Riordan arrays. Continued-fraction expansions for the area and kissing-number enumerators are also obtained.",0,arxiv,Matematik,CC-BY/arXiv,Enumeration of paths in a hexagonal circle packing
"For a finite group $G$, the notion of a $G$-transfer system provides homotopy theorists with a combinatorial way to study equivariant objects. In this paper, we focus on the properties of transfer systems for non-abelian groups. We explicitly describe the width of all dihedral groups, quaternion groups, and dicyclic groups. For a given $G$, the set of all $G$-transfer systems forms a poset lattice under inclusion; these are a useful resource to homotopical combinatorialists for detecting patterns and checking conjectures. We expand the suite of known transfer system lattices for non-abelian groups including those which are dihedral, dicyclic, Frobenius, and alternating.",0,arxiv,Matematik,CC-BY/arXiv,Characterizing Transfer Systems for Non-Abelian Groups
"An orientation-preserving mapping is not always defined by how it acts on triples of elements. Although this fact is simple and well-known, it sometimes gets overlooked, resulting in wrong statements in publications. Automated proof assistants are a technology that is supposed to ensure that proofs exactly match the results, thus pre-empting mistakes. In this paper we successfully formalise the definition of orientation-preserving mappings in proof assistant software Lean and construct a computer-verified proof of the above fact.",0,arxiv,Matematik,CC-BY/arXiv,Are automated proof assistants ready for semigroup research? Orientation-preserving mappings and proof assistant Lean
"Towards human-robot coexistence, socially aware navigation is significant for mobile robots. Yet existing studies on this area focus mainly on path efficiency and pedestrian collision avoidance, which are essential but represent only a fraction of social navigation. Beyond these basics, robots must also comply with user instructions, aligning their actions to task goals and social norms expressed by humans. In this work, we present LISN-Bench, the first simulation-based benchmark for language-instructed social navigation. Built on Rosnav-Arena 3.0, it is the first standardized social navigation benchmark to incorporate instruction following and scene understanding across diverse contexts. To address this task, we further propose Social-Nav-Modulator, a fast-slow hierarchical system where a VLM agent modulates costmaps and controller parameters. Decoupling low-level action generation from the slower VLM loop reduces reliance on high-frequency VLM inference while improving dynamic avoidance and perception adaptability. Our method achieves an average success rate of 91.3%, which is greater than 63% than the most competitive baseline, with most of the improvements observed in challenging tasks such as following a person in a crowd and navigating while strictly avoiding instruction-forbidden regions. The project website is at: https://social-nav.github.io/LISN-project/",0,arxiv,AI,CC-BY/arXiv,LISN: Language-Instructed Social Navigation with VLM-based Controller Modulating
"Scalable sampling of molecular states in thermodynamic equilibrium is a long-standing challenge in statistical physics. Boltzmann Generators tackle this problem by pairing a generative model, capable of exact likelihood computation, with importance sampling to obtain consistent samples under the target distribution. Current Boltzmann Generators primarily use continuous normalizing flows (CNFs) trained with flow matching for efficient training of powerful models. However, likelihood calculation for these models is extremely costly, requiring thousands of function evaluations per sample, severely limiting their adoption. In this work, we propose Few-step Accurate Likelihoods for Continuous Flows (FALCON), a method which allows for few-step sampling with a likelihood accurate enough for importance sampling applications by introducing a hybrid training objective that encourages invertibility. We show FALCON outperforms state-of-the-art normalizing flow models for molecular Boltzmann sampling and is two orders of magnitude faster than the equivalently performing CNF model.",0,arxiv,AI,CC-BY/arXiv,FALCON: Few-step Accurate Likelihoods for Continuous Flows
"In-context learning with attention enables large neural networks to make context-specific predictions by selectively focusing on relevant examples. Here, we adapt this idea to supervised learning procedures such as lasso regression and gradient boosting, for tabular data. Our goals are to (1) flexibly fit personalized models for each prediction point and (2) retain model simplicity and interpretability.   Our method fits a local model for each test observation by weighting the training data according to attention, a supervised similarity measure that emphasizes features and interactions that are predictive of the outcome. Attention weighting allows the method to adapt to heterogeneous data in a data-driven way, without requiring cluster or similarity pre-specification. Further, our approach is uniquely interpretable: for each test observation, we identify which features are most predictive and which training observations are most relevant. We then show how to use attention weighting for time series and spatial data, and we present a method for adapting pretrained tree-based models to distributional shift using attention-weighted residual corrections. Across real and simulated datasets, attention weighting improves predictive performance while preserving interpretability, and theory shows that attention-weighting linear models attain lower mean squared error than the standard linear model under mixture-of-models data-generating processes with known subgroup structure.",0,arxiv,AI,CC-BY/arXiv,Supervised learning pays attention
"Continual learning in Neural Machine Translation (NMT) faces the dual challenges of catastrophic forgetting and the high computational cost of retraining. This study establishes Low-Rank Adaptation (LoRA) as a parameter-efficient framework to address these challenges in dedicated NMT architectures. We first demonstrate that LoRA-based fine-tuning adapts NMT models to new languages and domains with performance on par with full-parameter techniques, while utilizing only a fraction of the parameter space. Second, we propose an interactive adaptation method using a calibrated linear combination of LoRA modules. This approach functions as a gate-free mixture of experts, enabling real-time, user-controllable adjustments to domain and style without retraining. Finally, to mitigate catastrophic forgetting, we introduce a novel gradient-based regularization strategy specifically designed for low-rank decomposition matrices. Unlike methods that regularize the full parameter set, our approach weights the penalty on the low-rank updates using historical gradient information. Experimental results indicate that this strategy efficiently preserves prior domain knowledge while facilitating the acquisition of new tasks, offering a scalable paradigm for interactive and continual NMT.",0,arxiv,AI,CC-BY/arXiv,Efficient Continual Learning in Neural Machine Translation: A Low-Rank Adaptation Approach
"Reinforcement learning agents often behave unexpectedly in sparse-reward or safety-critical environments, creating a strong need for reliable debugging and verification tools. In this paper, we propose STACHE, a comprehensive framework for generating local, black-box explanations for an agent's specific action within discrete Markov games. Our method produces a Composite Explanation consisting of two complementary components: (1) a Robustness Region, the connected neighborhood of states where the agent's action remains invariant, and (2) Minimal Counterfactuals, the smallest state perturbations required to alter that decision. By exploiting the structure of factored state spaces, we introduce an exact, search-based algorithm that circumvents the fidelity gaps of surrogate models. Empirical validation on Gymnasium environments demonstrates that our framework not only explains policy actions, but also effectively captures the evolution of policy logic during training - from erratic, unstable behavior to optimized, robust strategies - providing actionable insights into agent sensitivity and decision boundaries.",0,arxiv,AI,CC-BY/arXiv,STACHE: Local Black-Box Explanations for Reinforcement Learning Policies
"Moralisation and Triangulation are transformations allowing to switch between different ways of factoring a probability distribution into a graphical model. Moralisation allows to view a Bayesian network (a directed model) as a Markov network (an undirected model), whereas triangulation addresses the opposite direction. We present a categorical framework where these transformations are modelled as functors between a category of Bayesian networks and one of Markov networks. The two kinds of network (the objects of these categories) are themselves represented as functors from a `syntax' domain to a `semantics' codomain. Notably, moralisation and triangulation can be defined inductively on such syntax via functor pre-composition. Moreover, while moralisation is fully syntactic, triangulation relies on semantics. This leads to a discussion of the variable elimination algorithm, reinterpreted here as a functor in its own right, that splits the triangulation procedure in two: one purely syntactic, the other purely semantic. This approach introduces a functorial perspective into the theory of probabilistic graphical models, which highlights the distinctions between syntactic and semantic modifications.",0,arxiv,AI,CC-BY/arXiv,"Bayesian Networks, Markov Networks, Moralisation, Triangulation: a Categorical Perspective"
"The integration of Unmanned Aerial Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs) is increasingly central to the development of intelligent autonomous systems for applications such as search and rescue, environmental monitoring, and logistics. However, precise coordination between these platforms in real-time scenarios presents major challenges, particularly when external localization infrastructure such as GPS or GNSS is unavailable or degraded [1]. This paper proposes a vision-based, data-driven framework for real-time UAV-UGV integration, with a focus on robust UGV detection and heading angle prediction for navigation and coordination. The system employs a fine-tuned YOLOv5 model to detect UGVs and extract bounding box features, which are then used by a lightweight artificial neural network (ANN) to estimate the UAV's required heading angle. A VICON motion capture system was used to generate ground-truth data during training, resulting in a dataset of over 13,000 annotated images collected in a controlled lab environment. The trained ANN achieves a mean absolute error of 0.1506Â° and a root mean squared error of 0.1957Â°, offering accurate heading angle predictions using only monocular camera inputs. Experimental evaluations achieve 95% accuracy in UGV detection. This work contributes a vision-based, infrastructure- independent solution that demonstrates strong potential for deployment in GPS/GNSS-denied environments, supporting reliable multi-agent coordination under realistic dynamic conditions. A demonstration video showcasing the system's real-time performance, including UGV detection, heading angle prediction, and UAV alignment under dynamic conditions, is available at: https://github.com/Kooroshraf/UAV-UGV-Integration",0,arxiv,AI,CC-BY/arXiv,Visual Heading Prediction for Autonomous Aerial Vehicles
"Long-term planning in complex, text-based environments presents significant challenges due to open-ended action spaces, ambiguous observations, and sparse feedback. Recent research suggests that large language models (LLMs) encode rich semantic knowledge about the world, which can be valuable for guiding agents in high-level reasoning and planning across both embodied and purely textual settings. However, existing approaches often depend heavily on querying LLMs during training and inference, making them computationally expensive and difficult to deploy efficiently. In addition, these methods typically employ a pretrained, unaltered LLM whose parameters remain fixed throughout training, providing no opportunity for adaptation to the target task. To address these limitations, we introduce SCOPE (Subgoal-COnditioned Pretraining for Efficient planning), a one-shot hierarchical planner that leverages LLM-generated subgoals only at initialization to pretrain a lightweight student model. Unlike prior approaches that distill LLM knowledge by repeatedly prompting the model to adaptively generate subgoals during training, our method derives subgoals directly from example trajectories. This design removes the need for repeated LLM queries, significantly improving efficiency, though at the cost of reduced explainability and potentially suboptimal subgoals. Despite their suboptimality, our results on the TextCraft environment show that LLM-generated subgoals can still serve as a strong starting point for hierarchical goal decomposition in text-based planning tasks. Compared to the LLM-based hierarchical agent ADaPT (Prasad et al., 2024), which achieves a 0.52 success rate, our method reaches 0.56 and reduces inference time from 164.4 seconds to just 3.0 seconds.",0,arxiv,AI,CC-BY/arXiv,SCOPE: Language Models as One-Time Teacher for Hierarchical Planning in Text Environments
"Metadata vocabularies are essential for advancing FAIR and FARR data principles, but their development constrained by limited human resources and inconsistent standardization practices. This paper introduces MatSci-YAMZ, a platform that integrates artificial intelligence (AI) and human-in-the-loop (HILT), including crowdsourcing, to support metadata vocabulary development. The paper reports on a proof-of-concept use case evaluating the AI-HILT model in materials science, a highly interdisciplinary domain Six (6) participants affiliated with the NSF Institute for Data-Driven Dynamical Design (ID4) engaged with the MatSci-YAMZ plaform over several weeks, contributing term definitions and providing examples to prompt the AI-definitions refinement. Nineteen (19) AI-generated definitions were successfully created, with iterative feedback loops demonstrating the feasibility of AI-HILT refinement. Findings confirm the feasibility AI-HILT model highlighting 1) a successful proof of concept, 2) alignment with FAIR and open-science principles, 3) a research protocol to guide future studies, and 4) the potential for scalability across domains. Overall, MatSci-YAMZ's underlying model has the capacity to enhance semantic transparency and reduce time required for consensus building and metadata vocabulary development.",0,arxiv,AI,CC-BY/arXiv,Human-in-the-Loop and AI: Crowdsourcing Metadata Vocabulary for Materials Science
"While modern language models and their inner workings are incredibly complex, recent work (Golowich, Liu & Shetty; 2025) has proposed a simple and potentially tractable abstraction for them through the observation that empirically, these language models all seem to have approximately low logit rank. Roughly, this means that a matrix formed by the model's log probabilities of various tokens conditioned on certain sequences of tokens is well approximated by a low rank matrix.   In this paper, our focus is on understanding how this structure can be exploited algorithmically for obtaining provable learning guarantees. Since low logit rank models can encode hard-to-learn distributions such as noisy parities, we study a query learning model with logit queries that reflects the access model for common APIs. Our main result is an efficient algorithm for learning any approximately low logit rank model from queries. We emphasize that our structural assumption closely reflects the behavior that is empirically observed in modern language models. Thus, our result gives what we believe is the first end-to-end learning guarantee for a generative model that plausibly captures modern language models.",0,arxiv,AI,CC-BY/arXiv,Provably Learning from Modern Language Models via Low Logit Rank
"We present the first comprehensive evaluation of AI agents against human cybersecurity professionals in a live enterprise environment. We evaluate ten cybersecurity professionals alongside six existing AI agents and ARTEMIS, our new agent scaffold, on a large university network consisting of ~8,000 hosts across 12 subnets. ARTEMIS is a multi-agent framework featuring dynamic prompt generation, arbitrary sub-agents, and automatic vulnerability triaging. In our comparative study, ARTEMIS placed second overall, discovering 9 valid vulnerabilities with an 82% valid submission rate and outperforming 9 of 10 human participants. While existing scaffolds such as Codex and CyAgent underperformed relative to most human participants, ARTEMIS demonstrated technical sophistication and submission quality comparable to the strongest participants. We observe that AI agents offer advantages in systematic enumeration, parallel exploitation, and cost -- certain ARTEMIS variants cost $18/hour versus $60/hour for professional penetration testers. We also identify key capability gaps: AI agents exhibit higher false-positive rates and struggle with GUI-based tasks.",0,arxiv,AI,CC-BY/arXiv,Comparing AI Agents to Cybersecurity Professionals in Real-World Penetration Testing
"Generative Artificial Intelligence models, such as Large Language Models (LLMs) and Large Vision Models (VLMs), exhibit state-of-the-art performance but remain vulnerable to hardware-based threats, specifically bit-flip attacks (BFAs). Existing BFA discovery methods lack generalizability and struggle to scale, often failing to analyze the vast parameter space and complex interdependencies of modern foundation models in a reasonable time. This paper proposes FlipLLM, a reinforcement learning (RL) architecture-agnostic framework that formulates BFA discovery as a sequential decision-making problem. FlipLLM combines sensitivity-guided layer pruning with Q-learning to efficiently identify minimal, high-impact bit sets that can induce catastrophic failure. We demonstrate the effectiveness and generalizability of FlipLLM by applying it to a diverse set of models, including prominent text-only LLMs (GPT-2 Large, LLaMA 3.1 8B, and DeepSeek-V2 7B), VLMs such as LLaVA 1.6, and datasets, such as MMLU, MMLU-Pro, VQAv2, and TextVQA. Our results show that FlipLLM can identify critical bits that are vulnerable to BFAs up to 2.5x faster than SOTA methods. We demonstrate that flipping the FlipLLM-identified bits plummets the accuracy of LLaMA 3.1 8B from 69.9% to ~0.2%, and for LLaVA's VQA score from 78% to almost 0%, by flipping as few as 5 and 7 bits, respectively. Further analysis reveals that applying standard hardware protection mechanisms, such as ECC SECDED, to the FlipLLM-identified bit locations completely mitigates the BFA impact, demonstrating the practical value of our framework in guiding hardware-level defenses. FlipLLM offers the first scalable and adaptive methodology for exploring the BFA vulnerability of both language and multimodal foundation models, paving the way for comprehensive hardware-security evaluation.",0,arxiv,AI,CC-BY/arXiv,FlipLLM: Efficient Bit-Flip Attacks on Multimodal LLMs using Reinforcement Learning
"Pretrained Multimodal Large Language Models (MLLMs) are increasingly deployed in medical AI systems for clinical reasoning, diagnosis support, and report generation. However, their training on sensitive patient data raises critical privacy and compliance challenges under regulations such as HIPAA and GDPR, which enforce the ""right to be forgotten"". Unlearning, the process of tuning models to selectively remove the influence of specific training data points, offers a potential solution, yet its effectiveness in complex medical settings remains underexplored. To systematically study this, we introduce MedForget, a Hierarchy-Aware Multimodal Unlearning Testbed with explicit retain and forget splits and evaluation sets containing rephrased variants. MedForget models hospital data as a nested hierarchy (Institution -> Patient -> Study -> Section), enabling fine-grained assessment across eight organizational levels. The benchmark contains 3840 multimodal (image, question, answer) instances, each hierarchy level having a dedicated unlearning target, reflecting distinct unlearning challenges. Experiments with four SOTA unlearning methods on three tasks (generation, classification, cloze) show that existing methods struggle to achieve complete, hierarchy-aware forgetting without reducing diagnostic performance. To test whether unlearning truly deletes hierarchical pathways, we introduce a reconstruction attack that progressively adds hierarchical level context to prompts. Models unlearned at a coarse granularity show strong resistance, while fine-grained unlearning leaves models vulnerable to such reconstruction. MedForget provides a practical, HIPAA-aligned testbed for building compliant medical AI systems.",0,arxiv,AI,CC-BY/arXiv,MedForget: Hierarchy-Aware Multimodal Unlearning Testbed for Medical AI
"This paper develops a geometric framework for modeling belief, motivation, and influence across cognitively heterogeneous agents. Each agent is represented by a personalized value space, a vector space encoding the internal dimensions through which the agent interprets and evaluates meaning. Beliefs are formalized as structured vectors-abstract beings-whose transmission is mediated by linear interpretation maps. A belief survives communication only if it avoids the null spaces of these maps, yielding a structural criterion for intelligibility, miscommunication, and belief death.   Within this framework, I show how belief distortion, motivational drift, counterfactual evaluation, and the limits of mutual understanding arise from purely algebraic constraints. A central result-""the No-Null-Space Leadership Condition""-characterizes leadership as a property of representational reachability rather than persuasion or authority. More broadly, the model explains how abstract beings can propagate, mutate, or disappear as they traverse diverse cognitive geometries.   The account unifies insights from conceptual spaces, social epistemology, and AI value alignment by grounding meaning preservation in structural compatibility rather than shared information or rationality. I argue that this cognitive-geometric perspective clarifies the epistemic boundaries of influence in both human and artificial systems, and offers a general foundation for analyzing belief dynamics across heterogeneous agents.",0,arxiv,AI,CC-BY/arXiv,Interpretation as Linear Transformation: A Cognitive-Geometric Model of Belief and Meaning
"This chapter explores the application of Large Language Models in the legal domain, showcasing their potential to optimise and augment traditional legal tasks by analysing possible use cases, such as assisting in interpreting statutes, contracts, and case law, enhancing clarity in legal summarisation, contract negotiation, and information retrieval. There are several challenges that can arise from the application of such technologies, such as algorithmic monoculture, hallucinations, and compliance with existing regulations, including the EU's AI Act and recent U.S. initiatives, alongside the emerging approaches in China. Furthermore, two different benchmarks are presented.",0,arxiv,AI,CC-BY/arXiv,LLMs in Interpreting Legal Documents
"The massive scale of modern AI accelerators presents critical challenges to traditional fault assessment methodologies, which face prohibitive computational costs and provide poor coverage of critical failure modes. This paper introduces RIFT (Reinforcement Learning-guided Intelligent Fault Targeting), a scalable framework that automates the discovery of minimal, high-impact fault scenarios for efficient design-time fault assessment. RIFT transforms the complex search for worst-case faults into a sequential decision-making problem, combining hybrid sensitivity analysis for search space pruning with reinforcement learning to intelligently generate minimal, high-impact test suites. Evaluated on billion-parameter Large Language Model (LLM) workloads using NVIDIA A100 GPUs, RIFT achieves a \textbf{2.2$\times$} fault assessment speedup over evolutionary methods and reduces the required test vector volume by over \textbf{99\%} compared to random fault injection, all while achieving \textbf{superior fault coverage}. The proposed framework also provides actionable data to enable intelligent hardware protection strategies, demonstrating that RIFT-guided selective error correction code provides a \textbf{12.8$\times$} improvement in \textbf{cost-effectiveness} (coverage per unit area) compared to uniform triple modular redundancy protection. RIFT automatically generates UVM-compliant verification artifacts, ensuring its findings are directly actionable and integrable into commercial RTL verification workflows.",0,arxiv,AI,CC-BY/arXiv,RIFT: A Scalable Methodology for LLM Accelerator Fault Assessment using Reinforcement Learning
"Visual concept composition, which aims to integrate different elements from images and videos into a single, coherent visual output, still falls short in accurately extracting complex concepts from visual inputs and flexibly combining concepts from both images and videos. We introduce Bind & Compose, a one-shot method that enables flexible visual concept composition by binding visual concepts with corresponding prompt tokens and composing the target prompt with bound tokens from various sources. It adopts a hierarchical binder structure for cross-attention conditioning in Diffusion Transformers to encode visual concepts into corresponding prompt tokens for accurate decomposition of complex visual concepts. To improve concept-token binding accuracy, we design a Diversify-and-Absorb Mechanism that uses an extra absorbent token to eliminate the impact of concept-irrelevant details when training with diversified prompts. To enhance the compatibility between image and video concepts, we present a Temporal Disentanglement Strategy that decouples the training process of video concepts into two stages with a dual-branch binder structure for temporal modeling. Evaluations demonstrate that our method achieves superior concept consistency, prompt fidelity, and motion quality over existing approaches, opening up new possibilities for visual creativity.",0,arxiv,AI,CC-BY/arXiv,Composing Concepts from Images and Videos via Concept-prompt Binding
"U-Net and other U-shaped architectures have achieved significant success in image deconvolution tasks. However, challenges have emerged, as these methods might generate unrealistic artifacts or hallucinations, which can interfere with analysis in safety-critical scenarios. This paper introduces a novel approach for quantifying and comprehending hallucination artifacts to ensure trustworthy computer vision models. Our method, termed the Conformal Hallucination Estimation Metric (CHEM), is applicable to any image reconstruction model, enabling efficient identification and quantification of hallucination artifacts. It offers two key advantages: it leverages wavelet and shearlet representations to efficiently extract hallucinations of image features and uses conformalized quantile regression to assess hallucination levels in a distribution-free manner. Furthermore, from an approximation theoretical perspective, we explore the reasons why U-shaped networks are prone to hallucinations. We test the proposed approach on the CANDELS astronomical image dataset with models such as U-Net, SwinUNet, and Learnlets, and provide new perspectives on hallucination from different aspects in deep learning-based image processing.",0,arxiv,AI,CC-BY/arXiv,CHEM: Estimating and Understanding Hallucinations in Deep Learning for Image Processing
"Few-shot learning (FSL) mitigates data scarcity in cardiac MRI segmentation but typically relies on semi-supervised techniques sensitive to domain shifts and validation bias, restricting zero-shot generalizability. We propose PathCo-LatticE, a fully supervised FSL framework that replaces unlabeled data with pathology-guided synthetic supervision. First, our Virtual Patient Engine models continuous latent disease trajectories from sparse clinical anchors, using generative modeling to synthesize physiologically plausible, fully labeled 3D cohorts. Second, Self-Reinforcing Interleaved Validation (SIV) provides a leakage-free protocol that evaluates models online with progressively challenging synthetic samples, eliminating the need for real validation data. Finally, a dynamic Lattice-of-Experts (LoE) organizes specialized networks within a pathology-aware topology and activates the most relevant experts per input, enabling robust zero-shot generalization to unseen data without target-domain fine-tuning. We evaluated PathCo-LatticE in a strict out-of-distribution (OOD) setting, deriving all anchors and severity statistics from a single-source domain (ACDC) and performing zero-shot testing on the multi-center, multi-vendor M&Ms dataset. PathCo-LatticE outperforms four state-of-the-art FSL methods by 4.2-11% Dice starting from only 7 labeled anchors, and approaches fully supervised performance (within 1% Dice) with only 19 labeled anchors. The method shows superior harmonization across four vendors and generalization to unseen pathologies. [Code will be made publicly available].",0,arxiv,AI,CC-BY/arXiv,PathCo-LatticE: Pathology-Constrained Lattice-Of Experts Framework for Fully-supervised Few-Shot Cardiac MRI Segmentation
"The recent convergence of pervasive computing and machine learning has given rise to numerous services, impacting almost all areas of economic and social activity. However, the use of AI techniques precludes certain standard software development practices, which emphasize rigorous testing to ensure the elimination of all bugs and adherence to well-defined specifications. ML models are trained on numerous high-dimensional examples rather than being manually coded. Consequently, the boundaries of their operating range are uncertain, and they cannot guarantee absolute error-free performance. In this paper, we propose to quantify uncertainty in ML-based systems. To achieve this, we propose to adapt and jointly utilize a set of selected techniques to evaluate the relevance of model predictions at runtime. We apply and evaluate these proposals in the highly heterogeneous and evolving domain of Human Activity Recognition (HAR). The results presented demonstrate the relevance of the approach, and we discuss in detail the assistance provided to domain experts.",0,arxiv,AI,CC-BY/arXiv,Quantifying Uncertainty in Machine Learning-Based Pervasive Systems: Application to Human Activity Recognition
"Transformers generate valid and diverse chemical structures, but little is known about the mechanisms that enable these models to capture the rules of molecular representation. We present a mechanistic analysis of autoregressive transformers trained on drug-like small molecules to reveal the computational structure underlying their capabilities across multiple levels of abstraction. We identify computational patterns consistent with low-level syntactic parsing and more abstract chemical validity constraints. Using sparse autoencoders (SAEs), we extract feature dictionaries associated with chemically relevant activation patterns. We validate our findings on downstream tasks and find that mechanistic insights can translate to predictive performance in various practical settings.",0,arxiv,AI,CC-BY/arXiv,"Circuits, Features, and Heuristics in Molecular Transformers"
"Multi-Agent Path Finding (MAPF) algorithms are increasingly deployed in industrial warehouses and automated manufacturing facilities, where robots must operate reliably under real-world physical constraints. However, existing MAPF evaluation frameworks typically rely on simplified robot models, leaving a substantial gap between algorithmic benchmarks and practical performance. Recent frameworks such as SMART, incorporate kinodynamic modeling and offer the MAPF community a platform for large-scale, realistic evaluation. Building on this capability, this work investigates how key planner design choices influence performance under realistic execution settings. We systematically study three fundamental factors: (1) the relationship between solution optimality and execution performance, (2) the sensitivity of system performance to inaccuracies in kinodynamic modeling, and (3) the interaction between model accuracy and plan optimality. Empirically, we examine these factors to understand how these design choices affect performance in realistic scenarios. We highlight open challenges and research directions to steer the community toward practical, real-world deployment.",0,arxiv,AI,CC-BY/arXiv,Analyzing Planner Design Trade-offs for MAPF under Realistic Simulation
"We present Ethics Readiness Levels (ERLs), a four-level, iterative method to track how ethical reflection is implemented in the design of AI systems. ERLs bridge high-level ethical principles and everyday engineering by turning ethical values into concrete prompts, checks, and controls within real use cases. The evaluation is conducted using a dynamic, tree-like questionnaire built from context-specific indicators, ensuring relevance to the technology and application domain. Beyond being a managerial tool, ERLs help facilitate a structured dialogue between ethics experts and technical teams, while our scoring system helps track progress over time. We demonstrate the methodology through two case studies: an AI facial sketch generator for law enforcement and a collaborative industrial robot. The ERL tool effectively catalyzes concrete design changes and promotes a shift from narrow technological solutionism to a more reflective, ethics-by-design mindset.",0,arxiv,AI,CC-BY/arXiv,Ethics Readiness of Artificial Intelligence: A Practical Evaluation Method
"Monte Carlo Tree Search is a cornerstone algorithm for online planning, and its root-parallel variant is widely used when wall clock time is limited but best performance is desired. In environments with continuous action spaces, how to best aggregate statistics from different threads is an important yet underexplored question. In this work, we introduce a method that uses Gaussian Process Regression to obtain value estimates for promising actions that were not trialed in the environment. We perform a systematic evaluation across 6 different domains, demonstrating that our approach outperforms existing aggregation strategies while requiring a modest increase in inference time.",0,arxiv,AI,CC-BY/arXiv,Gaussian Process Aggregation for Root-Parallel Monte Carlo Tree Search with Continuous Actions
"This work presents a conceptual study on the application of Multi-Agent Reinforcement Learning (MARL) for decentralized control of unmanned aerial vehicles to relay a critical data package to a known position. For this purpose, a family of deterministic games is introduced, designed for scaling studies for MARL. A robust baseline policy is proposed, which is based on restricting agent motion envelopes and applying Dijkstra's algorithm. Experimental results show that two off-the-shelf MARL algorithms perform competitively with the baseline for a small number of agents, but scalability issues arise as the number of agents increase.",0,arxiv,AI,CC-BY/arXiv,Dynamic one-time delivery of critical data by small and sparse UAV swarms: a model problem for MARL scaling studies
"In this article, we explore the use of various matrix norms for optimizing functions of weight matrices, a crucial problem in training large language models. Moving beyond the spectral norm underlying the Muon update, we leverage duals of the Ky Fan $k$-norms to introduce a family of Muon-like algorithms we name Fanions, which are closely related to Dion. By working with duals of convex combinations of the Ky Fan $k$-norms with either the Frobenius norm or the $l_\infty$ norm, we construct the families of F-Fanions and S-Fanions, respectively. Their most prominent members are F-Muon and S-Muon. We complement our theoretical analysis with an extensive empirical study of these algorithms across a wide range of tasks and settings, demonstrating that F-Muon and S-Muon consistently match Muon's performance, while outperforming vanilla Muon on a synthetic linear least squares problem.",0,arxiv,AI,CC-BY/arXiv,The Ky Fan Norms and Beyond: Dual Norms and Combinations for Matrix Optimization
"Equivariant neural networks encode symmetry as an inductive bias and have achieved strong empirical performance in wide domains. However, their expressive power remains not well understood. Focusing on 2-layer ReLU networks, this paper investigates the impact of equivariance constraints on the expressivity of equivariant and layer-wise equivariant networks. By examining the boundary hyperplanes and the channel vectors of ReLU networks, we construct an example showing that equivariance constraints could strictly limit expressive power. However, we demonstrate that this drawback can be compensated via enlarging the model size. Furthermore, we show that despite a larger model size, the resulting architecture could still correspond to a hypothesis space with lower complexity, implying superior generalizability for equivariant networks.",0,arxiv,AI,CC-BY/arXiv,Drawback of Enforcing Equivariance and its Compensation via the Lens of Expressive Power
"Hate speech spreads widely online, harming individuals and communities, making automatic detection essential for large-scale moderation, yet detecting it remains difficult. Part of the challenge lies in subjectivity: what one person flags as hate speech, another may see as benign. Traditional annotation agreement metrics, such as Cohen's $Îº$, oversimplify this disagreement, treating it as an error rather than meaningful diversity. Meanwhile, Large Language Models (LLMs) promise scalable annotation, but prior studies demonstrate that they cannot fully replace human judgement, especially in subjective tasks. In this work, we reexamine LLM reliability using a subjectivity-aware framework, cross-Rater Reliability (xRR), revealing that even under fairer lens, LLMs still diverge from humans. Yet this limitation opens an opportunity: we find that LLM-generated annotations can reliably reflect performance trends across classification models, correlating with human evaluations. We test this by examining whether LLM-generated annotations preserve the relative ordering of model performance derived from human evaluation (i.e. whether models ranked as more reliable by human annotators preserve the same order when evaluated with LLM-generated labels). Our results show that, although LLMs differ from humans at the instance level, they reproduce similar ranking and classification patterns, suggesting their potential as proxy evaluators. While not a substitute for human annotators, they might serve as a scalable proxy for evaluation in subjective NLP tasks.",0,arxiv,AI,CC-BY/arXiv,Can LLMs Evaluate What They Cannot Annotate? Revisiting LLM Reliability in Hate Speech Detection
"We present an end-to-end framework for planning supported by verifiers. An orchestrator receives a human specification written in natural language and converts it into a PDDL (Planning Domain Definition Language) model, where the domain and problem are iteratively refined by sub-modules (agents) to address common planning requirements, such as time constraints and optimality, as well as ambiguities and contradictions that may exist in the human specification. The validated domain and problem are then passed to an external planning engine to generate a plan. The orchestrator and agents are powered by Large Language Models (LLMs) and require no human intervention at any stage of the process. Finally, a module translates the final plan back into natural language to improve human readability while maintaining the correctness of each step. We demonstrate the flexibility and effectiveness of our framework across various domains and tasks, including the Google NaturalPlan benchmark and PlanBench, as well as planning problems like Blocksworld and the Tower of Hanoi (where LLMs are known to struggle even with small instances). Our framework can be integrated with any PDDL planning engine and validator (such as Fast Downward, LPG, POPF, VAL, and uVAL, which we have tested) and represents a significant step toward end-to-end planning aided by LLMs.",0,arxiv,AI,CC-BY/arXiv,An End-to-end Planning Framework with Agentic LLMs and PDDL
"Chain-of-thought (CoT) reasoning has been highly successful in solving complex tasks in natural language processing, and recent multimodal large language models (MLLMs) have extended this paradigm to video reasoning. However, these models typically build on lengthy reasoning chains and large numbers of input visual tokens. Motivated by empirical observations from our benchmark study, we hypothesize that concise reasoning combined with a reduced set of visual tokens can be sufficient for effective video reasoning. To evaluate this hypothesis, we design and validate an efficient post-training and inference framework that enhances a video MLLM's reasoning capability. Our framework enables models to operate on compressed visual tokens and generate brief reasoning traces prior to answering. The resulting models achieve substantially improved inference efficiency, deliver competitive performance across diverse benchmarks, and avoid reliance on manual CoT annotations or supervised fine-tuning. Collectively, our results suggest that long, human-like CoT reasoning may not be necessary for general video reasoning, and that concise reasoning can be both effective and efficient. Our code will be released at https://github.com/LaVi-Lab/Rethink_CoT_Video.",0,arxiv,AI,CC-BY/arXiv,Rethinking Chain-of-Thought Reasoning for Videos
"People living with Motor Neuron Disease (plwMND) frequently encounter speech and motor impairments that necessitate a reliance on augmentative and alternative communication (AAC) systems. This paper tackles the main challenge that traditional symbol-based AAC systems offer a limited vocabulary, while text entry solutions tend to exhibit low communication rates. To help plwMND articulate their needs about the system efficiently and effectively, we iteratively design and develop a novel multimodal text generation system called ImageTalk through a tailored proxy-user-based and an end-user-based design phase. The system demonstrates pronounced keystroke savings of 95.6%, coupled with consistent performance and high user satisfaction. We distill three design guidelines for AI-assisted text generation systems design and outline four user requirement levels tailored for AAC purposes, guiding future research in this field.",0,arxiv,AI,CC-BY/arXiv,ImageTalk: Designing a Multimodal AAC Text Generation System Driven by Image Recognition and Natural Language Generation
"Polysomnography (PSG), the gold standard test for sleep analysis, generates vast amounts of multimodal clinical data, presenting an opportunity to leverage self-supervised representation learning (SSRL) for pre-training foundation models to enhance sleep analysis. However, progress in sleep foundation models is hindered by two key limitations: (1) the lack of a shared dataset and benchmark with diverse tasks for training and evaluation, and (2) the absence of a systematic evaluation of SSRL approaches across sleep-related tasks. To address these gaps, we introduce Stanford Sleep Bench, a large-scale PSG dataset comprising 17,467 recordings totaling over 163,000 hours from a major sleep clinic, including 13 clinical disease prediction tasks alongside canonical sleep-related tasks such as sleep staging, apnea diagnosis, and age estimation. We systematically evaluate SSRL pre-training methods on Stanford Sleep Bench, assessing downstream performance across four tasks: sleep staging, apnea diagnosis, age estimation, and disease and mortality prediction. Our results show that multiple pretraining methods achieve comparable performance for sleep staging, apnea diagnosis, and age estimation. However, for mortality and disease prediction, contrastive learning significantly outperforms other approaches while also converging faster during pretraining. To facilitate reproducibility and advance sleep research, we will release Stanford Sleep Bench along with pretrained model weights, training pipelines, and evaluation code.",0,arxiv,AI,CC-BY/arXiv,Stanford Sleep Bench: Evaluating Polysomnography Pre-training Methods for Sleep Foundation Models
"Quantum circuit design is a key bottleneck for practical quantum machine learning on complex, real-world data. We present an automated framework that discovers and refines variational quantum circuits (VQCs) using graph-based Bayesian optimization with a graph neural network (GNN) surrogate. Circuits are represented as graphs and mutated and selected via an expected improvement acquisition function informed by surrogate uncertainty with Monte Carlo dropout. Candidate circuits are evaluated with a hybrid quantum-classical variational classifier on the next generation firewall telemetry and network internet of things (NF-ToN-IoT-V2) cybersecurity dataset, after feature selection and scaling for quantum embedding. We benchmark our pipeline against an MLP-based surrogate, random search, and greedy GNN selection. The GNN-guided optimizer consistently finds circuits with lower complexity and competitive or superior classification accuracy compared to all baselines. Robustness is assessed via a noise study across standard quantum noise channels, including amplitude damping, phase damping, thermal relaxation, depolarizing, and readout bit flip noise. The implementation is fully reproducible, with time benchmarking and export of best found circuits, providing a scalable and interpretable route to automated quantum circuit discovery.",0,arxiv,AI,CC-BY/arXiv,Graph-Based Bayesian Optimization for Quantum Circuit Architecture Search with Uncertainty Calibrated Surrogates
"Convolutional Neural Networks (CNNs) for computer vision sometimes struggle with understanding images in a global context, as they mainly focus on local patterns. On the other hand, Vision Transformers (ViTs), inspired by models originally created for language processing, use self-attention mechanisms, which allow them to understand relationships across the entire image. In this paper, we compare different types of ViTs (pure, hierarchical, and hybrid) against traditional CNN models across various tasks, including object recognition, detection, and medical image classification. We conduct thorough tests on standard datasets like ImageNet for image classification and COCO for object detection. Additionally, we apply these models to medical imaging using the ChestX-ray14 dataset. We find that hybrid and hierarchical transformers, especially Swin and CvT, offer a strong balance between accuracy and computational resources. Furthermore, by experimenting with data augmentation techniques on medical images, we discover significant performance improvements, particularly with the Swin Transformer model. Overall, our results indicate that Vision Transformers are competitive and, in many cases, outperform traditional CNNs, especially in scenarios requiring the understanding of global visual contexts like medical imaging.",0,arxiv,AI,CC-BY/arXiv,Hands-on Evaluation of Visual Transformers for Object Recognition and Detection
"We present Auto-BenchmarkCard, a workflow for generating validated descriptions of AI benchmarks. Benchmark documentation is often incomplete or inconsistent, making it difficult to interpret and compare benchmarks across tasks or domains. Auto-BenchmarkCard addresses this gap by combining multi-agent data extraction from heterogeneous sources (e.g., Hugging Face, Unitxt, academic papers) with LLM-driven synthesis. A validation phase evaluates factual accuracy through atomic entailment scoring using the FactReasoner tool. This workflow has the potential to promote transparency, comparability, and reusability in AI benchmark reporting, enabling researchers and practitioners to better navigate and evaluate benchmark choices.",0,arxiv,AI,CC-BY/arXiv,Auto-BenchmarkCard: Automated Synthesis of Benchmark Documentation
"Turbulent flows posses broadband, power-law spectra in which multiscale interactions couple high-wavenumber fluctuations to large-scale dynamics. Although diffusion-based generative models offer a principled probabilistic forecasting framework, we show that standard DDPMs induce a fundamental \emph{spectral collapse}: a Fourier-space analysis of the forward SDE reveals a closed-form, mode-wise signal-to-noise ratio (SNR) that decays monotonically in wavenumber, $|k|$ for spectra $S(k)\!\propto\!|k|^{-Î»}$, rendering high-wavenumber modes indistinguishable from noise and producing an intrinsic spectral bias. We reinterpret the noise schedule as a spectral regularizer and introduce power-law schedules $Î²(Ï„)\!\propto\!Ï„^Î³$ that preserve fine-scale structure deeper into diffusion time, along with \emph{Lazy Diffusion}, a one-step distillation method that leverages the learned score geometry to bypass long reverse-time trajectories and prevent high-$k$ degradation. Applied to high-Reynolds-number 2D Kolmogorov turbulence and $1/12^\circ$ Gulf of Mexico ocean reanalysis, these methods resolve spectral collapse, stabilize long-horizon autoregression, and restore physically realistic inertial-range scaling. Together, they show that naÃ¯ve Gaussian scheduling is structurally incompatible with power-law physics and that physics-aware diffusion processes can yield accurate, efficient, and fully probabilistic surrogates for multiscale dynamical systems.",0,arxiv,AI,CC-BY/arXiv,Lazy Diffusion: Mitigating spectral collapse in generative diffusion-based stable autoregressive emulation of turbulent flows
"This paper examines how international AI governance frameworks address gender issues and gender-based harms. The analysis covers binding regulations, such as the EU AI Act; soft law instruments, like the UNESCO Recommendations on AI Ethics; and global initiatives, such as the Global Partnership on AI (GPAI). These instruments reveal emerging trends, including the integration of gender concerns into broader human rights frameworks, a shift toward explicit gender-related provisions, and a growing emphasis on inclusivity and diversity. Yet, some critical gaps persist, including inconsistent treatment of gender across governance documents, limited engagement with intersectionality, and a lack of robust enforcement mechanisms. However, this paper argues that effective AI governance must be intersectional, enforceable, and inclusive. This is key to moving beyond tokenism toward meaningful equity and preventing reinforcement of existing inequalities. The study contributes to ethical AI debates by highlighting the importance of gender-sensitive governance in building a just technological future.",0,arxiv,AI,CC-BY/arXiv,The Gender Code: Gendering the Global Governance of Artificial Intelligence
"Drug discovery is a time-consuming and expensive process, with traditional high-throughput and docking-based virtual screening hampered by low success rates and limited scalability. Recent advances in generative modelling, including autoregressive, diffusion, and flow-based approaches, have enabled de novo ligand design beyond the limits of enumerative screening. Yet these models often suffer from inadequate generalization, limited interpretability, and an overemphasis on binding affinity at the expense of key pharmacological properties, thereby restricting their translational utility. Here we present Trio, a molecular generation framework integrating fragment-based molecular language modeling, reinforcement learning, and Monte Carlo tree search, for effective and interpretable closed-loop targeted molecular design. Through the three key components, Trio enables context-aware fragment assembly, enforces physicochemical and synthetic feasibility, and guides a balanced search between the exploration of novel chemotypes and the exploitation of promising intermediates within protein binding pockets. Experimental results show that Trio reliably achieves chemically valid and pharmacologically enhanced ligands, outperforming state-of-the-art approaches with improved binding affinity (+7.85%), drug-likeness (+11.10%) and synthetic accessibility (+12.05%), while expanding molecular diversity more than fourfold.",0,arxiv,AI,CC-BY/arXiv,"Toward Closed-loop Molecular Discovery via Language Model, Property Alignment and Strategic Search"
"The proliferation of hate speech on Chinese social media poses urgent societal risks, yet traditional systems struggle to decode context-dependent rhetorical strategies and evolving slang. To bridge this gap, we propose a novel three-stage LLM-based framework: Prompt Engineering, Supervised Fine-tuning, and LLM Merging. First, context-aware prompts are designed to guide LLMs in extracting implicit hate patterns. Next, task-specific features are integrated during supervised fine-tuning to enhance domain adaptation. Finally, merging fine-tuned LLMs improves robustness against out-of-distribution cases. Evaluations on the STATE-ToxiCN benchmark validate the framework's effectiveness, demonstrating superior performance over baseline methods in detecting fine-grained hate speech.",0,arxiv,AI,CC-BY/arXiv,System Report for CCL25-Eval Task 10: Prompt-Driven Large Language Model Merge for Fine-Grained Chinese Hate Speech Detection
"Context. LLM-based autonomous agents in software engineering rely on large, proprietary models, limiting local deployment. This has spurred interest in Small Language Models (SLMs), but their practical effectiveness and efficiency within complex agentic frameworks for automated issue resolution remain poorly understood.   Goal. We investigate the performance, energy efficiency, and resource consumption of four leading agentic issue resolution frameworks when deliberately constrained to using SLMs. We aim to assess the viability of these systems for this task in resource-limited settings and characterize the resulting trade-offs.   Method. We conduct a controlled evaluation of four leading agentic frameworks (SWE-Agent, OpenHands, Mini SWE Agent, AutoCodeRover) using two SLMs (Gemma-3 4B, Qwen-3 1.7B) on the SWE-bench Verified Mini benchmark. On fixed hardware, we measure energy, duration, token usage, and memory over 150 runs per configuration.   Results. We find that framework architecture is the primary driver of energy consumption. The most energy-intensive framework, AutoCodeRover (Gemma), consumed 9.4x more energy on average than the least energy-intensive, OpenHands (Gemma). However, this energy is largely wasted. Task resolution rates were near-zero, demonstrating that current frameworks, when paired with SLMs, consume significant energy on unproductive reasoning loops. The SLM's limited reasoning was the bottleneck for success, but the framework's design was the bottleneck for efficiency.   Conclusions. Current agentic frameworks, designed for powerful LLMs, fail to operate efficiently with SLMs. We find that framework architecture is the primary driver of energy consumption, but this energy is largely wasted due to the SLMs' limited reasoning. Viable low-energy solutions require shifting from passive orchestration to architectures that actively manage SLM weaknesses.",0,arxiv,AI,CC-BY/arXiv,SWEnergy: An Empirical Study on Energy Efficiency in Agentic Issue Resolution Frameworks with SLMs
"Neural decoding, a critical component of Brain-Computer Interface (BCI), has recently attracted increasing research interest. Previous research has focused on leveraging signal processing and deep learning methods to enhance neural decoding performance. However, the in-depth exploration of model architectures remains underexplored, despite its proven effectiveness in other tasks such as energy forecasting and image classification. In this study, we propose NeuroSketch, an effective framework for neural decoding via systematic architecture optimization. Starting with the basic architecture study, we find that CNN-2D outperforms other architectures in neural decoding tasks and explore its effectiveness from temporal and spatial perspectives. Building on this, we optimize the architecture from macro- to micro-level, achieving improvements in performance at each step. The exploration process and model validations take over 5,000 experiments spanning three distinct modalities (visual, auditory, and speech), three types of brain signals (EEG, SEEG, and ECoG), and eight diverse decoding tasks. Experimental results indicate that NeuroSketch achieves state-of-the-art (SOTA) performance across all evaluated datasets, positioning it as a powerful tool for neural decoding. Our code and scripts are available at https://github.com/Galaxy-Dawn/NeuroSketch.",0,arxiv,AI,CC-BY/arXiv,NeuroSketch: An Effective Framework for Neural Decoding via Systematic Architectural Optimization
"Unequal representation of demographic groups in training data poses challenges to model generalisation across populations. Standard practice assumes that balancing subgroup representation optimises performance. However, recent empirical results contradict this assumption: in some cases, imbalanced data distributions actually improve subgroup performance, while in others, subgroup performance remains unaffected by the absence of an entire subgroup during training. We conduct a systematic study of subgroup allocation across four vision and language models, varying training data composition to characterise the sensitivity of subgroup performance to data balance. We propose the latent separation hypothesis, which states that a partially fine-tuned model's dependence on subgroup representation is determined by the degree of separation between subgroups in the latent space of the pre-trained model. We formalise this hypothesis, provide theoretical analysis, and validate it empirically. Finally, we present a practical application to foundation model fine-tuning, demonstrating that quantitative analysis of latent subgroup separation can inform data collection and balancing decisions.",0,arxiv,AI,CC-BY/arXiv,Representation Invariance and Allocation: When Subgroup Balance Matters
"Retrieval-Augmented Generation (RAG) integrates non-parametric knowledge into Large Language Models (LLMs), typically from unstructured texts and structured graphs. While recent progress has advanced text-based RAG to multi-turn reasoning through Reinforcement Learning (RL), extending these advances to hybrid retrieval introduces additional challenges. Existing graph-based or hybrid systems typically depend on fixed or handcrafted retrieval pipelines, lacking the ability to integrate supplementary evidence as reasoning unfolds. Besides, while graph evidence provides relational structures crucial for multi-hop reasoning, it is substantially more expensive to retrieve. To address these limitations, we introduce \model{}, an RL-based framework that enables LLMs to perform multi-turn and adaptive graph-text hybrid RAG. \model{} jointly optimizes the entire generation process via RL, allowing the model to learn when to reason, what to retrieve from either texts or graphs, and when to produce final answers, all within a unified generation policy. To guide this learning process, we design a two-stage training framework that accounts for both task outcome and retrieval efficiency, enabling the model to exploit hybrid evidence while avoiding unnecessary retrieval overhead. Experimental results across five question answering benchmarks demonstrate that \model{} significantly outperforms existing RAG baselines, highlighting the benefits of end-to-end RL in supporting adaptive and efficient retrieval for complex reasoning.",0,arxiv,AI,CC-BY/arXiv,RouteRAG: Efficient Retrieval-Augmented Generation from Text and Graph via Reinforcement Learning
"Zero-Touch Networks (ZTNs) represent a transformative paradigm toward fully automated and intelligent network management, providing the scalability and adaptability required for the complexity of sixth-generation (6G) networks. However, the distributed architecture, high openness, and deep heterogeneity of 6G networks expand the attack surface and pose unprecedented security challenges. To address this, security automation aims to enable intelligent security management across dynamic and complex environments, serving as a key capability for securing 6G ZTNs. Despite its promise, implementing security automation in 6G ZTNs presents two primary challenges: 1) automating the lifecycle from security strategy generation to validation and update under real-world, parallel, and adversarial conditions, and 2) adapting security strategies to evolving threats and dynamic environments. This motivates us to propose SecLoop and SA-GRPO. SecLoop constitutes the first fully automated framework that integrates large language models (LLMs) across the entire lifecycle of security strategy generation, orchestration, response, and feedback, enabling intelligent and adaptive defenses in dynamic network environments, thus tackling the first challenge. Furthermore, we propose SA-GRPO, a novel security-aware group relative policy optimization algorithm that iteratively refines security strategies by contrasting group feedback collected from parallel SecLoop executions, thereby addressing the second challenge. Extensive real-world experiments on five benchmarks, including 11 MITRE ATT&CK processes and over 20 types of attacks, demonstrate the superiority of the proposed SecLoop and SA-GRPO. We will release our platform to the community, facilitating the advancement of security automation towards next generation communications.",0,arxiv,AI,CC-BY/arXiv,Advancing LLM-Based Security Automation with Customized Group Relative Policy Optimization for Zero-Touch Networks
"Recent advances in diffusion-based generative models have achieved remarkable visual fidelity, yet a detailed understanding of how specific perceptual attributes - such as color and shape - are internally represented remains limited. This work explores how color is encoded in a generative model through a systematic analysis of the latent representations in Stable Diffusion. Through controlled synthetic datasets, principal component analysis (PCA) and similarity metrics, we reveal that color information is encoded along circular, opponent axes predominantly captured in latent channels c_3 and c_4, whereas intensity and shape are primarily represented in channels c_1 and c_2. Our findings indicate that the latent space of Stable Diffusion exhibits an interpretable structure aligned with a efficient coding representation. These insights provide a foundation for future work in model understanding, editing applications, and the design of more disentangled generative frameworks.",0,arxiv,AI,CC-BY/arXiv,Color encoding in Latent Space of Stable Diffusion Models
"Cloud cover in multispectral imagery (MSI) significantly hinders early-season crop mapping by corrupting spectral information. Existing Vision Transformer(ViT)-based time-series reconstruction methods, like SMTS-ViT, often employ coarse temporal embeddings that aggregate entire sequences, causing substantial information loss and reducing reconstruction accuracy. To address these limitations, a Video Vision Transformer (ViViT)-based framework with temporal-spatial fusion embedding for MSI reconstruction in cloud-covered regions is proposed in this study. Non-overlapping tubelets are extracted via 3D convolution with constrained temporal span $(t=2)$, ensuring local temporal coherence while reducing cross-day information degradation. Both MSI-only and SAR-MSI fusion scenarios are considered during the experiments. Comprehensive experiments on 2020 Traill County data demonstrate notable performance improvements: MTS-ViViT achieves a 2.23\% reduction in MSE compared to the MTS-ViT baseline, while SMTS-ViViT achieves a 10.33\% improvement with SAR integration over the SMTS-ViT baseline. The proposed framework effectively enhances spectral reconstruction quality for robust agricultural monitoring.",0,arxiv,AI,CC-BY/arXiv,Temporal-Spatial Tubelet Embedding for Cloud-Robust MSI Reconstruction using MSI-SAR Fusion: A Multi-Head Self-Attention Video Vision Transformer Approach
"The adoption of AI-powered computer vision in industry is often constrained by the need to balance operational utility with worker privacy. Building on our previously proposed privacy-preserving framework, this paper presents its first comprehensive validation on real-world data collected directly by industrial partners in active production environments. We evaluate the framework across three representative use cases: woodworking production monitoring, human-aware AGV navigation, and multi-camera ergonomic risk assessment. The approach employs learned visual transformations that obscure sensitive or task-irrelevant information while retaining features essential for task performance. Through both quantitative evaluation of the privacy-utility trade-off and qualitative feedback from industrial partners, we assess the framework's effectiveness, deployment feasibility, and trust implications. Results demonstrate that task-specific obfuscation enables effective monitoring with reduced privacy risks, establishing the framework's readiness for real-world adoption and providing cross-domain recommendations for responsible, human-centric AI deployment in industry.",0,arxiv,AI,CC-BY/arXiv,Privacy-Preserving Computer Vision for Industry: Three Case Studies in Human-Centric Manufacturing
"Infertility is a major global health issue, and while in-vitro fertilization has improved treatment outcomes, embryo selection remains a critical bottleneck. Time-lapse imaging enables continuous, non-invasive monitoring of embryo development, yet most automated assessment methods rely solely on conventional morphokinetic features and overlook emerging biomarkers. Cytoplasmic Strings, thin filamentous structures connecting the inner cell mass and trophectoderm in expanded blastocysts, have been associated with faster blastocyst formation, higher blastocyst grades, and improved viability. However, CS assessment currently depends on manual visual inspection, which is labor-intensive, subjective, and severely affected by detection and subtle visual appearance. In this work, we present, to the best of our knowledge, the first computational framework for CS analysis in human IVF embryos. We first design a human-in-the-loop annotation pipeline to curate a biologically validated CS dataset from TLI videos, comprising 13,568 frames with highly sparse CS-positive instances. Building on this dataset, we propose a two-stage deep learning framework that (i) classifies CS presence at the frame level and (ii) localizes CS regions in positive cases. To address severe imbalance and feature uncertainty, we introduce the Novel Uncertainty-aware Contractive Embedding (NUCE) loss, which couples confidence-aware reweighting with an embedding contraction term to form compact, well-separated class clusters. NUCE consistently improves F1-score across five transformer backbones, while RF-DETR-based localization achieves state-of-the-art (SOTA) detection performance for thin, low-contrast CS structures. The source code will be made publicly available at: https://github.com/HamadYA/CS_Detection.",0,arxiv,AI,CC-BY/arXiv,Cytoplasmic Strings Analysis in Human Embryo Time-Lapse Videos using Deep Learning Framework
"This chapter argues that the reliability of agentic and generative AI is chiefly an architectural property. We define agentic systems as goal-directed, tool-using decision makers operating in closed loops, and show how reliability emerges from principled componentisation (goal manager, planner, tool-router, executor, memory, verifiers, safety monitor, telemetry), disciplined interfaces (schema-constrained, validated, least-privilege tool calls), and explicit control and assurance loops. Building on classical foundations, we propose a practical taxonomy-tool-using agents, memory-augmented agents, planning and self-improvement agents, multi-agent systems, and embodied or web agents - and analyse how each pattern reshapes the reliability envelope and failure modes. We distil design guidance on typed schemas, idempotency, permissioning, transactional semantics, memory provenance and hygiene, runtime governance (budgets, termination conditions), and simulate-before-actuate safeguards.",0,arxiv,AI,CC-BY/arXiv,Architectures for Building Agentic AI
"We investigate how large language models can be used as research tools in scientific computing while preserving mathematical rigor. We propose a human-in-the-loop workflow for interactive theorem proving and discovery with LLMs. Human experts retain control over problem formulation and admissible assumptions, while the model searches for proofs or contradictions, proposes candidate properties and theorems, and helps construct structures and parameters that satisfy explicit constraints, supported by numerical experiments and simple verification checks. Experts treat these outputs as raw material, further refine them, and organize the results into precise statements and rigorous proofs. We instantiate this workflow in a case study on the connection between manifold optimization and Grover's quantum search algorithm, where the pipeline helps identify invariant subspaces, explore Grover-compatible retractions, and obtain convergence guarantees for the retraction-based gradient method. The framework provides a practical template for integrating large language models into frontier mathematical research, enabling faster exploration of proof space and algorithm design while maintaining transparent reasoning responsibilities. Although illustrated on manifold optimization problems in quantum computing, the principles extend to other core areas of scientific computing.",0,arxiv,AI,CC-BY/arXiv,Advancing Research via Human-AI Interactive Theorem Proving
"Class-incremental learning requires a learning system to continually learn knowledge of new classes and meanwhile try to preserve previously learned knowledge of old classes. As current state-of-the-art methods based on Vision-Language Models (VLMs) still suffer from the issue of differentiating classes across learning tasks. Here a novel VLM-based continual learning framework for image classification is proposed. In this framework, task-specific adapters are added to the pre-trained and frozen image encoder to learn new knowledge, and a novel cross-task representation calibration strategy based on a mixture of light-weight projectors is used to help better separate all learned classes in a unified feature space, alleviating class confusion across tasks. In addition, a novel inference strategy guided by prediction uncertainty is developed to more accurately select the most appropriate image feature for class prediction. Extensive experiments on multiple datasets under various settings demonstrate the superior performance of our method compared to existing ones.",0,arxiv,AI,CC-BY/arXiv,Representation Calibration and Uncertainty Guidance for Class-Incremental Learning based on Vision Language Model
"Official court press releases from Germany's highest courts present and explain judicial rulings to the public, as well as to expert audiences. Prior NLP efforts emphasize technical headnotes, ignoring citizen-oriented communication needs. We introduce CourtPressGER, a 6.4k dataset of triples: rulings, human-drafted press releases, and synthetic prompts for LLMs to generate comparable releases. This benchmark trains and evaluates LLMs in generating accurate, readable summaries from long judicial texts. We benchmark small and large LLMs using reference-based metrics, factual-consistency checks, LLM-as-judge, and expert ranking. Large LLMs produce high-quality drafts with minimal hierarchical performance loss; smaller models require hierarchical setups for long judgments. Initial benchmarks show varying model performance, with human-drafted releases ranking highest.",0,arxiv,AI,CC-BY/arXiv,CourtPressGER: A German Court Decision to Press Release Summarization Dataset
"Serving large language models (LLMs) on accelerators with poor random-access bandwidth (e.g., LPDDR5-based) is limited by current memory managers. Static pre-allocation wastes memory, while fine-grained paging (e.g., PagedAttention) is ill-suited due to high random-access costs. Existing HBM-centric solutions do not exploit the characteristics of random-access-constrained memory (RACM) accelerators like Cambricon MLU370. We present ODMA, an on-demand memory allocation framework for RACM. ODMA addresses distribution drift and heavy-tailed requests by coupling a lightweight length predictor with dynamic bucket partitioning and a large-bucket safeguard. Boundaries are periodically updated from live traces to maximize utilization. On Alpaca and Google-NQ, ODMA improves prediction accuracy of prior work significantly (e.g., from 82.68% to 93.36%). Serving DeepSeek-R1-Distill-Qwen-7B on Cambricon MLU370-X4, ODMA raises memory utilization from 55.05% to 72.45% and improves RPS and TPS by 29% and 27% over static baselines. This demonstrates that hardware-aware allocation unlocks efficient LLM serving on RACM platforms.",0,arxiv,AI,CC-BY/arXiv,ODMA: On-Demand Memory Allocation Framework for LLM Serving on LPDDR-Class Accelerators
"Robots that learn manipulation skills from everyday human videos could acquire broad capabilities without tedious robot data collection. We propose a video-to-video translation framework that converts ordinary human-object interaction videos into motion-consistent robot manipulation videos with realistic, physically grounded interactions. Our approach does not require any paired human-robot videos for training only a set of unpaired robot videos, making the system easy to scale. We introduce a transferable representation that bridges the embodiment gap: by inpainting the robot arm in training videos to obtain a clean background and overlaying a simple visual cue (a marker and arrow indicating the gripper's position and orientation), we can condition a generative model to insert the robot arm back into the scene. At test time, we apply the same process to human videos (inpainting the person and overlaying human pose cues) and generate high-quality robot videos that mimic the human's actions. We fine-tune a SOTA video diffusion model (Wan 2.2) in an in-context learning manner to ensure temporal coherence and leveraging of its rich prior knowledge. Empirical results demonstrate that our approach achieves significantly more realistic and grounded robot motions compared to baselines, pointing to a promising direction for scaling up robot learning from unlabeled human videos. Project page: https://showlab.github.io/H2R-Grounder/",0,arxiv,AI,CC-BY/arXiv,H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos
"Traffic prediction remains a key challenge in spatio-temporal data mining, despite progress in deep learning. Accurate forecasting is hindered by the complex influence of external factors such as traffic accidents and regulations, often overlooked by existing models due to limited data integration. To address these limitations, we present two enriched traffic datasets from Tokyo and California, incorporating traffic accident and regulation data. Leveraging these datasets, we propose ConFormer (Conditional Transformer), a novel framework that integrates graph propagation with guided normalization layer. This design dynamically adjusts spatial and temporal node relationships based on historical patterns, enhancing predictive accuracy. Our model surpasses the state-of-the-art STAEFormer in both predictive performance and efficiency, achieving lower computational costs and reduced parameter demands. Extensive evaluations demonstrate that ConFormer consistently outperforms mainstream spatio-temporal baselines across multiple metrics, underscoring its potential to advance traffic prediction research.",0,arxiv,AI,CC-BY/arXiv,Towards Resilient Transportation: A Conditional Transformer for Accident-Informed Traffic Forecasting
"Building AI systems for GUI automation task has attracted remarkable research efforts, where MLLMs are leveraged for processing user requirements and give operations. However, GUI automation includes a wide range of tasks, from document processing to online shopping, from CAD to video editing. Diversity between particular tasks requires MLLMs for GUI automation to have heterogeneous capabilities and master multidimensional expertise, raising problems on constructing such a model. To address such challenge, we propose GAIR: GUI Automation via Information-Joint Reasoning and Group Reflection, a novel MLLM-based GUI automation agent framework designed for integrating knowledge and combining capabilities from heterogeneous models to build GUI automation agent systems with higher performance. Since different GUI-specific MLLMs are trained on different dataset and thus have different strengths, GAIR introduced a general-purpose MLLM for jointly processing the information from multiple GUI-specific models, further enhancing performance of the agent framework. The general-purpose MLLM also serves as decision maker, trying to execute a reasonable operation based on previously gathered information. When the general-purpose model thinks that there isn't sufficient information for a reasonable decision, GAIR would transit into group reflection status, where the general-purpose model would provide GUI-specific models with different instructions and hints based on their strengths and weaknesses, driving them to gather information with more significance and accuracy that can support deeper reasoning and decision. We evaluated the effectiveness and reliability of GAIR through extensive experiments on GUI benchmarks.",0,arxiv,AI,CC-BY/arXiv,GAIR: GUI Automation via Information-Joint Reasoning and Group Reflection
"AI tasks differ in complexity and are best addressed with different computation strategies (e.g., combinations of models and decoding methods). Hence, an effective routing system that maps tasks to the appropriate strategies is crucial. Most prior methods build the routing framework by training a single model across all strategies, which demands full retraining whenever new strategies appear and leads to high overhead. Attempts at such continual routing, however, often face difficulties with generalization. Prior models also typically use a single input representation, limiting their ability to capture the full complexity of the routing problem and leading to sub-optimal routing decisions. To address these gaps, we propose CONCUR, a continual routing framework that supports both constrained and unconstrained routing (i.e., routing with or without a budget). Our modular design trains a separate predictor model for each strategy, enabling seamless incorporation of new strategies with low additional training cost. Our predictors also leverage multiple representations of both tasks and computation strategies to better capture overall problem complexity. Experiments on both in-distribution and out-of-distribution, knowledge- and reasoning-intensive tasks show that our method outperforms the best single strategy and strong existing routing techniques with higher end-to-end accuracy and lower inference cost in both continual and non-continual settings, while also reducing training cost in the continual setting.",0,arxiv,AI,CC-BY/arXiv,CONCUR: A Framework for Continual Constrained and Unconstrained Routing
"The rapid growth of Ethereum has made it more important to quickly and accurately detect smart contract vulnerabilities. While machine-learning-based methods have shown some promise, many still rely on rule-based preprocessing designed by domain experts. Rule-based preprocessing methods often discard crucial context from the source code, potentially causing certain vulnerabilities to be overlooked and limiting adaptability to newly emerging threats. We introduce BugSweeper, an end-to-end deep learning framework that detects vulnerabilities directly from the source code without manual engineering. BugSweeper represents each Solidity function as a Function-Level Abstract Syntax Graph (FLAG), a novel graph that combines its Abstract Syntax Tree (AST) with enriched control-flow and data-flow semantics. Then, our two-stage Graph Neural Network (GNN) analyzes these graphs. The first-stage GNN filters noise from the syntax graphs, while the second-stage GNN conducts high-level reasoning to detect diverse vulnerabilities. Extensive experiments on real-world contracts show that BugSweeper significantly outperforms all state-of-the-art detection methods. By removing the need for handcrafted rules, our approach offers a robust, automated, and scalable solution for securing smart contracts without any dependence on security experts.",0,arxiv,AI,CC-BY/arXiv,BugSweeper: Function-Level Detection of Smart Contract Vulnerabilities Using Graph Neural Networks
"Neural Radiance Fields (NeRF) have achieved remarkable results in novel view synthesis, typically using sRGB images for supervision. However, little attention has been paid to the color space in which the network is learning the radiance field representation. Inspired by the BiIlluminant Dichromatic Reflection (BIDR) model, which suggests that a logarithmic transformation simplifies the separation of illumination and reflectance, we hypothesize that log RGB space enables NeRF to learn a more compact and effective representation of scene appearance. To test this, we captured approximately 30 videos using a GoPro camera, ensuring linear data recovery through inverse encoding. We trained NeRF models under various color space interpretations linear, sRGB, GPLog, and log RGB by converting each network output to a common color space before rendering and loss computation, enforcing representation learning in different color spaces. Quantitative and qualitative evaluations demonstrate that using a log RGB color space consistently improves rendering quality, exhibits greater robustness across scenes, and performs particularly well in low light conditions while using the same bit-depth input images. Further analysis across different network sizes and NeRF variants confirms the generalization and stability of the log space advantage.",0,arxiv,AI,CC-BY/arXiv,Log NeRF: Comparing Spaces for Learning Radiance Fields
"Graph Neural Networks (GNNs) have emerged as a promising approach for ``learning to branch'' in Mixed-Integer Linear Programming (MILP). While standard Message-Passing GNNs (MPNNs) are efficient, they theoretically lack the expressive power to fully represent MILP structures. Conversely, higher-order GNNs (like 2-FGNNs) are expressive but computationally prohibitive. In this work, we investigate Subgraph GNNs as a theoretical middle ground. Crucially, while previous work [Chen et al., 2025] demonstrated that GNNs with 3-WL expressive power can approximate Strong Branching, we prove a sharper result: node-anchored Subgraph GNNs whose expressive power is strictly lower than 3-WL [Zhang et al., 2023] are sufficient to approximate Strong Branching scores. However, our extensive empirical evaluation on four benchmark datasets reveals a stark contrast between theory and practice. While node-anchored Subgraph GNNs theoretically offer superior branching decisions, their $O(n)$ complexity overhead results in significant memory bottlenecks and slower solving times than MPNNs and heuristics. Our results indicate that for MILP branching, the computational cost of expressive GNNs currently outweighs their gains in decision quality, suggesting that future research must focus on efficiency-preserving expressivity.",0,arxiv,AI,CC-BY/arXiv,Branching Strategies Based on Subgraph GNNs: A Study on Theoretical Promise versus Practical Reality
"Understanding how humans and AI systems interpret ambiguous visual stimuli offers critical insight into the nature of perception, reasoning, and decision-making. This paper examines image labeling performance across human participants and deep neural networks, focusing on low-resolution, perceptually degraded stimuli. Drawing from computational cognitive science, cognitive architectures, and connectionist-symbolic hybrid models, we contrast human strategies such as analogical reasoning, shape-based recognition, and confidence modulation with AI's feature-based processing. Grounded in Marr's tri-level hypothesis, Simon's bounded rationality, and Thagard's frameworks of representation and emotion, we analyze participant responses in relation to Grad-CAM visualizations of model attention. Human behavior is further interpreted through cognitive principles modeled in ACT-R and Soar, revealing layered and heuristic decision strategies under uncertainty. Our findings highlight key parallels and divergences between biological and artificial systems in representation, inference, and confidence calibration. The analysis motivates future neuro-symbolic architectures that unify structured symbolic reasoning with connectionist representations. Such architectures, informed by principles of embodiment, explainability, and cognitive alignment, offer a path toward AI systems that are not only performant but also interpretable and cognitively grounded.",0,arxiv,AI,CC-BY/arXiv,Visual Categorization Across Minds and Models: Cognitive Analysis of Human Labeling and Neuro-Symbolic Integration
"Industrial cyber physical systems operate under heterogeneous sensing, stochastic dynamics, and shifting process conditions, producing data that are often incomplete, unlabeled, imbalanced, and domain shifted. High-fidelity datasets remain costly, confidential, and slow to obtain, while edge devices face strict limits on latency, bandwidth, and energy. These factors restrict the practicality of centralized deep learning, hinder the development of reliable digital twins, and increase the risk of error escape in safety-critical applications. Motivated by these challenges, this dissertation develops an efficiency grounded computational framework that enables data lean, physics-aware, and deployment ready intelligence for modern manufacturing environments. The research advances methods that collectively address core bottlenecks across multimodal and multiscale industrial scenarios. Generative strategies mitigate data scarcity and imbalance, while semi-supervised learning integrates unlabeled information to reduce annotation and simulation demands. Physics-informed representation learning strengthens interpretability and improves condition monitoring under small-data regimes. Spatially aware graph-based surrogate modeling provides efficient approximation of complex processes, and an edge cloud collaborative compression scheme supports real-time signal analytics under resource constraints. The dissertation also extends visual understanding through zero-shot vision language reasoning augmented by domain specific retrieval, enabling generalizable assessment in previously unseen scenarios. Together, these developments establish a unified paradigm of data efficient and resource aware intelligence that bridges laboratory learning with industrial deployment, supporting reliable decision-making across diverse manufacturing systems.",0,arxiv,AI,CC-BY/arXiv,Efficiency-Aware Computational Intelligence for Resource-Constrained Manufacturing Toward Edge-Ready Deployment
"The reliance of organisations on computer networks is enabled by network programmability, which is typically achieved through Service Function Chaining. These chains virtualise network functions, link them, and programmatically embed them on networking infrastructure. Optimal embedding of Service Function Chains is an NP-hard problem, with three sub-problems, chain composition, virtual network function embedding, and link embedding, that have to be optimised simultaneously, rather than sequentially, for optimal results. Genetic Algorithms have been employed for this, but existing approaches either do not optimise all three sub-problems or do not optimise all three sub-problems simultaneously. We propose a Genetic Algorithm-based approach called GENESIS, which evolves three sine-function-activated Neural Networks, and funnels their output to a Gaussian distribution and an A* algorithm to optimise all three sub-problems simultaneously. We evaluate GENESIS on an emulator across 48 different data centre scenarios and compare its performance to two state-of-the-art Genetic Algorithms and one greedy algorithm. GENESIS produces an optimal solution for 100% of the scenarios, whereas the second-best method optimises only 71% of the scenarios. Moreover, GENESIS is the fastest among all Genetic Algorithms, averaging 15.84 minutes, compared to an average of 38.62 minutes for the second-best Genetic Algorithm.",0,arxiv,AI,CC-BY/arXiv,Simultaneous Genetic Evolution of Neural Networks for Optimal SFC Embedding
"Understanding the physical constraints and minimal conditions that enable information processing in extended systems remains a central challenge across disciplines, from neuroscience and artificial intelligence to social and physical networks. Here we study how network connectivity both limits and enables information processing by analyzing random networks across the structural percolation transition. Using cascade-mediated dynamics as a minimal and universal mechanism for propagating state-dependent responses, we examine structural, functional, and information-theoretic observables as functions of mean degree in Erdos-Renyi networks. We find that the emergence of a giant connected component coincides with a sharp transition in realizable information processing: complex input-output response functions become accessible, functional diversity increases rapidly, output entropy rises, and directed information flow quantified by transfer entropy extends beyond local neighborhoods. These coincident transitions define a regime of functional percolation, referring to a sharp expansion of the space of realizable input-output functions at the structural percolation transition. Near criticality, networks exhibit a Pareto-optimal tradeoff between functional complexity and diversity, suggesting that percolation criticality provides a universal organizing principle for information processing in systems with local interactions and propagating influences.",0,arxiv,AI,CC-BY/arXiv,Functional Percolation: A Perspective on Criticality of Form and Function
"The continuous scaling of deep neural networks has fundamentally transformed machine learning, with larger models demonstrating improved performance across diverse tasks. This growth in model size has dramatically increased the computational resources required for the training process. Consequently, distributed approaches, such as Federated Learning and Split Learning, have become essential paradigms for scalable deployment. However, existing Split Learning approaches assume client homogeneity and uniform split points across all participants. This critically limits their applicability to real-world IoT systems where devices exhibit heterogeneity in computational resources. To address this limitation, this paper proposes Hetero-SplitEE, a novel method that enables heterogeneous IoT devices to train a shared deep neural network in parallel collaboratively. By integrating heterogeneous early exits into hierarchical training, our approach allows each client to select distinct split points (cut layers) tailored to its computational capacity. In addition, we propose two cooperative training strategies, the Sequential strategy and the Averaging strategy, to facilitate this collaboration among clients with different split points. The Sequential strategy trains clients sequentially with a shared server model to reduce computational overhead. The Averaging strategy enables parallel client training with periodic cross-layer aggregation. Extensive experiments on CIFAR-10, CIFAR-100, and STL-10 datasets using ResNet-18 demonstrate that our method maintains competitive accuracy while efficiently supporting diverse computational constraints, enabling practical deployment of collaborative deep learning in heterogeneous IoT ecosystems.",0,arxiv,AI,CC-BY/arXiv,Hetero-SplitEE: Split Learning of Neural Networks with Early Exits for Heterogeneous IoT Devices
"The meteoric rise in text generation capability has been accompanied by parallel growth in interest in machine-generated text detection: the capability to identify whether a given text was generated using a model or written by a person. While detection models show strong performance, they have the capacity to cause significant negative impacts. We explore potential biases in English machine-generated text detection systems. We curate a dataset of student essays and assess 16 different detection systems for bias across four attributes: gender, race/ethnicity, English-language learner (ELL) status, and economic status. We evaluate these attributes using regression-based models to determine the significance and power of the effects, as well as performing subgroup analysis. We find that while biases are generally inconsistent across systems, there are several key issues: several models tend to classify disadvantaged groups as machine-generated, ELL essays are more likely to be classified as machine-generated, economically disadvantaged students' essays are less likely to be classified as machine-generated, and non-White ELL essays are disproportionately classified as machine-generated relative to their White counterparts. Finally, we perform human annotation and find that while humans perform generally poorly at the detection task, they show no significant biases on the studied attributes.",0,arxiv,AI,CC-BY/arXiv,Identifying Bias in Machine-generated Text Detection
"The prosperous development of Artificial Intelligence-Generated Content (AIGC) has brought people's anxiety about the spread of false information on social media. Designing detectors for filtering is an effective defense method, but most detectors will be compromised by adversarial samples. Currently, most studies exposing AIGC security issues assume information on model structure and data distribution. In real applications, attackers query and interfere with models that provide services in the form of application programming interfaces (APIs), which constitutes the black-box decision-based attack paradigm. However, to the best of our knowledge, decision-based attacks on AIGC detectors remain unexplored. In this study, we propose \textbf{FBA$^2$D}: a frequency-based black-box attack method for AIGC detection to fill the research gap. Motivated by frequency-domain discrepancies between generated and real images, we develop a decision-based attack that leverages the Discrete Cosine Transform (DCT) for fine-grained spectral partitioning and selects frequency bands as query subspaces, improving both query efficiency and image quality. Moreover, attacks on AIGC detectors should mitigate initialization failures, preserve image quality, and operate under strict query budgets. To address these issues, we adopt an ``adversarial example soup'' method, averaging candidates from successive surrogate iterations and using the result as the initialization to accelerate the query-based attack. The empirical study on the Synthetic LSUN dataset and GenImage dataset demonstrate the effectiveness of our prosed method. This study shows the urgency of addressing practical AIGC security problems.",0,arxiv,AI,CC-BY/arXiv,FBA$^2$D: Frequency-based Black-box Attack for AI-generated Image Detection
"Glacial lake monitoring bears great significance in mitigating the anticipated risk of Glacial Lake Outburst Floods. However, existing segmentation methods based on convolutional neural networks (CNNs) and Vision Transformers (ViTs), remain constrained to pixel-level predictions, lacking high-level global scene semantics and human-interpretable reasoning. To address this, we introduce GLACIA (\textbf{G}lacial \textbf{LA}ke segmentation with \textbf{C}ontextual \textbf{I}nstance \textbf{A}wareness), the first framework that integrates large language models with segmentation capabilities to produce both accurate segmentation masks and corresponding spatial reasoning outputs. We construct the Glacial Lake Position Reasoning (GLake-Pos) dataset pipeline, which provides diverse, spatially grounded question-answer pairs designed to overcome the lack of instance-aware positional reasoning data in remote sensing. Comparative evaluation demonstrate that GLACIA (mIoU: 87.30) surpasses state-of-the-art method based on CNNs (mIoU: 78.55 - 79.01), ViTs (mIoU: 69.27 - 81.75), Geo-foundation models (mIoU: 76.37 - 87.10), and reasoning based segmentation methods (mIoU: 60.12 - 75.66). Our approach enables intuitive disaster preparedness and informed policy-making in the context of rapidly changing glacial environments by facilitating natural language interaction, thereby supporting more efficient and interpretable decision-making. The code is released on https://github.com/lalitmaurya47/GLACIA",0,arxiv,AI,CC-BY/arXiv,GLACIA: Instance-Aware Positional Reasoning for Glacial Lake Segmentation via Multimodal Large Language Model
"Chronic Kidney Disease (CKD) constitutes a major global medical burden, marked by the gradual deterioration of renal function, which results in the impaired clearance of metabolic waste and disturbances in systemic fluid homeostasis. Owing to its substantial contribution to worldwide morbidity and mortality, the development of reliable and efficient diagnostic approaches is critically important to facilitate early detection and prompt clinical management. This study presents a deep convolutional neural network (CNN) for early CKD detection from CT kidney images, complemented by class balancing using Synthetic Minority Over-sampling Technique (SMOTE) and interpretability via Gradient-weighted Class Activation Mapping (Grad-CAM). The model was trained and evaluated on the CT KIDNEY DATASET, which contains 12,446 CT images, including 3,709 cyst, 5,077 normal, 1,377 stone, and 2,283 tumor cases. The proposed deep CNN achieved a remarkable classification performance, attaining 100% accuracy in the early detection of chronic kidney disease (CKD). This significant advancement demonstrates strong potential for addressing critical clinical diagnostic challenges and enhancing early medical intervention strategies.",0,arxiv,AI,CC-BY/arXiv,A Clinically Interpretable Deep CNN Framework for Early Chronic Kidney Disease Prediction Using Grad-CAM-Based Explainable AI
"Large language models handle single-turn generation well, but multi-turn interactions still require the model to reconstruct user intent and task state from an expanding token history because internal representations do not persist across turns. This token-first paradigm leads to drift, inconsistent reasoning modes, and growing prompts as conversations deepen. We propose CORE, a concept-first interaction layer that improves multi-turn stability without modifying model weights. CORE combines a small library of universal cognitive operators with a persistent Local Concept - a compact semantic state capturing the task, constraints, preferences, and intermediate results. Each model call receives only this concept state, the user's latest instruction, and the selected operator, eliminating the need to replay full history. A preliminary prototype simulating CORE's behavior shows about 42% reduction in cumulative prompt tokens, though this number reflects prototype conditions and should not be interpreted as a real-world performance estimate. CORE offers a model-agnostic mechanism that separates conceptual reasoning from language generation, suggesting a scalable direction for more stable multi-turn systems.",0,arxiv,AI,CC-BY/arXiv,CORE: A Conceptual Reasoning Layer for Large Language Models
"Physics-Informed Neural Networks (PINNs) have emerged as a promising paradigm for solving partial differential equations (PDEs) by embedding physical laws into neural network training objectives. However, their deployment on resource-constrained platforms is hindered by substantial computational and memory overhead, primarily stemming from higher-order automatic differentiation, intensive tensor operations, and reliance on full-precision arithmetic. To address these challenges, we present a framework that enables scalable and energy-efficient PINN training on edge devices. This framework integrates fully quantized training, Stein's estimator (SE)-based residual loss computation, and tensor-train (TT) decomposition for weight compression. It contributes three key innovations: (1) a mixed-precision training method that use a square-block MX (SMX) format to eliminate data duplication during backpropagation; (2) a difference-based quantization scheme for the Stein's estimator that mitigates underflow; and (3) a partial-reconstruction scheme (PRS) for TT-Layers that reduces quantization-error accumulation. We further design PINTA, a precision-scalable hardware accelerator, to fully exploit the performance of the framework. Experiments on the 2-D Poisson, 20-D Hamilton-Jacobi-Bellman (HJB), and 100-D Heat equations demonstrate that the proposed framework achieves accuracy comparable to or better than full-precision, uncompressed baselines while delivering 5.5x to 83.5x speedups and 159.6x to 2324.1x energy savings. This work enables real-time PDE solving on edge devices and paves the way for energy-efficient scientific computing at scale.",0,arxiv,AI,CC-BY/arXiv,Tensor-Compressed and Fully-Quantized Training of Neural PDE Solvers
"Large Language Models (LLMs) and transformer architectures have shown impressive reasoning and generation capabilities across diverse natural language tasks. However, their reliability and robustness in real-world engineering domains remain largely unexplored, limiting their practical utility in human-centric workflows. In this work, we investigate the applicability and consistency of LLMs for analog circuit design -- a task requiring domain-specific reasoning, adherence to physical constraints, and structured representations -- focusing on AI-assisted design where humans remain in the loop. We study how different data representations influence model behavior and compare smaller models (e.g., T5, GPT-2) with larger foundation models (e.g., Mistral-7B, GPT-oss-20B) under varying training conditions. Our results highlight key reliability challenges, including sensitivity to data format, instability in generated designs, and limited generalization to unseen circuit configurations. These findings provide early evidence on the limits and potential of LLMs as tools to enhance human capabilities in complex engineering tasks, offering insights into designing reliable, deployable foundation models for structured, real-world applications.",0,arxiv,AI,CC-BY/arXiv,LLMs for Analog Circuit Design Continuum (ACDC)
"Transcatheter Aortic Valve Replacement (TAVR) has emerged as a minimally invasive treatment option for patients with severe aortic stenosis, a life-threatening cardiovascular condition. Multiple transcatheter heart valves (THV) have been approved for use in TAVR, but current guidelines regarding valve type prescription remain an active topic of debate. We propose a data-driven clinical support tool to identify the optimal valve type with the objective of minimizing the risk of permanent pacemaker implantation (PPI), a predominant postoperative complication. We synthesize a novel dataset that combines U.S. and Greek patient populations and integrates three distinct data sources (patient demographics, computed tomography scans, echocardiograms) while harmonizing differences in each country's record system. We introduce a leaf-level analysis to leverage population heterogeneity and avoid benchmarking against uncertain counterfactual risk estimates. The final prescriptive model shows a reduction in PPI rates of 26% and 16% compared with the current standard of care in our internal U.S. population and external Greek validation cohort, respectively. To the best of our knowledge, this work represents the first unified, personalized prescription strategy for THV selection in TAVR.",0,arxiv,AI,CC-BY/arXiv,Towards Optimal Valve Prescription for Transcatheter Aortic Valve Replacement (TAVR) Surgery: A Machine Learning Approach
"Understanding how driver mental states differ between active and autonomous driving is critical for designing safe human-vehicle interfaces. This paper presents the first EEG-based comparison of cognitive load, fatigue, valence, and arousal across the two driving modes. Using data from 31 participants performing identical tasks in both scenarios of three different complexity levels, we analyze temporal patterns, task-complexity effects, and channel-wise activation differences. Our findings show that although both modes evoke similar trends across complexity levels, the intensity of mental states and the underlying neural activation differ substantially, indicating a clear distribution shift between active and autonomous driving. Transfer-learning experiments confirm that models trained on active driving data generalize poorly to autonomous driving and vice versa. We attribute this distribution shift primarily to differences in motor engagement and attentional demands between the two driving modes, which lead to distinct spatial and temporal EEG activation patterns. Although autonomous driving results in lower overall cortical activation, participants continue to exhibit measurable fluctuations in cognitive load, fatigue, valence, and arousal associated with readiness to intervene, task-evoked emotional responses, and monotony-related passive fatigue. These results emphasize the need for scenario-specific data and models when developing next-generation driver monitoring systems for autonomous vehicles.",0,arxiv,AI,CC-BY/arXiv,Understanding Mental States in Active and Autonomous Driving with EEG
"Deception is a fundamental challenge for multi-agent reasoning: effective systems must strategically conceal information while detecting misleading behavior in others. Yet most evaluations reduce deception to static classification, ignoring the interactive, adversarial, and longitudinal nature of real deceptive dynamics. Large language models (LLMs) can deceive convincingly but remain weak at detecting deception in peers. We present WOLF, a multi-agent social deduction benchmark based on Werewolf that enables separable measurement of deception production and detection. WOLF embeds role-grounded agents (Villager, Werewolf, Seer, Doctor) in a programmable LangGraph state machine with strict night-day cycles, debate turns, and majority voting. Every statement is a distinct analysis unit, with self-assessed honesty from speakers and peer-rated deceptiveness from others. Deception is categorized via a standardized taxonomy (omission, distortion, fabrication, misdirection), while suspicion scores are longitudinally smoothed to capture both immediate judgments and evolving trust dynamics. Structured logs preserve prompts, outputs, and state transitions for full reproducibility. Across 7,320 statements and 100 runs, Werewolves produce deceptive statements in 31% of turns, while peer detection achieves 71-73% precision with ~52% overall accuracy. Precision is higher for identifying Werewolves, though false positives occur against Villagers. Suspicion toward Werewolves rises from ~52% to over 60% across rounds, while suspicion toward Villagers and the Doctor stabilizes near 44-46%. This divergence shows that extended interaction improves recall against liars without compounding errors against truthful roles. WOLF moves deception evaluation beyond static datasets, offering a dynamic, controlled testbed for measuring deceptive and detective capacity in adversarial multi-agent interaction.",0,arxiv,AI,CC-BY/arXiv,WOLF: Werewolf-based Observations for LLM Deception and Falsehoods
"Understanding disease progression is a central clinical challenge with direct implications for early diagnosis and personalized treatment. While recent generative approaches have attempted to model progression, key mismatches remain: disease dynamics are inherently continuous and monotonic, yet latent representations are often scattered, lacking semantic structure, and diffusion-based models disrupt continuity with random denoising process. In this work, we propose to treat the disease dynamic as a velocity field and leverage Flow Matching (FM) to align the temporal evolution of patient data. Unlike prior methods, it captures the intrinsic dynamic of disease, making the progression more interpretable. However, a key challenge remains: in latent space, Auto-Encoders (AEs) do not guarantee alignment across patients or correlation with clinical-severity indicators (e.g., age and disease conditions). To address this, we propose to learn patient-specific latent alignment, which enforces patient trajectories to lie along a specific axis, with magnitude increasing monotonically with disease severity. This leads to a consistent and semantically meaningful latent space. Together, we present $Î”$-LFM, a framework for modeling patient-specific latent progression with flow matching. Across three longitudinal MRI benchmarks, $Î”$-LFM demonstrates strong empirical performance and, more importantly, offers a new framework for interpreting and visualizing disease dynamics.",0,arxiv,AI,CC-BY/arXiv,Learning Patient-Specific Disease Dynamics with Latent Flow Matching for Longitudinal Imaging Generation
"We tackle continual adaptation of vision-language models to new attributes, objects, and their compositions in Compositional Zero-Shot Learning (CZSL), while preventing forgetting of prior knowledge. Unlike classical continual learning where classes are disjoint, CCZSL is more complex as attributes and objects may reoccur across sessions while compositions remain unique. Built on a frozen VLM backbone, we propose the first Prompt-based Continual Compositional Zero-Shot Learning (PromptCCZSL) framework that retains prior knowledge through recency-weighted multi-teacher distillation. It employs session-aware compositional prompts to fuse multimodal features for new compositions, while attribute and object prompts are learned through session-agnostic fusion to maintain global semantic consistency, which is further stabilized by a Cosine Anchor Loss (CAL) to preserve prior knowledge. To enhance adaptation in the current session, an Orthogonal Projection Loss (OPL) ensures that new attribute and object embeddings remain distinct from previous ones, preventing overlap, while an Intra-Session Diversity Loss (IDL) promotes variation among current-session embeddings for richer, more discriminative representations. We also introduce a comprehensive protocol that jointly measures catastrophic forgetting and compositional generalization. Extensive experiments on UT-Zappos and C-GQA benchmarks demonstrate that PromptCCZSL achieves substantial improvements over prior VLM-based and non-VLM baselines, setting a new benchmark for CCZSL in closed-world settings.",0,arxiv,AI,CC-BY/arXiv,Prompt-Based Continual Compositional Zero-Shot Learning
"We present a novel multi-stage workflow for computational materials discovery that achieves a 99% success rate in identifying compounds within 100 meV/atom of thermodynamic stability, with a threefold improvement over previous approaches. By combining the Matra-Genoa generative model, Orb-v2 universal machine learning interatomic potential, and ALIGNN graph neural network for energy prediction, we generated 119 million candidate structures and added 1.3 million DFT-validated compounds to the ALEXANDRIA database, including 74 thousand new stable materials. The expanded ALEXANDRIA database now contains 5.8 million structures with 175 thousand compounds on the convex hull. Predicted structural disorder rates (37-43%) match experimental databases, unlike other recent AI-generated datasets. Analysis reveals fundamental patterns in space group distributions, coordination environments, and phase stability networks, including sub-linear scaling of convex hull connectivity. We release the complete dataset, including sAlex25 with 14 million out-of-equilibrium structures containing forces and stresses for training universal force fields. We demonstrate that fine-tuning a GRACE model on this data improves benchmark accuracy. All data, models, and workflows are freely available under Creative Commons licenses.",0,arxiv,AI,CC-BY/arXiv,AI-Driven Expansion and Application of the Alexandria Database
"We present WonderZoom, a novel approach to generating 3D scenes with contents across multiple spatial scales from a single image. Existing 3D world generation models remain limited to single-scale synthesis and cannot produce coherent scene contents at varying granularities. The fundamental challenge is the lack of a scale-aware 3D representation capable of generating and rendering content with largely different spatial sizes. WonderZoom addresses this through two key innovations: (1) scale-adaptive Gaussian surfels for generating and real-time rendering of multi-scale 3D scenes, and (2) a progressive detail synthesizer that iteratively generates finer-scale 3D contents. Our approach enables users to ""zoom into"" a 3D region and auto-regressively synthesize previously non-existent fine details from landscapes to microscopic features. Experiments demonstrate that WonderZoom significantly outperforms state-of-the-art video and 3D models in both quality and alignment, enabling multi-scale 3D world creation from a single image. We show video results and an interactive viewer of generated multi-scale 3D worlds in https://wonderzoom.github.io/",0,arxiv,AI,CC-BY/arXiv,WonderZoom: Multi-Scale 3D World Generation
"Large language models (LLMs) hold the potential to absorb and reflect personality traits and attitudes specified by users. In our study, we investigated this potential using robust psychometric measures. We adapted the most studied test in psychological literature, namely Minnesota Multiphasic Personality Inventory (MMPI) and examined LLMs' behavior to identify traits. To asses the sensitivity of LLMs' prompts and psychological biases we created personality-oriented prompts, crafting a detailed set of personas that vary in trait intensity. This enables us to measure how well LLMs follow these roles. Our study introduces MindShift, a benchmark for evaluating LLMs' psychological adaptability. The results highlight a consistent improvement in LLMs' role perception, attributed to advancements in training datasets and alignment techniques. Additionally, we observe significant differences in responses to psychometric assessments across different model types and families, suggesting variability in their ability to emulate human-like personality traits. MindShift prompts and code for LLM evaluation will be publicly available.",0,arxiv,AI,CC-BY/arXiv,MindShift: Analyzing Language Models' Reactions to Psychological Prompts
"Graph-based Retrieval-Augmented Generation (GraphRAG) enhances Large Language Models (LLMs) by incorporating external knowledge from linearized subgraphs retrieved from knowledge graphs. However, LLMs struggle to interpret the relational and topological information in these inputs, resulting in hallucinations that are inconsistent with the retrieved knowledge. To analyze how LLMs attend to and retain structured knowledge during generation, we propose two lightweight interpretability metrics: Path Reliance Degree (PRD), which measures over-reliance on shortest-path triples, and Semantic Alignment Score (SAS), which assesses how well the model's internal representations align with the retrieved knowledge. Through empirical analysis on a knowledge-based QA task, we identify failure patterns associated with over-reliance on salient paths and weak semantic grounding, as indicated by high PRD and low SAS scores. We further develop a lightweight post-hoc hallucination detector, Graph Grounding and Alignment (GGA), which outperforms strong semantic and confidence-based baselines across AUC and F1. By grounding hallucination analysis in mechanistic interpretability, our work offers insights into how structural limitations in LLMs contribute to hallucinations, informing the design of more reliable GraphRAG systems in the future.",0,arxiv,AI,CC-BY/arXiv,Detecting Hallucinations in Graph Retrieval-Augmented Generation via Attention Patterns and Semantic Alignment
"We present SDialog, an MIT-licensed open-source Python toolkit that unifies dialog generation, evaluation and mechanistic interpretability into a single end-to-end framework for building and analyzing LLM-based conversational agents. Built around a standardized \texttt{Dialog} representation, SDialog provides: (1) persona-driven multi-agent simulation with composable orchestration for controlled, synthetic dialog generation, (2) comprehensive evaluation combining linguistic metrics, LLM-as-a-judge and functional correctness validators, (3) mechanistic interpretability tools for activation inspection and steering via feature ablation and induction, and (4) audio generation with full acoustic simulation including 3D room modeling and microphone effects. The toolkit integrates with all major LLM backends, enabling mixed-backend experiments under a unified API. By coupling generation, evaluation, and interpretability in a dialog-centric architecture, SDialog enables researchers to build, benchmark and understand conversational systems more systematically.",0,arxiv,AI,CC-BY/arXiv,"SDialog: A Python Toolkit for End-to-End Agent Building, User Simulation, Dialog Generation, and Evaluation"
"Coronary angiography is the main tool for assessing coronary artery disease, but visual grading of stenosis is variable and only moderately related to ischaemia. Wire based fractional flow reserve (FFR) improves lesion selection but is not used systematically. Angiography derived indices such as quantitative flow ratio (QFR) offer wire free physiology, yet many tools are workflow intensive and separate from automated anatomy analysis and virtual PCI planning. We developed AngioAI-QFR, an end to end angiography only pipeline combining deep learning stenosis detection, lumen segmentation, centreline and diameter extraction, per millimetre Relative Flow Capacity profiling, and virtual stenting with automatic recomputation of angiography derived QFR. The system was evaluated in 100 consecutive vessels with invasive FFR as reference. Primary endpoints were agreement with FFR (correlation, mean absolute error) and diagnostic performance for FFR <= 0.80. On held out frames, stenosis detection achieved precision 0.97 and lumen segmentation Dice 0.78. Across 100 vessels, AngioAI-QFR correlated strongly with FFR (r = 0.89, MAE 0.045). The AUC for detecting FFR <= 0.80 was 0.93, with sensitivity 0.88 and specificity 0.86. The pipeline completed fully automatically in 93 percent of vessels, with median time to result 41 s. RFC profiling distinguished focal from diffuse capacity loss, and virtual stenting predicted larger QFR gain in focal than in diffuse disease. AngioAI-QFR provides a practical, near real time pipeline that unifies computer vision, functional profiling, and virtual PCI with automated angiography derived physiology.",0,arxiv,AI,CC-BY/arXiv,"Integrated Pipeline for Coronary Angiography With Automated Lesion Profiling, Virtual Stenting, and 100-Vessel FFR Validation"
"Accurate interpretation of pediatric dental clinical records and safe antibiotic prescribing remain persistent challenges in dental informatics. Traditional rule-based clinical decision support systems struggle with unstructured dental narratives, incomplete radiographic descriptions, and complex safety constraints. To address these limitations, this study proposes a Knowledge-Guided Large Language Model (KG-LLM) that integrates a pediatric dental knowledge graph, retrieval-augmented generation (RAG), and a multi-stage safety validation pipeline for evidence-grounded antibiotic recommendation. The framework first employs a clinical NER/RE module to extract structured entities and relations from dental notes and radiology reports. Relevant guidelines, drug-safety rules, and analogous historical cases are subsequently retrieved from the knowledge graph and supplied to the LLM for diagnostic summarization and dose-drug-duration prediction. Safety assurance is achieved through a dual-layer validation mechanism combining deterministic rule checking with a learned classifier for detecting allergies, contraindications, and dosing errors. Experiments on 32,000 de-identified pediatric dental visit records demonstrate the effectiveness of the proposed approach. Compared with a domain-adapted Llama-2 clinical baseline, KG-LLM improves record-understanding performance (F1: 0.914 vs. 0.867), drug-dose-duration accuracy (Top-1: 0.782 vs. 0.716), and reduces unsafe antibiotic suggestions by 50%. Additional evaluation across summary quality, recommendation accuracy, and global safety scores further confirms the robustness of the system. Ablation analyses indicate that the knowledge graph, RAG, and safety modules each contribute substantially to clinical reliability and interpretability.",0,arxiv,AI,CC-BY/arXiv,Knowledge-Guided Large Language Model for Automatic Pediatric Dental Record Understanding and Safe Antibiotic Recommendation
"This paper presents a formal, categorical framework for analysing how humans and large language models (LLMs) transform content into truth-evaluated propositions about a state space of possible worlds W , in order to argue that LLMs do not solve but circumvent the symbol grounding problem.",0,arxiv,AI,CC-BY/arXiv,A Categorical Analysis of Large Language Models and Why LLMs Circumvent the Symbol Grounding Problem
"The deployment of AI systems faces three critical governance challenges that current frameworks fail to adequately address. First, organizations struggle with inadequate risk assessment at the use case level, exemplified by the Humana class action lawsuit and other high impact cases where an AI system deployed to production exhibited both significant bias and high error rates, resulting in improper healthcare claim denials. Each AI use case presents unique risk profiles requiring tailored governance, yet most frameworks provide one size fits all guidance. Second, existing frameworks like ISO 42001 and NIST AI RMF remain at high conceptual levels, offering principles without actionable controls, leaving practitioners unable to translate governance requirements into specific technical implementations. Third, organizations lack mechanisms for operationalizing governance at scale, with no systematic approach to embed trustworthy AI practices throughout the development lifecycle, measure compliance quantitatively, or provide role-appropriate visibility from boards to data scientists. We present AI TIPS, Artificial Intelligence Trust-Integrated Pillars for Sustainability 2.0, update to the comprehensive operational framework developed in 2019,four years before NIST's AI Risk Management Framework, that directly addresses these challenges.",0,arxiv,AI,CC-BY/arXiv,AI TIPS 2.0: A Comprehensive Framework for Operationalizing AI Governance
"Reliable real-time trajectory generation is essential for future autonomous spacecraft. While recent progress in nonconvex guidance and control is paving the way for onboard autonomous trajectory optimization, these methods still rely on extensive expert input (e.g., waypoints, constraints, mission timelines, etc.), which limits the operational scalability in real rendezvous missions.This paper introduces SAGES (Semantic Autonomous Guidance Engine for Space), a trajectory-generation framework that translates natural-language commands into spacecraft trajectories that reflect high-level intent while respecting nonconvex constraints. Experiments in two settings -- fault-tolerant proximity operations with continuous-time constraint enforcement and a free-flying robotic platform -- demonstrate that SAGES reliably produces trajectories aligned with human commands, achieving over 90\% semantic-behavioral consistency across diverse behavior modes. Ultimately, this work marks an initial step toward language-conditioned, constraint-aware spacecraft trajectory generation, enabling operators to interactively guide both safety and behavior through intuitive natural-language commands with reduced expert burden.",0,arxiv,AI,CC-BY/arXiv,Semantic Trajectory Generation for Goal-Oriented Spacecraft Rendezvous
"Agentic AI systems built on large language models (LLMs) offer significant potential for automating complex workflows, from software development to customer support. However, LLM agents often underperform due to suboptimal configurations; poorly tuned prompts, tool descriptions, and parameters that typically require weeks of manual refinement. Existing optimization methods either are too complex for general use or treat components in isolation, missing critical interdependencies.   We present ARTEMIS, a no-code evolutionary optimization platform that jointly optimizes agent configurations through semantically-aware genetic operators. Given only a benchmark script and natural language goals, ARTEMIS automatically discovers configurable components, extracts performance signals from execution logs, and evolves configurations without requiring architectural modifications.   We evaluate ARTEMIS on four representative agent systems: the \emph{ALE Agent} for competitive programming on AtCoder Heuristic Contest, achieving a \textbf{$13.6\%$ improvement} in acceptance rate; the \emph{Mini-SWE Agent} for code optimization on SWE-Perf, with a statistically significant \textbf{10.1\% performance gain}; and the \emph{CrewAI Agent} for cost and mathematical reasoning on Math Odyssey, achieving a statistically significant \textbf{$36.9\%$ reduction} in the number of tokens required for evaluation. We also evaluate the \emph{MathTales-Teacher Agent} powered by a smaller open-source model (Qwen2.5-7B) on GSM8K primary-level mathematics problems, achieving a \textbf{22\% accuracy improvement} and demonstrating that ARTEMIS can optimize agents based on both commercial and local models.",0,arxiv,AI,CC-BY/arXiv,Evolving Excellence: Automated Optimization of LLM-based Agents
"We present Masked Generative Policy (MGP), a novel framework for visuomotor imitation learning. We represent actions as discrete tokens, and train a conditional masked transformer that generates tokens in parallel and then rapidly refines only low-confidence tokens. We further propose two new sampling paradigms: MGP-Short, which performs parallel masked generation with score-based refinement for Markovian tasks, and MGP-Long, which predicts full trajectories in a single pass and dynamically refines low-confidence action tokens based on new observations. With globally coherent prediction and robust adaptive execution capabilities, MGP-Long enables reliable control on complex and non-Markovian tasks that prior methods struggle with. Extensive evaluations on 150 robotic manipulation tasks spanning the Meta-World and LIBERO benchmarks show that MGP achieves both rapid inference and superior success rates compared to state-of-the-art diffusion and autoregressive policies. Specifically, MGP increases the average success rate by 9% across 150 tasks while cutting per-sequence inference time by up to 35x. It further improves the average success rate by 60% in dynamic and missing-observation environments, and solves two non-Markovian scenarios where other state-of-the-art methods fail.",0,arxiv,AI,CC-BY/arXiv,Masked Generative Policy for Robotic Control
"Hallucinations are outputs by Large Language Models (LLMs) that are factually incorrect yet appear plausible [1]. This paper investigates how such hallucinations influence users' trust in LLMs and users' interaction with LLMs. To explore this in everyday use, we conducted a qualitative study with 192 participants. Our findings show that hallucinations do not result in blanket mistrust but instead lead to context-sensitive trust calibration. Building on the calibrated trust model by Lee & See [2] and Afroogh et al.'s trust-related factors [3], we confirm expectancy [3], [4], prior experience [3], [4], [5], and user expertise & domain knowledge [3], [4] as userrelated (human) trust factors, and identify intuition as an additional factor relevant for hallucination detection. Additionally, we found that trust dynamics are further influenced by contextual factors, particularly perceived risk [3] and decision stakes [6]. Consequently, we validate the recursive trust calibration process proposed by BlÃ¶baum [7] and extend it by including intuition as a user-related trust factor. Based on these insights, we propose practical recommendations for responsible and reflective LLM use.",0,arxiv,AI,CC-BY/arXiv,Calibrated Trust in Dealing with LLM Hallucinations: A Qualitative Study
"Narratives about artificial intelligence (AI) entangle autonomy, the capacity to self-govern, with sentience, the capacity to sense and feel. AI agents that perform tasks autonomously and companions that recognize and express emotions may activate mental models of autonomy and sentience, respectively, provoking distinct reactions. To examine this possibility, we conducted three pilot studies (N = 374) and four preregistered vignette experiments describing an AI as autonomous, sentient, both, or neither (N = 2,702). Activating a mental model of sentience increased general mind perception (cognition and emotion) and moral consideration more than autonomy, but autonomy increased perceived threat more than sentience. Sentience also increased perceived autonomy more than vice versa. Based on a within-paper meta-analysis, sentience changed reactions more than autonomy on average. By disentangling different mental models of AI, we can study human-AI interaction with more precision to better navigate the detailed design of anthropomorphized AI and prompting interfaces.",0,arxiv,AI,CC-BY/arXiv,Mental Models of Autonomy and Sentience Shape Reactions to AI
"Accurate forecasting of urban air pollution is essential for protecting public health and guiding mitigation policies. While Deep Learning (DL) and hybrid pipelines dominate recent research, their complexity and limited interpretability hinder operational use. This study investigates whether lightweight additive models -- Facebook Prophet (FBP) and NeuralProphet (NP) -- can deliver competitive forecasts for particulate matter (PM$_{2.5}$, PM$_{10}$) in Beijing, China. Using multi-year pollutant and meteorological data, we applied systematic feature selection (correlation, mutual information, mRMR), leakage-safe scaling, and chronological data splits. Both models were trained with pollutant and precursor regressors, with NP additionally leveraging lagged dependencies. For context, two machine learning baselines (LSTM, LightGBM) and one traditional statistical model (SARIMAX) were also implemented. Performance was evaluated on a 7-day holdout using MAE, RMSE, and $R^2$. Results show that FBP consistently outperformed NP, SARIMAX, and the learning-based baselines, achieving test $R^2$ above 0.94 for both pollutants. These findings demonstrate that interpretable additive models remain competitive with both traditional and complex approaches, offering a practical balance of accuracy, transparency, and ease of deployment.",0,arxiv,AI,CC-BY/arXiv,Beyond the Hype: Comparing Lightweight and Deep Learning Models for Air Quality Forecasting
"Age-related macular degeneration (AMD) and choroidal neovascularization (CNV)-related conditions are leading causes of vision loss worldwide, with optical coherence tomography (OCT) serving as a cornerstone for early detection and management. However, deploying state-of-the-art deep learning models like ConvNeXtV2-Large in clinical settings is hindered by their computational demands. Therefore, it is desirable to develop efficient models that maintain high diagnostic performance while enabling real-time deployment. In this study, a novel knowledge distillation framework, termed KD-OCT, is proposed to compress a high-performance ConvNeXtV2-Large teacher model, enhanced with advanced augmentations, stochastic weight averaging, and focal loss, into a lightweight EfficientNet-B2 student for classifying normal, drusen, and CNV cases. KD-OCT employs real-time distillation with a combined loss balancing soft teacher knowledge transfer and hard ground-truth supervision. The effectiveness of the proposed method is evaluated on the Noor Eye Hospital (NEH) dataset using patient-level cross-validation. Experimental results demonstrate that KD-OCT outperforms comparable multi-scale or feature-fusion OCT classifiers in efficiency- accuracy balance, achieving near-teacher performance with substantial reductions in model size and inference time. Despite the compression, the student model exceeds most existing frameworks, facilitating edge deployment for AMD screening. Code is available at https://github.com/erfan-nourbakhsh/KD- OCT.",0,arxiv,AI,CC-BY/arXiv,KD-OCT: Efficient Knowledge Distillation for Clinical-Grade Retinal OCT Classification
"Many indoor workspaces are quasi-static: global layout is stable but local semantics change continually, producing repetitive geometry, dynamic clutter, and perceptual noise that defeat vision-based localization. We present ShelfAware, a semantic particle filter for robust global localization that treats scene semantics as statistical evidence over object categories rather than fixed landmarks. ShelfAware fuses a depth likelihood with a category-centric semantic similarity and uses a precomputed bank of semantic viewpoints to perform inverse semantic proposals inside MCL, yielding fast, targeted hypothesis generation on low-cost, vision-only hardware. Across 100 global-localization trials spanning four conditions (cart-mounted, wearable, dynamic obstacles, and sparse semantics) in a semantically dense, retail environment, ShelfAware achieves a 96% success rate (vs. 22% MCL and 10% AMCL) with a mean time-to-convergence of 1.91s, attains the lowest translational RMSE in all conditions, and maintains stable tracking in 80% of tested sequences, all while running in real time on a consumer laptop-class platform. By modeling semantics distributionally at the category level and leveraging inverse proposals, ShelfAware resolves geometric aliasing and semantic drift common to quasi-static domains. Because the method requires only vision sensors and VIO, it integrates as an infrastructure-free building block for mobile robots in warehouses, labs, and retail settings; as a representative application, it also supports the creation of assistive devices providing start-anytime, shared-control assistive navigation for people with visual impairments.",0,arxiv,AI,CC-BY/arXiv,ShelfAware: Real-Time Visual-Inertial Semantic Localization in Quasi-Static Environments with Low-Cost Sensors
"Post-deployment monitoring of artificial intelligence (AI) systems in health care is essential to ensure their safety, quality, and sustained benefit-and to support governance decisions about which systems to update, modify, or decommission. Motivated by these needs, we developed a framework for monitoring deployed AI systems grounded in the mandate to take specific actions when they fail to behave as intended. This framework, which is now actively used at Stanford Health Care, is organized around three complementary principles: system integrity, performance, and impact. System integrity monitoring focuses on maximizing system uptime, detecting runtime errors, and identifying when changes to the surrounding IT ecosystem have unintended effects. Performance monitoring focuses on maintaining accurate system behavior in the face of changing health care practices (and thus input data) over time. Impact monitoring assesses whether a deployed system continues to have value in the form of benefit to clinicians and patients. Drawing on examples of deployed AI systems at our academic medical center, we provide practical guidance for creating monitoring plans based on these principles that specify which metrics to measure, when those metrics should be reviewed, who is responsible for acting when metrics change, and what concrete follow-up actions should be taken-for both traditional and generative AI. We also discuss challenges to implementing this framework, including the effort and cost of monitoring for health systems with limited resources and the difficulty of incorporating data-driven monitoring practices into complex organizations where conflicting priorities and definitions of success often coexist. This framework offers a practical template and starting point for health systems seeking to ensure that AI deployments remain safe and effective over time.",0,arxiv,AI,CC-BY/arXiv,Monitoring Deployed AI Systems in Health Care
"Recent advances in diffusion transformers have empowered video generation models to generate high-quality video clips from texts or images. However, world models with the ability to predict long-horizon futures from past observations and actions remain underexplored, especially for general-purpose scenarios and various forms of actions. To bridge this gap, we introduce Astra, an interactive general world model that generates real-world futures for diverse scenarios (e.g., autonomous driving, robot grasping) with precise action interactions (e.g., camera motion, robot action). We propose an autoregressive denoising architecture and use temporal causal attention to aggregate past observations and support streaming outputs. We use a noise-augmented history memory to avoid over-reliance on past frames to balance responsiveness with temporal coherence. For precise action control, we introduce an action-aware adapter that directly injects action signals into the denoising process. We further develop a mixture of action experts that dynamically route heterogeneous action modalities, enhancing versatility across diverse real-world tasks such as exploration, manipulation, and camera control. Astra achieves interactive, consistent, and general long-term video prediction and supports various forms of interactions. Experiments across multiple datasets demonstrate the improvements of Astra in fidelity, long-range prediction, and action alignment over existing state-of-the-art world models.",0,arxiv,AI,CC-BY/arXiv,Astra: General Interactive World Model with Autoregressive Denoising
"We introduce two new benchmarks REST and REST+(Render-Equivalence Stress Tests) to enable systematic evaluation of cross-modal inconsistency in multimodal large language models (MLLMs). MLLMs are trained to represent vision and language in the same embedding space, yet they cannot perform the same tasks in both modalities. Our benchmarks contain samples with the same semantic information in three modalities (image, text, mixed) and we show that state-of-the-art MLLMs cannot consistently reason over these different modalities. We evaluate 15 MLLMs and find that the degree of modality inconsistency varies substantially, even when accounting for problems with text recognition (OCR). Neither rendering text as image nor rendering an image as text solves the inconsistency. Even if OCR is correct, we find that visual characteristics (text colour and resolution, but not font) and the number of vision tokens have an impact on model performance. Finally, we find that our consistency score correlates with the modality gap between text and images, highlighting a mechanistic interpretation of cross-modal inconsistent MLLMs.",0,arxiv,AI,CC-BY/arXiv,"Same Content, Different Answers: Cross-Modal Inconsistency in MLLMs"
"Quantum Error Correction (QEC) decoding faces a fundamental accuracy-efficiency tradeoff. Classical methods like Minimum Weight Perfect Matching (MWPM) exhibit variable performance across noise models and suffer from polynomial complexity, while tensor network decoders achieve high accuracy but at prohibitively high computational cost. Recent neural decoders reduce complexity but lack the accuracy needed to compete with computationally expensive classical methods. We introduce SAQ-Decoder, a unified framework combining transformer-based learning with constraint aware post-processing that achieves both near Maximum Likelihood (ML) accuracy and linear computational scalability with respect to the syndrome size. Our approach combines a dual-stream transformer architecture that processes syndromes and logical information with asymmetric attention patterns, and a novel differentiable logical loss that directly optimizes Logical Error Rates (LER) through smooth approximations over finite fields. SAQ-Decoder achieves near-optimal performance, with error thresholds of 10.99% (independent noise) and 18.6% (depolarizing noise) on toric codes that approach the ML bounds of 11.0% and 18.9% while outperforming existing neural and classical baselines in accuracy, complexity, and parameter efficiency. Our findings establish that learned decoders can simultaneously achieve competitive decoding accuracy and computational efficiency, addressing key requirements for practical fault-tolerant quantum computing systems.",0,arxiv,AI,CC-BY/arXiv,SAQ: Stabilizer-Aware Quantum Error Correction Decoder
"While scaling laws for Large Language Models (LLMs) traditionally focus on proxy metrics like pretraining loss, predicting downstream task performance has been considered unreliable. This paper challenges that view by proposing a direct framework to model the scaling of benchmark performance from the training budget. We find that for a fixed token-to-parameter ratio, a simple power law can accurately describe the scaling behavior of log accuracy on multiple popular downstream tasks. Our results show that the direct approach extrapolates better than the previously proposed two-stage procedure, which is prone to compounding errors. Furthermore, we introduce functional forms that predict accuracy across token-to-parameter ratios and account for inference compute under repeated sampling. We validate our findings on models with up to 17B parameters trained on up to 350B tokens across two dataset mixtures. To support reproducibility and encourage future research, we release the complete set of pretraining losses and downstream evaluation results.",0,arxiv,AI,CC-BY/arXiv,Revisiting the Scaling Properties of Downstream Metrics in Large Language Model Training
"Retrieval-Augmented Generation (RAG) improves the factuality of large language models (LLMs) by grounding outputs in retrieved evidence, but faithfulness failures, where generations contradict or extend beyond the provided sources, remain a critical challenge. Existing hallucination detection methods for RAG often rely either on large-scale detector training, which requires substantial annotated data, or on querying external LLM judges, which leads to high inference costs. Although some approaches attempt to leverage internal representations of LLMs for hallucination detection, their accuracy remains limited. Motivated by recent advances in mechanistic interpretability, we employ sparse autoencoders (SAEs) to disentangle internal activations, successfully identifying features that are specifically triggered during RAG hallucinations. Building on a systematic pipeline of information-based feature selection and additive feature modeling, we introduce RAGLens, a lightweight hallucination detector that accurately flags unfaithful RAG outputs using LLM internal representations. RAGLens not only achieves superior detection performance compared to existing methods, but also provides interpretable rationales for its decisions, enabling effective post-hoc mitigation of unfaithful RAG. Finally, we justify our design choices and reveal new insights into the distribution of hallucination-related signals within LLMs. The code is available at https://github.com/Teddy-XiongGZ/RAGLens.",0,arxiv,AI,CC-BY/arXiv,Toward Faithful Retrieval-Augmented Generation with Sparse Autoencoders
"Visual reasoning is challenging, requiring both precise object grounding and understanding complex spatial relationships. Existing methods fall into two camps: language-only chain-of-thought approaches, which demand large-scale (image, query, answer) supervision, and program-synthesis approaches which use pre-trained models and avoid training, but suffer from flawed logic and erroneous grounding. We propose an annotation-free training framework that improves both reasoning and grounding. Our framework uses AI-powered verifiers: an LLM verifier refines LLM reasoning via reinforcement learning, while a VLM verifier strengthens visual grounding through automated hard-negative mining, eliminating the need for ground truth labels. This design combines the strengths of modern AI systems: advanced language-only reasoning models for decomposing spatial queries into simpler subtasks, and strong vision specialist models improved via performant VLM critics. We evaluate our approach across diverse spatial reasoning tasks, and show that our method improves visual reasoning and surpasses open-source and proprietary models, while with our improved visual grounding model we further outperform recent text-only visual reasoning methods. Project webpage: https://glab-caltech.github.io/valor/",0,arxiv,AI,CC-BY/arXiv,"No Labels, No Problem: Training Visual Reasoners with Multimodal Verifiers"
"Real-world datasets often exhibit temporal dynamics characterized by evolving data distributions. Disregarding this phenomenon, commonly referred to as concept drift, can significantly diminish a model's predictive accuracy. Furthermore, the presence of hyperparameters in online models exacerbates this issue. These parameters are typically fixed and cannot be dynamically adjusted by the user in response to the evolving data distribution. Gaussian Process (GP) models offer powerful non-parametric regression capabilities with uncertainty quantification, making them ideal for modeling complex data relationships in an online setting. However, conventional online GP methods face several critical limitations, including a lack of drift-awareness, reliance on fixed hyperparameters, vulnerability to data snooping, absence of a principled decay mechanism, and memory inefficiencies. In response, we propose DAO-GP (Drift-Aware Online Gaussian Process), a novel, fully adaptive, hyperparameter-free, decayed, and sparse non-linear regression model. DAO-GP features a built-in drift detection and adaptation mechanism that dynamically adjusts model behavior based on the severity of drift. Extensive empirical evaluations confirm DAO-GP's robustness across stationary conditions, diverse drift types (abrupt, incremental, gradual), and varied data characteristics. Analyses demonstrate its dynamic adaptation, efficient in-memory and decay-based management, and evolving inducing points. Compared with state-of-the-art parametric and non-parametric models, DAO-GP consistently achieves superior or competitive performance, establishing it as a drift-resilient solution for online non-linear regression.",0,arxiv,AI,CC-BY/arXiv,DAO-GP Drift Aware Online Non-Linear Regression Gaussian-Process
"Large Language Models (LLMs) have recently demonstrated remarkable performance in generating high-quality tabular synthetic data. In practice, two primary approaches have emerged for adapting LLMs to tabular data generation: (i) fine-tuning smaller models directly on tabular datasets, and (ii) prompting larger models with examples provided in context. In this work, we show that popular implementations from both regimes exhibit a tendency to compromise privacy by reproducing memorized patterns of numeric digits from their training data. To systematically analyze this risk, we introduce a simple No-box Membership Inference Attack (MIA) called LevAtt that assumes adversarial access to only the generated synthetic data and targets the string sequences of numeric digits in synthetic observations. Using this approach, our attack exposes substantial privacy leakage across a wide range of models and datasets, and in some cases, is even a perfect membership classifier on state-of-the-art models. Our findings highlight a unique privacy vulnerability of LLM-based synthetic data generation and the need for effective defenses. To this end, we propose two methods, including a novel sampling strategy that strategically perturbs digits during generation. Our evaluation demonstrates that this approach can defeat these attacks with minimal loss of fidelity and utility of the synthetic data.",0,arxiv,AI,CC-BY/arXiv,When Tables Leak: Attacking String Memorization in LLM-Based Tabular Data Generation
"Image captioning is essential in many fields including assisting visually impaired individuals, improving content management systems, and enhancing human-computer interaction. However, a recent challenge in this domain is dealing with low-resolution image (LRI). While performance can be improved by using larger models like transformers for encoding, these models are typically heavyweight, demanding significant computational resources and memory, leading to challenges in retraining. To address this, the proposed SOLI (Siamese-Driven Optimization for Low-Resolution Image Latent Embedding in Image Captioning) approach presents a solution specifically designed for lightweight, low-resolution images captioning. It employs a Siamese network architecture to optimize latent embeddings, enhancing the efficiency and accuracy of the image-to-text translation process. By focusing on a dual-pathway neural network structure, SOLI minimizes computational overhead without sacrificing performance, making it an ideal choice for training on resource-constrained scenarios.",0,arxiv,AI,CC-BY/arXiv,Siamese-Driven Optimization for Low-Resolution Image Latent Embedding in Image Captioning
"LLM agents are widely deployed in complex interactive tasks, yet privacy constraints often preclude centralized optimization and co-evolution across dynamic environments. While Federated Learning (FL) has proven effective on static datasets, its extension to the open-ended self-evolution of agents remains underexplored. Directly applying standard FL is challenging: heterogeneous tasks and sparse, trajectory-level rewards introduce severe gradient conflicts, destabilizing the global optimization process. To bridge this gap, we propose Fed-SE, a Federated Self-Evolution framework for LLM agents. Fed-SE establishes a local evolution-global aggregation paradigm. Locally, agents employ parameter-efficient fine-tuning on filtered, high-return trajectories to achieve stable gradient updates. Globally, Fed-SE aggregates updates within a low-rank subspace that disentangles environment-specific dynamics, effectively reducing negative transfer across clients. Experiments across five heterogeneous environments demonstrate that Fed-SE improves average task success rates by approximately 18% over federated baselines, validating its effectiveness in robust cross-environment knowledge transfer in privacy-constrained deployments.",0,arxiv,AI,CC-BY/arXiv,Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents
"The widespread use of big data across sectors has raised major privacy concerns, especially when sensitive information is shared or analyzed. Regulations such as GDPR and HIPAA impose strict controls on data handling, making it difficult to balance the need for insights with privacy requirements. Synthetic data offers a promising solution by creating artificial datasets that reflect real patterns without exposing sensitive information. However, traditional synthetic data methods often fail to capture complex, implicit rules that link different elements of the data and are essential in domains like healthcare. They may reproduce explicit patterns but overlook domain-specific constraints that are not directly stated yet crucial for realism and utility. For example, prescription guidelines that restrict certain medications for specific conditions or prevent harmful drug interactions may not appear explicitly in the original data. Synthetic data generated without these implicit rules can lead to medically inappropriate or unrealistic profiles. To address this gap, we propose ContextGAN, a Context-Aware Differentially Private Generative Adversarial Network that integrates domain-specific rules through a constraint matrix encoding both explicit and implicit knowledge. The constraint-aware discriminator evaluates synthetic data against these rules to ensure adherence to domain constraints, while differential privacy protects sensitive details from the original data. We validate ContextGAN across healthcare, security, and finance, showing that it produces high-quality synthetic data that respects domain rules and preserves privacy. Our results demonstrate that ContextGAN improves realism and utility by enforcing domain constraints, making it suitable for applications that require compliance with both explicit patterns and implicit rules under strict privacy guarantees.",0,arxiv,AI,CC-BY/arXiv,Differentially Private Synthetic Data Generation Using Context-Aware GANs
"Foundation agents have rapidly advanced in their ability to reason and interact with real environments, making the evaluation of their core capabilities increasingly important. While many benchmarks have been developed to assess agent performance, most concentrate on academic settings or artificially designed scenarios while overlooking the challenges that arise in real applications. To address this issue, we focus on a highly practical real-world setting, the e-commerce domain, which involves a large volume of diverse user interactions, dynamic market conditions, and tasks directly tied to real decision-making processes. To this end, we introduce EcomBench, a holistic E-commerce Benchmark designed to evaluate agent performance in realistic e-commerce environments. EcomBench is built from genuine user demands embedded in leading global e-commerce ecosystems and is carefully curated and annotated through human experts to ensure clarity, accuracy, and domain relevance. It covers multiple task categories within e-commerce scenarios and defines three difficulty levels that evaluate agents on key capabilities such as deep information retrieval, multi-step reasoning, and cross-source knowledge integration. By grounding evaluation in real e-commerce contexts, EcomBench provides a rigorous and dynamic testbed for measuring the practical capabilities of agents in modern e-commerce.",0,arxiv,AI,CC-BY/arXiv,EcomBench: Towards Holistic Evaluation of Foundation Agents in E-commerce
"Craig interpolation and uniform interpolation have many applications in knowledge representation, including explainability, forgetting, modularization and reuse, and even learning. At the same time, many relevant knowledge representation formalisms do in general not have Craig or uniform interpolation, and computing interpolants in practice is challenging. We have a closer look at two prominent knowledge representation formalisms, description logics and logic programming, and discuss theoretical results and practical methods for computing interpolants.",0,arxiv,AI,CC-BY/arXiv,Interpolation in Knowledge Representation
"Window attention and linear attention represent two principal strategies for mitigating the quadratic complexity and ever-growing KV cache in Vision-Language Models (VLMs). However, we observe that window-based VLMs suffer performance degradation when sequence length exceeds the window size, while linear attention underperforms on information-intensive tasks such as OCR and document understanding. To overcome these limitations, we propose InfiniteVL, a linear-complexity VLM architecture that synergizes sliding window attention (SWA) with Gated DeltaNet. For achieving competitive multimodal performance under constrained resources, we design a three-stage training strategy comprising distillation pretraining, instruction tuning, and long-sequence SFT. Remarkably, using less than 2\% of the training data required by leading VLMs, InfiniteVL not only substantially outperforms previous linear-complexity VLMs but also matches the performance of leading Transformer-based VLMs, while demonstrating effective long-term memory retention. Compared to similar-sized Transformer-based VLMs accelerated by FlashAttention-2, InfiniteVL achieves over 3.6\times inference speedup while maintaining constant latency and memory footprint. In streaming video understanding scenarios, it sustains a stable 24 FPS real-time prefill speed while preserving long-term memory cache. Code and models are available at https://github.com/hustvl/InfiniteVL.",0,arxiv,AI,CC-BY/arXiv,"InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models"
"The rapid proliferation of generative components, such as LoRAs, has created a vast but unstructured ecosystem. Existing discovery methods depend on unreliable user descriptions or biased popularity metrics, hindering usability. We present CARLoS, a large-scale framework for characterizing LoRAs without requiring additional metadata. Analyzing over 650 LoRAs, we employ them in image generation over a variety of prompts and seeds, as a credible way to assess their behavior. Using CLIP embeddings and their difference to a base-model generation, we concisely define a three-part representation: Directions, defining semantic shift; Strength, quantifying the significance of the effect; and Consistency, quantifying how stable the effect is. Using these representations, we develop an efficient retrieval framework that semantically matches textual queries to relevant LoRAs while filtering overly strong or unstable ones, outperforming textual baselines in automated and human evaluations. While retrieval is our primary focus, the same representation also supports analyses linking Strength and Consistency to legal notions of substantiality and volition, key considerations in copyright, positioning CARLoS as a practical system with broader relevance for LoRA analysis.",0,arxiv,AI,CC-BY/arXiv,CARLoS: Retrieval via Concise Assessment Representation of LoRAs at Scale
"Recent research in Vision-Language Models (VLMs) has significantly advanced our capabilities in cross-modal reasoning. However, existing methods suffer from performance degradation with domain changes or require substantial computational resources for fine-tuning in new domains. To address this issue, we develop a new adaptation method for large vision-language models, called \textit{Training-free Dual Hyperbolic Adapters} (T-DHA). We characterize the vision-language relationship between semantic concepts, which typically has a hierarchical tree structure, in the hyperbolic space instead of the traditional Euclidean space. Hyperbolic spaces exhibit exponential volume growth with radius, unlike the polynomial growth in Euclidean space. We find that this unique property is particularly effective for embedding hierarchical data structures using the PoincarÃ© ball model, achieving significantly improved representation and discrimination power. Coupled with negative learning, it provides more accurate and robust classifications with fewer feature dimensions. Our extensive experimental results on various datasets demonstrate that the T-DHA method significantly outperforms existing state-of-the-art methods in few-shot image recognition and domain generalization tasks.",0,arxiv,AI,CC-BY/arXiv,Training-Free Dual Hyperbolic Adapters for Better Cross-Modal Reasoning
"Gradually growing the depth of Transformers during training can not only reduce training cost but also lead to improved reasoning performance, as shown by MIDAS (Saunshi et al., 2024). Thus far, however, a mechanistic understanding of these gains has been missing. In this work, we establish a connection to recent work showing that layers in the second half of non-grown, pre-layernorm Transformers contribute much less to the final output distribution than those in the first half - also known as the Curse of Depth (Sun et al., 2025, CsordÃ¡s et al., 2025). Using depth-wise analyses, we demonstrate that growth via gradual middle stacking yields more effective utilization of model depth, alters the residual stream structure, and facilitates the formation of permutable computational blocks. In addition, we propose a lightweight modification of MIDAS that yields further improvements in downstream reasoning benchmarks. Overall, this work highlights how the gradual growth of model depth can lead to the formation of distinct computational circuits and overcome the limited depth utilization seen in standard non-grown models.",0,arxiv,AI,CC-BY/arXiv,Do Depth-Grown Models Overcome the Curse of Depth? An In-Depth Analysis
"As AI-based code generation becomes widespread, researchers are investigating the calibration of code LLMs - ensuring their confidence scores faithfully represent the true likelihood of code correctness. To do so, we investigate multicalibration, which can capture additional factors about a coding problem, such as complexity, code length, or programming language used. We study four multicalibration approaches on three function synthesis benchmarks, using latest-generation code LLMs (Qwen3 Coder, GPT-OSS, DeepSeek-R1-Distill). Our results demonstrate that multicalibration can yield distinct improvements over both uncalibrated token likelihoods (+1.03 in skill score) and baseline calibrations (+0.37 in skill score). We study the influence of the aforementioned factors in ablations, and make our dataset (consisting of code generations, likelihoods, and correctness labels) available for future research on code LLM calibration.",0,arxiv,AI,CC-BY/arXiv,Multicalibration for LLM-based Code Generation
"With the rise of large language models, service providers offer language models as a service, enabling users to fine-tune customized models via uploaded private datasets. However, this raises concerns about sensitive data leakage. Prior methods, relying on differential privacy within device-cloud collaboration frameworks, struggle to balance privacy and utility, exposing users to inference attacks or degrading fine-tuning performance. To address this, we propose PrivTune, an efficient and privacy-preserving fine-tuning framework via Split Learning (SL). The key idea of PrivTune is to inject crafted noise into token representations from the SL bottom model, making each token resemble the $n$-hop indirect neighbors. PrivTune formulates this as an optimization problem to compute the optimal noise vector, aligning with defense-utility goals. On this basis, it then adjusts the parameters (i.e., mean) of the $d_Ï‡$-Privacy noise distribution to align with the optimization direction and scales the noise according to token importance to minimize distortion. Experiments on five datasets (covering both classification and generation tasks) against three embedding inversion and three attribute inference attacks show that, using RoBERTa on the Stanford Sentiment Treebank dataset, PrivTune reduces the attack success rate to 10% with only a 3.33% drop in utility performance, outperforming state-of-the-art baselines.",0,arxiv,AI,CC-BY/arXiv,PrivTune: Efficient and Privacy-Preserving Fine-Tuning of Large Language Models via Device-Cloud Collaboration
"Despite advancements in machine learning for security, rule-based detection remains prevalent in Security Operations Centers due to the resource intensiveness and skill gap associated with ML solutions. While traditional rule-based methods offer efficiency, their rigidity leads to high false positives or negatives and requires continuous manual maintenance. This paper proposes a novel, two-stage hybrid framework to democratize ML-based threat detection. The first stage employs intentionally loose YARA rules for coarse-grained filtering, optimized for high recall. The second stage utilizes an ML classifier to filter out false positives from the first stage's output. To overcome data scarcity, the system leverages Simula, a seedless synthetic data generation framework, enabling security analysts to create high-quality training datasets without extensive data science expertise or pre-labeled examples. A continuous feedback loop incorporates real-time investigation results to adaptively tune the ML model, preventing rule degradation.   This proposed model with active learning has been rigorously tested for a prolonged time in a production environment spanning tens of thousands of systems. The system handles initial raw log volumes often reaching 250 billion events per day, significantly reducing them through filtering and ML inference to a handful of daily tickets for human investigation. Live experiments over an extended timeline demonstrate a general improvement in the model's precision over time due to the active learning feature. This approach offers a self-sustained, low-overhead, and low-maintenance solution, allowing security professionals to guide model learning as expert ``teachers''.",0,arxiv,AI,CC-BY/arXiv,Democratizing ML for Enterprise Security: A Self-Sustained Attack Detection Framework
"Foundation models pretrained on large data have demonstrated remarkable zero-shot generalization capabilities across domains. Building on the success of TabPFN for tabular data and its recent extension to time series, we investigate whether graph node classification can be effectively reformulated as a tabular learning problem. We introduce TabPFN-GN, which transforms graph data into tabular features by extracting node attributes, structural properties, positional encodings, and optionally smoothed neighborhood features. This enables TabPFN to perform direct node classification without any graph-specific training or language model dependencies. Our experiments on 12 benchmark datasets reveal that TabPFN-GN achieves competitive performance with GNNs on homophilous graphs and consistently outperforms them on heterophilous graphs. These results demonstrate that principled feature engineering can bridge the gap between tabular and graph domains, providing a practical alternative to task-specific GNN training and LLM-dependent graph foundation models.",0,arxiv,AI,CC-BY/arXiv,Can TabPFN Compete with GNNs for Node Classification via Graph Tabularization?
"Document shadow removal is essential for enhancing the clarity of digitized documents. Preserving high-frequency details (e.g., text edges and lines) is critical in this process because shadows often obscure or distort fine structures. This paper proposes a matte vision transformer (MatteViT), a novel shadow removal framework that applies spatial and frequency-domain information to eliminate shadows while preserving fine-grained structural details. To effectively retain these details, we employ two preservation strategies. First, our method introduces a lightweight high-frequency amplification module (HFAM) that decomposes and adaptively amplifies high-frequency components. Second, we present a continuous luminance-based shadow matte, generated using a custom-built matte dataset and shadow matte generator, which provides precise spatial guidance from the earliest processing stage. These strategies enable the model to accurately identify fine-grained regions and restore them with high fidelity. Extensive experiments on public benchmarks (RDD and Kligler) demonstrate that MatteViT achieves state-of-the-art performance, providing a robust and practical solution for real-world document shadow removal. Furthermore, the proposed method better preserves text-level details in downstream tasks, such as optical character recognition, improving recognition performance over prior methods.",0,arxiv,AI,CC-BY/arXiv,MatteViT: High-Frequency-Aware Document Shadow Removal with Shadow Matte Guidance
"This paper addresses the challenge of aligning large language models (LLMs) with diverse human preferences within federated learning (FL) environments, where standard methods often fail to adequately represent diverse viewpoints. We introduce a comprehensive evaluation framework that systematically assesses the trade-off between alignment quality and fairness when using different aggregation strategies for human preferences. In our federated setting, each group locally evaluates rollouts and produces reward signals, and the server aggregates these group-level rewards without accessing any raw data. Specifically, we evaluate standard reward aggregation techniques (min, max, and average) and introduce a novel adaptive scheme that dynamically adjusts preference weights based on a group's historical alignment performance. Our experiments on question-answering (Q/A) tasks using a PPO-based RLHF pipeline demonstrate that our adaptive approach consistently achieves superior fairness while maintaining competitive alignment scores. This work offers a robust methodology for evaluating LLM behavior across diverse populations and provides a practical solution for developing truly pluralistic and fairly aligned models.",0,arxiv,AI,CC-BY/arXiv,A Systematic Evaluation of Preference Aggregation in Federated RLHF for Pluralistic Alignment of LLMs
"We propose a post-training method for lower-resource languages that preserves fluency of language models even when aligned by disfluent reward models. Preference-optimization is now a well-researched topic, but previous work has mostly addressed models for English and Chinese. Lower-resource languages lack both datasets written by native speakers and language models capable of generating fluent synthetic data. Thus, in this work, we focus on developing a fluent preference-aligned language model without any instruction-tuning data in the target language. Our approach uses an on-policy training method, which we compare with two common approaches: supervised finetuning on machine-translated data and multilingual finetuning. We conduct a case study on Norwegian BokmÃ¥l and evaluate fluency through native-speaker assessments. The results show that the on-policy aspect is crucial and outperforms the alternatives without relying on any hard-to-obtain data.",0,arxiv,AI,CC-BY/arXiv,Fluent Alignment with Disfluent Judges: Post-training for Lower-resource Languages
"Diffusion models have achieved remarkable success in image synthesis. However, addressing artifacts and unrealistic regions remains a critical challenge. We propose self-refining diffusion, a novel framework that enhances image generation quality by detecting these flaws. The framework employs an explainable artificial intelligence (XAI)-based flaw highlighter to produce flaw activation maps (FAMs) that identify artifacts and unrealistic regions. These FAMs improve reconstruction quality by amplifying noise in flawed regions during the forward process and by focusing on these regions during the reverse process. The proposed approach achieves up to a 27.3% improvement in FrÃ©chet inception distance across various diffusion-based models, demonstrating consistently strong performance on diverse datasets. It also shows robust effectiveness across different tasks, including image generation, text-to-image generation, and inpainting. These results demonstrate that explainable AI techniques can extend beyond interpretability to actively contribute to image refinement. The proposed framework offers a versatile and effective approach applicable to various diffusion models and tasks, significantly advancing the field of image synthesis.",0,arxiv,AI,CC-BY/arXiv,Refining Visual Artifacts in Diffusion Models via Explainable AI-based Flaw Activation Maps
"Agentic AI marks a major shift in how autonomous systems reason, plan, and execute multi-step tasks. Unlike traditional single model prompting, agentic workflows integrate multiple specialized agents with different Large Language Models(LLMs), tool-augmented capabilities, orchestration logic, and external system interactions to form dynamic pipelines capable of autonomous decision-making and action. As adoption accelerates across industry and research, organizations face a central challenge: how to design, engineer, and operate production-grade agentic AI workflows that are reliable, observable, maintainable, and aligned with safety and governance requirements. This paper provides a practical, end-to-end guide for designing, developing, and deploying production-quality agentic AI systems. We introduce a structured engineering lifecycle encompassing workflow decomposition, multi-agent design patterns, Model Context Protocol(MCP), and tool integration, deterministic orchestration, Responsible-AI considerations, and environment-aware deployment strategies. We then present nine core best practices for engineering production-grade agentic AI workflows, including tool-first design over MCP, pure-function invocation, single-tool and single-responsibility agents, externalized prompt management, Responsible-AI-aligned model-consortium design, clean separation between workflow logic and MCP servers, containerized deployment for scalable operations, and adherence to the Keep it Simple, Stupid (KISS) principle to maintain simplicity and robustness. To demonstrate these principles in practice, we present a comprehensive case study: a multimodal news-analysis and media-generation workflow. By combining architectural guidance, operational patterns, and practical implementation insights, this paper offers a foundational reference to build robust, extensible, and production-ready agentic AI workflows.",0,arxiv,AI,CC-BY/arXiv,"A Practical Guide for Designing, Developing, and Deploying Production-Grade Agentic AI Workflows"
"Bridging the sim-to-real gap remains a fundamental challenge in robotics, as accurate dynamic parameter estimation is essential for reliable model-based control, realistic simulation, and safe deployment of manipulators. Traditional analytical approaches often fall short when faced with complex robot structures and interactions. Data-driven methods offer a promising alternative, yet conventional neural networks such as recurrent models struggle to capture long-range dependencies critical for accurate estimation. In this study, we propose a Transformer-based approach for dynamic parameter estimation, supported by an automated pipeline that generates diverse robot models and enriched trajectory data using Jacobian-derived features. The dataset consists of 8,192 robots with varied inertial and frictional properties. Leveraging attention mechanisms, our model effectively captures both temporal and spatial dependencies. Experimental results highlight the influence of sequence length, sampling rate, and architecture, with the best configuration (sequence length 64, 64 Hz, four layers, 32 heads) achieving a validation R2 of 0.8633. Mass and inertia are estimated with near-perfect accuracy, Coulomb friction with moderate-to-high accuracy, while viscous friction and distal link center-of-mass remain more challenging. These results demonstrate that combining Transformers with automated dataset generation and kinematic enrichment enables scalable, accurate dynamic parameter estimation, contributing to improved sim-to-real transfer in robotic systems",0,arxiv,AI,CC-BY/arXiv,Data-Driven Dynamic Parameter Learning of manipulator robots
"Reconfigurable intelligent surface (RIS) and simultaneously transmitting and reflecting RIS (STAR-RIS) have emerged as key enablers for enhancing wireless coverage and capacity in next-generation networks. When mounted on unmanned aerial vehicles (UAVs), they benefit from flexible deployment and improved line-of-sight conditions. Despite their promising potential, a comprehensive performance comparison between aerial RIS and STAR-RIS architectures has not been thoroughly investigated. This letter presents a detailed performance comparison between aerial RIS and STAR-RIS in three-dimensional wireless environments. Accurate channel models incorporating directional radiation patterns are established, and the influence of deployment altitude and orientation is thoroughly examined. To optimize the system sum-rate, we formulate joint optimization problems for both architectures and propose an efficient solution based on the weighted minimum mean square error and block coordinate descent algorithms. Simulation results reveal that STAR-RIS outperforms RIS in low-altitude scenarios due to its full-space coverage capability, whereas RIS delivers better performance near the base station at higher altitudes. The findings provide practical insights for the deployment of aerial intelligent surfaces in future 6G communication systems.",0,arxiv,AI,CC-BY/arXiv,Performance Comparison of Aerial RIS and STAR-RIS in 3D Wireless Environments
"Foundation models (FMs) are increasingly assuming the role of the ""brain"" of AI agents. While recent efforts have begun to equip FMs with native single-agent abilities -- such as GUI interaction or integrated tool use -- we argue that the next frontier is endowing FMs with native multi-agent intelligence. We identify four core capabilities of FMs in multi-agent contexts: understanding, planning, efficient communication, and adaptation. Contrary to assumptions about the spontaneous emergence of such abilities, we provide extensive empirical evidence across 41 large language models showing that strong single-agent performance alone does not automatically yield robust multi-agent intelligence. To address this gap, we outline key research directions -- spanning dataset construction, evaluation, training paradigms, and safety considerations -- for building FMs with native multi-agent intelligence.",0,arxiv,AI,CC-BY/arXiv,Towards Foundation Models with Native Multi-Agent Intelligence
"Currently, there exists a fundamental divide between the ""cognitive black box"" (implicit intuition) of human experts and the ""computational black box"" (untrustworthy decision-making) of artificial intelligence (AI). This paper proposes a new paradigm of ""human-AI collaborative cognitive enhancement,"" aiming to transform the dual black boxes into a composable, auditable, and extensible ""functional white-box"" system through structured ""meta-interaction."" The core breakthrough lies in the ""plug-and-play cognitive framework""--a computable knowledge package that can be extracted from expert dialogues and loaded into the Recursive Adversarial Meta-Thinking Network (RAMTN). This enables expert thinking, such as medical diagnostic logic and teaching intuition, to be converted into reusable and scalable public assets, realizing a paradigm shift from ""AI as a tool"" to ""AI as a thinking partner."" This work not only provides the first engineering proof for ""cognitive equity"" but also opens up a new path for AI governance: constructing a verifiable and intervenable governance paradigm through ""transparency of interaction protocols"" rather than prying into the internal mechanisms of models. The framework is open-sourced to promote technology for good and cognitive inclusion. This paper is an independent exploratory research conducted by the author. All content presented, including the theoretical framework (RAMTN), methodology (meta-interaction), system implementation, and case validation, constitutes the author's individual research achievements.",0,arxiv,AI,CC-BY/arXiv,Deconstructing the Dual Black Box:A Plug-and-Play Cognitive Framework for Human-AI Collaborative Enhancement and Its Implications for AI Governance
"Skin color has historically been a focal point of discrimination, yet fairness research in machine learning for medical imaging often relies on coarse subgroup categories, overlooking individual-level variations. Such group-based approaches risk obscuring biases faced by outliers within subgroups. This study introduces a distribution-based framework for evaluating and mitigating individual fairness in skin lesion classification. We treat skin tone as a continuous attribute rather than a categorical label, and employ kernel density estimation (KDE) to model its distribution. We further compare twelve statistical distance metrics to quantify disparities between skin tone distributions and propose a distance-based reweighting (DRW) loss function to correct underrepresentation in minority tones. Experiments across CNN and Transformer models demonstrate: (i) the limitations of categorical reweighting in capturing individual-level disparities, and (ii) the superior performance of distribution-based reweighting, particularly with Fidelity Similarity (FS), Wasserstein Distance (WD), Hellinger Metric (HM), and Harmonic Mean Similarity (HS). These findings establish a robust methodology for advancing fairness at individual level in dermatological AI systems, and highlight broader implications for sensitive continuous attributes in medical image analysis.",0,arxiv,AI,CC-BY/arXiv,Mitigating Individual Skin Tone Bias in Skin Lesion Classification through Distribution-Aware Reweighting
"Visual language models encounter challenges in computational efficiency and latency, primarily due to the substantial redundancy in the token representations of high-resolution images and videos. Current attention/similarity-based compression algorithms suffer from either position bias or class imbalance, leading to significant accuracy degradation. They also fail to generalize to shallow LLM layers, which exhibit weaker cross-modal interactions. To address this, we extend token compression to the visual encoder through an effective iterative merging scheme that is orthogonal in spatial axes to accelerate the computation across the entire VLM. Furthermoer, we integrate a spectrum pruning unit into LLM through an attention/similarity-free low-pass filter, which gradually prunes redundant visual tokens and is fully compatible to modern FlashAttention. On this basis, we propose Lossless Ultimate Vision tokens Compression (LUVC) framework. LUVC systematically compresses visual tokens until complete elimination at the final layer of LLM, so that the high-dimensional visual features are gradually fused into the multimodal queries. The experiments show that LUVC achieves a 2 speedup inference in language model with negligible accuracy degradation, and the training-free characteristic enables immediate deployment across multiple VLMs.",0,arxiv,AI,CC-BY/arXiv,Towards Lossless Ultimate Vision Token Compression for VLMs
"The performance of algorithms, methods, and models tends to depend heavily on the distribution of cases on which they are applied, this distribution being specific to the applicative domain. After performing an evaluation in several domains, it is highly informative to compute a (weighted) mean performance and, as shown in this paper, to scrutinize what happens during this averaging. To achieve this goal, we adopt a probabilistic framework and consider a performance as a probability measure (e.g., a normalized confusion matrix for a classification task). It appears that the corresponding weighted mean is known to be the summarization, and that only some remarkable scores assign to the summarized performance a value equal to a weighted arithmetic mean of the values assigned to the domain-specific performances. These scores include the family of ranking scores, a continuum parameterized by user preferences, and that the weights to consider in the arithmetic mean depend on the user preferences. Based on this, we rigorously define four domains, named easiest, most difficult, preponderant, and bottleneck domains, as functions of user preferences. After establishing the theory in a general setting, regardless of the task, we develop new visual tools for two-class classification.",0,arxiv,AI,CC-BY/arXiv,Multi-domain performance analysis with scores tailored to user preferences
"This paper introduces the first publicly available dataset for Automatic Essay Scoring (AES) and feedback generation in Basque, targeting the CEFR C1 proficiency level. The dataset comprises 3,200 essays from HABE, each annotated by expert evaluators with criterion specific scores covering correctness, richness, coherence, cohesion, and task alignment enriched with detailed feedback and error examples. We fine-tune open-source models, including RoBERTa-EusCrawl and Latxa 8B/70B, for both scoring and explanation generation. Our experiments show that encoder models remain highly reliable for AES, while supervised fine-tuning (SFT) of Latxa significantly enhances performance, surpassing state-of-the-art (SoTA) closed-source systems such as GPT-5 and Claude Sonnet 4.5 in scoring consistency and feedback quality. We also propose a novel evaluation methodology for assessing feedback generation, combining automatic consistency metrics with expert-based validation of extracted learner errors. Results demonstrate that the fine-tuned Latxa model produces criterion-aligned, pedagogically meaningful feedback and identifies a wider range of error types than proprietary models. This resource and benchmark establish a foundation for transparent, reproducible, and educationally grounded NLP research in low-resource languages such as Basque.",0,arxiv,AI,CC-BY/arXiv,Automatic Essay Scoring and Feedback Generation in Basque Language Learning
"Multimodal clinical reasoning in the field of gastrointestinal (GI) oncology necessitates the integrated interpretation of endoscopic imagery, radiological data, and biochemical markers. Despite the evident potential exhibited by Multimodal Large Language Models (MLLMs), they frequently encounter challenges such as context dilution and hallucination when confronted with intricate, heterogeneous medical histories. In order to address these limitations, a hierarchical Multi-Agent Framework is proposed, which emulates the collaborative workflow of a human Multidisciplinary Team (MDT). The system attained a composite expert evaluation score of 4.60/5.00, thereby demonstrating a substantial improvement over the monolithic baseline. It is noteworthy that the agent-based architecture yielded the most substantial enhancements in reasoning logic and medical accuracy. The findings indicate that mimetic, agent-based collaboration provides a scalable, interpretable, and clinically robust paradigm for automated decision support in oncology.",0,arxiv,AI,CC-BY/arXiv,Multi-Agent Intelligence for Multidisciplinary Decision-Making in Gastrointestinal Oncology
"ML-Enabled Systems (MLES) are inherently complex since they require multiple components to achieve their business goal. This experience report showcases the software architecture reusability techniques applied while building Ocean Guard, an MLES for anomaly detection in the maritime domain. In particular, it highlights the challenges and lessons learned to reuse the Ports and Adapters pattern to support building multiple microservices from a single codebase. This experience report hopes to inspire software engineers, machine learning engineers, and data scientists to apply the Hexagonal Architecture pattern to build their MLES.",0,arxiv,AI,CC-BY/arXiv,Reusability in MLOps: Leveraging Ports and Adapters to Build a Microservices Architecture for the Maritime Domain
"Aerial Vision-and-Language Navigation (VLN) aims to enable unmanned aerial vehicles (UAVs) to interpret natural language instructions and navigate complex urban environments using onboard visual observation. This task holds promise for real-world applications such as low-altitude inspection, search-and-rescue, and autonomous aerial delivery. Existing methods often rely on panoramic images, depth inputs, or odometry to support spatial reasoning and action planning. These requirements increase system cost and integration complexity, thus hindering practical deployment for lightweight UAVs. We present a unified aerial VLN framework that operates solely on egocentric monocular RGB observations and natural language instructions. The model formulates navigation as a next-token prediction problem, jointly optimizing spatial perception, trajectory reasoning, and action prediction through prompt-guided multi-task learning. Moreover, we propose a keyframe selection strategy to reduce visual redundancy by retaining semantically informative frames, along with an action merging and label reweighting mechanism that mitigates long-tailed supervision imbalance and facilitates stable multi-task co-training. Extensive experiments on the Aerial VLN benchmark validate the effectiveness of our method. Under the challenging monocular RGB-only setting, our model achieves strong results across both seen and unseen environments. It significantly outperforms existing RGB-only baselines and narrows the performance gap with state-of-the-art panoramic RGB-D counterparts. Comprehensive ablation studies further demonstrate the contribution of our task design and architectural choices.",0,arxiv,AI,CC-BY/arXiv,"Aerial Vision-Language Navigation with a Unified Framework for Spatial, Temporal and Embodied Reasoning"
"Recent advances in Multimodal Large Language Models (MLLMs) have enabled their use as intelligent agents for smartphone operation. However, existing methods depend on the Android Debug Bridge (ADB) for data transmission and action execution, limiting their applicability to Android devices. In this work, we introduce the novel Embodied Smartphone Operation (ESO) task and present See-Control, a framework that enables smartphone operation via direct physical interaction with a low-DoF robotic arm, offering a platform-agnostic solution. See-Control comprises three key components: (1) an ESO benchmark with 155 tasks and corresponding evaluation metrics; (2) an MLLM-based embodied agent that generates robotic control commands without requiring ADB or system back-end access; and (3) a richly annotated dataset of operation episodes, offering valuable resources for future research. By bridging the gap between digital agents and the physical world, See-Control provides a concrete step toward enabling home robots to perform smartphone-dependent tasks in realistic environments.",0,arxiv,AI,CC-BY/arXiv,See-Control: A Multimodal Agent Framework for Smartphone Interaction with a Robotic Arm
"Predicting protein secondary structures such as alpha helices, beta sheets, and coils from amino acid sequences is essential for understanding protein function. This work presents a transformer-based model that applies attention mechanisms to protein sequence data to predict structural motifs. A sliding-window data augmentation technique is used on the CB513 dataset to expand the training samples. The transformer shows strong ability to generalize across variable-length sequences while effectively capturing both local and long-range residue interactions.",0,arxiv,AI,CC-BY/arXiv,Protein Secondary Structure Prediction Using Transformers
"Automatic Heuristic Design (AHD) is an effective1 framework for solving complex optimization prob-2 lems. The development of large language mod-3 els (LLMs) enables the automated generation of4 heuristics. Existing LLM-based evolutionary meth-5 ods rely on population strategies and are prone6 to local optima. Integrating LLMs with Monte7 Carlo Tree Search (MCTS) improves the trade-off8 between exploration and exploitation, but multi-9 round cognitive integration remains limited and10 search diversity is constrained. To overcome these11 limitations, this paper proposes a novel cognitive-12 guided MCTS framework (CogMCTS). CogMCTS13 tightly integrates the cognitive guidance mecha-14 nism of LLMs with MCTS to achieve efficient au-15 tomated heuristic optimization. The framework16 employs multi-round cognitive feedback to incor-17 porate historical experience, node information, and18 negative outcomes, dynamically improving heuris-19 tic generation. Dual-track node expansion com-20 bined with elite heuristic management balances the21 exploration of diverse heuristics and the exploita-22 tion of high-quality experience. In addition, strate-23 gic mutation modifies the heuristic forms and pa-24 rameters to further enhance the diversity of the so-25 lution and the overall optimization performance.26 The experimental results indicate that CogMCTS27 outperforms existing LLM-based AHD methods in28 stability, efficiency, and solution quality.",0,arxiv,AI,CC-BY/arXiv,CogMCTS: A Novel Cognitive-Guided Monte Carlo Tree Search Framework for Iterative Heuristic Evolution with Large Language Models
"The Contrastive Language-Image Pre-Training (CLIP) model excels in few-shot learning by aligning visual and textual representations. Our study shows that template-sample similarity (TSS), defined as the resemblance between a text template and an image sample, introduces bias. This bias leads the model to rely on template proximity rather than true sample-to-category alignment, reducing both accuracy and robustness in classification. We present a framework that uses empty prompts, textual inputs that convey the idea of ""emptiness"" without category information. These prompts capture unbiased template features and offset TSS bias. The framework employs two stages. During pre-training, empty prompts reveal and reduce template-induced bias within the CLIP encoder. During few-shot fine-tuning, a bias calibration loss enforces correct alignment between images and their categories, ensuring the model focuses on relevant visual cues. Experiments across multiple benchmarks demonstrate that our template correction method significantly reduces performance fluctuations caused by TSS, yielding higher classification accuracy and stronger robustness. The repository of this project is available at https://github.com/zhenyuZ-HUST/Decoupling-Template-Bias-in-CLIP.",0,arxiv,AI,CC-BY/arXiv,Decoupling Template Bias in CLIP: Harnessing Empty Prompts for Enhanced Few-Shot Learning
"The dynamic nature of interactions between students and GenAI, as well as their relationship to writing quality, remains underexplored. While most research has examined how general-purpose GenAI can support writing, fewer studies have investigated how students interact with pedagogically designed systems across different phases of the writing process. To address this gap, we evaluated a GenAI-driven essay-writing assistant (EWA) designed to support higher education students in argumentative writing. Drawing on 1,282 interaction logs from 32 undergraduates during a two-hour writing session, Sequential Pattern Mining and K-Means clustering were used to identify behavioral patterns. Two clusters emerged: Cluster 1 emphasized outline planning and essay structure, while Cluster 2 focused on content development. A Mann-Whitney U test revealed a moderate effect size (r = 0.36) in the essay Organization dimension, with Cluster 1 showing higher scores. Qualitative analysis indicated that students with better performance actively wrote and shared essay sections with EWA for feedback, rather than interacted passively by asking questions. These findings suggest implications for teaching and system design. Teachers can encourage active engagement, while future EWAs may integrate automatic labeling and monitoring to prompt students to move from questioning to writing, enabling fuller benefits from GenAI-supported learning.",0,arxiv,AI,CC-BY/arXiv,Examining Student Interactions with a Pedagogical AI-Assistant for Essay Writing and their Impact on Students Writing Quality
"Artificial Intelligence (AI) systems are now an integral part of multiple industries. In clinical research, AI supports automated adverse event detection in clinical trials, patient eligibility screening for protocol enrollment, and data quality validation. Beyond healthcare, AI is transforming finance through real-time fraud detection, automated loan risk assessment, and algorithmic decision-making. Similarly, in manufacturing, AI enables predictive maintenance to reduce equipment downtime, enhances quality control through computer-vision inspection, and optimizes production workflows using real-time operational data. While these technologies enhance operational efficiency, they introduce new challenges regarding safety, accountability, and regulatory compliance. To address these concerns, we introduce the SMART+ Framework - a structured model built on the pillars of Safety, Monitoring, Accountability, Reliability, and Transparency, and further enhanced with Privacy & Security, Data Governance, Fairness & Bias, and Guardrails. SMART+ offers a practical, comprehensive approach to evaluating and governing AI systems across industries. This framework aligns with evolving mechanisms and regulatory guidance to integrate operational safeguards, oversight procedures, and strengthened privacy and governance controls. SMART+ demonstrates risk mitigation, trust-building, and compliance readiness. By enabling responsible AI adoption and ensuring auditability, SMART+ provides a robust foundation for effective AI governance in clinical research.",0,arxiv,AI,CC-BY/arXiv,The SMART+ Framework for AI Systems
"Humans act with context and intention, with reasoning playing a central role. While internet-scale data has enabled broad reasoning capabilities in AI systems, grounding these abilities in physical action remains a major challenge. We introduce Lumo-1, a generalist vision-language-action (VLA) model that unifies robot reasoning (""mind"") with robot action (""hand""). Our approach builds upon the general multi-modal reasoning capabilities of pre-trained vision-language models (VLMs), progressively extending them to embodied reasoning and action prediction, and ultimately towards structured reasoning and reasoning-action alignment. This results in a three-stage pre-training pipeline: (1) Continued VLM pre-training on curated vision-language data to enhance embodied reasoning skills such as planning, spatial understanding, and trajectory prediction; (2) Co-training on cross-embodiment robot data alongside vision-language data; and (3) Action training with reasoning process on trajectories collected on Astribot S1, a bimanual mobile manipulator with human-like dexterity and agility. Finally, we integrate reinforcement learning to further refine reasoning-action consistency and close the loop between semantic inference and motor control. Extensive experiments demonstrate that Lumo-1 achieves significant performance improvements in embodied vision-language reasoning, a critical component for generalist robotic control. Real-world evaluations further show that Lumo-1 surpasses strong baselines across a wide range of challenging robotic tasks, with strong generalization to novel objects and environments, excelling particularly in long-horizon tasks and responding to human-natural instructions that require reasoning over strategy, concepts and space.",0,arxiv,AI,CC-BY/arXiv,Mind to Hand: Purposeful Robotic Control via Embodied Reasoning
"Video recordings of open surgeries are greatly required for education and research purposes. However, capturing unobstructed videos is challenging since surgeons frequently block the camera field of view. To avoid occlusion, the positions and angles of the camera must be frequently adjusted, which is highly labor-intensive. Prior work has addressed this issue by installing multiple cameras on a shadowless lamp and arranging them to fully surround the surgical area. This setup increases the chances of some cameras capturing an unobstructed view. However, manual image alignment is needed in post-processing since camera configurations change every time surgeons move the lamp for optimal lighting. This paper aims to fully automate this alignment task. The proposed method identifies frames in which the lighting system moves, realigns them, and selects the camera with the least occlusion to generate a video that consistently presents the surgical field from a fixed perspective. A user study involving surgeons demonstrated that videos generated by our method were superior to those produced by conventional methods in terms of the ease of confirming the surgical area and the comfort during video viewing. Additionally, our approach showed improvements in video quality over existing techniques. Furthermore, we implemented several synthesis options for the proposed view-synthesis method and conducted a user study to assess surgeons' preferences for each option.",0,arxiv,AI,CC-BY/arXiv,Disturbance-Free Surgical Video Generation from Multi-Camera Shadowless Lamps for Open Surgery
"Stock market prediction is a long-standing challenge in finance, as accurate forecasts support informed investment decisions. Traditional models rely mainly on historical prices, but recent work shows that financial news can provide useful external signals. This paper investigates a multimodal approach that integrates companies' news articles with their historical stock data to improve prediction performance. We compare a Graph Neural Network (GNN) model with a baseline LSTM model. Historical data for each company is encoded using an LSTM, while news titles are embedded with a language model. These embeddings form nodes in a heterogeneous graph, and GraphSAGE is used to capture interactions between articles, companies, and industries. We evaluate two targets: a binary direction-of-change label and a significance-based label. Experiments on the US equities and Bloomberg datasets show that the GNN outperforms the LSTM baseline, achieving 53% accuracy on the first target and a 4% precision gain on the second. Results also indicate that companies with more associated news yield higher prediction accuracy. Moreover, headlines contain stronger predictive signals than full articles, suggesting that concise news summaries play an important role in short-term market reactions.",0,arxiv,AI,CC-BY/arXiv,A Hybrid Model for Stock Market Forecasting: Integrating News Sentiment and Time Series Data with Graph Neural Networks
"Recent end-to-end robotic manipulation research increasingly adopts architectures inspired by large language models to enable robust manipulation. However, a critical challenge arises from severe distribution shifts between robotic action data, primarily due to substantial numerical variations in action commands across diverse robotic platforms and tasks, hindering the effective transfer of pretrained knowledge. To address this limitation, we propose a semantically grounded linguistic representation to normalize actions for efficient pretraining. Unlike conventional discretized action representations that are sensitive to numerical scales, the motion representation specifically disregards numeric scale effects, emphasizing directionality instead. This abstraction mitigates distribution shifts, yielding a more generalizable pretraining representation. Moreover, using the motion representation narrows the feature distance between action tokens and standard vocabulary tokens, mitigating modality gaps. Multi-task experiments on two benchmarks demonstrate that the proposed method significantly improves generalization performance and transferability in robotic manipulation tasks.",0,arxiv,AI,CC-BY/arXiv,Bridging Scale Discrepancies in Robotic Control via Language-Based Action Representations
"Large Language Models and multi-agent systems have shown promise in decomposing complex tasks, yet they struggle with long-horizon reasoning tasks and escalating computation cost. This work introduces a hierarchical multi-agent architecture that distributes reasoning across a 64*64 grid of lightweight agents, supported by a selective oracle. A spatial curriculum progressively expands the operational region of the grid, ensuring that agents master easier central tasks before tackling harder peripheral ones. To improve reliability, the system integrates Negative Log-Likelihood as a measure of confidence, allowing the curriculum to prioritize regions where agents are both accurate and well calibrated. A Thompson Sampling curriculum manager adaptively chooses training zones based on competence and NLL-driven reward signals. We evaluate the approach on a spatially grounded Tower of Hanoi benchmark, which mirrors the long-horizon structure of many robotic manipulation and planning tasks. Results demonstrate improved stability, reduced oracle usage, and stronger long-range reasoning from distributed agent cooperation.",0,arxiv,AI,CC-BY/arXiv,Curriculum Guided Massive Multi Agent System Solving For Robust Long Horizon Tasks
"Color image generation has a wide range of applications, but the existing generation models ignore the correlation among color channels, which may lead to chromatic aberration problems. In addition, the data distribution problem of color images has not been systematically elaborated and explained, so that there is still the lack of the theory about measuring different color images datasets. In this paper, we define a new quaternion Wasserstein distance and develop its dual theory. To deal with the quaternion linear programming problem, we derive the strong duality form with helps of quaternion convex set separation theorem and quaternion Farkas lemma. With using quaternion Wasserstein distance, we propose a novel Wasserstein quaternion generative adversarial network. Experiments demonstrate that this novel model surpasses both the (quaternion) generative adversarial networks and the Wasserstein generative adversarial network in terms of generation efficiency and image quality.",0,arxiv,AI,CC-BY/arXiv,A Novel Wasserstein Quaternion Generative Adversarial Network for Color Image Generation
"Ethical awareness is critical for robots operating in human environments, yet existing automated planning tools provide little support. Manually specifying ethical rules is labour-intensive and highly context-specific. We present Principles2Plan, an interactive research prototype demonstrating how a human and a Large Language Model (LLM) can collaborate to produce context-sensitive ethical rules and guide automated planning. A domain expert provides the planning domain, problem details, and relevant high-level principles such as beneficence and privacy. The system generates operationalisable ethical rules consistent with these principles, which the user can review, prioritise, and supply to a planner to produce ethically-informed plans. To our knowledge, no prior system supports users in generating principle-grounded rules for classical planning contexts. Principles2Plan showcases the potential of human-LLM collaboration for making ethical automated planning more practical and feasible.",0,arxiv,AI,CC-BY/arXiv,Principles2Plan: LLM-Guided System for Operationalising Ethical Principles into Plans
"The significant increase in software production, driven by the acceleration of development cycles over the past two decades, has led to a steady rise in software vulnerabilities, as shown by statistics published yearly by the CVE program. The automation of the source code vulnerability detection (CVD) process has thus become essential, and several methods have been proposed ranging from the well established program analysis techniques to the more recent AI-based methods. Our research investigates Large Language Models (LLMs), which are considered among the most performant AI models to date, for the CVD task. The objective is to study their performance and apply different state-of-the-art techniques to enhance their effectiveness for this task. We explore various fine-tuning and prompt engineering settings. We particularly suggest one novel approach for fine-tuning LLMs which we call Double Fine-tuning, and also test the understudied Test-Time fine-tuning approach. We leverage the recent open-source Llama-3.1 8B, with source code samples extracted from BigVul and PrimeVul datasets. Our conclusions highlight the importance of fine-tuning to resolve the task, the performance of Double tuning, as well as the potential of Llama models for CVD. Though prompting proved ineffective, Retrieval augmented generation (RAG) performed relatively well as an example selection technique. Overall, some of our research questions have been answered, and many are still on hold, which leaves us many future work perspectives. Code repository is available here: https://github.com/DynaSoumhaneOuchebara/Llama-based-vulnerability-detection.",0,arxiv,AI,CC-BY/arXiv,Llama-based source code vulnerability detection: Prompt engineering vs Fine tuning
"Social robots must adjust to human proxemic norms to ensure user comfort and engagement. While prior research demonstrates that eye-tracking features reliably estimate comfort in human-human interactions, their applicability to interactions with humanoid robots remains unexplored. In this study, we investigate user comfort with the robot ""Ameca"" across four experimentally controlled distances (0.5 m to 2.0 m) using mobile eye-tracking and subjective reporting (N=19). We evaluate multiple machine learning and deep learning models to estimate comfort based on gaze features. Contrary to previous human-human studies where Transformer models excelled, a Decision Tree classifier achieved the highest performance (F1-score = 0.73), with minimum pupil diameter identified as the most critical predictor. These findings suggest that physiological comfort thresholds in human-robot interaction differ from human-human dynamics and can be effectively modeled using interpretable logic.",0,arxiv,AI,CC-BY/arXiv,SensHRPS: Sensing Comfortable Human-Robot Proxemics and Personal Space With Eye-Tracking
"Accurate and rapid state-of-health (SOH) monitoring plays an important role in indicating energy information for lithium-ion battery-powered portable mobile devices. To confront their variable working conditions, transfer learning (TL) emerges as a promising technique for leveraging knowledge from data-rich source working conditions, significantly reducing the training data required for SOH monitoring from target working conditions. However, traditional TL-based SOH monitoring is infeasible when applied in portable mobile devices since substantial computational resources are consumed during the TL stage and unexpectedly reduce the working endurance. To address these challenges, this paper proposes a lightweight TL-based SOH monitoring approach with constructive incremental transfer learning (CITL). First, taking advantage of the unlabeled data in the target domain, a semi-supervised TL mechanism is proposed to minimize the monitoring residual in a constructive way, through iteratively adding network nodes in the CITL. Second, the cross-domain learning ability of node parameters for CITL is comprehensively guaranteed through structural risk minimization, transfer mismatching minimization, and manifold consistency maximization. Moreover, the convergence analysis of the CITL is given, theoretically guaranteeing the efficacy of TL performance and network compactness. Finally, the proposed approach is verified through extensive experiments with a realistic unmanned air vehicles (UAV) battery dataset collected from dozens of flight missions. Specifically, the CITL outperforms SS-TCA, MMD-LSTM-DA, DDAN, BO-CNN-TL, and AS$^3$LSTM, in SOH estimation by 83.73%, 61.15%, 28.24%, 87.70%, and 57.34%, respectively, as evaluated using the index root mean square error.",0,arxiv,AI,CC-BY/arXiv,A Lightweight Transfer Learning-Based State-of-Health Monitoring with Application to Lithium-ion Batteries in Unmanned Air Vehicles
"Multi-modal large reasoning models (MLRMs) pose significant privacy risks by inferring precise geographic locations from personal images through hierarchical chain-of-thought reasoning. Existing privacy protection techniques, primarily designed for perception-based models, prove ineffective against MLRMs' sophisticated multi-step reasoning processes that analyze environmental cues. We introduce \textbf{ReasonBreak}, a novel adversarial framework specifically designed to disrupt hierarchical reasoning in MLRMs through concept-aware perturbations. Our approach is founded on the key insight that effective disruption of geographic reasoning requires perturbations aligned with conceptual hierarchies rather than uniform noise. ReasonBreak strategically targets critical conceptual dependencies within reasoning chains, generating perturbations that invalidate specific inference steps and cascade through subsequent reasoning stages. To facilitate this approach, we contribute \textbf{GeoPrivacy-6K}, a comprehensive dataset comprising 6,341 ultra-high-resolution images ($\geq$2K) with hierarchical concept annotations. Extensive evaluation across seven state-of-the-art MLRMs (including GPT-o3, GPT-5, Gemini 2.5 Pro) demonstrates ReasonBreak's superior effectiveness, achieving a 14.4\% improvement in tract-level protection (33.8\% vs 19.4\%) and nearly doubling block-level protection (33.5\% vs 16.8\%). This work establishes a new paradigm for privacy protection against reasoning-based threats.",0,arxiv,AI,CC-BY/arXiv,Disrupting Hierarchical Reasoning: Adversarial Protection for Geographic Privacy in Multimodal Reasoning Models
"Accurate and uncertainty-aware degradation estimation is essential for predictive maintenance in safety-critical systems like rotating machinery with rolling-element bearings. Many existing uncertainty methods lack confidence calibration, are costly to run, are not distance-aware, and fail to generalize under out-of-distribution data. We introduce two distance-aware uncertainty methods for deterministic physics-guided neural networks: PG-SNGP, based on Spectral Normalization Gaussian Process, and PG-SNER, based on Deep Evidential Regression. We apply spectral normalization to the hidden layers so the network preserves distances from input to latent space. PG-SNGP replaces the final dense layer with a Gaussian Process layer for distance-sensitive uncertainty, while PG-SNER outputs Normal Inverse Gamma parameters to model uncertainty in a coherent probabilistic form. We assess performance using standard accuracy metrics and a new distance-aware metric based on the Pearson Correlation Coefficient, which measures how well predicted uncertainty tracks the distance between test and training samples. We also design a dynamic weighting scheme in the loss to balance data fidelity and physical consistency. We test our methods on rolling-element bearing degradation using the PRONOSTIA dataset and compare them with Monte Carlo and Deep Ensemble PGNNs. Results show that PG-SNGP and PG-SNER improve prediction accuracy, generalize reliably under OOD conditions, and remain robust to adversarial attacks and noise.",0,arxiv,AI,CC-BY/arXiv,Developing Distance-Aware Uncertainty Quantification Methods in Physics-Guided Neural Networks for Reliable Bearing Health Prediction
"Vulnerability code-bases often suffer from severe imbalance, limiting the effectiveness of Deep Learning-based vulnerability classifiers. Data Augmentation could help solve this by mitigating the scarcity of under-represented CWEs. In this context, we investigate LLM-based augmentation for vulnerable functions, comparing controlled generation of new vulnerable samples with semantics-preserving refactoring of existing ones. Using Qwen2.5-Coder to produce augmented data and CodeBERT as a vulnerability classifier on the SVEN dataset, we find that our approaches are indeed effective in enriching vulnerable code-bases through a simple process and with reasonable quality, and that a hybrid strategy best boosts vulnerability classifiers' performance.",0,arxiv,AI,CC-BY/arXiv,LLM-based Vulnerable Code Augmentation: Generate or Refactor?
"Recent advances in Large Language Models have revolutionized function-level code generation; however, repository-scale Automated Program Repair (APR) remains a significant challenge. Current approaches typically employ a control-centric paradigm, forcing agents to navigate complex directory structures and irrelevant control logic. In this paper, we propose a paradigm shift from the standard Code Property Graphs (CPGs) to the concept of Data Transformation Graph (DTG) that inverts the topology by modeling data states as nodes and functions as edges, enabling agents to trace logic defects through data lineage rather than control flow. We introduce a multi-agent framework that reconciles data integrity navigation with control flow logic. Our theoretical analysis and case studies demonstrate that this approach resolves the ""Semantic Trap"" inherent in standard RAG systems in modern coding agents. We provide a comprehensive implementation in the form of Autonomous Issue Resolver (AIR), a self-improvement system for zero-touch code maintenance that utilizes neuro-symbolic reasoning and uses the DTG structure for scalable logic repair. Our approach has demonstrated good results on several SWE benchmarks, reaching a resolution rate of 87.1% on SWE-Verified benchmark. Our approach directly addresses the core limitations of current AI code-assistant tools and tackles the critical need for a more robust foundation for our increasingly software-dependent world.",0,arxiv,AI,CC-BY/arXiv,Autonomous Issue Resolver: Towards Zero-Touch Code Maintenance
"Neural rendering, particularly 3D Gaussian Splatting (3DGS), has evolved rapidly and become a key component for building world models. However, existing viewer solutions remain fragmented, heavy, or constrained by legacy pipelines, resulting in high deployment friction and limited support for dynamic content and generative models. In this work, we present Visionary, an open, web-native platform for real-time various Gaussian Splatting and meshes rendering. Built on an efficient WebGPU renderer with per-frame ONNX inference, Visionary enables dynamic neural processing while maintaining a lightweight, ""click-to-run"" browser experience. It introduces a standardized Gaussian Generator contract, which not only supports standard 3DGS rendering but also allows plug-and-play algorithms to generate or update Gaussians each frame. Such inference also enables us to apply feedforward generative post-processing. The platform further offers a plug in three.js library with a concise TypeScript API for seamless integration into existing web applications. Experiments show that, under identical 3DGS assets, Visionary achieves superior rendering efficiency compared to current Web viewers due to GPU-based primitive sorting. It already supports multiple variants, including MLP-based 3DGS, 4DGS, neural avatars, and style transformation or enhancement networks. By unifying inference and rendering directly in the browser, Visionary significantly lowers the barrier to reproduction, comparison, and deployment of 3DGS-family methods, serving as a unified World Model Carrier for both reconstructive and generative paradigms.",0,arxiv,AI,CC-BY/arXiv,Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform
"Drag-based image editing aims to modify visual content followed by user-specified drag operations. Despite existing methods having made notable progress, they still fail to fully exploit the contextual information in the reference image, including fine-grained texture details, leading to edits with limited coherence and fidelity. To address this challenge, we introduce ContextDrag, a new paradigm for drag-based editing that leverages the strong contextual modeling capability of editing models, such as FLUX-Kontext. By incorporating VAE-encoded features from the reference image, ContextDrag can leverage rich contextual cues and preserve fine-grained details, without the need for finetuning or inversion. Specifically, ContextDrag introduced a novel Context-preserving Token Injection (CTI) that injects noise-free reference features into their correct destination locations via a Latent-space Reverse Mapping (LRM) algorithm. This strategy enables precise drag control while preserving consistency in both semantics and texture details. Second, ContextDrag adopts a novel Position-Consistent Attention (PCA), which positional re-encodes the reference tokens and applies overlap-aware masking to eliminate interference from irrelevant reference features. Extensive experiments on DragBench-SR and DragBench-DR demonstrate that our approach surpasses all existing SOTA methods. Code will be publicly available.",0,arxiv,AI,CC-BY/arXiv,ContextDrag: Precise Drag-Based Image Editing via Context-Preserving Token Injection and Position-Consistent Attention
"Many high-performance human activities are executed with little or no external feedback: think of a figure skater landing a triple jump, a pitcher throwing a curveball for a strike, or a barista pouring latte art. To study the process of skill acquisition under fully controlled conditions, we bypass human subjects. Instead, we directly interface a generalist reinforcement learning agent with a spinning cylinder in a tabletop circulating water channel to maximize or minimize drag. This setup has several desirable properties. First, it is a physical system, with the rich interactions and complex dynamics that only the physical world has: the flow is highly chaotic and extremely difficult, if not impossible, to model or simulate accurately. Second, the objective -- drag minimization or maximization -- is easy to state and can be captured directly in the reward, yet good strategies are not obvious beforehand. Third, decades-old experimental studies provide recipes for simple, high-performance open-loop policies. Finally, the setup is inexpensive and far easier to reproduce than human studies. In our experiments we find that high-dimensional flow feedback lets the agent discover high-performance drag-control strategies with only minutes of real-world interaction. When we later replay the same action sequences without any feedback, we obtain almost identical performance. This shows that feedback, and in particular flow feedback, is not needed to execute the learned policy. Surprisingly, without flow feedback during training the agent fails to discover any well-performing policy in drag maximization, but still succeeds in drag minimization, albeit more slowly and less reliably. Our studies show that learning a high-performance skill can require richer information than executing it, and learning conditions can be kind or wicked depending solely on the goal, not on dynamics or policy complexity.",0,arxiv,AI,CC-BY/arXiv,Using reinforcement learning to probe the role of feedback in skill acquisition
"The potential for rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons has generated significant policy, academic, and public concern. Both model developers and policymakers seek to quantify and mitigate any risk, with an important element of such efforts being the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper discusses the pilot implementation of the Bacterial Biothreat Benchmark (B3) dataset. It is the third in a series of three papers describing an overall Biothreat Benchmark Generation (BBG) framework, with previous papers detailing the development of the B3 dataset. The pilot involved running the benchmarks through a sample frontier AI model, followed by human evaluation of model responses, and an applied risk analysis of the results along several dimensions. Overall, the pilot demonstrated that the B3 dataset offers a viable, nuanced method for rapidly assessing the biosecurity risk posed by a LLM, identifying the key sources of that risk and providing guidance for priority areas of mitigation priority.",0,arxiv,AI,CC-BY/arXiv,Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models III: Implementing the Bacterial Biothreat Benchmark (B3) Dataset
"The potential for rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons has generated significant policy, academic, and public concern. Both model developers and policymakers seek to quantify and mitigate any risk, with an important element of such efforts being the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper, the second in a series of three, describes the second component of a novel Biothreat Benchmark Generation (BBG) framework: the generation of the Bacterial Biothreat Benchmark (B3) dataset. The development process involved three complementary approaches: 1) web-based prompt generation, 2) red teaming, and 3) mining existing benchmark corpora, to generate over 7,000 potential benchmarks linked to the Task-Query Architecture that was developed during the first component of the project. A process of de-duplication, followed by an assessment of uplift diagnosticity, and general quality control measures, reduced the candidates to a set of 1,010 final benchmarks. This procedure ensured that these benchmarks are a) diagnostic in terms of providing uplift; b) directly relevant to biosecurity threats; and c) are aligned with a larger biosecurity architecture permitting nuanced analysis at different levels of analysis.",0,arxiv,AI,CC-BY/arXiv,Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models II: Benchmark Generation Process
"This paper introduces the Impact-Driven AI Framework (IDAIF), a novel architectural methodology that integrates Theory of Change (ToC) principles with modern artificial intelligence system design. As AI systems increasingly influence high-stakes domains including healthcare, finance, and public policy, the alignment problem--ensuring AI behavior corresponds with human values and intentions--has become critical. Current approaches predominantly optimize technical performance metrics while neglecting the sociotechnical dimensions of AI deployment. IDAIF addresses this gap by establishing a systematic mapping between ToC's five-stage model (Inputs-Activities-Outputs-Outcomes-Impact) and corresponding AI architectural layers (Data Layer-Pipeline Layer-Inference Layer-Agentic Layer-Normative Layer). Each layer incorporates rigorous theoretical foundations: multi-objective Pareto optimization for value alignment, hierarchical multi-agent orchestration for outcome achievement, causal directed acyclic graphs (DAGs) for hallucination mitigation, and adversarial debiasing with Reinforcement Learning from Human Feedback (RLHF) for fairness assurance. We provide formal mathematical formulations for each component and introduce an Assurance Layer that manages assumption failures through guardian architectures. Three case studies demonstrate IDAIF application across healthcare, cybersecurity, and software engineering domains. This framework represents a paradigm shift from model-centric to impact-centric AI development, providing engineers with concrete architectural patterns for building ethical, trustworthy, and socially beneficial AI systems.",0,arxiv,AI,CC-BY/arXiv,From Accuracy to Impact: The Impact-Driven AI Framework (IDAIF) for Aligning Engineering Architecture with Theory of Change
"Model-based planning in robotic domains is fundamentally challenged by the hybrid nature of physical dynamics, where continuous motion is punctuated by discrete events such as contacts and impacts. Conventional latent world models typically employ monolithic neural networks that enforce global continuity, inevitably over-smoothing the distinct dynamic modes (e.g., sticking vs. sliding, flight vs. stance). For a planner, this smoothing results in catastrophic compounding errors during long-horizon lookaheads, rendering the search process unreliable at physical boundaries. To address this, we introduce the Prismatic World Model (PRISM-WM), a structured architecture designed to decompose complex hybrid dynamics into composable primitives. PRISM-WM leverages a context-aware Mixture-of-Experts (MoE) framework where a gating mechanism implicitly identifies the current physical mode, and specialized experts predict the associated transition dynamics. We further introduce a latent orthogonalization objective to ensure expert diversity, effectively preventing mode collapse. By accurately modeling the sharp mode transitions in system dynamics, PRISM-WM significantly reduces rollout drift. Extensive experiments on challenging continuous control benchmarks, including high-dimensional humanoids and diverse multi-task settings, demonstrate that PRISM-WM provides a superior high-fidelity substrate for trajectory optimization algorithms (e.g., TD-MPC), proving its potential as a powerful foundational model for next-generation model-based agents.",0,arxiv,AI,CC-BY/arXiv,Prismatic World Model: Learning Compositional Dynamics for Planning in Hybrid Systems
"This paper investigates bias in GLLM annotations by conceptually replicating manual annotations of Boukes (2024). Using various GLLMs (Llama3.1:8b, Llama3.3:70b, GPT4o, Qwen2.5:72b) in combination with five different prompts for five concepts (political content, interactivity, rationality, incivility, and ideology). We find GLLMs perform adequate in terms of F1 scores, but differ from manual annotations in terms of prevalence, yield substantively different downstream results, and display systematic bias in that they overlap more with each other than with manual annotations. Differences in F1 scores fail to account for the degree of bias.",0,arxiv,AI,CC-BY/arXiv,Are generative AI text annotations systematically biased?
"Biosignals collected from wearable devices are widely utilized in healthcare applications. Machine learning models used in these applications often rely on features extracted from biosignals due to their effectiveness, lower data dimensionality, and wide compatibility across various model architectures. However, existing feature extraction methods often lack task-specific contextual knowledge, struggle to identify optimal feature extraction settings in high-dimensional feature space, and are prone to code generation and automation errors. In this paper, we propose DeepFeature, the first LLM-empowered, context-aware feature generation framework for wearable biosignals. DeepFeature introduces a multi-source feature generation mechanism that integrates expert knowledge with task settings. It also employs an iterative feature refinement process that uses feature assessment-based feedback for feature re-selection. Additionally, DeepFeature utilizes a robust multi-layer filtering and verification approach for robust feature-to-code translation to ensure that the extraction functions run without crashing. Experimental evaluation results show that DeepFeature achieves an average AUROC improvement of 4.21-9.67% across eight diverse tasks compared to baseline methods. It outperforms state-of-the-art approaches on five tasks while maintaining comparable performance on the remaining tasks.",0,arxiv,AI,CC-BY/arXiv,DeepFeature: Iterative Context-aware Feature Generation for Wearable Biosignals
"Large language model (LLM) agents often rely on external demonstrations or retrieval-augmented planning, leading to brittleness, poor generalization, and high computational overhead. Inspired by human problem-solving, we propose DuSAR (Dual-Strategy Agent with Reflecting) - a demonstration-free framework that enables a single frozen LLM to perform co-adaptive reasoning via two complementary strategies: a high-level holistic plan and a context-grounded local policy. These strategies interact through a lightweight reflection mechanism, where the agent continuously assesses progress via a Strategy Fitness Score and dynamically revises its global plan when stuck or refines it upon meaningful advancement, mimicking human metacognitive behavior. On ALFWorld and Mind2Web, DuSAR achieves state-of-the-art performance with open-source LLMs (7B-70B), reaching 37.1% success on ALFWorld (Llama3.1-70B) - more than doubling the best prior result (13.0%) - and 4.02% on Mind2Web, also more than doubling the strongest baseline. Remarkably, it reduces per-step token consumption by 3-9X while maintaining strong performance. Ablation studies confirm the necessity of dual-strategy coordination. Moreover, optional integration of expert demonstrations further boosts results, highlighting DuSAR's flexibility and compatibility with external knowledge.",0,arxiv,AI,CC-BY/arXiv,Reflecting with Two Voices: A Co-Adaptive Dual-Strategy Framework for LLM-Based Agent Decision Making
"Biological systems exhibit remarkable morphogenetic plasticity, where a single genome can encode various specialized cellular structures triggered by local chemical signals. In the domain of Deep Learning, Differentiable Neural Cellular Automata (NCA) have emerged as a paradigm to mimic this self-organization. However, existing NCA research has predominantly focused on continuous texture synthesis or single-target object recovery, leaving the challenge of class-conditional structural generation largely unexplored. In this work, we propose a novel Conditional Neural Cellular Automata (c-NCA) architecture capable of growing distinct topological structures - specifically MNIST digits - from a single generic seed, guided solely by a spatially broadcasted class vector. Unlike traditional generative models (e.g., GANs, VAEs) that rely on global reception fields, our model enforces strict locality and translation equivariance. We demonstrate that by injecting a one-hot condition into the cellular perception field, a single set of local rules can learn to break symmetry and self-assemble into ten distinct geometric attractors. Experimental results show that our c-NCA achieves stable convergence, correctly forming digit topologies from a single pixel, and exhibits robustness characteristic of biological systems. This work bridges the gap between texture-based NCAs and structural pattern formation, offering a lightweight, biologically plausible alternative for conditional generation.",0,arxiv,AI,CC-BY/arXiv,Conditional Morphogenesis: Emergent Generation of Structural Digits via Neural Cellular Automata
"Workplace toxicity is widely recognized as detrimental to organizational culture, yet quantifying its direct impact on operational efficiency remains methodologically challenging due to the ethical and practical difficulties of reproducing conflict in human subjects. This study leverages Large Language Model (LLM) based Multi-Agent Systems to simulate 1-on-1 adversarial debates, creating a controlled ""sociological sandbox"". We employ a Monte Carlo method to simulate hundrets of discussions, measuring the convergence time (defined as the number of arguments required to reach a conclusion) between a baseline control group and treatment groups involving agents with ""toxic"" system prompts. Our results demonstrate a statistically significant increase of approximately 25\% in the duration of conversations involving toxic participants. We propose that this ""latency of toxicity"" serves as a proxy for financial damage in corporate and academic settings. Furthermore, we demonstrate that agent-based modeling provides a reproducible, ethical alternative to human-subject research for measuring the mechanics of social friction.",0,arxiv,AI,CC-BY/arXiv,The High Cost of Incivility: Quantifying Interaction Inefficiency via Multi-Agent Monte Carlo Simulations
"Graph Neural Networks (GNNs) have become a powerful tool for modeling and analyzing data with graph structures. The wide adoption in numerous applications underscores the value of these models. However, the complexity of these methods often impedes understanding their decision-making processes. Current Explainable AI (XAI) methods struggle to untangle the intricate relationships and interactions within graphs. Several methods have tried to bridge this gap via a post-hoc approach or self-interpretable design. Most of them focus on graph structure analysis to determine essential patterns that correlate with prediction outcomes. While post-hoc explanation methods are adaptable, they require extra computational resources and may be less reliable due to limited access to the model's internal workings. Conversely, Interpretable models can provide immediate explanations, but their generalizability to different scenarios remains a major concern. To address these shortcomings, this thesis seeks to develop a novel XAI framework tailored for graph-based machine learning. The proposed framework aims to offer adaptable, computationally efficient explanations for GNNs, moving beyond individual feature analysis to capture how graph structure influences predictions.",0,arxiv,AI,CC-BY/arXiv,Enhancing Explainability of Graph Neural Networks Through Conceptual and Structural Analyses and Their Extensions
"Soil compaction is critical in construction engineering to ensure the stability of structures like road embankments and earth dams. Traditional methods for determining optimum moisture content (OMC) and maximum dry density (MDD) involve labor-intensive laboratory experiments, and empirical regression models have limited applicability and accuracy across diverse soil types. In recent years, artificial intelligence (AI) and machine learning (ML) techniques have emerged as alternatives for predicting these compaction parameters. However, ML models often struggle with prediction accuracy and generalizability, particularly with heterogeneous datasets representing various soil types. This study proposes an automated machine learning (AutoML) approach to predict OMC and MDD. AutoML automates algorithm selection and hyperparameter optimization, potentially improving accuracy and scalability. Through extensive experimentation, the study found that the Extreme Gradient Boosting (XGBoost) algorithm provided the best performance, achieving R-squared values of 80.4% for MDD and 89.1% for OMC on a separate dataset. These results demonstrate the effectiveness of AutoML in predicting compaction parameters across different soil types. The study also highlights the importance of heterogeneous datasets in improving the generalization and performance of ML models. Ultimately, this research contributes to more efficient and reliable construction practices by enhancing the prediction of soil compaction parameters.",0,arxiv,AI,CC-BY/arXiv,Soil Compaction Parameters Prediction Based on Automated Machine Learning Approach
"The California Bearing Ratio (CBR) is a key geotechnical indicator used to assess the load-bearing capacity of subgrade soils, especially in transportation infrastructure and foundation design. Traditional CBR determination relies on laboratory penetration tests. Despite their accuracy, these tests are often time-consuming, costly, and can be impractical, particularly for large-scale or diverse soil profiles. Recent progress in artificial intelligence, especially machine learning (ML), has enabled data-driven approaches for modeling complex soil behavior with greater speed and precision. This study introduces a comprehensive ML framework for CBR prediction using a dataset of 382 soil samples collected from various geoclimatic regions in TÃ¼rkiye. The dataset includes physicochemical soil properties relevant to bearing capacity, allowing multidimensional feature representation in a supervised learning context. Twelve ML algorithms were tested, including decision tree, random forest, extra trees, gradient boosting, xgboost, k-nearest neighbors, support vector regression, multi-layer perceptron, adaboost, bagging, voting, and stacking regressors. Each model was trained, validated, and evaluated to assess its generalization and robustness. Among them, the random forest regressor performed the best, achieving strong R2 scores of 0.95 (training), 0.76 (validation), and 0.83 (test). These outcomes highlight the model's powerful nonlinear mapping ability, making it a promising tool for predictive geotechnical tasks. The study supports the integration of intelligent, data-centric models in geotechnical engineering, offering an effective alternative to traditional methods and promoting digital transformation in infrastructure analysis and design.",0,arxiv,AI,CC-BY/arXiv,Predicting California Bearing Ratio with Ensemble and Neural Network Models: A Case Study from TÃ¼rkiye
"Generalist robot policies, trained on large and diverse datasets, have demonstrated the ability to generalize across a wide spectrum of behaviors, enabling a single policy to act in varied real-world environments. However, they still fall short on new tasks not covered in the training data. When finetuned on limited demonstrations of a new task, these policies often overfit to the specific demonstrations--not only losing their prior abilities to solve a wide variety of generalist tasks but also failing to generalize within the new task itself. In this work, we aim to develop a method that preserves the generalization capabilities of the generalist policy during finetuning, allowing a single policy to robustly incorporate a new skill into its repertoire. Our goal is a single policy that both learns to generalize to variations of the new task and retains the broad competencies gained from pretraining. We show that this can be achieved through a simple yet effective strategy: interpolating the weights of a finetuned model with that of the pretrained model. We show, across extensive simulated and real-world experiments, that such model merging produces a single model that inherits the generalist abilities of the base model and learns to solve the new task robustly, outperforming both the pretrained and finetuned model on out-of-distribution variations of the new task. Moreover, we show that model merging enables continual acquisition of new skills in a lifelong learning setting, without sacrificing previously learned generalist abilities.",0,arxiv,AI,CC-BY/arXiv,Robust Finetuning of Vision-Language-Action Robot Policies via Parameter Merging
"Recent image protection mechanisms such as Glaze and Nightshade introduce imperceptible, adversarially designed perturbations intended to disrupt downstream text-to-image generative models. While their empirical effectiveness is known, the internal structure, detectability, and representational behavior of these perturbations remain poorly understood. This study provides a systematic, explainable AI analysis using a unified framework that integrates white-box feature-space inspection and black-box signal-level probing. Through latent-space clustering, feature-channel activation analysis, occlusion-based spatial sensitivity mapping, and frequency-domain characterization, we show that protection mechanisms operate as structured, low-entropy perturbations tightly coupled to underlying image content across representational, spatial, and spectral domains. Protected images preserve content-driven feature organization with protection-specific substructure rather than inducing global representational drift. Detectability is governed by interacting effects of perturbation entropy, spatial deployment, and frequency alignment, with sequential protection amplifying detectable structure rather than suppressing it. Frequency-domain analysis shows that Glaze and Nightshade redistribute energy along dominant image-aligned frequency axes rather than introducing diffuse noise. These findings indicate that contemporary image protection operates through structured feature-level deformation rather than semantic dislocation, explaining why protection signals remain visually subtle yet consistently detectable. This work advances the interpretability of adversarial image protection and informs the design of future defenses and detection strategies for generative AI systems.",0,arxiv,AI,CC-BY/arXiv,Interpreting Structured Perturbations in Image Protection Methods for Diffusion Models
"Spatial transcriptomics (ST) enables simultaneous mapping of tissue morphology and spatially resolved gene expression, offering unique opportunities to study tumor microenvironment heterogeneity. Here, we introduce a computational framework that predicts spatial pathway activity directly from hematoxylin-and-eosin-stained histology images at microscale resolution 55 and 100 um. Using image features derived from a computational pathology foundation model, we found that TGFb signaling was the most accurately predicted pathway across three independent breast and lung cancer ST datasets. In 87-88% of reliably predicted cases, the resulting spatial TGFb activity maps reflected the expected contrast between tumor and adjacent non-tumor regions, consistent with the known role of TGFb in regulating interactions within the tumor microenvironment. Notably, linear and nonlinear predictive models performed similarly, suggesting that image features may relate to pathway activity in a predominantly linear fashion or that nonlinear structure is small relative to measurement noise. These findings demonstrate that features extracted from routine histopathology may recover spatially coherent and biologically interpretable pathway patterns, offering a scalable strategy for integrating image-based inference with ST information in tumor microenvironment studies.",0,arxiv,AI,CC-BY/arXiv,Digital Modeling of Spatial Pathway Activity from Histology Reveals Tumor Microenvironment Heterogeneity
"Sensitive information leakage in code repositories has emerged as a critical security challenge. Traditional detection methods that rely on regular expressions, fingerprint features, and high-entropy calculations often suffer from high false-positive rates. This not only reduces detection efficiency but also significantly increases the manual screening burden on developers. Recent advances in large language models (LLMs) and multi-agent collaborative architectures have demonstrated remarkable potential for tackling complex tasks, offering a novel technological perspective for sensitive information detection. In response to these challenges, we propose Argus, a multi-agent collaborative framework for detecting sensitive information. Argus employs a three-tier detection mechanism that integrates key content, file context, and project reference relationships to effectively reduce false positives and enhance overall detection accuracy. To comprehensively evaluate Argus in real-world repository environments, we developed two new benchmarks, one to assess genuine leak detection capabilities and another to evaluate false-positive filtering performance. Experimental results show that Argus achieves up to 94.86% accuracy in leak detection, with a precision of 96.36%, recall of 94.64%, and an F1 score of 0.955. Moreover, the analysis of 97 real repositories incurred a total cost of only 2.2$. All code implementations and related datasets are publicly available at https://github.com/TheBinKing/Argus-Guard for further research and application.",0,arxiv,AI,CC-BY/arXiv,Argus: A Multi-Agent Sensitive Information Leakage Detection Framework Based on Hierarchical Reference Relationships
"Dataset distillation aims to synthesize a compact subset of the original data, enabling models trained on it to achieve performance comparable to those trained on the original large dataset. Existing distribution-matching methods are confined to Euclidean spaces, making them only capture linear structures and overlook the intrinsic geometry of real data, e.g., curvature. However, high-dimensional data often lie on low-dimensional manifolds, suggesting that dataset distillation should have the distilled data manifold aligned with the original data manifold. In this work, we propose a geometry-aware distribution-matching framework, called \textbf{GeoDM}, which operates in the Cartesian product of Euclidean, hyperbolic, and spherical manifolds, with flat, hierarchical, and cyclical structures all captured by a unified representation. To adapt to the underlying data geometry, we introduce learnable curvature and weight parameters for three kinds of geometries. At the same time, we design an optimal transport loss to enhance the distribution fidelity. Our theoretical analysis shows that the geometry-aware distribution matching in a product space yields a smaller generalization error bound than the Euclidean counterparts. Extensive experiments conducted on standard benchmarks demonstrate that our algorithm outperforms state-of-the-art data distillation methods and remains effective across various distribution-matching strategies for the single geometries.",0,arxiv,AI,CC-BY/arXiv,GeoDM: Geometry-aware Distribution Matching for Dataset Distillation
"For decades, procedural worlds have been built on procedural noise functions such as Perlin noise, which are fast and infinite, yet fundamentally limited in realism and large-scale coherence. We introduce Terrain Diffusion, an AI-era successor to Perlin noise that bridges the fidelity of diffusion models with the properties that made procedural noise indispensable: seamless infinite extent, seed-consistency, and constant-time random access. At its core is InfiniteDiffusion, a novel algorithm for infinite generation, enabling seamless, real-time synthesis of boundless landscapes. A hierarchical stack of diffusion models couples planetary context with local detail, while a compact Laplacian encoding stabilizes outputs across Earth-scale dynamic ranges. An open-source infinite-tensor framework supports constant-memory manipulation of unbounded tensors, and few-step consistency distillation enables efficient generation. Together, these components establish diffusion models as a practical foundation for procedural world generation, capable of synthesizing entire planets coherently, controllably, and without limits.",0,arxiv,AI,CC-BY/arXiv,"Terrain Diffusion: A Diffusion-Based Successor to Perlin Noise in Infinite, Real-Time Terrain Generation"
"Large language models (LLMs) are post-trained through reinforcement learning (RL) to evolve into Reasoning Language Models (RLMs), where the hallmark of this advanced reasoning is ``aha'' moments when they start to perform strategies, such as self-reflection and deep thinking, within chain of thoughts (CoTs). Motivated by this, this paper proposes a novel reinforced strategy injection mechanism (rSIM), that enables any LLM to become an RLM by employing a small planner to guide the LLM's CoT through the adaptive injection of reasoning strategies. To achieve this, the planner (leader agent) is jointly trained with an LLM (follower agent) using multi-agent RL (MARL), based on a leader-follower framework and straightforward rule-based rewards. Experimental results show that rSIM enables Qwen2.5-0.5B to become an RLM and significantly outperform Qwen2.5-14B. Moreover, the planner is generalizable: it only needs to be trained once and can be applied as a plug-in to substantially improve the reasoning capabilities of existing LLMs. In addition, the planner supports continual learning across various tasks, allowing its planning abilities to gradually improve and generalize to a wider range of problems.",0,arxiv,AI,CC-BY/arXiv,rSIM: Incentivizing Reasoning Capabilities of LLMs via Reinforced Strategy Injection
"Agents, language model (LM)-based systems that are capable of reasoning, planning, and acting are becoming the dominant paradigm for real-world AI applications. Despite this widespread adoption, the principles that determine their performance remain underexplored, leaving practitioners to rely on heuristics rather than principled design choices. We address this gap by deriving quantitative scaling principles for agent systems. We evaluate this across four diverse benchmarks: Finance-Agent, BrowseComp-Plus, PlanCraft, and Workbench. Using five canonical architectures (Single, Independent, Centralized, Decentralized, Hybrid) instantiated across three LLM families, we perform a controlled evaluation spanning 180 configurations with standardized tools and token budgets. We derive a predictive model using empirical coordination metrics, including efficiency, overhead, error amplification, and redundancy, that achieves cross-validated R^2=0.513. We identify three dominant effects: (1) a tool-coordination trade-off: under fixed computational budgets, tool-heavy tasks suffer disproportionately from multi-agent overhead. (2) a capability saturation: coordination yields diminishing or negative returns (beta=-0.408, p<0.001) once single-agent baselines exceed ~45%. (3) topology-dependent error amplification: independent agents amplify errors 17.2x through unchecked propagation, while centralized coordination contains this to 4.4x. Centralized coordination improves performance by 80.9% on parallelizable tasks like financial reasoning, while decentralized coordination excels on dynamic web navigation (+9.2% vs. +0.2%). Yet for sequential reasoning tasks, all multi-agent variants degraded performance by 39-70%. The framework predicts the optimal coordination strategy for 87% of held-out configurations, providing a predictive principle of agentic scaling based on measurable task properties.",0,arxiv,AI,CC-BY/arXiv,Towards a Science of Scaling Agent Systems
"The Model Context Protocol (MCP) has emerged as the de facto standard for connecting Large Language Models (LLMs) to external data and tools, effectively functioning as the ""USB-C for Agentic AI."" While this decoupling of context and execution solves critical interoperability challenges, it introduces a profound new threat landscape where the boundary between epistemic errors (hallucinations) and security breaches (unauthorized actions) dissolves. This Systematization of Knowledge (SoK) aims to provide a comprehensive taxonomy of risks in the MCP ecosystem, distinguishing between adversarial security threats (e.g., indirect prompt injection, tool poisoning) and epistemic safety hazards (e.g., alignment failures in distributed tool delegation). We analyze the structural vulnerabilities of MCP primitives, specifically Resources, Prompts, and Tools, and demonstrate how ""context"" can be weaponized to trigger unauthorized operations in multi-agent environments. Furthermore, we survey state-of-the-art defenses, ranging from cryptographic provenance (ETDI) to runtime intent verification, and conclude with a roadmap for securing the transition from conversational chatbots to autonomous agentic operating systems.",0,arxiv,AI,CC-BY/arXiv,Systematization of Knowledge: Security and Safety in the Model Context Protocol Ecosystem
"The integration of Large Language Models (LLMs) into mobile and software development workflows faces a persistent tension among three demands: semantic awareness, developer productivity, and data privacy. Traditional cloud-based tools offer strong reasoning but risk data exposure and latency, while on-device solutions lack full-context understanding across codebase and developer tooling. We introduce SolidGPT, an open-source, edge-cloud hybrid developer assistant built on GitHub, designed to enhance code and workspace semantic search. SolidGPT enables developers to: talk to your codebase: interactively query code and project structure, discovering the right methods and modules without manual searching. Automate software project workflows: generate PRDs, task breakdowns, Kanban boards, and even scaffold web app beginnings, with deep integration via VSCode and Notion. Configure private, extensible agents: onboard private code folders (up to approximately 500 files), connect Notion, customize AI agent personas via embedding and in-context training, and deploy via Docker, CLI, or VSCode extension. In practice, SolidGPT empowers developer productivity through: Semantic-rich code navigation: no more hunting through files or wondering where a feature lives. Integrated documentation and task management: seamlessly sync generated PRD content and task boards into developer workflows. Privacy-first design: running locally via Docker or VSCode, with full control over code and data, while optionally reaching out to LLM APIs as needed. By combining interactive code querying, automated project scaffolding, and human-AI collaboration, SolidGPT provides a practical, privacy-respecting edge assistant that accelerates real-world development workflows, ideal for intelligent mobile and software engineering contexts.",0,arxiv,AI,CC-BY/arXiv,Empowering smart app development with SolidGPT: an edge-cloud hybrid AI agent framework
"Offline decision-making requires synthesizing reliable behaviors from fixed datasets without further interaction, yet existing generative approaches often yield trajectories that are dynamically infeasible. We propose Model Predictive Diffuser (MPDiffuser), a compositional model-based diffusion framework consisting of: (i) a planner that generates diverse, task-aligned trajectories; (ii) a dynamics model that enforces consistency with the underlying system dynamics; and (iii) a ranker module that selects behaviors aligned with the task objectives. MPDiffuser employs an alternating diffusion sampling scheme, where planner and dynamics updates are interleaved to progressively refine trajectories for both task alignment and feasibility during the sampling process. We also provide a theoretical rationale for this procedure, showing how it balances fidelity to data priors with dynamics consistency. Empirically, the compositional design improves sample efficiency, as it leverages even low-quality data for dynamics learning and adapts seamlessly to novel dynamics. We evaluate MPDiffuser on both unconstrained (D4RL) and constrained (DSRL) offline decision-making benchmarks, demonstrating consistent gains over existing approaches. Furthermore, we present a preliminary study extending MPDiffuser to vision-based control tasks, showing its potential to scale to high-dimensional sensory inputs. Finally, we deploy our method on a real quadrupedal robot, showcasing its practicality for real-world control.",0,arxiv,AI,CC-BY/arXiv,Model-Based Diffusion Sampling for Predictive Control in Offline Decision Making
"The efficacy of Artificial Intelligence (AI) in micro/nano manufacturing is fundamentally constrained by the scarcity of high-quality and physically grounded training data for defect inspection. Lithography defect data from semiconductor industry are rarely accessible for research use, resulting in a shortage of publicly available datasets. To address this bottleneck in lithography, this study proposes a novel methodology for generating large-scale, physically valid defect datasets with pixel-level annotations. The framework begins with the ab initio synthesis of defect layouts using controllable, physics-constrained mathematical morphology operations (erosion and dilation) applied to the original design-level layout. These synthesized layouts, together with their defect-free counterparts, are fabricated into physical samples via high-fidelity digital micromirror device (DMD)-based lithography. Optical micrographs of the synthesized defect samples and their defect-free references are then compared to create consistent defect delineation annotations. Using this methodology, we constructed a comprehensive dataset of 3,530 Optical micrographs containing 13,365 annotated defect instances including four classes: bridge, burr, pinch, and contamination. Each defect instance is annotated with a pixel-accurate segmentation mask, preserving full contour and geometry. The segmentation-based Mask R-CNN achieves AP@0.5 of 0.980, 0.965, and 0.971, compared with 0.740, 0.719, and 0.717 for Faster R-CNN on bridge, burr, and pinch classes, representing a mean AP@0.5 improvement of approximately 34%. For the contamination class, Mask R-CNN achieves an AP@0.5 roughly 42% higher than Faster R-CNN. These consistent gains demonstrate that our proposed methodology to generate defect datasets with pixel-level annotations is feasible for robust AI-based Measurement/Inspection (MI) in semiconductor fabrication.",0,arxiv,AI,CC-BY/arXiv,"A Physics-Constrained, Design-Driven Methodology for Defect Dataset Generation in Optical Lithography"
"Modern businesses are increasingly challenged by the time and expense required to generate and assess high-quality content. Human writers face time constraints, and extrinsic evaluations can be costly. While Large Language Models (LLMs) offer potential in content creation, concerns about the quality of AI-generated content persist. Traditional evaluation methods, like human surveys, further add operational costs, highlighting the need for efficient, automated solutions. This research introduces Generative Agents as a means to tackle these challenges. These agents can rapidly and cost-effectively evaluate AI-generated content, simulating human judgment by rating aspects such as coherence, interestingness, clarity, fairness, and relevance. By incorporating these agents, businesses can streamline content generation and ensure consistent, high-quality output while minimizing reliance on costly human evaluations. The study provides critical insights into enhancing LLMs for producing business-aligned, high-quality content, offering significant advancements in automated content generation and evaluation.",0,arxiv,AI,CC-BY/arXiv,AgentEval: Generative Agents as Reliable Proxies for Human Evaluation of AI-Generated Content
"Previous research has reported that large language models (LLMs) demonstrate poor performance on the Chartered Financial Analyst (CFA) exams. However, recent reasoning models have achieved strong results on graduate-level academic and professional examinations across various disciplines. In this paper, we evaluate state-of-the-art reasoning models on a set of mock CFA exams consisting of 980 questions across three Level I exams, two Level II exams, and three Level III exams. Using the same pass/fail criteria from prior studies, we find that most models clear all three levels. The models that pass, ordered by overall performance, are Gemini 3.0 Pro, Gemini 2.5 Pro, GPT-5, Grok 4, Claude Opus 4.1, and DeepSeek-V3.1. Specifically, Gemini 3.0 Pro achieves a record score of 97.6% on Level I. Performance is also strong on Level II, led by GPT-5 at 94.3%. On Level III, Gemini 2.5 Pro attains the highest score with 86.4% on multiple-choice questions while Gemini 3.0 Pro achieves 92.0% on constructed-response questions.",0,arxiv,AI,CC-BY/arXiv,Reasoning Models Ace the CFA Exams
"Predicting diseases solely from patient-side information, such as demographics and self-reported symptoms, has attracted significant research attention due to its potential to enhance patient awareness, facilitate early healthcare engagement, and improve healthcare system efficiency. However, existing approaches encounter critical challenges, including imbalanced disease distributions and a lack of interpretability, resulting in biased or unreliable predictions. To address these issues, we propose the Knowledge graph-enhanced, Prototype-aware, and Interpretable (KPI) framework. KPI systematically integrates structured and trusted medical knowledge into a unified disease knowledge graph, constructs clinically meaningful disease prototypes, and employs contrastive learning to enhance predictive accuracy, which is particularly important for long-tailed diseases. Additionally, KPI utilizes large language models (LLMs) to generate patient-specific, medically relevant explanations, thereby improving interpretability and reliability. Extensive experiments on real-world datasets demonstrate that KPI outperforms state-of-the-art methods in predictive accuracy and provides clinically valid explanations that closely align with patient narratives, highlighting its practical value for patient-centered healthcare delivery.",0,arxiv,AI,CC-BY/arXiv,Beyond Traditional Diagnostics: Transforming Patient-Side Information into Predictive Insights with Knowledge Graphs and Prototypes
"Camera-based temporal 3D object detection has shown impressive results in autonomous driving, with offline models improving accuracy by using future frames. Knowledge distillation (KD) can be an appealing framework for transferring rich information from offline models to online models. However, existing KD methods overlook future frames, as they mainly focus on spatial feature distillation under strict frame alignment or on temporal relational distillation, thereby making it challenging for online models to effectively learn future knowledge. To this end, we propose a sparse query-based approach, Future Temporal Knowledge Distillation (FTKD), which effectively transfers future frame knowledge from an offline teacher model to an online student model. Specifically, we present a future-aware feature reconstruction strategy to encourage the student model to capture future features without strict frame alignment. In addition, we further introduce future-guided logit distillation to leverage the teacher's stable foreground and background context. FTKD is applied to two high-performing 3D object detection baselines, achieving up to 1.3 mAP and 1.3 NDS gains on the nuScenes dataset, as well as the most accurate velocity estimation, without increasing inference cost.",0,arxiv,AI,CC-BY/arXiv,Distilling Future Temporal Knowledge with Masked Feature Reconstruction for 3D Object Detection
"A novel deep hybrid Residual-SwinCA-Net segmentation framework is proposed in the study for addressing such challenges by extracting locally correlated and robust features, incorporating residual CNN modules. Furthermore, for learning global dependencies, Swin Transformer blocks are customized using internal residual pathways, which reinforce gradient stability, refine local patterns, and facilitate global feature fusion. Formerly, for enhancing tissue continuity, ultrasound noise suppressions, and accentuating fine structural transitions Laplacian-of-Gaussian regional operator is applied, and for maintaining the morphological integrity of malignant lesion contours, a boundary-oriented operator has been incorporated. Subsequently, a contraction strategy was applied stage-wise by progressively reducing features-map progressively for capturing scale invariance and enhancing the robustness of structural variability. In addition, each decoder level prior augmentation integrates a new Multi-Scale Channel Attention and Squeezing (MSCAS) module. The MSCAS selectively emphasizes encoder salient maps, retains discriminative global context, and complementary local structures with minimal computational cost while suppressing redundant activations. Finally, the Pixel-Attention module encodes class-relevant spatial cues by adaptively weighing malignant lesion pixels while suppressing background interference. The Residual-SwinCA-Net and existing CNNs/ViTs techniques have been implemented on the publicly available BUSI dataset. The proposed Residual-SwinCA-Net framework outperformed and achieved 99.29% mean accuracy, 98.74% IoU, and 0.9041 Dice for breast lesion segmentation. The proposed Residual-SwinCA-Net framework improves the BUSI lesion diagnostic performance and strengthens timely clinical decision-making.",0,arxiv,AI,CC-BY/arXiv,Residual-SwinCA-Net: A Channel-Aware Integrated Residual CNN-Swin Transformer for Malignant Lesion Segmentation in BUSI
"Vision-language models (VLMs) have transformed multimodal reasoning, but feeding hundreds of visual patch tokens into LLMs incurs quadratic computational costs, straining memory and context windows. Traditional approaches face a trade-off: continuous compression dilutes high-level semantics such as object identities, while discrete quantization loses fine-grained details such as textures. We introduce HTC-VLM, a hybrid framework that disentangles semantics and appearance through dual channels, i.e., a continuous pathway for fine-grained details via ViT patches and a discrete pathway for symbolic anchors using MGVQ quantization projected to four tokens. These are fused into a 580-token hybrid sequence and compressed into a single voco token via a disentanglement attention mask and bottleneck, ensuring efficient and grounded representations. HTC-VLM achieves an average performance retention of 87.2 percent across seven benchmarks (GQA, VQAv2, MMBench, MME, POPE, SEED-Bench, ScienceQA-Image), outperforming the leading continuous baseline at 81.0 percent with a 580-to-1 compression ratio. Attention analyses show that the compressed token prioritizes the discrete anchor, validating its semantic guidance. Our work demonstrates that a minimalist hybrid design can resolve the efficiency-fidelity dilemma and advance scalable VLMs.",0,arxiv,AI,CC-BY/arXiv,HybridToken-VLM: Hybrid Token Compression for Vision-Language Models
"Objective speech quality assessment is central to telephony, VoIP, and streaming systems, where large volumes of degraded audio must be monitored and optimized at scale. Classical metrics such as PESQ and POLQA approximate human mean opinion scores (MOS) but require carefully controlled conditions and expensive listening tests, while learning-based models such as NISQA regress MOS and multiple perceptual dimensions from waveforms or spectrograms, achieving high correlation with subjective ratings yet remaining rigid: they do not support interactive, natural-language queries and do not natively provide textual rationales. In this work, we introduce SpeechQualityLLM, a multimodal speech quality question-answering (QA) system that couples an audio encoder with a language model and is trained on the NISQA corpus using template-based question-answer pairs covering overall MOS and four perceptual dimensions (noisiness, coloration, discontinuity, and loudness) in both single-ended (degraded only) and double-ended (degraded plus clean reference) setups. Instead of directly regressing scores, our system is supervised to generate textual answers from which numeric predictions are parsed and evaluated with standard regression and ranking metrics; on held-out NISQA clips, the double-ended model attains a MOS mean absolute error (MAE) of 0.41 with Pearson correlation of 0.86, with competitive performance on dimension-wise tasks. Beyond these quantitative gains, it offers a flexible natural-language interface in which the language model acts as an audio quality expert: practitioners can query arbitrary aspects of degradations, prompt the model to emulate different listener profiles to capture human variability and produce diverse but plausible judgments rather than a single deterministic score, and thereby reduce reliance on large-scale crowdsourced tests and their monetary cost.",0,arxiv,AI,CC-BY/arXiv,SpeechQualityLLM: LLM-Based Multimodal Assessment of Speech Quality
"Learning about the causal structure of the world is a fundamental problem for human cognition. Causal models and especially causal learning have proved to be difficult for large pretrained models using standard techniques of deep learning. In contrast, cognitive scientists have applied advances in our formal understanding of causation in computer science, particularly within the Causal Bayes Net formalism, to understand human causal learning. In the very different tradition of reinforcement learning, researchers have described an intrinsic reward signal called ""empowerment"" which maximizes mutual information between actions and their outcomes. ""Empowerment"" may be an important bridge between classical Bayesian causal learning and reinforcement learning and may help to characterize causal learning in humans and enable it in machines. If an agent learns an accurate causal world model, they will necessarily increase their empowerment, and increasing empowerment will lead to a more accurate causal world model. Empowerment may also explain distinctive features of childrens causal learning, as well as providing a more tractable computational account of how that learning is possible. In an empirical study, we systematically test how children and adults use cues to empowerment to infer causal relations, and design effective causal interventions.",0,arxiv,AI,CC-BY/arXiv,Empowerment Gain and Causal Model Construction: Children and adults are sensitive to controllability and variability in their causal interventions
"The ability to perform Chain-of-Thought (CoT) reasoning marks a major milestone for multimodal models (MMs), enabling them to solve complex visual reasoning problems. Yet a critical question remains: is such reasoning genuinely grounded in visual evidence and logically coherent? Existing benchmarks emphasize generation but neglect verification, i.e., the capacity to assess whether a reasoning chain is both visually consistent and logically valid. To fill this gap, we introduce MM-CoT, a diagnostic benchmark specifically designed to probe the visual grounding and logical coherence of CoT reasoning in MMs. Instead of generating free-form explanations, models must select the sole event chain that satisfies two orthogonal constraints: (i) visual consistency, ensuring all steps are anchored in observable evidence, and (ii) logical coherence, ensuring causal and commonsense validity. Adversarial distractors are engineered to violate one of these constraints, exposing distinct reasoning failures. We evaluate leading vision-language models on MM-CoT and find that even the most advanced systems struggle, revealing a sharp discrepancy between generative fluency and true reasoning fidelity. MM-CoT shows low correlation with existing benchmarks, confirming that it measures a unique combination of visual grounding and logical reasoning. This benchmark provides a foundation for developing future models that reason not just plausibly, but faithfully and coherently within the visual world.",0,arxiv,AI,CC-BY/arXiv,MM-CoT:A Benchmark for Probing Visual Chain-of-Thought Reasoning in Multimodal Models
"Capsule Networks (CapsNets) show exceptional graph representation capacity via dynamic routing and vectorized hierarchical representations, but they model the complex geometries of real\-world graphs poorly by fixed\-curvature space due to the inherent geodesical disconnectedness issues, leading to suboptimal performance. Recent works find that non\-Euclidean pseudo\-Riemannian manifolds provide specific inductive biases for embedding graph data, but how to leverage them to improve CapsNets is still underexplored. Here, we extend the Euclidean capsule routing into geodesically disconnected pseudo\-Riemannian manifolds and derive a Pseudo\-Riemannian Capsule Network (PR\-CapsNet), which models data in pseudo\-Riemannian manifolds of adaptive curvature, for graph representation learning. Specifically, PR\-CapsNet enhances the CapsNet with Adaptive Pseudo\-Riemannian Tangent Space Routing by utilizing pseudo\-Riemannian geometry. Unlike single\-curvature or subspace\-partitioning methods, PR\-CapsNet concurrently models hierarchical and cluster or cyclic graph structures via its versatile pseudo\-Riemannian metric. It first deploys Pseudo\-Riemannian Tangent Space Routing to decompose capsule states into spherical\-temporal and Euclidean\-spatial subspaces with diffeomorphic transformations. Then, an Adaptive Curvature Routing is developed to adaptively fuse features from different curvature spaces for complex graphs via a learnable curvature tensor with geometric attention from local manifold properties. Finally, a geometric properties\-preserved Pseudo\-Riemannian Capsule Classifier is developed to project capsule embeddings to tangent spaces and use curvature\-weighted softmax for classification. Extensive experiments on node and graph classification benchmarks show PR\-CapsNet outperforms SOTA models, validating PR\-CapsNet's strong representation power for complex graph structures.",0,arxiv,AI,CC-BY/arXiv,PR-CapsNet: Pseudo-Riemannian Capsule Network with Adaptive Curvature Routing for Graph Learning
"We present ClinicalTrialsHub, an interactive search-focused platform that consolidates all data from ClinicalTrials.gov and augments it by automatically extracting and structuring trial-relevant information from PubMed research articles. Our system effectively increases access to structured clinical trial data by 83.8% compared to relying on ClinicalTrials.gov alone, with potential to make access easier for patients, clinicians, researchers, and policymakers, advancing evidence-based medicine. ClinicalTrialsHub uses large language models such as GPT-5.1 and Gemini-3-Pro to enhance accessibility. The platform automatically parses full-text research articles to extract structured trial information, translates user queries into structured database searches, and provides an attributed question-answering system that generates evidence-grounded answers linked to specific source sentences. We demonstrate its utility through a user study involving clinicians, clinical researchers, and PhD students of pharmaceutical sciences and nursing, and a systematic automatic evaluation of its information extraction and question answering capabilities.",0,arxiv,AI,CC-BY/arXiv,ClinicalTrialsHub: Bridging Registries and Literature for Comprehensive Clinical Trial Access
"World models have emerged as a pivotal component in robot manipulation planning, enabling agents to predict future environmental states and reason about the consequences of actions before execution. While video-generation models are increasingly adopted, they often lack rigorous physical grounding, leading to hallucinations and a failure to maintain consistency in long-horizon physical constraints. To address these limitations, we propose Embodied Tree of Thoughts (EToT), a novel Real2Sim2Real planning framework that leverages a physics-based interactive digital twin as an embodied world model. EToT formulates manipulation planning as a tree search expanded through two synergistic mechanisms: (1) Priori Branching, which generates diverse candidate execution paths based on semantic and spatial analysis; and (2) Reflective Branching, which utilizes VLMs to diagnose execution failures within the simulator and iteratively refine the planning tree with corrective actions. By grounding high-level reasoning in a physics simulator, our framework ensures that generated plans adhere to rigid-body dynamics and collision constraints. We validate EToT on a suite of short- and long-horizon manipulation tasks, where it consistently outperforms baselines by effectively predicting physical dynamics and adapting to potential failures. Website at https://embodied-tree-of-thoughts.github.io .",0,arxiv,AI,CC-BY/arXiv,Embodied Tree of Thoughts: Deliberate Manipulation Planning with Embodied World Model
"Medical Large Language Models (LLMs) are increasingly deployed for clinical decision support across diverse specialties, yet systematic evaluation of their robustness to adversarial misuse and privacy leakage remains inaccessible to most researchers. Existing security benchmarks require GPU clusters, commercial API access, or protected health data -- barriers that limit community participation in this critical research area. We propose a practical, fully reproducible framework for evaluating medical AI security under realistic resource constraints. Our framework design covers multiple medical specialties stratified by clinical risk -- from high-risk domains such as emergency medicine and psychiatry to general practice -- addressing jailbreaking attacks (role-playing, authority impersonation, multi-turn manipulation) and privacy extraction attacks. All evaluation utilizes synthetic patient records requiring no IRB approval. The framework is designed to run entirely on consumer CPU hardware using freely available models, eliminating cost barriers. We present the framework specification including threat models, data generation methodology, evaluation protocols, and scoring rubrics. This proposal establishes a foundation for comparative security assessment of medical-specialist models and defense mechanisms, advancing the broader goal of ensuring safe and trustworthy medical AI systems.",0,arxiv,AI,CC-BY/arXiv,A Practical Framework for Evaluating Medical AI Security: Reproducible Assessment of Jailbreaking and Privacy Vulnerabilities Across Clinical Specialties
"Security Operations Centers face massive, heterogeneous alert streams under minute-level service windows, creating the Alert Triage Latency Paradox: verbose reasoning chains ensure accuracy and compliance but incur prohibitive latency and token costs, while minimal chains sacrifice transparency and auditability. Existing solutions fail: signature systems are brittle, anomaly methods lack actionability, and fully cloud-hosted LLMs raise latency, cost, and privacy concerns. We propose AIDR, a hybrid cloud-edge framework that addresses this trade-off through constrained information-density optimization. The core innovation is gradient-based compression of reasoning chains to retain only decision-critical steps--minimal evidence sufficient to justify predictions while respecting token and latency budgets. We demonstrate that this approach preserves decision-relevant information while minimizing complexity. We construct compact datasets by distilling alerts into 3-5 high-information bullets (68% token reduction), train domain-specialized experts via LoRA, and deploy a cloud-edge architecture: a cloud LLM routes alerts to on-premises experts generating SOAR-ready JSON. Experiments demonstrate AIDR achieves higher accuracy and 40.6% latency reduction versus Chain-of-Thought, with robustness to data corruption and out-of-distribution generalization, enabling auditable and efficient SOC triage with full data residency compliance.",0,arxiv,AI,CC-BY/arXiv,Information-Dense Reasoning for Efficient and Auditable Security Alert Triage
"In our prior work, LayerPipe, we had introduced an approach to accelerate training of convolutional, fully connected, and spiking neural networks by overlapping forward and backward computation. However, despite empirical success, a principled understanding of how much gradient delay needs to be introduced at each layer to achieve desired level of pipelining was not addressed. This paper, LayerPipe2, fills that gap by formally deriving LayerPipe using variable delayed gradient adaptation and retiming. We identify where delays may be legally inserted and show that the required amount of delay follows directly from the network structure where inner layers require fewer delays and outer layers require longer delays. When pipelining is applied at every layer, the amount of delay depends only on the number of remaining downstream stages. When layers are pipelined in groups, all layers in the group share the same assignment of delays. These insights not only explain previously observed scheduling patterns but also expose an often overlooked challenge that pipelining implicitly requires storage of historical weights. We overcome this storage bottleneck by developing a pipeline--aware moving average that reconstructs the required past states rather than storing them explicitly. This reduces memory cost without sacrificing the accuracy guarantees that makes pipelined learning viable. The result is a principled framework that illustrates how to construct LayerPipe architectures, predicts their delay requirements, and mitigates their storage burden, thereby enabling scalable pipelined training with controlled communication computation tradeoffs.",0,arxiv,AI,CC-BY/arXiv,LayerPipe2: Multistage Pipelining and Weight Recompute via Improved Exponential Moving Average for Training Neural Networks
"Reinforcement learning (RL) post-training is crucial for aligning generative models with human preferences, but its prohibitive computational cost remains a major barrier to widespread adoption. We introduce \textbf{TreeGRPO}, a novel RL framework that dramatically improves training efficiency by recasting the denoising process as a search tree. From shared initial noise samples, TreeGRPO strategically branches to generate multiple candidate trajectories while efficiently reusing their common prefixes. This tree-structured approach delivers three key advantages: (1) \emph{High sample efficiency}, achieving better performance under same training samples (2) \emph{Fine-grained credit assignment} via reward backpropagation that computes step-specific advantages, overcoming the uniform credit assignment limitation of trajectory-based methods, and (3) \emph{Amortized computation} where multi-child branching enables multiple policy updates per forward pass. Extensive experiments on both diffusion and flow-based models demonstrate that TreeGRPO achieves \textbf{2.4$\times$ faster training} while establishing a superior Pareto frontier in the efficiency-reward trade-off space. Our method consistently outperforms GRPO baselines across multiple benchmarks and reward models, providing a scalable and effective pathway for RL-based visual generative model alignment. The project website is available at treegrpo.github.io.",0,arxiv,AI,CC-BY/arXiv,TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models
"The rising global prevalence of diabetes necessitates early detection to prevent severe complications. While AI-powered prediction applications offer a promising solution, they require a responsive and scalable back-end architecture to serve a large user base effectively. This paper details the development and evaluation of a scalable back-end system designed for a mobile diabetes prediction application. The primary objective was to maintain a failure rate below 5% and an average latency of under 1000 ms. The architecture leverages horizontal scaling, database sharding, and asynchronous communication via a message queue. Performance evaluation showed that 83% of the system's features (20 out of 24) met the specified performance targets. Key functionalities such as user profile management, activity tracking, and read-intensive prediction operations successfully achieved the desired performance. The system demonstrated the ability to handle up to 10,000 concurrent users without issues, validating its scalability. The implementation of asynchronous communication using RabbitMQ proved crucial in minimizing the error rate for computationally intensive prediction requests, ensuring system reliability by queuing requests and preventing data loss under heavy load.",0,arxiv,AI,CC-BY/arXiv,Scalable Back-End for an AI-Based Diabetes Prediction Application
"The future of UAV interaction systems is evolving from engineer-driven to user-driven, aiming to replace traditional predefined Human-UAV Interaction designs. This shift focuses on enabling more personalized task planning and design, thereby achieving a higher quality of interaction experience and greater flexibility, which can be used in many fileds, such as agriculture, aerial photography, logistics, and environmental monitoring. However, due to the lack of a common language between users and the UAVs, such interactions are often difficult to be achieved. The developments of Large Language Models possess the ability to understand nature languages and Robots' (UAVs') behaviors, marking the possibility of personalized Human-UAV Interaction. Recently, some HUI frameworks based on LLMs have been proposed, but they commonly suffer from difficulties in mixed task planning and execution, leading to low adaptability in complex scenarios. In this paper, we propose a novel dual-agent HUI framework. This framework constructs two independent LLM agents (a task planning agent, and an execution agent) and applies different Prompt Engineering to separately handle the understanding, planning, and execution of tasks. To verify the effectiveness and performance of the framework, we have built a task database covering four typical application scenarios of UAVs and quantified the performance of the HUI framework using three independent metrics. Meanwhile different LLM models are selected to control the UAVs with compared performance. Our user study experimental results demonstrate that the framework improves the smoothness of HUI and the flexibility of task execution in the tasks scenario we set up, effectively meeting users' personalized needs.",0,arxiv,AI,CC-BY/arXiv,Chat with UAV -- Human-UAV Interaction Based on Large Language Models
"Our work introduces the DermETAS-SNA LLM Assistant that integrates Dermatology-focused Evolutionary Transformer Architecture Search with StackNet Augmented LLM. The assistant dynamically learns skin-disease classifiers and provides medically informed descriptions to facilitate clinician-patient interpretation. Contributions include: (1) Developed an ETAS framework on the SKINCON dataset to optimize a Vision Transformer (ViT) tailored for dermatological feature representation and then fine-tuned binary classifiers for each of the 23 skin disease categories in the DermNet dataset to enhance classification performance; (2) Designed a StackNet architecture that integrates multiple fine-tuned binary ViT classifiers to enhance predictive robustness and mitigate class imbalance issues; (3) Implemented a RAG pipeline, termed Diagnostic Explanation and Retrieval Model for Dermatology, which harnesses the capabilities of the Google Gemini 2.5 Pro LLM architecture to generate personalized, contextually informed diagnostic descriptions and explanations for patients, leveraging a repository of verified dermatological materials; (4) Performed extensive experimental evaluations on 23 skin disease categories to demonstrate performance increase, achieving an overall F1-score of 56.30% that surpasses SkinGPT-4 (48.51%) by a considerable margin, representing a performance increase of 16.06%; (5) Conducted a domain-expert evaluation, with eight licensed medical doctors, of the clinical responses generated by our AI assistant for seven dermatological conditions. Our results show a 92% agreement rate with the assessments provided by our AI assistant (6) Created a proof-of-concept prototype that fully integrates our DermETAS-SNA LLM into our AI assistant to demonstrate its practical feasibility for real-world clinical and educational applications.",0,arxiv,AI,CC-BY/arXiv,DermETAS-SNA LLM: A Dermatology Focused Evolutionary Transformer Architecture Search with StackNet Augmented LLM Assistant
"Both model developers and policymakers seek to quantify and mitigate the risk of rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons. An important element of such efforts is the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper describes the first component of a novel Biothreat Benchmark Generation (BBG) Framework. The BBG approach is designed to help model developers and evaluators reliably measure and assess the biosecurity risk uplift and general harm potential of existing and future AI models, while accounting for key aspects of the threat itself that are often overlooked in other benchmarking efforts, including different actor capability levels, and operational (in addition to purely technical) risk factors. As a pilot, the BBG is first being developed to address bacterial biological threats only. The BBG is built upon a hierarchical structure of biothreat categories, elements and tasks, which then serves as the basis for the development of task-aligned queries. This paper outlines the development of this biothreat task-query architecture, which we have named the Bacterial Biothreat Schema, while future papers will describe follow-on efforts to turn queries into model prompts, as well as how the resulting benchmarks can be implemented for model evaluation. Overall, the BBG Framework, including the Bacterial Biothreat Schema, seeks to offer a robust, re-usable structure for evaluating bacterial biological risks arising from LLMs across multiple levels of aggregation, which captures the full scope of technical and operational requirements for biological adversaries, and which accounts for a wide spectrum of biological adversary capabilities.",0,arxiv,AI,CC-BY/arXiv,Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models I: The Task-Query Architecture
"This paper will propose a novel machine learning based portfolio management method in the context of the cryptocurrency market. Previous researchers mainly focus on the prediction of the movement for specific cryptocurrency such as the bitcoin(BTC) and then trade according to the prediction. In contrast to the previous work that treats the cryptocurrencies independently, this paper manages a group of cryptocurrencies by analyzing the relative relationship. Specifically, in each time step, we utilize the neural network to predict the rank of the future return of the managed cryptocurrencies and place weights accordingly. By incorporating such cross-sectional information, the proposed methods is shown to profitable based on the backtesting experiments on the real daily cryptocurrency market data from May, 2020 to Nov, 2023. During this 3.5 years, the market experiences the full cycle of bullish, bearish and stagnant market conditions. Despite under such complex market conditions, the proposed method outperforms the existing methods and achieves a Sharpe ratio of 1.01 and annualized return of 64.26%. Additionally, the proposed method is shown to be robust to the increase of transaction fee.",0,arxiv,AI,CC-BY/arXiv,Long-only cryptocurrency portfolio management by ranking the assets: a neural network approach
"Rigorous evaluation of large language models (LLMs) relies on comparing models by the prevalence of desirable or undesirable behaviors, such as task pass rates or policy violations. These prevalence estimates are produced by a classifier, either an LLM-as-a-judge or human annotators, making the choice of classifier central to trustworthy evaluation. Common metrics used for this choice, such as Accuracy, Precision, and F1, are sensitive to class imbalance and to arbitrary choices of positive class, and can favor judges that distort prevalence estimates. We show that Youden's $J$ statistic is theoretically aligned with choosing the best judge to compare models, and that Balanced Accuracy is an equivalent linear transformation of $J$. Through both analytical arguments and empirical examples and simulations, we demonstrate how selecting judges using Balanced Accuracy leads to better, more robust classifier selection.",0,arxiv,AI,CC-BY/arXiv,Balanced Accuracy: The Right Metric for Evaluating LLM Judges -- Explained through Youden's J statistic
"In this paper, we study whether model-based reinforcement learning (RL), in particular model-based value expansion, can provide a scalable recipe for tackling complex, long-horizon tasks in offline RL. Model-based value expansion fits an on-policy value function using length-n imaginary rollouts generated by the current policy and a learned dynamics model. While larger n reduces bias in value bootstrapping, it amplifies accumulated model errors over long horizons, degrading future predictions. We address this trade-off with an \emph{action-chunk} model that predicts a future state from a sequence of actions (an ""action chunk"") instead of a single action, which reduces compounding errors. In addition, instead of directly training a policy to maximize rewards, we employ rejection sampling from an expressive behavioral action-chunk policy, which prevents model exploitation from out-of-distribution actions. We call this recipe \textbf{Model-Based RL with Action Chunks (MAC)}. Through experiments on highly challenging tasks with large-scale datasets of up to 100M transitions, we show that MAC achieves the best performance among offline model-based RL algorithms, especially on challenging long-horizon tasks.",0,arxiv,AI,CC-BY/arXiv,Scalable Offline Model-Based RL with Action Chunks
"Large language models (LLMs) can be dishonest when reporting on their actions and beliefs -- for example, they may overstate their confidence in factual claims or cover up evidence of covert actions. Such dishonesty may arise due to the effects of reinforcement learning (RL), where challenges with reward shaping can result in a training process that inadvertently incentivizes the model to lie or misrepresent its actions.   In this work we propose a method for eliciting an honest expression of an LLM's shortcomings via a self-reported *confession*. A confession is an output, provided upon request after a model's original answer, that is meant to serve as a full account of the model's compliance with the letter and spirit of its policies and instructions. The reward assigned to a confession during training is solely based on its honesty, and does not impact positively or negatively the main answer's reward. As long as the ""path of least resistance"" for maximizing confession reward is to surface misbehavior rather than covering it up, this incentivizes models to be honest in their confessions. Our findings provide some justification this empirical assumption, especially in the case of egregious model misbehavior.   To demonstrate the viability of our approach, we train GPT-5-Thinking to produce confessions, and we evaluate its honesty in out-of-distribution scenarios measuring hallucination, instruction following, scheming, and reward hacking. We find that when the model lies or omits shortcomings in its ""main"" answer, it often confesses to these behaviors honestly, and this confession honesty modestly improves with training. Confessions can enable a number of inference-time interventions including monitoring, rejection sampling, and surfacing issues to the user.",0,arxiv,AI,CC-BY/arXiv,Training LLMs for Honesty via Confessions
"We investigate the short-context dominance hypothesis: that for most sequences, a small local prefix suffices to predict their next tokens. Using large language models as statistical oracles, we measure the minimum context length (MCL) needed to reproduce accurate full-context predictions across datasets with sequences of varying lengths. For sequences with 1-7k tokens from long-context documents, we consistently find that 75-80% require only the last 96 tokens at most. Given the dominance of short-context tokens, we then ask whether it is possible to detect challenging long-context sequences for which a short local prefix does not suffice for prediction. We introduce a practical proxy to MCL, called Distributionally Aware MCL (DaMCL), that does not require knowledge of the actual next-token and is compatible with sampling strategies beyond greedy decoding. Our experiments validate that simple thresholding of the metric defining DaMCL achieves high performance in detecting long vs. short context sequences. Finally, to counter the bias that short-context dominance induces in LLM output distributions, we develop an intuitive decoding algorithm that leverages our detector to identify and boost tokens that are long-range-relevant. Across Q&A tasks and model architectures, we confirm that mitigating the bias improves performance.",0,arxiv,AI,CC-BY/arXiv,Short-Context Dominance: How Much Local Context Natural Language Actually Needs?
"Pretrained Large Language Models (LLMs) have achieved remarkable success across diverse domains, with education and research emerging as particularly impactful areas. Among current state-of-the-art LLMs, ChatGPT and DeepSeek exhibit strong capabilities in mathematics, science, medicine, literature, and programming. In this study, we present a comprehensive evaluation of these two LLMs through background technology analysis, empirical experiments, and a real-world user survey. The evaluation explores trade-offs among model accuracy, computational efficiency, and user experience in educational and research affairs. We benchmarked these LLMs performance in text generation, programming, and specialized problem-solving. Experimental results show that ChatGPT excels in general language understanding and text generation, while DeepSeek demonstrates superior performance in programming tasks due to its efficiency-focused design. Moreover, both models deliver medically accurate diagnostic outputs and effectively solve complex mathematical problems. Complementing these quantitative findings, a survey of students, educators, and researchers highlights the practical benefits and limitations of these models, offering deeper insights into their role in advancing education and research.",0,arxiv,AI,CC-BY/arXiv,Large Language Models for Education and Research: An Empirical and User Survey-based Analysis
"Joint activity describes when more than one agent (human or machine) contributes to the completion of a task or activity. Designing for joint activity focuses on explicitly supporting the interdependencies between agents necessary for effective coordination among agents engaged in the joint activity. This builds and expands upon designing for usability to further address how technologies can be designed to act as effective team players. Effective joint activity requires supporting, at minimum, five primary macrocognitive functions within teams: Event Detection, Sensemaking, Adaptability, Perspective-Shifting, and Coordination. Supporting these functions is equally as important as making technologies usable. We synthesized fourteen heuristics from relevant literature including display design, human factors, cognitive systems engineering, cognitive psychology, and computer science to aid the design, development, and evaluation of technologies that support joint human-machine activity.",0,arxiv,AI,CC-BY/arXiv,Joint Activity Design Heuristics for Enhancing Human-Machine Collaboration
"Screening patients for clinical trial eligibility remains a manual, time-consuming, and resource-intensive process. We present a secure, scalable proof-of-concept system for Artificial Intelligence (AI)-augmented patient-trial matching that addresses key implementation challenges: integrating heterogeneous electronic health record (EHR) data, facilitating expert review, and maintaining rigorous security standards. Leveraging open-source, reasoning-enabled large language models (LLMs), the system moves beyond binary classification to generate structured eligibility assessments with interpretable reasoning chains that support human-in-the-loop review. This decision support tool represents eligibility as a dynamic state rather than a fixed determination, identifying matches when available and offering actionable recommendations that could render a patient eligible in the future. The system aims to reduce coordinator burden, intelligently broaden the set of trials considered for each patient and guarantee comprehensive auditability of all AI-generated outputs.",0,arxiv,AI,CC-BY/arXiv,Toward an AI Reasoning-Enabled System for Patient-Clinical Trial Matching
"Cartographic reasoning is the skill of interpreting geographic relationships by aligning legends, map scales, compass directions, map texts, and geometries across one or more map images. Although essential as a concrete cognitive capability and for critical tasks such as disaster response and urban planning, it remains largely unevaluated. Building on progress in chart and infographic understanding, recent large vision language model studies on map visual question-answering often treat maps as a special case of charts. In contrast, map VQA demands comprehension of layered symbology (e.g., symbols, geometries, and text labels) as well as spatial relations tied to orientation and distance that often span multiple maps and are not captured by chart-style evaluations. To address this gap, we introduce FRIEDA, a benchmark for testing complex open-ended cartographic reasoning in LVLMs. FRIEDA sources real map images from documents and reports in various domains and geographical areas. Following classifications in Geographic Information System (GIS) literature, FRIEDA targets all three categories of spatial relations: topological (border, equal, intersect, within), metric (distance), and directional (orientation). All questions require multi-step inference, and many require cross-map grounding and reasoning. We evaluate eleven state-of-the-art LVLMs under two settings: (1) the direct setting, where we provide the maps relevant to the question, and (2) the contextual setting, where the model may have to identify the maps relevant to the question before reasoning. Even the strongest models, Gemini-2.5-Pro and GPT-5-Think, achieve only 38.20% and 37.20% accuracy, respectively, far below human performance of 84.87%. These results reveal a persistent gap in multi-step cartographic reasoning, positioning FRIEDA as a rigorous benchmark to drive progress on spatial intelligence in LVLMs.",0,arxiv,AI,CC-BY/arXiv,FRIEDA: Benchmarking Multi-Step Cartographic Reasoning in Vision-Language Models
"Large reasoning models (LRMs) often cost significant key-value (KV) cache overhead, due to their linear growth with the verbose chain-of-thought (CoT) reasoning process. This costs both memory and throughput bottleneck limiting their efficient deployment. Towards reducing KV cache size during inference, we first investigate the effectiveness of existing KV cache eviction methods for CoT reasoning. Interestingly, we find that due to unstable token-wise scoring and the reduced effective KV budget caused by padding tokens, state-of-the-art (SoTA) eviction methods fail to maintain accuracy in the multi-batch setting. Additionally, these methods often generate longer sequences than the original model, as semantic-unaware token-wise eviction leads to repeated revalidation during reasoning. To address these issues, we present \textbf{SkipKV}, a \textbf{\textit{training-free}} KV compression method for selective \textit{eviction} and \textit{generation} operating at a coarse-grained sentence-level sequence removal for efficient CoT reasoning. In specific, it introduces a \textit{sentence-scoring metric} to identify and remove highly similar sentences while maintaining semantic coherence. To suppress redundant generation, SkipKV dynamically adjusts a steering vector to update the hidden activation states during inference enforcing the LRM to generate concise response. Extensive evaluations on multiple reasoning benchmarks demonstrate the effectiveness of SkipKV in maintaining up to $\mathbf{26.7}\%$ improved accuracy compared to the alternatives, at a similar compression budget. Additionally, compared to SoTA, SkipKV yields up to $\mathbf{1.6}\times$ fewer generation length while improving throughput up to $\mathbf{1.7}\times$.",0,arxiv,AI,CC-BY/arXiv,SkipKV: Selective Skipping of KV Generation and Storage for Efficient Inference with Large Reasoning Models
"Today, with the growing obsession with applying Artificial Intelligence (AI), particularly Machine Learning (ML), to software across various contexts, much of the focus has been on the effectiveness of AI models, often measured through common metrics such as F1- score, while fairness receives relatively little attention. This paper presents a review of existing gray literature, examining fairness requirements in AI context, with a focus on how they are defined across various application domains, managed throughout the Software Development Life Cycle (SDLC), and the causes, as well as the corresponding consequences of their violation by AI models. Our gray literature investigation shows various definitions of fairness requirements in AI systems, commonly emphasizing non-discrimination and equal treatment across different demographic and social attributes. Fairness requirement management practices vary across the SDLC, particularly in model training and bias mitigation, fairness monitoring and evaluation, and data handling practices. Fairness requirement violations are frequently linked, but not limited, to data representation bias, algorithmic and model design bias, human judgment, and evaluation and transparency gaps. The corresponding consequences include harm in a broad sense, encompassing specific professional and societal impacts as key examples, stereotype reinforcement, data and privacy risks, and loss of trust and legitimacy in AI-supported decisions. These findings emphasize the need for consistent frameworks and practices to integrate fairness into AI software, paying as much attention to fairness as to effectiveness.",0,arxiv,AI,CC-BY/arXiv,A Gray Literature Study on Fairness Requirements in AI-enabled Software Engineering
"Accurate understanding of anatomical structures is essential for reliably staging certain dental diseases. A way of introducing this within semantic segmentation models is by utilising hierarchy-aware methodologies. However, existing hierarchy-aware segmentation methods largely encode anatomical structure through the loss functions, providing weak and indirect supervision. We introduce a general framework that embeds an explicit anatomical hierarchy into semantic segmentation by coupling a recurrent, level-wise prediction scheme with restrictive output heads and top-down feature conditioning. At each depth of the class tree, the backbone is re-run on the original image concatenated with logits from the previous level. Child class features are conditioned using Feature-wise Linear Modulation of their parent class probabilities, to modulate child feature spaces for fine grained detection. A probabilistic composition rule enforces consistency between parent and descendant classes. Hierarchical loss combines per-level class weighted Dice and cross entropy loss and a consistency term loss, ensuring parent predictions are the sum of their children. We validate our approach on our proposed dataset, TL-pano, containing 194 panoramic radiographs with dense instance and semantic segmentation annotations, of tooth layers and alveolar bone. Utilising UNet and HRNet as donor models across a 5-fold cross validation scheme, the hierarchical variants consistently increase IoU, Dice, and recall, particularly for fine-grained anatomies, and produce more anatomically coherent masks. However, hierarchical variants also demonstrated increased recall over precision, implying increased false positives. The results demonstrate that explicit hierarchical structuring improves both performance and clinical plausibility, especially in low data dental imaging regimes.",0,arxiv,AI,CC-BY/arXiv,Restrictive Hierarchical Semantic Segmentation for Stratified Tooth Layer Detection
"As machine learning (ML) becomes an integral part of high-autonomy systems, it is critical to ensure the trustworthiness of learning-enabled software systems (LESS). Yet, the nondeterministic and run-time-defined semantics of ML complicate traditional software refactoring. We define semantic preservation in LESS as the property that optimizations of intelligent components do not alter the system's overall functional behavior. This paper introduces an empirical framework to evaluate semantic preservation in LESS by mining model evolution data from HuggingFace. We extract commit histories, $\textit{Model Cards}$, and performance metrics from a large number of models. To establish baselines, we conducted case studies in three domains, tracing performance changes across versions. Our analysis demonstrates how $\textit{semantic drift}$ can be detected via evaluation metrics across commits and reveals common refactoring patterns based on commit message analysis. Although API constraints limited the possibility of estimating a full-scale threshold, our pipeline offers a foundation for defining community-accepted boundaries for semantic preservation. Our contributions include: (1) a large-scale dataset of ML model evolution, curated from 1.7 million Hugging Face entries via a reproducible pipeline using the native HF hub API, (2) a practical pipeline for the evaluation of semantic preservation for a subset of 536 models and 4000+ metrics and (3) empirical case studies illustrating semantic drift in practice. Together, these contributions advance the foundations for more maintainable and trustworthy ML systems.",0,arxiv,AI,CC-BY/arXiv,An Empirical Framework for Evaluating Semantic Preservation Using Hugging Face
"Humans do not just see attribute similarity -- we also see relational similarity. An apple is like a peach because both are reddish fruit, but the Earth is also like a peach: its crust, mantle, and core correspond to the peach's skin, flesh, and pit. This ability to perceive and recognize relational similarity, is arguable by cognitive scientist to be what distinguishes humans from other species. Yet, all widely used visual similarity metrics today (e.g., LPIPS, CLIP, DINO) focus solely on perceptual attribute similarity and fail to capture the rich, often surprising relational similarities that humans perceive. How can we go beyond the visible content of an image to capture its relational properties? How can we bring images with the same relational logic closer together in representation space? To answer these questions, we first formulate relational image similarity as a measurable problem: two images are relationally similar when their internal relations or functions among visual elements correspond, even if their visual attributes differ. We then curate 114k image-caption dataset in which the captions are anonymized -- describing the underlying relational logic of the scene rather than its surface content. Using this dataset, we finetune a Vision-Language model to measure the relational similarity between images. This model serves as the first step toward connecting images by their underlying relational structure rather than their visible appearance. Our study shows that while relational similarity has a lot of real-world applications, existing image similarity models fail to capture it -- revealing a critical gap in visual computing.",0,arxiv,AI,CC-BY/arXiv,Relational Visual Similarity
"Enterprise data management is a monumental task. It spans data architecture and systems, integration, quality, governance, and continuous improvement. While AI assistants can help specific persona, such as data engineers and stewards, to navigate and configure the data stack, they fall far short of full automation. However, as AI becomes increasingly capable of tackling tasks that have previously resisted automation due to inherent complexities, we believe there is an imminent opportunity to target fully autonomous data estates. Currently, AI is used in different parts of the data stack, but in this paper, we argue for a paradigm shift from the use of AI in independent data component operations towards a more holistic and autonomous handling of the entire data lifecycle. Towards that end, we explore how each stage of the modern data stack can be autonomously managed by intelligent agents to build self-sufficient systems that can be used not only by human end-users, but also by AI itself. We begin by describing the mounting forces and opportunities that demand this paradigm shift, examine how agents can streamline the data lifecycle, and highlight open questions and areas where additional research is needed. We hope this work will inspire lively debate, stimulate further research, motivate collaborative approaches, and facilitate a more autonomous future for data systems.",0,arxiv,AI,CC-BY/arXiv,"Can AI autonomously build, operate, and use the entire data stack?"
"Visual generative models (e.g., diffusion models) typically operate in compressed latent spaces to balance training efficiency and sample quality. In parallel, there has been growing interest in leveraging high-quality pre-trained visual representations, either by aligning them inside VAEs or directly within the generative model. However, adapting such representations remains challenging due to fundamental mismatches between understanding-oriented features and generation-friendly latent spaces. Representation encoders benefit from high-dimensional latents that capture diverse hypotheses for masked regions, whereas generative models favor low-dimensional latents that must faithfully preserve injected noise. This discrepancy has led prior work to rely on complex objectives and architectures. In this work, we propose FAE (Feature Auto-Encoder), a simple yet effective framework that adapts pre-trained visual representations into low-dimensional latents suitable for generation using as little as a single attention layer, while retaining sufficient information for both reconstruction and understanding. The key is to couple two separate deep decoders: one trained to reconstruct the original feature space, and a second that takes the reconstructed features as input for image generation. FAE is generic; it can be instantiated with a variety of self-supervised encoders (e.g., DINO, SigLIP) and plugged into two distinct generative families: diffusion models and normalizing flows. Across class-conditional and text-to-image benchmarks, FAE achieves strong performance. For example, on ImageNet 256x256, our diffusion model with CFG attains a near state-of-the-art FID of 1.29 (800 epochs) and 1.70 (80 epochs). Without CFG, FAE reaches the state-of-the-art FID of 1.48 (800 epochs) and 2.08 (80 epochs), demonstrating both high quality and fast learning.",0,arxiv,AI,CC-BY/arXiv,One Layer Is Enough: Adapting Pretrained Visual Encoders for Image Generation
"The challenges of ongoing war in Sudan highlight the need for rapid monitoring and analysis of such conflicts. Advances in deep learning and readily available satellite remote sensing imagery allow for near real-time monitoring. This paper uses 4-band imagery from Planet Labs with a deep learning model to show that fire damage in armed conflicts can be monitored with minimal delay. We demonstrate the effectiveness of our approach using five case studies in Sudan. We show that, compared to a baseline, the automated method captures the active fires and charred areas more accurately. Our results indicate that using 8-band imagery or time series of such imagery only result in marginal gains.",0,arxiv,AI,CC-BY/arXiv,Near-real time fires detection using satellite imagery in Sudan conflict
"Recent video generators achieve striking photorealism, yet remain fundamentally inconsistent in 3D. We present WorldReel, a 4D video generator that is natively spatio-temporally consistent. WorldReel jointly produces RGB frames together with 4D scene representations, including pointmaps, camera trajectory, and dense flow mapping, enabling coherent geometry and appearance modeling over time. Our explicit 4D representation enforces a single underlying scene that persists across viewpoints and dynamic content, yielding videos that remain consistent even under large non-rigid motion and significant camera movement. We train WorldReel by carefully combining synthetic and real data: synthetic data providing precise 4D supervision (geometry, motion, and camera), while real videos contribute visual diversity and realism. This blend allows WorldReel to generalize to in-the-wild footage while preserving strong geometric fidelity. Extensive experiments demonstrate that WorldReel sets a new state-of-the-art for consistent video generation with dynamic scenes and moving cameras, improving metrics of geometric consistency, motion coherence, and reducing view-time artifacts over competing methods. We believe that WorldReel brings video generation closer to 4D-consistent world modeling, where agents can render, interact, and reason about scenes through a single and stable spatiotemporal representation.",0,arxiv,AI,CC-BY/arXiv,WorldReel: 4D Video Generation with Consistent Geometry and Motion Modeling
"Why do modern language models, trained to do well on next-word prediction, appear to generate coherent documents and capture long-range structure? Here we show that next-token prediction is provably powerful for learning longer-range structure, even with common neural network architectures. Specifically, we prove that optimizing next-token prediction over a Recurrent Neural Network (RNN) yields a model that closely approximates the training distribution: for held-out documents sampled from the training distribution, no algorithm of bounded description length limited to examining the next $k$ tokens, for any $k$, can distinguish between $k$ consecutive tokens of such documents and $k$ tokens generated by the learned language model following the same prefix. We provide polynomial bounds (in $k$, independent of the document length) on the model size needed to achieve such $k$-token indistinguishability, offering a complexity-theoretic explanation for the long-range coherence observed in practice.",0,arxiv,AI,CC-BY/arXiv,Provable Long-Range Benefits of Next-Token Prediction
"Large language models for code (LLM4Code) have greatly improved developer productivity but also raise privacy concerns due to their reliance on open-source repositories containing abundant personally identifiable information (PII). Prior work shows that commercial models can reproduce sensitive PII, yet existing studies largely treat PII as a single category and overlook the heterogeneous risks among different types. We investigate whether distinct PII types vary in their likelihood of being learned and leaked by LLM4Code, and whether this relationship is causal. Our methodology includes building a dataset with diverse PII types, fine-tuning representative models of different scales, computing training dynamics on real PII data, and formulating a structural causal model to estimate the causal effect of learnability on leakage. Results show that leakage risks differ substantially across PII types and correlate with their training dynamics: easy-to-learn instances such as IP addresses exhibit higher leakage, while harder types such as keys and passwords leak less frequently. Ambiguous types show mixed behaviors. This work provides the first causal evidence that leakage risks are type-dependent and offers guidance for developing type-aware and learnability-aware defenses for LLM4Code.",0,arxiv,AI,CC-BY/arXiv,Understanding Privacy Risks in Code Models Through Training Dynamics: A Causal Approach
"Future AI systems could conceal their capabilities ('sandbagging') during evaluations, potentially misleading developers and auditors. We stress-tested sandbagging detection techniques using an auditing game. First, a red team fine-tuned five models, some of which conditionally underperformed, as a proxy for sandbagging. Second, a blue team used black-box, model-internals, or training-based approaches to identify sandbagging models. We found that the blue team could not reliably discriminate sandbaggers from benign models. Black-box approaches were defeated by effective imitation of a weaker model. Linear probes, a model-internals approach, showed more promise but their naive application was vulnerable to behaviours instilled by the red team. We also explored capability elicitation as a strategy for detecting sandbagging. Although Prompt-based elicitation was not reliable, training-based elicitation consistently elicited full performance from the sandbagging models, using only a single correct demonstration of the evaluation task. However the performance of benign models was sometimes also raised, so relying on elicitation as a detection strategy was prone to false-positives. In the short-term, we recommend developers remove potential sandbagging using on-distribution training for elicitation. In the longer-term, further research is needed to ensure the efficacy of training-based elicitation, and develop robust methods for sandbagging detection. We open source our model organisms at https://github.com/AI-Safety-Institute/sandbagging_auditing_games and select transcripts and results at https://huggingface.co/datasets/sandbagging-games/evaluation_logs . A demo illustrating the game can be played at https://sandbagging-demo.far.ai/ .",0,arxiv,AI,CC-BY/arXiv,Auditing Games for Sandbagging
"We present GRAPE (Group RepresentAtional Position Encoding), a unified framework for positional encoding based on group actions. GRAPE brings together two families of mechanisms: (i) multiplicative rotations (Multiplicative GRAPE) in $\mathrm{SO}(d)$ and (ii) additive logit biases (Additive GRAPE) arising from unipotent actions in the general linear group $\mathrm{GL}$. In Multiplicative GRAPE, a position $n \in \mathbb{Z}$ (or $t \in \mathbb{R}$) acts as $\mathbf{G}(n)=\exp(n\,Ï‰\,\mathbf{L})$ with a rank-2 skew generator $\mathbf{L} \in \mathbb{R}^{d \times d}$, yielding a relative, compositional, norm-preserving map with a closed-form matrix exponential. RoPE is recovered exactly when the $d/2$ planes are the canonical coordinate pairs with log-uniform spectrum. Learned commuting subspaces and compact non-commuting mixtures strictly extend this geometry to capture cross-subspace feature coupling at $O(d)$ and $O(r d)$ cost per head, respectively. In Additive GRAPE, additive logits arise as rank-1 (or low-rank) unipotent actions, recovering ALiBi and the Forgetting Transformer (FoX) as exact special cases while preserving an exact relative law and streaming cacheability. Altogether, GRAPE supplies a principled design space for positional geometry in long-context models, subsuming RoPE and ALiBi as special cases. Project Page: https://github.com/model-architectures/GRAPE.",0,arxiv,AI,CC-BY/arXiv,Group Representational Position Encoding
"LLM-based agents are increasingly deployed for expert decision support, yet human-AI teams in high-stakes settings do not yet reliably outperform the best individual. We argue this complementarity gap reflects a fundamental mismatch: current agents are trained as answer engines, not as partners in the collaborative sensemaking through which experts actually make decisions. Sensemaking (the ability to co-construct causal explanations, surface uncertainties, and adapt goals) is the key capability that current training pipelines do not explicitly develop or evaluate. We propose Collaborative Causal Sensemaking (CCS) as a research agenda to develop this capability from the ground up, spanning new training environments that reward collaborative thinking, representations for shared human-AI mental models, and evaluation centred on trust and complementarity. Taken together, these directions shift MAS research from building oracle-like answer engines to cultivating AI teammates that co-reason with their human partners over the causal structure of shared decisions, advancing the design of effective human-AI teams.",0,arxiv,AI,CC-BY/arXiv,Collaborative Causal Sensemaking: Closing the Complementarity Gap in Human-AI Decision Support
"We introduce a new paradigm for building large causal models (LCMs) that exploits the enormous potential latent in today's large language models (LLMs). We describe our ongoing experiments with an implemented system called DEMOCRITUS (Decentralized Extraction of Manifold Ontologies of Causal Relations Integrating Topos Universal Slices) aimed at building, organizing, and visualizing LCMs that span disparate domains extracted from carefully targeted textual queries to LLMs. DEMOCRITUS is methodologically distinct from traditional narrow domain and hypothesis centered causal inference that builds causal models from experiments that produce numerical data. A high-quality LLM is used to propose topics, generate causal questions, and extract plausible causal statements from a diverse range of domains. The technical challenge is then to take these isolated, fragmented, potentially ambiguous and possibly conflicting causal claims, and weave them into a coherent whole, converting them into relational causal triples and embedding them into a LCM. Addressing this technical challenge required inventing new categorical machine learning methods, which we can only briefly summarize in this paper, as it is focused more on the systems side of building DEMOCRITUS. We describe the implementation pipeline for DEMOCRITUS comprising of six modules, examine its computational cost profile to determine where the current bottlenecks in scaling the system to larger models. We describe the results of using DEMOCRITUS over a wide range of domains, spanning archaeology, biology, climate change, economics, medicine and technology. We discuss the limitations of the current DEMOCRITUS system, and outline directions for extending its capabilities.",0,arxiv,AI,CC-BY/arXiv,Large Causal Models from Large Language Models
"Large language models (LLMs) are increasingly deployed in settings where reasoning, such as multi-step problem solving and chain-of-thought, is essential. Yet, current evaluation practices overwhelmingly report single-run accuracy while ignoring the intrinsic uncertainty that naturally arises from stochastic decoding. This omission creates a blind spot because practitioners cannot reliably assess whether a method's reported performance is stable, reproducible, or cost-consistent. We introduce ReasonBENCH, the first benchmark designed to quantify the underlying instability in LLM reasoning. ReasonBENCH provides (i) a modular evaluation library that standardizes reasoning frameworks, models, and tasks, (ii) a multi-run protocol that reports statistically reliable metrics for both quality and cost, and (iii) a public leaderboard to encourage variance-aware reporting. Across tasks from different domains, we find that the vast majority of reasoning strategies and models exhibit high instability. Notably, even strategies with similar average performance can display confidence intervals up to four times wider, and the top-performing methods often incur higher and less stable costs. Such instability compromises reproducibility across runs and, consequently, the reliability of reported performance. To better understand these dynamics, we further analyze the impact of prompts, model families, and scale on the trade-off between solve rate and stability. Our results highlight reproducibility as a critical dimension for reliable LLM reasoning and provide a foundation for future reasoning methods and uncertainty quantification techniques. ReasonBENCH is publicly available at https://github.com/au-clan/ReasonBench .",0,arxiv,AI,CC-BY/arXiv,ReasonBENCH: Benchmarking the (In)Stability of LLM Reasoning
"We present a proof-of-principle study demonstrating the use of large language model (LLM) agents to automate a representative high energy physics (HEP) analysis. Using the Higgs boson diphoton cross-section measurement as a case study with ATLAS Open Data, we design a hybrid system that combines an LLM-based supervisor-coder agent with the Snakemake workflow manager. In this architecture, the workflow manager enforces reproducibility and determinism, while the agent autonomously generates, executes, and iteratively corrects analysis code in response to user instructions. We define quantitative evaluation metrics including success rate, error distribution, costs per specific task, and average number of API calls, to assess agent performance across multi-stage workflows. To characterize variability across architectures, we benchmark a representative selection of state-of-the-art LLMs spanning the Gemini and GPT-5 series, the Claude family, and leading open-weight models. While the workflow manager ensures deterministic execution of all analysis steps, the final outputs still show stochastic variation. Although we set the temperature to zero, other sampling parameters (e.g., top-p, top-k) remained at their defaults, and some reasoning-oriented models internally adjust these settings. Consequently, the models do not produce fully deterministic results. This study establishes the first LLM-agent-driven automated data-analysis framework in HEP, enabling systematic benchmarking of model capabilities, stability, and limitations in real-world scientific computing environments. The baseline code used in this work is available at https://huggingface.co/HWresearch/LLM4HEP. This work was accepted as a poster at the Machine Learning and the Physical Sciences (ML4PS) workshop at NeurIPS 2025. The initial submission was made on August 30, 2025.",0,arxiv,AI,CC-BY/arXiv,Automating High Energy Physics Data Analysis with LLM-Powered Agents
"Large language models are vulnerable to jailbreak attacks, threatening their safe deployment in real-world applications. This paper studies black-box multi-turn jailbreaks, aiming to train attacker LLMs to elicit harmful content from black-box models through a sequence of prompt-output interactions. Existing approaches typically rely on single turn optimization, which is insufficient for learning long-term attack strategies. To bridge this gap, we formulate the problem as a multi-turn reinforcement learning task, directly optimizing the harmfulness of the final-turn output as the outcome reward. To mitigate sparse supervision and promote long-term attack strategies, we propose two heuristic process rewards: (1) controlling the harmfulness of intermediate outputs to prevent triggering the black-box model's rejection mechanisms, and (2) maintaining the semantic relevance of intermediate outputs to avoid drifting into irrelevant content. Experimental results on multiple benchmarks show consistently improved attack success rates across multiple models, highlighting the effectiveness of our approach. The code is available at https://github.com/xxiqiao/RL-MTJail. Warning: This paper contains examples of harmful content.",0,arxiv,AI,CC-BY/arXiv,RL-MTJail: Reinforcement Learning for Automated Black-Box Multi-Turn Jailbreaking of Large Language Models
"Although Multimodal Large Language Models (MLLMs) have advanced substantially, they remain vulnerable to object hallucination caused by language priors and visual information loss. To address this, we propose SAVE (Sparse Autoencoder-Driven Visual Information Enhancement), a framework that mitigates hallucination by steering the model along Sparse Autoencoder (SAE) latent features. A binary object-presence question-answering probe identifies the SAE features most indicative of the model's visual information processing, referred to as visual understanding features. Steering the model along these identified features reinforces grounded visual understanding and effectively reduces hallucination. With its simple design, SAVE outperforms state-of-the-art training-free methods on standard benchmarks, achieving a 10\%p improvement in CHAIR\_S and consistent gains on POPE and MMHal-Bench. Extensive evaluations across multiple models and layers confirm the robustness and generalizability of our approach. Further analysis reveals that steering along visual understanding features suppresses the generation of uncertain object tokens and increases attention to image tokens, mitigating hallucination. Code is released at https://github.com/wiarae/SAVE.",0,arxiv,AI,CC-BY/arXiv,SAVE: Sparse Autoencoder-Driven Visual Information Enhancement for Mitigating Object Hallucination
"Action recognition is also key for applications ranging from robotics to healthcare monitoring. Action information can be extracted from the body pose and movements, as well as from the background scene. However, the extent to which deep neural networks (DNNs) make use of information about the body and information about the background remains unclear. Since these two sources of information may be correlated within a training dataset, DNNs might learn to rely predominantly on one of them, without taking full advantage of the other. Unlike DNNs, humans have domain-specific brain regions selective for perceiving bodies, and regions selective for perceiving scenes. The present work tests whether humans are thus more effective at extracting information from both body and background, and whether building brain-inspired deep network architectures with separate domain-specific streams for body and scene perception endows them with more human-like performance. We first demonstrate that DNNs trained using the HAA500 dataset perform almost as accurately on versions of the stimuli that show both body and background and on versions of the stimuli from which the body was removed, but are at chance-level for versions of the stimuli from which the background was removed. Conversely, human participants (N=28) can recognize the same set of actions accurately with all three versions of the stimuli, and perform significantly better on stimuli that show only the body than on stimuli that show only the background. Finally, we implement and test a novel architecture patterned after domain specificity in the brain with separate streams to process body and background information. We show that 1) this architecture improves action recognition performance, and 2) its accuracy across different versions of the stimuli follows a pattern that matches more closely the pattern of accuracy observed in human participants.",0,arxiv,AI,CC-BY/arXiv,Improving action classification with brain-inspired deep networks
"The 2025 Nobel Prize in Chemistry for Metal-Organic Frameworks (MOFs) and recent breakthroughs by Huanting Wang's team at Monash University establish angstrom-scale channels as promising post-silicon substrates with native integrate-and-fire (IF) dynamics. However, utilizing these stochastic, analog materials for deterministic, bit-exact AI workloads (e.g., FP8) remains a paradox. Existing neuromorphic methods often settle for approximation, failing Transformer precision standards. To traverse the gap ""from stochastic ions to deterministic floats,"" we propose a Native Spiking Microarchitecture. Treating noisy neurons as logic primitives, we introduce a Spatial Combinational Pipeline and a Sticky-Extra Correction mechanism. Validation across all 16,129 FP8 pairs confirms 100% bit-exact alignment with PyTorch. Crucially, our architecture reduces Linear layer latency to O(log N), yielding a 17x speedup. Physical simulations further demonstrate robustness against extreme membrane leakage (beta approx 0.01), effectively immunizing the system against the stochastic nature of the hardware.",0,arxiv,AI,CC-BY/arXiv,The Native Spiking Microarchitecture: From Iontronic Primitives to Bit-Exact FP8 Arithmetic
"Electric vehicles (EVs) are key to sustainable mobility, yet their lithium-ion batteries (LIBs) degrade more rapidly under prolonged high states of charge (SOC). This can be mitigated by delaying full charging \ours until just before departure, which requires accurate prediction of user departure times. In this work, we propose Transformer-based real-time-to-event (TTE) model for accurate EV departure prediction. Our approach represents each day as a TTE sequence by discretizing time into grid-based tokens. Unlike previous methods primarily dependent on temporal dependency from historical patterns, our method leverages streaming contextual information to predict departures. Evaluation on a real-world study involving 93 users and passive smartphone data demonstrates that our method effectively captures irregular departure patterns within individual routines, outperforming baseline models. These results highlight the potential for practical deployment of the \ours algorithm and its contribution to sustainable transportation systems.",0,arxiv,AI,CC-BY/arXiv,Enabling Delayed-Full Charging Through Transformer-Based Real-Time-to-Departure Modeling for EV Battery Longevity
"We present CompassMax-V3-Thinking, a hundred-billion-scale MoE reasoning model trained with a new RL framework built on one principle: each prompt must matter. Scaling RL to this size exposes critical inefficiencies-zero-variance prompts that waste rollouts, unstable importance sampling over long horizons, advantage inversion from standard reward models, and systemic bottlenecks in rollout processing. To overcome these challenges, we introduce several unified innovations: (1) Multi-Stage Zero-Variance Elimination, which filters out non-informative prompts and stabilizes group-based policy optimization (e.g. GRPO) by removing wasted rollouts; (2) ESPO, an entropy-adaptive optimization method that balances token-level and sequence-level importance sampling to maintain stable learning dynamics; (3) a Router Replay strategy that aligns training-time MoE router decisions with inference-time behavior to mitigate train-infer discrepancies, coupled with a reward model adjustment to prevent advantage inversion; (4) a high-throughput RL system with FP8-precision rollouts, overlapped reward computation, and length-aware scheduling to eliminate performance bottlenecks. Together, these contributions form a cohesive pipeline that makes RL on hundred-billion-scale MoE models stable and efficient. The resulting model delivers strong performance across both internal and public evaluations.",0,arxiv,AI,CC-BY/arXiv,Each Prompt Matters: Scaling Reinforcement Learning Without Wasting Rollouts on Hundred-Billion-Scale MoE
"Existing data-driven approaches in modeling and predicting time series data include ARIMA (Autoregressive Integrated Moving Average), Transformer-based models, LSTM (Long Short-Term Memory) and TCN (Temporal Convolutional Network). These approaches, and in particular deep learning-based models such as LSTM and TCN, have shown great results in predicting time series data. With the advancement of leveraging pre-trained foundation models such as Large Language Models (LLMs) and more notably Google's recent foundation model for time series data, {\it TimesFM} (Time Series Foundation Model), it is of interest to investigate whether these foundation models have the capability of outperforming existing modeling approaches in analyzing and predicting time series data.   This paper investigates the performance of using LLM models for time series data prediction. We investigate the in-context learning methodology in the training of LLM models that are specific to the underlying application domain. More specifically, the paper explores training LLMs through in-context, zero-shot and few-shot learning and forecasting time series data with OpenAI {\tt o4-mini} and Gemini 2.5 Flash Lite, as well as the recent Google's Transformer-based TimesFM, a time series-specific foundation model, along with two deep learning models, namely TCN and LSTM networks. The findings indicate that TimesFM has the best overall performance with the lowest RMSE value (0.3023) and the competitive inference time (266 seconds). Furthermore, OpenAI's o4-mini also exhibits a good performance based on Zero Shot learning.   These findings highlight pre-trained time series foundation models as a promising direction for real-time forecasting, enabling accurate and scalable deployment with minimal model adaptation.",0,arxiv,AI,CC-BY/arXiv,In-Context and Few-Shots Learning for Forecasting Time Series Data based on Large Language Models
"Radiotherapy planning is a highly complex process that often varies significantly across institutions and individual planners. Most existing deep learning approaches for 3D dose prediction rely on reference plans as ground truth during training, which can inadvertently bias models toward specific planning styles or institutional preferences. In this study, we introduce a novel generative model that predicts 3D dose distributions based solely on user-defined preference flavors. These customizable preferences enable planners to prioritize specific trade-offs between organs-at-risk (OARs) and planning target volumes (PTVs), offering greater flexibility and personalization. Designed for seamless integration with clinical treatment planning systems, our approach assists users in generating high-quality plans efficiently. Comparative evaluations demonstrate that our method can surpasses the Varian RapidPlan model in both adaptability and plan quality in some scenarios.",0,arxiv,AI,CC-BY/arXiv,Demo: Generative AI helps Radiotherapy Planning with User Preference
"Despite substantial progress in text-to-image generation, achieving precise text-image alignment remains challenging, particularly for prompts with rich compositional structure or imaginative elements. To address this, we introduce Negative Prompting for Image Correction (NPC), an automated pipeline that improves alignment by identifying and applying negative prompts that suppress unintended content. We begin by analyzing cross-attention patterns to explain why both targeted negatives-those directly tied to the prompt's alignment error-and untargeted negatives-tokens unrelated to the prompt but present in the generated image-can enhance alignment. To discover useful negatives, NPC generates candidate prompts using a verifier-captioner-proposer framework and ranks them with a salient text-space score, enabling effective selection without requiring additional image synthesis. On GenEval++ and Imagine-Bench, NPC outperforms strong baselines, achieving 0.571 vs. 0.371 on GenEval++ and the best overall performance on Imagine-Bench. By guiding what not to generate, NPC provides a principled, fully automated route to stronger text-image alignment in diffusion models. Code is released at https://github.com/wiarae/NPC.",0,arxiv,AI,CC-BY/arXiv,Guiding What Not to Generate: Automated Negative Prompting for Text-Image Alignment
"Online incivility has emerged as a widespread and persistent problem in digital communities, imposing substantial social and psychological burdens on users. Although many platforms attempt to curb incivility through moderation and automated detection, the performance of existing approaches often remains limited in both accuracy and efficiency. To address this challenge, we propose a Graph Neural Network (GNN) framework for detecting three types of uncivil behavior (i.e., toxicity, aggression, and personal attacks) within the English Wikipedia community. Our model represents each user comment as a node, with textual similarity between comments defining the edges, allowing the network to jointly learn from both linguistic content and relational structures among comments. We also introduce a dynamically adjusted attention mechanism that adaptively balances nodal and topological features during information aggregation. Empirical evaluations demonstrate that our proposed architecture outperforms 12 state-of-the-art Large Language Models (LLMs) across multiple metrics while requiring significantly lower inference cost. These findings highlight the crucial role of structural context in detecting online incivility and address the limitations of text-only LLM paradigms in behavioral prediction. All datasets and comparative outputs will be publicly available in our repository to support further research and reproducibility.",0,arxiv,AI,CC-BY/arXiv,When Large Language Models Do Not Work: Online Incivility Prediction through Graph Neural Networks
"Deep learning holds immense promise for transforming medical image analysis, yet its clinical generalization remains profoundly limited. A major barrier is data heterogeneity. This is particularly true in Magnetic Resonance Imaging, where scanner hardware differences, diverse acquisition protocols, and varying sequence parameters introduce substantial domain shifts that obscure underlying biological signals. Data harmonization methods aim to reduce these instrumental and acquisition variability, but existing approaches remain insufficient. When applied to imaging data, image-based harmonization approaches are often restricted by the need for target images, while existing text-guided methods rely on simplistic labels that fail to capture complex acquisition details or are typically restricted to datasets with limited variability, failing to capture the heterogeneity of real-world clinical environments. To address these limitations, we propose DIST-CLIP (Disentangled Style Transfer with CLIP Guidance), a unified framework for MRI harmonization that flexibly uses either target images or DICOM metadata for guidance. Our framework explicitly disentangles anatomical content from image contrast, with the contrast representations being extracted using pre-trained CLIP encoders. These contrast embeddings are then integrated into the anatomical content via a novel Adaptive Style Transfer module. We trained and evaluated DIST-CLIP on diverse real-world clinical datasets, and showed significant improvements in performance when compared against state-of-the-art methods in both style translation fidelity and anatomical preservation, offering a flexible solution for style transfer and standardizing MRI data. Our code and weights will be made publicly available upon publication.",0,arxiv,AI,CC-BY/arXiv,DIST-CLIP: Arbitrary Metadata and Image Guided MRI Harmonization via Disentangled Anatomy-Contrast Representations
"Recent advances in large language models (LLMs) have given rise to powerful coding agents, making it possible for code assistants to evolve into code engineers. However, existing methods still face significant challenges in achieving high-fidelity document-to-codebase synthesis--such as scientific papers to code--primarily due to a fundamental conflict between information overload and the context bottlenecks of LLMs. In this work, we introduce DeepCode, a fully autonomous framework that fundamentally addresses this challenge through principled information-flow management. By treating repository synthesis as a channel optimization problem, DeepCode seamlessly orchestrates four information operations to maximize task-relevant signals under finite context budgets: source compression via blueprint distillation, structured indexing using stateful code memory, conditional knowledge injection via retrieval-augmented generation, and closed-loop error correction. Extensive evaluations on the PaperBench benchmark demonstrate that DeepCode achieves state-of-the-art performance, decisively outperforming leading commercial agents such as Cursor and Claude Code, and crucially, surpassing PhD-level human experts from top institutes on key reproduction metrics. By systematically transforming paper specifications into production-grade implementations comparable to human expert quality, this work establishes new foundations for autonomous scientific reproduction that can accelerate research evaluation and discovery.",0,arxiv,AI,CC-BY/arXiv,DeepCode: Open Agentic Coding
"Traditional sea exploration faces significant challenges due to extreme conditions, limited visibility, and high costs, resulting in vast unexplored ocean regions. This paper presents an innovative AI-powered Autonomous Underwater Vehicle (AUV) system designed to overcome these limitations by automating underwater object detection, analysis, and reporting. The system integrates YOLOv12 Nano for real-time object detection, a Convolutional Neural Network (CNN) (ResNet50) for feature extraction, Principal Component Analysis (PCA) for dimensionality reduction, and K-Means++ clustering for grouping marine objects based on visual characteristics. Furthermore, a Large Language Model (LLM) (GPT-4o Mini) is employed to generate structured reports and summaries of underwater findings, enhancing data interpretation. The system was trained and evaluated on a combined dataset of over 55,000 images from the DeepFish and OzFish datasets, capturing diverse Australian marine environments. Experimental results demonstrate the system's capability to detect marine objects with a mAP@0.5 of 0.512, a precision of 0.535, and a recall of 0.438. The integration of PCA effectively reduced feature dimensionality while preserving 98% variance, facilitating K-Means clustering which successfully grouped detected objects based on visual similarities. The LLM integration proved effective in generating insightful summaries of detections and clusters, supported by location data. This integrated approach significantly reduces the risks associated with human diving, increases mission efficiency, and enhances the speed and depth of underwater data analysis, paving the way for more effective scientific research and discovery in challenging marine environments.",0,arxiv,AI,CC-BY/arXiv,An AI-Powered Autonomous Underwater System for Sea Exploration and Scientific Research
"We develop a unified mathematical framework for certified Top-$k$ attention truncation that quantifies approximation error at both the distribution and output levels. For a single attention distribution $P$ and its Top-$k$ truncation $\hat P$, we show that the total-variation distance coincides with the discarded softmax tail mass and satisfies $\mathrm{TV}(P,\hat P)=1-e^{-\mathrm{KL}(\hat P\Vert P)}$, yielding sharp Top-$k$-specific bounds in place of generic inequalities. From this we derive non-asymptotic deterministic bounds -- from a single boundary gap through multi-gap and blockwise variants -- that control $\mathrm{TV}(P,\hat P)$ using only the ordered logits. Using an exact head-tail decomposition, we prove that the output error factorizes as $\|\mathrm{Attn}(q,K,V)-\mathrm{Attn}_k(q,K,V)\|_2=Ï„\|Î¼_{\mathrm{tail}}-Î¼_{\mathrm{head}}\|_2$ with $Ï„=\mathrm{TV}(P,\hat P)$, yielding a new head-tail diameter bound $\|\mathrm{Attn}(q,K,V)-\mathrm{Attn}_k(q,K,V)\|_2\leÏ„\,\mathrm{diam}_{H,T}$ and refinements linking the error to $\mathrm{Var}_P(V)$. Under an i.i.d. Gaussian score model $s_i\sim\mathcal N(Î¼,Ïƒ^2)$ we derive closed-form tail masses and an asymptotic rule for the minimal $k_\varepsilon$ ensuring $\mathrm{TV}(P,\hat P)\le\varepsilon$, namely $k_\varepsilon/n\approxÎ¦_c(Ïƒ+Î¦^{-1}(\varepsilon))$. Experiments on bert-base-uncased and synthetic logits confirm the predicted scaling of $k_\varepsilon/n$ and show that certified Top-$k$ can reduce scored keys by 2-4$\times$ on average while meeting the prescribed total-variation budget.",0,arxiv,AI,CC-BY/arXiv,A Mathematical Theory of Top-$k$ Sparse Attention via Total Variation Distance
"When should an autonomous agent commit resources to a task? We introduce the Agent Capability Problem (ACP), a framework for predicting whether an agent can solve a problem under resource constraints. Rather than relying on empirical heuristics, ACP frames problem-solving as information acquisition: an agent requires $\Itotal$ bits to identify a solution and gains $\Istep$ bits per action at cost $\Cstep$, yielding an effective cost $\Ceff = (\Itotal/\Istep), \Cstep$ that predicts resource requirements before search. We prove that $\Ceff$ lower-bounds expected cost and provide tight probabilistic upper bounds. Experimental validation shows that ACP predictions closely track actual agent performance, consistently bounding search effort while improving efficiency over greedy and random strategies. The framework generalizes across LLM-based and agentic workflows, linking principles from active learning, Bayesian optimization, and reinforcement learning through a unified information-theoretic lens. \",0,arxiv,AI,CC-BY/arXiv,The Agent Capability Problem: Predicting Solvability Through Information-Theoretic Bounds
"Process Model Forecasting (PMF) aims to predict how the control-flow structure of a process evolves over time by modeling the temporal dynamics of directly-follows (DF) relations, complementing predictive process monitoring that focuses on single-case prefixes. Prior benchmarks show that machine learning and deep learning models provide only modest gains over statistical baselines, mainly due to the sparsity and heterogeneity of the DF time series. We investigate Time Series Foundation Models (TSFMs), large pre-trained models for generic time series, as an alternative for PMF. Using DF time series derived from real-life event logs, we compare zero-shot use of TSFMs, without additional training, with fine-tuned variants adapted on PMF-specific data. TSFMs generally achieve lower forecasting errors (MAE and RMSE) than traditional and specialized models trained from scratch on the same logs, indicating effective transfer of temporal structure from non-process domains. While fine-tuning can further improve accuracy, the gains are often small and may disappear on smaller or more complex datasets, so zero-shot use remains a strong default. Our study highlights the generalization capability and data efficiency of TSFMs for process-related time series and, to the best of our knowledge, provides the first systematic evaluation of temporal foundation models for PMF.",0,arxiv,AI,CC-BY/arXiv,Time Series Foundation Models for Process Model Forecasting
"The rapid advancement of Large Language Models (LLMs) has resulted in a significant knowledge gap between the open-source community and industry, primarily because the latter relies on closed-source, high-quality data and training recipes. To address this, we introduce PCMind-2.1-Kaiyuan-2B, a fully open-source 2-billion-parameter model focused on improving training efficiency and effectiveness under resource constraints. Our methodology includes three key innovations: a Quantile Data Benchmarking method for systematically comparing heterogeneous open-source datasets and providing insights on data mixing strategies; a Strategic Selective Repetition scheme within a multi-phase paradigm to effectively leverage sparse, high-quality data; and a Multi-Domain Curriculum Training policy that orders samples by quality. Supported by a highly optimized data preprocessing pipeline and architectural modifications for FP16 stability, Kaiyuan-2B achieves performance competitive with state-of-the-art fully open-source models, demonstrating practical and scalable solutions for resource-limited pretraining. We release all assets (including model weights, data, and code) under Apache 2.0 license at https://huggingface.co/thu-pacman/PCMind-2.1-Kaiyuan-2B.",0,arxiv,AI,CC-BY/arXiv,PCMind-2.1-Kaiyuan-2B Technical Report
"This study presents a systematic comparison of three Reinforcement Learning (RL) algorithms (PPO, GRPO, and DAPO) for improving complex reasoning in large language models (LLMs). Our main contribution is a controlled transfer-learning evaluation: models are first fine-tuned on the specialized Countdown Game and then assessed on a suite of general-purpose reasoning benchmarks. Across all tasks, RL-trained models outperform their corresponding base models, although the degree of improvement differs by benchmark.   Our parametric analysis offers practical guidance for RL-based LLM training. Increasing the group size in GRPO and DAPO leads to more stable training dynamics and higher accuracy, while the impact of the KL-penalty coefficient is non-monotonic. Additionally, we find that the Dynamic Sampling (DS) component in DAPO does not improve performance; in fact, the best overall results are achieved with DAPO when DS is disabled.",0,arxiv,AI,CC-BY/arXiv,"Comparative Analysis and Parametric Tuning of PPO, GRPO, and DAPO for LLM Reasoning Enhancement"
"We introduce \emph{Metric-Fair Prompting}, a fairness-aware prompting framework that guides large language models (LLMs) to make decisions under metric-fairness constraints. In the application of multiple-choice medical question answering, each {(question, option)} pair is treated as a binary instance with label $+1$ (correct) or $-1$ (incorrect). To promote {individual fairness}~--~treating similar instances similarly~--~we compute question similarity using NLP embeddings and solve items in \emph{joint pairs of similar questions} rather than in isolation. The prompt enforces a global decision protocol: extract decisive clinical features, map each \((\text{question}, \text{option})\) to a score $f(x)$ that acts as confidence, and impose a Lipschitz-style constraint so that similar inputs receive similar scores and, hence, consistent outputs. Evaluated on the {MedQA (US)} benchmark, Metric-Fair Prompting is shown to improve performance over standard single-item prompting, demonstrating that fairness-guided, confidence-oriented reasoning can enhance LLM accuracy on high-stakes clinical multiple-choice questions.",0,arxiv,AI,CC-BY/arXiv,Metric-Fair Prompting: Treating Similar Samples Similarly
"In this study, we propose a structured methodology that utilizes large language models (LLMs) in a cost-efficient and parsimonious manner, integrating the strengths of scholars and machines while offsetting their respective weaknesses. Our methodology, facilitated through a chain of thought and few-shot learning prompting from computer science, extends best practices for co-author teams in qualitative research to human-machine teams in quantitative research. This allows humans to utilize abductive reasoning and natural language to interrogate not just what the machine has done but also what the human has done. Our method highlights how scholars can manage inherent weaknesses OF LLMs using careful, low-cost techniques. We demonstrate how to use the methodology to interrogate human-machine rating discrepancies for a sample of 1,934 press releases announcing pharmaceutical alliances (1990-2017).",0,arxiv,AI,CC-BY/arXiv,Complementary Learning Approach for Text Classification using Large Language Models
"Accurate segmentation of spinal structures in X-ray images is a prerequisite for quantitative scoliosis assessment, including Cobb angle measurement, vertebral translation estimation and curvature classification. In routine practice, clinicians acquire coronal, left-bending and right-bending radiographs to jointly evaluate deformity severity and spinal flexibility. However, the segmentation step remains heavily manual, time-consuming and non-reproducible, particularly in low-contrast images and in the presence of rib shadows or overlapping tissues. To address these limitations, this paper proposes R2MF-Net, a recurrent residual multi-path encoder--decoder network tailored for automatic segmentation of multi-directional spine X-ray images. The overall design consists of a coarse segmentation network and a fine segmentation network connected in cascade. Both stages adopt an improved Inception-style multi-branch feature extractor, while a recurrent residual jump connection (R2-Jump) module is inserted into skip paths to gradually align encoder and decoder semantics. A multi-scale cross-stage skip (MC-Skip) mechanism allows the fine network to reuse hierarchical representations from multiple decoder levels of the coarse network, thereby strengthening the stability of segmentation across imaging directions and contrast conditions. Furthermore, a lightweight spatial-channel squeeze-and-excitation block (SCSE-Lite) is employed at the bottleneck to emphasize spine-related activations and suppress irrelevant structures and background noise. We evaluate R2MF-Net on a clinical multi-view radiograph dataset comprising 228 sets of coronal, left-bending and right-bending spine X-ray images with expert annotations.",0,arxiv,AI,CC-BY/arXiv,R2MF-Net: A Recurrent Residual Multi-Path Fusion Network for Robust Multi-directional Spine X-ray Segmentation
"Reliable forecasting of multivariate time series under anomalous conditions is crucial in applications such as ATM cash logistics, where sudden demand shifts can disrupt operations. Modern deep forecasters achieve high accuracy on normal data but often fail when distribution shifts occur. We propose Weighted Contrastive Adaptation (WECA), a Weighted contrastive objective that aligns normal and anomaly-augmented representations, preserving anomaly-relevant information while maintaining consistency under benign variations. Evaluations on a nationwide ATM transaction dataset with domain-informed anomaly injection show that WECA improves SMAPE on anomaly-affected data by 6.1 percentage points compared to a normally trained baseline, with negligible degradation on normal data. These results demonstrate that WECA enhances forecasting reliability under anomalies without sacrificing performance during regular operations.",0,arxiv,AI,CC-BY/arXiv,Weighted Contrastive Learning for Anomaly-Aware Time-Series Forecasting
"Cross-modal learning has become a fundamental paradigm for integrating heterogeneous information sources such as images, text, and structured attributes. However, multimodal representations often suffer from modality dominance, redundant information coupling, and spurious cross-modal correlations, leading to suboptimal generalization and limited interpretability. In particular, high-variance modalities tend to overshadow weaker but semantically important signals, while naÃ¯ve fusion strategies entangle modality-shared and modality-specific factors in an uncontrolled manner. This makes it difficult to understand which modality actually drives a prediction and to maintain robustness when some modalities are noisy or missing. To address these challenges, we propose a Dual-Stream Residual Semantic Decorrelation Network (DSRSD-Net), a simple yet effective framework that disentangles modality-specific and modality-shared information through residual decomposition and explicit semantic decorrelation constraints. DSRSD-Net introduces: (1) a dual-stream representation learning module that separates intra-modal (private) and inter-modal (shared) latent factors via residual projection; (2) a residual semantic alignment head that maps shared factors from different modalities into a common space using a combination of contrastive and regression-style objectives; and (3) a decorrelation and orthogonality loss that regularizes the covariance structure of the shared space while enforcing orthogonality between shared and private streams, thereby suppressing cross-modal redundancy and preventing feature collapse. Experimental results on two large-scale educational benchmarks demonstrate that DSRSD-Net consistently improves next-step prediction and final outcome prediction over strong single-modality, early-fusion, late-fusion, and co-attention baselines.",0,arxiv,AI,CC-BY/arXiv,Dual-Stream Cross-Modal Representation Learning via Residual Semantic Decorrelation
"Vision-language models (VLMs) frequently generate hallucinated content plausible but incorrect claims about image content. We propose a training-free self-correction framework enabling VLMs to iteratively refine responses through uncertainty-guided visual re-attention. Our method combines multidimensional uncertainty quantification (token entropy, attention dispersion, semantic consistency, claim confidence) with attention-guided cropping of under-explored regions. Operating entirely with frozen, pretrained VLMs, our framework requires no gradient updates. We validate our approach on the POPE and MMHAL BENCH benchmarks using the Qwen2.5-VL-7B [23] architecture. Experimental results demonstrate that our method reduces hallucination rates by 9.8 percentage points compared to the baseline, while improving object existence accuracy by 4.7 points on adversarial splits. Furthermore, qualitative analysis confirms that uncertainty-guided re-attention successfully grounds corrections in visual evidence where standard decoding fails. We validate our approach on Qwen2.5-VL-7B [23], with plans to extend validation across diverse architectures in future versions. We release our code and methodology to facilitate future research in trustworthy multimodal systems.",0,arxiv,AI,CC-BY/arXiv,Toward More Reliable Artificial Intelligence: Reducing Hallucinations in Vision-Language Models
"As dialogue systems become increasingly important across various domains, a key challenge in persona-based dialogue is generating engaging and context-specific interactions while ensuring the model acts with a coherent personality. However, existing persona-based dialogue datasets lack explicit relations between persona sentences and responses, which makes it difficult for models to effectively capture persona information. To address these issues, we propose MoCoRP (Modeling Consistent Relations between Persona and Response), a framework that incorporates explicit relations into language models. MoCoRP leverages an NLI expert to explicitly extract the NLI relations between persona sentences and responses, enabling the model to effectively incorporate appropriate persona information from the context into its responses. We applied this framework to pre-trained models like BART and further extended it to modern large language models (LLMs) through alignment tuning. Experimental results on the public datasets ConvAI2 and MPChat demonstrate that MoCoRP outperforms existing baselines, achieving superior persona consistency and engaging, context-aware dialogue generation. Furthermore, our model not only excels in quantitative metrics but also shows significant improvements in qualitative aspects. These results highlight the effectiveness of explicitly modeling persona-response relations in persona-based dialogue. The source codes of MoCoRP are available at https://github.com/DMCB-GIST/MoCoRP.",0,arxiv,AI,CC-BY/arXiv,MoCoRP: Modeling Consistent Relations between Persona and Response for Persona-based Dialogue
"Error Span Detection (ESD) is a subtask of automatic machine translation evaluation that localizes error spans in translations and labels their severity. State-of-the-art generative ESD methods typically decode using Maximum a Posteriori (MAP), assuming that model-estimated probabilities are perfectly correlated with similarity to human annotation. However, we observed that annotations dissimilar to the human annotation could achieve a higher model likelihood than the human annotation. We address this issue by applying Minimum Bayes Risk (MBR) decoding to generative ESD models. Specifically, we employ sentence- and span-level similarity metrics as utility functions to select candidate hypotheses based on their approximate similarity to the human annotation. Extensive experimental results show that our MBR decoding outperforms the MAP baseline at the system, sentence, and span-levels. Furthermore, to mitigate the computational cost of MBR decoding, we demonstrate that applying MBR distillation enables a standard greedy model to match MBR decoding performance, effectively eliminating the inference-time latency bottleneck.",0,arxiv,AI,CC-BY/arXiv,Minimum Bayes Risk Decoding for Error Span Detection in Reference-Free Automatic Machine Translation Evaluation
"We propose VulnLLM-R, the~\emph{first specialized reasoning LLM} for vulnerability detection. Our key insight is that LLMs can reason about program states and analyze the potential vulnerabilities, rather than simple pattern matching. This can improve the model's generalizability and prevent learning shortcuts. However, SOTA reasoning LLMs are typically ultra-large, closed-source, or have limited performance in vulnerability detection. To address this, we propose a novel training recipe with specialized data selection, reasoning data generation, reasoning data filtering and correction, and testing-phase optimization. Using our proposed methodology, we train a reasoning model with seven billion parameters. Through extensive experiments on SOTA datasets across Python, C/C++, and Java, we show that VulnLLM-R has superior effectiveness and efficiency than SOTA static analysis tools and both open-source and commercial large reasoning models. We further conduct a detailed ablation study to validate the key designs in our training recipe. Finally, we construct an agent scaffold around our model and show that it outperforms CodeQL and AFL++ in real-world projects. Our agent further discovers a set of zero-day vulnerabilities in actively maintained repositories. This work represents a pioneering effort to enable real-world, project-level vulnerability detection using AI agents powered by specialized reasoning models. The code is available at~\href{https://github.com/ucsb-mlsec/VulnLLM-R}{github}.",0,arxiv,AI,CC-BY/arXiv,VulnLLM-R: Specialized Reasoning LLM with Agent Scaffold for Vulnerability Detection
"We investigate model-based reinforcement learning in contextual Markov decision processes (C-MDPs) in which the context is unobserved and induces confounding in the offline dataset. In such settings, conventional model-learning methods are fundamentally inconsistent, as the transition and reward mechanisms generated under a behavioral policy do not correspond to the interventional quantities required for evaluating a state-based policy. To address this issue, we adapt a proximal off-policy evaluation approach that identifies the confounded reward expectation using only observable state-action-reward trajectories under mild invertibility conditions on proxy variables. When combined with a behavior-averaged transition model, this construction yields a surrogate MDP whose Bellman operator is well defined and consistent for state-based policies, and which integrates seamlessly with the maximum causal entropy (MaxCausalEnt) model-learning framework. The proposed formulation enables principled model learning and planning in confounded environments where contextual information is unobserved, unavailable, or impractical to collect.",0,arxiv,AI,CC-BY/arXiv,Model-Based Reinforcement Learning Under Confounding
"Pre-training decoder-only language models relies on vast amounts of high-quality data, yet the availability of such data is increasingly reaching its limits. While metadata is commonly used to create and curate these datasets, its potential as a direct training signal remains under-explored. We challenge this status quo and propose LIME (Linguistic Metadata Embeddings), a method that enriches token embeddings with metadata capturing syntax, semantics, and contextual properties. LIME substantially improves pre-training efficiency. Specifically, it adapts up to 56% faster to the training data distribution, while introducing only 0.01% additional parameters at negligible compute overhead. Beyond efficiency, LIME improves tokenization, leading to remarkably stronger language modeling capabilities and generative task performance. These benefits persist across model scales (500M to 2B). In addition, we develop a variant with shifted metadata, LIME+1, that can guide token generation. Given prior metadata for the next token, LIME+1 improves reasoning performance by up to 38% and arithmetic accuracy by up to 35%.",0,arxiv,AI,CC-BY/arXiv,LIME: Making LLM Data More Efficient with Linguistic Metadata Embeddings
"Detecting hallucinations in Retrieval-Augmented Generation (RAG) remains a challenge. Prior approaches attribute hallucinations to a binary conflict between internal knowledge (stored in FFNs) and retrieved context. However, this perspective is incomplete, failing to account for the impact of other components in the generative process, such as the user query, previously generated tokens, the current token itself, and the final LayerNorm adjustment. To address this, we introduce SPAD. First, we mathematically attribute each token's probability into seven distinct sources: Query, RAG, Past, Current Token, FFN, Final LayerNorm, and Initial Embedding. This attribution quantifies how each source contributes to the generation of the current token. Then, we aggregate these scores by POS tags to quantify how different components drive specific linguistic categories. By identifying anomalies, such as Nouns relying on Final LayerNorm, SPAD effectively detects hallucinations. Extensive experiments demonstrate that SPAD achieves state-of-the-art performance",0,arxiv,AI,CC-BY/arXiv,SPAD: Seven-Source Token Probability Attribution with Syntactic Aggregation for Detecting Hallucinations in RAG
"The overall neural network (NN) performance is closely related to the properties of its embedding distribution in latent space (LS). It has recently been shown that predefined vector systems, specifically An root system vectors, can be used as targets for latent space configurations (LSC) to ensure the desired LS structure. One of the main LSC advantage is the possibility of training classifier NNs without classification layers, which facilitates training NNs on datasets with extremely large numbers of classes. This paper provides a more general overview of possible vector systems for NN training along with their properties and methods for vector system construction. These systems are used to configure LS of encoders and visual transformers to significantly speed up ImageNet-1K and 50k-600k classes LSC training. It is also shown that using the minimum number of LS dimensions for a specific number of classes results in faster convergence. The latter has potential advantages for reducing the size of vector databases used to store NN embeddings.",0,arxiv,AI,CC-BY/arXiv,Exploring possible vector systems for faster training of neural networks with preconfigured latent spaces
"Automatically synthesizing verifiable code from natural language requirements ensures software correctness and reliability while significantly lowering the barrier to adopting the techniques of formal methods. With the rise of large language models (LLMs), long-standing efforts at autoformalization have gained new momentum. However, existing approaches suffer from severe syntactic and semantic errors due to the scarcity of domain-specific pre-training corpora and often fail to formalize implicit knowledge effectively. In this paper, we propose AutoICE, an LLM-driven evolutionary search for synthesizing verifiable C code. It introduces the diverse individual initialization and the collaborative crossover to enable diverse iterative updates, thereby mitigating error propagation inherent in single-agent iterations. Besides, it employs the self-reflective mutation to facilitate the discovery of implicit knowledge. Evaluation results demonstrate the effectiveness of AutoICE: it successfully verifies $90.36$\% of code, outperforming the state-of-the-art (SOTA) approach. Besides, on a developer-friendly dataset variant, AutoICE achieves a $88.33$\% verification success rate, significantly surpassing the $65$\% success rate of the SOTA approach.",0,arxiv,AI,CC-BY/arXiv,AutoICE: Automatically Synthesizing Verifiable C Code via LLM-driven Evolution
"We investigate how large language models (LLMs) fail when operating as autonomous agents with tool-use capabilities. Using the Kamiwaza Agentic Merit Index (KAMI) v0.1 benchmark, we analyze 900 execution traces from three representative models - Granite 4 Small, Llama 4 Maverick, and DeepSeek V3.1 - across filesystem, text extraction, CSV analysis, and SQL scenarios. Rather than focusing on aggregate scores, we perform fine-grained, per-trial behavioral analysis to surface the strategies that enable successful multi-step tool execution and the recurrent failure modes that undermine reliability. Our findings show that model scale alone does not predict agentic robustness: Llama 4 Maverick (400B) performs only marginally better than Granite 4 Small (32B) in some uncertainty-driven tasks, while DeepSeek V3.1's superior reliability derives primarily from post-training reinforcement learning rather than architecture or size. Across models, we identify four recurring failure archetypes: premature action without grounding, over-helpfulness that substitutes missing entities, vulnerability to distractor-induced context pollution, and fragile execution under load. These patterns highlight the need for agentic evaluation methods that emphasize interactive grounding, recovery behavior, and environment-aware adaptation, suggesting that reliable enterprise deployment requires not just stronger models but deliberate training and design choices that reinforce verification, constraint discovery, and adherence to source-of-truth data.",0,arxiv,AI,CC-BY/arXiv,How Do LLMs Fail In Agentic Scenarios? A Qualitative Analysis of Success and Failure Scenarios of Various LLMs in Agentic Simulations
"A robust nonproliferation regime has contained the spread of nuclear weapons to just nine states. Yet, emerging and disruptive technologies are reshaping the landscape of nuclear risks, presenting a critical juncture for decision makers. This article lays out the contours of an overlooked but intensifying technological arms race for nuclear (in)visibility, driven by the interplay between proliferation-enabling technologies (PETs) and detection-enhancing technologies (DETs). We argue that the strategic pattern of proliferation will be increasingly shaped by the innovation pace in these domains. Artificial intelligence (AI) introduces unprecedented complexity to this equation, as its rapid scaling and knowledge substitution capabilities accelerate PET development and challenge traditional monitoring and verification methods. To analyze this dynamic, we develop a formal model centered on a Relative Advantage Index (RAI), quantifying the shifting balance between PETs and DETs. Our model explores how asymmetric technological advancement, particularly logistic AI-driven PET growth versus stepwise DET improvements, expands the band of uncertainty surrounding proliferation detectability. Through replicable scenario-based simulations, we evaluate the impact of varying PET growth rates and DET investment strategies on cumulative nuclear breakout risk. We identify a strategic fork ahead, where detection may no longer suffice without broader PET governance. Governments and international organizations should accordingly invest in policies and tools agile enough to keep pace with tomorrow's technology.",0,arxiv,AI,CC-BY/arXiv,Artificial Intelligence and Nuclear Weapons Proliferation: The Technological Arms Race for (In)visibility
"The reliable operation of autonomous vehicles, automated driving functions, and advanced driver assistance systems across a wide range of relevant scenarios is critical for their development and deployment. Identifying a near-complete set of relevant driving scenarios for such functionalities is challenging due to numerous degrees of freedom involved, each affecting the outcomes of the driving scenario differently. Moreover, with increasing technical complexity of new functionalities, the number of potentially relevant, particularly ""unknown unsafe"" scenarios is increasing. To enhance validation efficiency, it is essential to identify relevant scenarios in advance, starting with simpler domains like highways before moving to more complex environments such as urban traffic. To address this, this paper focuses on analyzing lane change scenarios in highway traffic, which involve multiple degrees of freedom and present numerous safetyrelevant scenarios. We describe the process of data acquisition and processing of real-world data from public highway traffic, followed by the application of criticality measures on trajectory data to evaluate scenarios, as conducted within the AVEAS project (www.aveas.org). By linking the calculated measures to specific lane change driving scenarios and the conditions under which the data was collected, we facilitate the identification of safetyrelevant driving scenarios for various applications. Further, to tackle the extensive range of ""unknown unsafe"" scenarios, we propose a way to generate relevant scenarios by creating synthetic scenarios based on recorded ones. Consequently, we demonstrate and evaluate a processing chain that enables the identification of safety-relevant scenarios, the development of data-driven methods for extracting these scenarios, and the generation of synthetic critical scenarios via sampling on highways.",0,arxiv,AI,CC-BY/arXiv,From Real-World Traffic Data to Relevant Critical Scenarios
"Configuring computational fluid dynamics (CFD) simulations requires significant expertise in physics modeling and numerical methods, posing a barrier to non-specialists. Although automating scientific tasks with large language models (LLMs) has attracted attention, applying them to the complete, end-to-end CFD workflow remains a challenge due to its stringent domain-specific requirements. We introduce CFD-copilot, a domain-specialized LLM framework designed to facilitate natural language-driven CFD simulation from setup to post-processing. The framework employs a fine-tuned LLM to directly translate user descriptions into executable CFD setups. A multi-agent system integrates the LLM with simulation execution, automatic error correction, and result analysis. For post-processing, the framework utilizes the model context protocol (MCP), an open standard that decouples LLM reasoning from external tool execution. This modular design allows the LLM to interact with numerous specialized post-processing functions through a unified and scalable interface, improving the automation of data extraction and analysis. The framework was evaluated on benchmarks including the NACA~0012 airfoil and the three-element 30P-30N airfoil. The results indicate that domain-specific adaptation and the incorporation of the MCP jointly enhance the reliability and efficiency of LLM-driven engineering workflows.",0,arxiv,AI,CC-BY/arXiv,CFD-copilot: leveraging domain-adapted large language model and model context protocol to enhance simulation automation
"As Large Language Models (LLMs) increasingly operate as autonomous decision-makers in interactive and multi-agent systems and human societies, understanding their strategic behaviour has profound implications for safety, coordination, and the design of AI-driven social and economic infrastructures. Assessing such behaviour requires methods that capture not only what LLMs output, but the underlying intentions that guide their decisions. In this work, we extend the FAIRGAME framework to systematically evaluate LLM behaviour in repeated social dilemmas through two complementary advances: a payoff-scaled Prisoners Dilemma isolating sensitivity to incentive magnitude, and an integrated multi-agent Public Goods Game with dynamic payoffs and multi-agent histories. These environments reveal consistent behavioural signatures across models and languages, including incentive-sensitive cooperation, cross-linguistic divergence and end-game alignment toward defection. To interpret these patterns, we train traditional supervised classification models on canonical repeated-game strategies and apply them to FAIRGAME trajectories, showing that LLMs exhibit systematic, model- and language-dependent behavioural intentions, with linguistic framing at times exerting effects as strong as architectural differences. Together, these findings provide a unified methodological foundation for auditing LLMs as strategic agents and reveal systematic cooperation biases with direct implications for AI governance, collective decision-making, and the design of safe multi-agent systems.",0,arxiv,AI,CC-BY/arXiv,"Understanding LLM Agent Behaviours via Game Theory: Strategy Recognition, Biases and Multi-Agent Dynamics"
"The democratization of AI is currently hindered by the immense computational costs required to train Large Language Models (LLMs) for low-resource languages. This paper presents Persian-Phi, a 3.8B parameter model that challenges the assumption that robust multilingual capabilities require massive model sizes or multilingual baselines. We demonstrate how Microsoft Phi-3 Mini -- originally a monolingual English model -- can be effectively adapted to Persian through a novel, resource-efficient curriculum learning pipeline. Our approach employs a unique ""warm-up"" stage using bilingual narratives (Tiny Stories) to align embeddings prior to heavy training, followed by continual pretraining and instruction tuning via Parameter-Efficient Fine-Tuning (PEFT). Despite its compact size, Persian-Phi achieves competitive results on Open Persian LLM Leaderboard in HuggingFace. Our findings provide a validated, scalable framework for extending the reach of state-of-the-art LLMs to underrepresented languages with minimal hardware resources. The Persian-Phi model is publicly available at https://huggingface.co/amirakhlaghiqqq/PersianPhi.",0,arxiv,AI,CC-BY/arXiv,Persian-Phi: Efficient Cross-Lingual Adaptation of Compact LLMs via Curriculum Learning
"Research on promoting cooperation among autonomous, self-regarding agents has often focused on the bi-objective optimisation problem: minimising the total incentive cost while maximising the frequency of cooperation. However, the optimal value of social welfare under such constraints remains largely unexplored. In this work, we hypothesise that achieving maximal social welfare is not guaranteed at the minimal incentive cost required to drive agents to a desired cooperative state. To address this gap, we adopt to a single-objective approach focused on maximising social welfare, building upon foundational evolutionary game theory models that examined cost efficiency in finite populations, in both well-mixed and structured population settings. Our analytical model and agent-based simulations show how different interference strategies, including rewarding local versus global behavioural patterns, affect social welfare and dynamics of cooperation. Our results reveal a significant gap in the per-individual incentive cost between optimising for pure cost efficiency or cooperation frequency and optimising for maximal social welfare. Overall, our findings indicate that incentive design, policy, and benchmarking in multi-agent systems and human societies should prioritise welfare-centric objectives over proxy targets of cost or cooperation frequency.",0,arxiv,AI,CC-BY/arXiv,Social welfare optimisation in well-mixed and structured populations
"Graph neural networks (GNNs) are increasingly used to model complex patterns in graph-structured data. However, enabling them to ""forget"" designated information remains challenging, especially under privacy regulations such as the GDPR. Existing unlearning methods largely optimize for efficiency and scalability, yet they offer little transparency, and the black-box nature of GNNs makes it difficult to verify whether forgetting has truly occurred. We propose an explainability-driven verifier for GNN unlearning that snapshots the model before and after deletion, using attribution shifts and localized structural changes (for example, graph edit distance) as transparent evidence. The verifier uses five explainability metrics: residual attribution, heatmap shift, explainability score deviation, graph edit distance, and a diagnostic graph rule shift. We evaluate two backbones (GCN, GAT) and four unlearning strategies (Retrain, GraphEditor, GNNDelete, IDEA) across five benchmarks (Cora, Citeseer, Pubmed, Coauthor-CS, Coauthor-Physics). Results show that Retrain and GNNDelete achieve near-complete forgetting, GraphEditor provides partial erasure, and IDEA leaves residual signals. These explanation deltas provide the primary, human-readable evidence of forgetting; we also report membership-inference ROC-AUC as a complementary, graph-wide privacy signal.",0,arxiv,AI,CC-BY/arXiv,Forget and Explain: Transparent Verification of GNN Unlearning
"DreamerV3 is a state-of-the-art online model-based reinforcement learning (MBRL) algorithm known for remarkable sample efficiency. Concurrently, Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to Multi-Layer Perceptrons (MLPs), offering superior parameter efficiency and interpretability. To mitigate KANs' computational overhead, variants like FastKAN leverage Radial Basis Functions (RBFs) to accelerate inference. In this work, we investigate integrating KAN architectures into the DreamerV3 framework. We introduce KAN-Dreamer, replacing specific MLP and convolutional components of DreamerV3 with KAN and FastKAN layers. To ensure efficiency within the JAX-based World Model, we implement a tailored, fully vectorized version with simplified grid management. We structure our investigation into three subsystems: Visual Perception, Latent Prediction, and Behavior Learning. Empirical evaluations on the DeepMind Control Suite (walker_walk) analyze sample efficiency, training time, and asymptotic performance. Experimental results demonstrate that utilizing our adapted FastKAN as a drop-in replacement for the Reward and Continue predictors yields performance on par with the original MLP-based architecture, maintaining parity in both sample efficiency and training speed. This report serves as a preliminary study for future developments in KAN-based world models.",0,arxiv,AI,CC-BY/arXiv,KAN-Dreamer: Benchmarking Kolmogorov-Arnold Networks as Function Approximators in World Models
"Recent advances in large reasoning models (LRMs) have enabled agentic search systems to perform complex multi-step reasoning across multiple sources. However, most studies focus on general information retrieval and rarely explores vertical domains with unique challenges. In this work, we focus on local life services and introduce LocalSearchBench, which encompass diverse and complex business scenarios. Real-world queries in this domain are often ambiguous and require multi-hop reasoning across merchants and products, remaining challenging and not fully addressed. As the first comprehensive benchmark for agentic search in local life services, LocalSearchBench includes over 150,000 high-quality entries from various cities and business types. We construct 300 multi-hop QA tasks based on real user queries, challenging agents to understand questions and retrieve information in multiple steps. We also developed LocalPlayground, a unified environment integrating multiple tools for agent interaction. Experiments show that even state-of-the-art LRMs struggle on LocalSearchBench: the best model (DeepSeek-V3.1) achieves only 34.34% correctness, and most models have issues with completeness (average 77.33%) and faithfulness (average 61.99%). This highlights the need for specialized benchmarks and domain-specific agent training in local life services. Code, Benchmark, and Leaderboard are available at localsearchbench.github.io.",0,arxiv,AI,CC-BY/arXiv,LocalSearchBench: Benchmarking Agentic Search in Real-World Local Life Services
"Existing methods in domain generalization for Multimodal Sentiment Analysis (MSA) often overlook inter-modal synergies during invariant features extraction, which prevents the accurate capture of the rich semantic information within multimodal data. Additionally, while knowledge injection techniques have been explored in MSA, they often suffer from fragmented cross-modal knowledge, overlooking specific representations that exist beyond the confines of unimodal. To address these limitations, we propose a novel MSA framework designed for domain generalization. Firstly, the framework incorporates a Mixture of Invariant Experts model to extract domain-invariant features, thereby enhancing the model's capacity to learn synergistic relationships between modalities. Secondly, we design a Cross-Modal Adapter to augment the semantic richness of multimodal representations through cross-modal knowledge injection. Extensive domain experiments conducted on three datasets demonstrate that the proposed MIDG achieves superior performance.",0,arxiv,AI,CC-BY/arXiv,MIDG: Mixture of Invariant Experts with knowledge injection for Domain Generalization in Multimodal Sentiment Analysis
"Whole slide image (WSI) normalization remains a vital preprocessing step in computational pathology. Increasingly driven by deep learning, these models learn to approximate data distributions from training examples. This often results in outputs that gravitate toward the average, potentially masking diagnostically important features. More critically, they can introduce hallucinated content, artifacts that appear realistic but are not present in the original tissue, posing a serious threat to downstream analysis. These hallucinations are nearly impossible to detect visually, and current evaluation practices often overlook them. In this work, we demonstrate that the risk of hallucinations is real and underappreciated. While many methods perform adequately on public datasets, we observe a concerning frequency of hallucinations when these same models are retrained and evaluated on real-world clinical data. To address this, we propose a novel image comparison measure designed to automatically detect hallucinations in normalized outputs. Using this measure, we systematically evaluate several well-cited normalization methods retrained on real-world data, revealing significant inconsistencies and failures that are not captured by conventional metrics. Our findings underscore the need for more robust, interpretable normalization techniques and stricter validation protocols in clinical deployment.",0,arxiv,AI,CC-BY/arXiv,When normalization hallucinates: unseen risks in AI-powered whole slide image processing
"Understanding the movement behaviours of individuals and the way they react to the external world is a key component of any problem that involves the modelling of human dynamics at a physical level. In particular, it is crucial to capture the influence that the presence of an individual can have on the others. Important examples of applications include crowd simulation and emergency management, where the simulation of the mass of people passes through the simulation of the individuals, taking into consideration the others as part of the general context. While existing solutions basically start from some preconceived behavioural model, in this work we propose an approach that starts directly from the data, adopting a data mining perspective. Our method searches the mobility events in the data that might be possible evidences of mutual interactions between individuals, and on top of them looks for complex, persistent patterns and time evolving configurations of events. The study of these patterns can provide new insights on the mechanics of mobility interactions between individuals, which can potentially help in improving existing simulation models. We instantiate the general methodology on two real case studies, one on cars and one on pedestrians, and a full experimental evaluation is performed, both in terms of performances, parameter sensitivity and interpretation of sample results.",0,arxiv,AI,CC-BY/arXiv,Data-driven Exploration of Mobility Interaction Patterns
"Despite the effectiveness of large language models (LLMs) for code generation, they often output incorrect code. One reason is that model output probabilities are often not well-correlated with correctness, and reflect only the final output of the generation process. Inspired by findings that LLMs internally encode concepts like truthfulness, this paper explores if LLMs similarly represent code correctness. Specifically, we identify a correctness representation inside LLMs by contrasting the hidden states between pairs of correct and incorrect code for the same programming tasks. By experimenting on four LLMs, we show that exploiting this extracted correctness representation outperforms standard log-likelihood ranking, as well as verbalized model confidence. Furthermore, we explore how this internal correctness signal can be used to select higher-quality code samples, without requiring test execution. Ultimately, this work demonstrates how leveraging internal representations can enhance code generation systems and make LLMs more reliable, thus improving confidence in automatically generated code.",0,arxiv,AI,CC-BY/arXiv,Do LLMs Trust the Code They Write?
"A persistent paradox in continual learning (CL) is that neural networks often retain linearly separable representations of past tasks even when their output predictions fail. We formalize this distinction as the gap between deep feature-space and shallow classifier-level forgetting. We reveal a critical asymmetry in Experience Replay: while minimal buffers successfully anchor feature geometry and prevent deep forgetting, mitigating shallow forgetting typically requires substantially larger buffer capacities. To explain this, we extend the Neural Collapse framework to the sequential setting. We characterize deep forgetting as a geometric drift toward out-of-distribution subspaces and prove that any non-zero replay fraction asymptotically guarantees the retention of linear separability. Conversely, we identify that the ""strong collapse"" induced by small buffers leads to rank-deficient covariances and inflated class means, effectively blinding the classifier to true population boundaries. By unifying CL with out-of-distribution detection, our work challenges the prevailing reliance on large buffers, suggesting that explicitly correcting these statistical artifacts could unlock robust performance with minimal replay.",0,arxiv,AI,CC-BY/arXiv,Asymptotic analysis of shallow and deep forgetting in replay with Neural Collapse
"Behavior-cloning based visuomotor policies enable precise manipulation but often inherit the slow, cautious tempo of human demonstrations, limiting practical deployment. However, prior studies on acceleration methods mainly rely on statistical or heuristic cues that ignore task semantics and can fail across diverse manipulation settings. We present ESPADA, a semantic and spatially aware framework that segments demonstrations using a VLM-LLM pipeline with 3D gripper-object relations, enabling aggressive downsampling only in non-critical segments while preserving precision-critical phases, without requiring extra data or architectural modifications, or any form of retraining. To scale from a single annotated episode to the full dataset, ESPADA propagates segment labels via Dynamic Time Warping (DTW) on dynamics-only features. Across both simulation and real-world experiments with ACT and DP baselines, ESPADA achieves approximately a 2x speed-up while maintaining success rates, narrowing the gap between human demonstrations and efficient robot control.",0,arxiv,AI,CC-BY/arXiv,ESPADA: Execution Speedup via Semantics Aware Demonstration Data Downsampling for Imitation Learning
"Benefiting from the inductive biases learned from large-scale datasets, open-vocabulary semantic segmentation (OVSS) leverages the power of vision-language models, such as CLIP, to achieve remarkable progress without requiring task-specific training. However, due to CLIP's pre-training nature on image-text pairs, it tends to focus on global semantic alignment, resulting in suboptimal performance when associating fine-grained visual regions with text. This leads to noisy and inconsistent predictions, particularly in local areas. We attribute this to a dispersed bias stemming from its contrastive training paradigm, which is difficult to alleviate using CLIP features alone. To address this, we propose a structure-aware feature rectification approach that incorporates instance-specific priors derived directly from the image. Specifically, we construct a region adjacency graph (RAG) based on low-level features (e.g., colour and texture) to capture local structural relationships and use it to refine CLIP features by enhancing local discrimination. Extensive experiments show that our method effectively suppresses segmentation noise, improves region-level consistency, and achieves strong performance on multiple open-vocabulary segmentation benchmarks.",0,arxiv,AI,CC-BY/arXiv,Structure-Aware Feature Rectification with Region Adjacency Graphs for Training-Free Open-Vocabulary Semantic Segmentation
"Two traditions of interpretability have evolved side by side but seldom spoken to each other: Concept Bottleneck Models (CBMs), which prescribe what a concept should be, and Sparse Autoencoders (SAEs), which discover what concepts emerge. While CBMs use supervision to align activations with human-labeled concepts, SAEs rely on sparse coding to uncover emergent ones. We show that both paradigms instantiate the same geometric structure: each learns a set of linear directions in activation space whose nonnegative combinations form a concept cone. Supervised and unsupervised methods thus differ not in kind but in how they select this cone. Building on this view, we propose an operational bridge between the two paradigms. CBMs provide human-defined reference geometries, while SAEs can be evaluated by how well their learned cones approximate or contain those of CBMs. This containment framework yields quantitative metrics linking inductive biases -- such as SAE type, sparsity, or expansion ratio -- to emergence of plausible\footnote{We adopt the terminology of \citet{jacovi2020towards}, who distinguish between faithful explanations (accurately reflecting model computations) and plausible explanations (aligning with human intuition and domain knowledge). CBM concepts are plausible by construction -- selected or annotated by humans -- though not necessarily faithful to the true latent factors that organise the data manifold.} concepts. Using these metrics, we uncover a ``sweet spot'' in both sparsity and expansion factor that maximizes both geometric and semantic alignment with CBM concepts. Overall, our work unifies supervised and unsupervised concept discovery through a shared geometric framework, providing principled metrics to measure SAE progress and assess how well discovered concept align with plausible human concepts.",0,arxiv,AI,CC-BY/arXiv,A Geometric Unification of Concept Learning with Concept Cones
"The increasing use of synthetic media, particularly deepfakes, is an emerging challenge for digital content verification. Although recent studies use both audio and visual information, most integrate these cues within a single model, which remains vulnerable to modality mismatches, noise, and manipulation. To address this gap, we propose DeepAgent, an advanced multi-agent collaboration framework that simultaneously incorporates both visual and audio modalities for the effective detection of deepfakes. DeepAgent consists of two complementary agents. Agent-1 examines each video with a streamlined AlexNet-based CNN to identify the symbols of deepfake manipulation, while Agent-2 detects audio-visual inconsistencies by combining acoustic features, audio transcriptions from Whisper, and frame-reading sequences of images through EasyOCR. Their decisions are fused through a Random Forest meta-classifier that improves final performance by taking advantage of the different decision boundaries learned by each agent. This study evaluates the proposed framework using three benchmark datasets to demonstrate both component-level and fused performance. Agent-1 achieves a test accuracy of 94.35% on the combined Celeb-DF and FakeAVCeleb datasets. On the FakeAVCeleb dataset, Agent-2 and the final meta-classifier attain accuracies of 93.69% and 81.56%, respectively. In addition, cross-dataset validation on DeepFakeTIMIT confirms the robustness of the meta-classifier, which achieves a final accuracy of 97.49%, and indicates a strong capability across diverse datasets. These findings confirm that hierarchy-based fusion enhances robustness by mitigating the weaknesses of individual modalities and demonstrate the effectiveness of a multi-agent approach in addressing diverse types of manipulations in deepfakes.",0,arxiv,AI,CC-BY/arXiv,DeepAgent: A Dual Stream Multi Agent Fusion for Robust Multimodal Deepfake Detection
"Vision-language models (VLMs) have demonstrated impressive multimodal comprehension capabilities and are being deployed in an increasing number of online video understanding applications. While recent efforts extensively explore advancing VLMs' reasoning power in these cases, deployment constraints are overlooked, leading to overwhelming system overhead in real-world deployments. To address that, we propose Venus, an on-device memory-and-retrieval system for efficient online video understanding. Venus proposes an edge-cloud disaggregated architecture that sinks memory construction and keyframe retrieval from cloud to edge, operating in two stages. In the ingestion stage, Venus continuously processes streaming edge videos via scene segmentation and clustering, where the selected keyframes are embedded with a multimodal embedding model to build a hierarchical memory for efficient storage and retrieval. In the querying stage, Venus indexes incoming queries from memory, and employs a threshold-based progressive sampling algorithm for keyframe selection that enhances diversity and adaptively balances system cost and reasoning accuracy. Our extensive evaluation shows that Venus achieves a 15x-131x speedup in total response latency compared to state-of-the-art methods, enabling real-time responses within seconds while maintaining comparable or even superior reasoning accuracy.",0,arxiv,AI,CC-BY/arXiv,Venus: An Efficient Edge Memory-and-Retrieval System for VLM-based Online Video Understanding
"Knowledge graph embedding (KGE) relies on the geometry of the embedding space to encode semantic and structural relations. Existing methods place all entities on one homogeneous manifold, Euclidean, spherical, hyperbolic, or their product/multi-curvature variants, to model linear, symmetric, or hierarchical patterns. Yet a predefined, homogeneous manifold cannot accommodate the sharply varying curvature that real-world graphs exhibit across local regions. Since this geometry is imposed a priori, any mismatch with the knowledge graph's local curvatures will distort distances between entities and hurt the expressiveness of the resulting KGE. To rectify this, we propose RicciKGE to have the KGE loss gradient coupled with local curvatures in an extended Ricci flow such that entity embeddings co-evolve dynamically with the underlying manifold geometry towards mutual adaptation. Theoretically, when the coupling coefficient is bounded and properly selected, we rigorously prove that i) all the edge-wise curvatures decay exponentially, meaning that the manifold is driven toward the Euclidean flatness; and ii) the KGE distances strictly converge to a global optimum, which indicates that geometric flattening and embedding optimization are promoting each other. Experimental improvements on link prediction and node classification benchmarks demonstrate RicciKGE's effectiveness in adapting to heterogeneous knowledge graph structures.",0,arxiv,AI,CC-BY/arXiv,Local-Curvature-Aware Knowledge Graph Embedding: An Extended Ricci Flow Approach
"Text-to-video (T2V) generation has advanced rapidly, yet maintaining consistent character identities across scenes remains a major challenge. Existing personalization methods often focus on facial identity but fail to preserve broader contextual cues such as hairstyle, outfit, and body shape, which are critical for visual coherence. We propose \textbf{ContextAnyone}, a context-aware diffusion framework that achieves character-consistent video generation from text and a single reference image. Our method jointly reconstructs the reference image and generates new video frames, enabling the model to fully perceive and utilize reference information. Reference information is effectively integrated into a DiT-based diffusion backbone through a novel Emphasize-Attention module that selectively reinforces reference-aware features and prevents identity drift across frames. A dual-guidance loss combines diffusion and reference reconstruction objectives to enhance appearance fidelity, while the proposed Gap-RoPE positional embedding separates reference and video tokens to stabilize temporal modeling. Experiments demonstrate that ContextAnyone outperforms existing reference-to-video methods in identity consistency and visual quality, generating coherent and context-preserving character videos across diverse motions and scenes. Project page: \href{https://github.com/ziyang1106/ContextAnyone}{https://github.com/ziyang1106/ContextAnyone}.",0,arxiv,AI,CC-BY/arXiv,ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation
"Modeling human mobility is vital for extensive applications such as transportation planning and epidemic modeling. With the rise of the Artificial Intelligence Generated Content (AIGC) paradigm, recent works explore synthetic trajectory generation using autoregressive and diffusion models. While these methods show promise for generating single-day trajectories, they remain limited by inefficiencies in long-term generation (e.g., weekly trajectories) and a lack of explicit spatiotemporal multi-scale modeling. This study proposes Multi-Scale Spatio-Temporal AutoRegression (M-STAR), a new framework that generates long-term trajectories through a coarse-to-fine spatiotemporal prediction process. M-STAR combines a Multi-scale Spatiotemporal Tokenizer that encodes hierarchical mobility patterns with a Transformer-based decoder for next-scale autoregressive prediction. Experiments on two real-world datasets show that M-STAR outperforms existing methods in fidelity and significantly improves generation speed. The data and codes are available at https://github.com/YuxiaoLuo0013/M-STAR.",0,arxiv,AI,CC-BY/arXiv,M-STAR: Multi-Scale Spatiotemporal Autoregression for Human Mobility Modeling
"The rapid adoption of large language models (LLMs) is pushing AI accelerators toward increasingly powerful and specialized designs. Instead of further complicating software development with deeply hierarchical scratchpad memories (SPMs) and their asynchronous management, we investigate the opposite point of the design spectrum: a multi-core AI accelerator equipped with a shared system-level cache and application-aware management policies, which keeps the programming effort modest. Our approach exploits dataflow information available in the software stack to guide cache replacement (including dead-block prediction), in concert with bypass decisions and mechanisms that alleviate cache thrashing.   We assess the proposal using a cycle-accurate simulator and observe substantial performance gains (up to 1.80x speedup) compared with conventional cache architectures. In addition, we build and validate an analytical model that takes into account the actual overlapping behaviors to extend the measurement results of our policies to real-world larger-scale workloads. Experiment results show that when functioning together, our bypassing and thrashing mitigation strategies can handle scenarios both with and without inter-core data sharing and achieve remarkable speedups.   Finally, we implement the design in RTL and the area of our design is $\mathbf{0.064mm^2}$ with 15nm process, which can run at 2 GHz clock frequency. Our findings explore the potential of the shared cache design to assist the development of future AI accelerator systems.",0,arxiv,AI,CC-BY/arXiv,DCO: Dynamic Cache Orchestration for LLM Accelerators through Predictive Management
"Radio frequency (RF)-based indoor localization offers significant promise for applications such as indoor navigation, augmented reality, and pervasive computing. While deep learning has greatly enhanced localization accuracy and robustness, existing localization models still face major challenges in cross-scene generalization due to their reliance on scene-specific labeled data. To address this, we introduce Radiance-Field Reinforced Pretraining (RFRP). This novel self-supervised pretraining framework couples a large localization model (LM) with a neural radio-frequency radiance field (RF-NeRF) in an asymmetrical autoencoder architecture. In this design, the LM encodes received RF spectra into latent, position-relevant representations, while the RF-NeRF decodes them to reconstruct the original spectra. This alignment between input and output enables effective representation learning using large-scale, unlabeled RF data, which can be collected continuously with minimal effort. To this end, we collected RF samples at 7,327,321 positions across 100 diverse scenes using four common wireless technologies--RFID, BLE, WiFi, and IIoT. Data from 75 scenes were used for training, and the remaining 25 for evaluation. Experimental results show that the RFRP-pretrained LM reduces localization error by over 40% compared to non-pretrained models and by 21% compared to those pretrained using supervised learning.",0,arxiv,AI,CC-BY/arXiv,Radiance-Field Reinforced Pretraining: Scaling Localization Models with Unlabeled Wireless Signals
"We introduce a constraint-programming framework for generating synthetic populations that reproduce target statistics with high precision while enforcing full individual consistency. Unlike data-driven approaches that infer distributions from samples, our method directly encodes aggregated statistics and structural relations, enabling exact control of demographic profiles without requiring any microdata. We validate the approach on official demographic sources and study the impact of distributional deviations on downstream analyses. This work is conducted within the Pollitics project developed by Emotia, where synthetic populations can be queried through large language models to model societal behaviors, explore market and policy scenarios, and provide reproducible decision-grade insights without personal data.",0,arxiv,AI,CC-BY/arXiv,Exact Synthetic Populations for Scalable Societal and Market Modeling
"Existing image perception methods based on VLMs generally follow a paradigm wherein models extract and analyze image content based on user-provided textual task prompts. However, such methods face limitations when applied to UAV imagery, which presents challenges like target confusion, scale variations, and complex backgrounds. These challenges arise because VLMs' understanding of image content depends on the semantic alignment between visual and textual tokens. When the task prompt is simplistic and the image content is complex, achieving effective alignment becomes difficult, limiting the model's ability to focus on task-relevant information. To address this issue, we introduce AerialVP, the first agent framework for task prompt enhancement in UAV image perception. AerialVP proactively extracts multi-dimensional auxiliary information from UAV images to enhance task prompts, overcoming the limitations of traditional VLM-based approaches. Specifically, the enhancement process includes three stages: (1) analyzing the task prompt to identify the task type and enhancement needs, (2) selecting appropriate tools from the tool repository, and (3) generating enhanced task prompts based on the analysis and selected tools. To evaluate AerialVP, we introduce AerialSense, a comprehensive benchmark for UAV image perception that includes Aerial Visual Reasoning, Aerial Visual Question Answering, and Aerial Visual Grounding tasks. AerialSense provides a standardized basis for evaluating model generalization and performance across diverse resolutions, lighting conditions, and both urban and natural scenes. Experimental results demonstrate that AerialVP significantly enhances task prompt guidance, leading to stable and substantial performance improvements in both open-source and proprietary VLMs. Our work will be available at https://github.com/lostwolves/AerialVP.",0,arxiv,AI,CC-BY/arXiv,Towards Accurate UAV Image Perception: Guiding Vision-Language Models with Stronger Task Prompts
"Despite impressive advances in agent systems, multi-turn tool-use scenarios remain challenging. It is mainly because intent is clarified progressively and the environment evolves with each tool call. While reusing past experience is natural, current LLM agents either treat entire trajectories or pre-defined subtasks as indivisible units, or solely exploit tool-to-tool dependencies, hindering adaptation as states and information evolve across turns. In this paper, we propose a State Integrated Tool Graph (SIT-Graph), which enhances multi-turn tool use by exploiting partially overlapping experience. Inspired by human decision-making that integrates episodic and procedural memory, SIT-Graph captures both compact state representations (episodic-like fragments) and tool-to-tool dependencies (procedural-like routines) from historical trajectories. Specifically, we first build a tool graph from accumulated tool-use sequences, and then augment each edge with a compact state summary of the dialog and tool history that may shape the next action. At inference time, SIT-Graph enables a human-like balance between episodic recall and procedural execution: when the next decision requires recalling prior context, the agent retrieves the state summaries stored on relevant edges and uses them to guide its next action; when the step is routine, it follows high-confidence tool dependencies without explicit recall. Experiments across multiple stateful multi-turn tool-use benchmarks show that SIT-Graph consistently outperforms strong memory- and graph-based baselines, delivering more robust tool selection and more effective experience transfer.",0,arxiv,AI,CC-BY/arXiv,SIT-Graph: State Integrated Tool Graph for Multi-Turn Agents
"In the field of healthcare, precise skin lesion segmentation is crucial for the early detection and accurate diagnosis of skin diseases. Despite significant advances in deep learning for image processing, existing methods have yet to effectively address the challenges of irregular lesion shapes and low contrast. To address these issues, this paper proposes an innovative encoder-decoder network architecture based on multi-scale residual structures, capable of extracting rich feature information from different receptive fields to effectively identify lesion areas. By introducing a Multi-Resolution Multi-Channel Fusion (MRCF) module, our method captures cross-scale features, enhancing the clarity and accuracy of the extracted information. Furthermore, we propose a Cross-Mix Attention Module (CMAM), which redefines the attention scope and dynamically calculates weights across multiple contexts, thus improving the flexibility and depth of feature capture and enabling deeper exploration of subtle features. To overcome the information loss caused by skip connections in traditional U-Net, an External Attention Bridge (EAB) is introduced, facilitating the effective utilization of information in the decoder and compensating for the loss during upsampling. Extensive experimental evaluations on several skin lesion segmentation datasets demonstrate that the proposed model significantly outperforms existing transformer and convolutional neural network-based models, showcasing exceptional segmentation accuracy and robustness.",0,arxiv,AI,CC-BY/arXiv,Effective Attention-Guided Multi-Scale Medical Network for Skin Lesion Segmentation
"Integrating autonomous mobile robots into human environments requires human-like decision-making and energy-efficient, event-based computation. Despite progress, neuromorphic methods are rarely applied to Deep Reinforcement Learning (DRL) navigation approaches due to unstable training. We address this gap with a hybrid socially integrated DRL actor-critic approach that combines Spiking Neural Networks (SNNs) in the actor with Artificial Neural Networks (ANNs) in the critic and a neuromorphic feature extractor to capture temporal crowd dynamics and human-robot interactions. Our approach enhances social navigation performance and reduces estimated energy consumption by approximately 1.69 orders of magnitude.",0,arxiv,AI,CC-BY/arXiv,SINRL: Socially Integrated Navigation with Reinforcement Learning using Spiking Neural Networks
"Endoscopic surgery relies on intraoperative video, making image quality a decisive factor for surgical safety and efficacy. Yet, endoscopic videos are often degraded by uneven illumination, tissue scattering, occlusions, and motion blur, which obscure critical anatomical details and complicate surgical manipulation. Although deep learning-based methods have shown promise in image enhancement, most existing approaches remain too computationally demanding for real-time surgical use. To address this challenge, we propose a degradation-aware framework for endoscopic video enhancement, which enables real-time, high-quality enhancement by propagating degradation representations across frames. In our framework, degradation representations are first extracted from images using contrastive learning. We then introduce a fusion mechanism that modulates image features with these representations to guide a single-frame enhancement model, which is trained with a cycle-consistency constraint between degraded and restored images to improve robustness and generalization. Experiments demonstrate that our framework achieves a superior balance between performance and efficiency compared with several state-of-the-art methods. These results highlight the effectiveness of degradation-aware modeling for real-time endoscopic video enhancement. Nevertheless, our method suggests that implicitly learning and propagating degradation representation offer a practical pathway for clinical application.",0,arxiv,AI,CC-BY/arXiv,DGGAN: Degradation Guided Generative Adversarial Network for Real-time Endoscopic Video Enhancement
"Because machine learning has significantly improved efficiency and convenience in the society, it's increasingly used to assist or replace human decision-making. However, the data-based pattern makes related algorithms learn and even exacerbate potential bias in samples, resulting in discriminatory decisions against certain unprivileged groups, depriving them of the rights to equal treatment, thus damaging the social well-being and hindering the development of related applications. Therefore, we propose a pre-processing method IFFair based on the influence function. Compared with other fairness optimization approaches, IFFair only uses the influence disparity of training samples on different groups as a guidance to dynamically adjust the sample weights during training without modifying the network structure, data features and decision boundaries. To evaluate the validity of IFFair, we conduct experiments on multiple real-world datasets and metrics. The experimental results show that our approach mitigates bias of multiple accepted metrics in the classification setting, including demographic parity, equalized odds, equality of opportunity and error rate parity without conflicts. It also demonstrates that IFFair achieves better trade-off between multiple utility and fairness metrics compared with previous pre-processing methods.",0,arxiv,AI,CC-BY/arXiv,IFFair: Influence Function-driven Sample Reweighting for Fair Classification
"Dropout is a widely used regularization technique which improves the generalization ability of a model by randomly dropping neurons. In light of this, we propose Dropout Prompt Learning, which aims for applying dropout to improve the robustness of the vision-language models. Different from the vanilla dropout, we apply dropout on the tokens of the textual and visual branches, where we evaluate the token significance considering both intra-modal context and inter-modal alignment, enabling flexible dropout probabilities for each token. Moreover, to maintain semantic alignment for general knowledge transfer while encouraging the diverse representations that dropout introduces, we further propose residual entropy regularization. Experiments on 15 benchmarks show our method's effectiveness in challenging scenarios like low-shot learning, long-tail classification, and out-of-distribution generalization. Notably, our method surpasses regularization-based methods including KgCoOp by 5.10% and PromptSRC by 2.13% in performance on base-to-novel generalization.",0,arxiv,AI,CC-BY/arXiv,Dropout Prompt Learning: Towards Robust and Adaptive Vision-Language Models
"Product matching aims to identify identical or similar products sold on different platforms. By building knowledge graphs (KGs), the product matching problem can be converted to the Entity Alignment (EA) task, which aims to discover the equivalent entities from diverse KGs. The existing EA methods inadequately utilize both attribute triples and relation triples simultaneously, especially the interactions between them. This paper introduces a two-stage pipeline consisting of rough filter and fine filter to match products from eBay and Amazon. For fine filtering, a new framework for Entity Alignment, Relation-aware and Attribute-aware Graph Attention Networks for Entity Alignment (RAEA), is employed. RAEA focuses on the interactions between attribute triples and relation triples, where the entity representation aggregates the alignment signals from attributes and relations with Attribute-aware Entity Encoder and Relation-aware Graph Attention Networks. The experimental results indicate that the RAEA model achieves significant improvements over 12 baselines on EA task in the cross-lingual dataset DBP15K (6.59% on average Hits@1) and delivers competitive results in the monolingual dataset DWY100K. The source code for experiments on DBP15K and DWY100K is available at github (https://github.com/Mockingjay-liu/RAEA-model-for-Entity-Alignment).",0,arxiv,AI,CC-BY/arXiv,Cross-platform Product Matching Based on Entity Alignment of Knowledge Graph with RAEA model
"DeepFake face swapping enables highly realistic identity forgeries, posing serious privacy and security risks. A common defence embeds invisible perturbations into images, but these are fragile and often destroyed by basic transformations such as compression or resizing. In this paper, we first conduct a systematic analysis of 30 transformations across six categories and show that protection robustness is highly sensitive to the choice of training transformations, making the standard Expectation over Transformation (EOT) with uniform sampling fundamentally suboptimal. Motivated by this, we propose Expectation Over Learned distribution of Transformation (EOLT), the framework to treat transformation distribution as a learnable component rather than a fixed design choice. Specifically, EOLT employs a policy network that learns to automatically prioritize critical transformations and adaptively generate instance-specific perturbations via reinforcement learning, enabling explicit modeling of defensive bottlenecks while maintaining broad transferability. Extensive experiments demonstrate that our method achieves substantial improvements over state-of-the-art approaches, with 26% higher average robustness and up to 30% gains on challenging transformation categories.",0,arxiv,AI,CC-BY/arXiv,Towards Robust Protective Perturbation against DeepFake Face Swapping
"Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing tasks. However, temporal reasoning, particularly under complex temporal constraints, remains a major challenge. To this end, existing approaches have explored symbolic methods, which encode temporal structure explicitly, and reflective mechanisms, which revise reasoning errors through multi-step inference. Nonetheless, symbolic approaches often underutilize the reasoning capabilities of LLMs, while reflective methods typically lack structured temporal representations, which can result in inconsistent or hallucinated reasoning. As a result, even when the correct temporal context is available, LLMs may still misinterpret or misapply time-related information, leading to incomplete or inaccurate answers. To address these limitations, in this work, we propose Neuro-Symbolic Temporal Reasoning (NeSTR), a novel framework that integrates structured symbolic representations with hybrid reflective reasoning to enhance the temporal sensitivity of LLM inference. NeSTR preserves explicit temporal relations through symbolic encoding, enforces logical consistency via verification, and corrects flawed inferences using abductive reflection. Extensive experiments on diverse temporal question answering benchmarks demonstrate that NeSTR achieves superior zero-shot performance and consistently improves temporal reasoning without any fine-tuning, showcasing the advantage of neuro-symbolic integration in enhancing temporal understanding in large language models.",0,arxiv,AI,CC-BY/arXiv,NeSTR: A Neuro-Symbolic Abductive Framework for Temporal Reasoning in Large Language Models
"Vision Foundation Models (VFMs) and Vision Language Models (VLMs) have revolutionized computer vision by providing rich semantic and geometric representations. This paper presents a comprehensive visual comparison between CLIP based and DINOv2 based approaches for 3D pose estimation in hand object grasping scenarios. We evaluate both models on the task of 6D object pose estimation and demonstrate their complementary strengths: CLIP excels in semantic understanding through language grounding, while DINOv2 provides superior dense geometric features. Through extensive experiments on benchmark datasets, we show that CLIP based methods achieve better semantic consistency, while DINOv2 based approaches demonstrate competitive performance with enhanced geometric precision. Our analysis provides insights for selecting appropriate vision models for robotic manipulation and grasping, picking applications.",0,arxiv,AI,CC-BY/arXiv,VFM-VLM: Vision Foundation Model and Vision Language Model based Visual Comparison for 3D Pose Estimation
"Imitation learning with diffusion models has advanced robotic control by capturing multi-modal action distributions. However, existing approaches typically treat observations as high-level conditioning inputs to the denoising network, rather than integrating them into the stochastic dynamics of the diffusion process itself. As a result, sampling must begin from random Gaussian noise, weakening the coupling between perception and control and often yielding suboptimal performance. We introduce BridgePolicy, a generative visuomotor policy that explicitly embeds observations within the stochastic differential equation via a diffusion-bridge formulation. By constructing an observation-informed trajectory, BridgePolicy enables sampling to start from a rich, informative prior rather than random noise, substantially improving precision and reliability in control. A key challenge is that classical diffusion bridges connect distributions with matched dimensionality, whereas robotic observations are heterogeneous and multi-modal and do not naturally align with the action space. To address this, we design a multi-modal fusion module and a semantic aligner that unify visual and state inputs and align observation and action representations, making the bridge applicable to heterogeneous robot data. Extensive experiments across 52 simulation tasks on three benchmarks and five real-world tasks demonstrate that BridgePolicy consistently outperforms state-of-the-art generative policies.",0,arxiv,AI,CC-BY/arXiv,Sample from What You See: Visuomotor Policy Learning via Diffusion Bridge with Observation-Embedded Stochastic Differential Equation
"Federated Prompt Learning (FPL) offers a parameter-efficient solution for collaboratively training large models, but its performance is severely hindered by data heterogeneity, which causes locally trained prompts to become biased. Existing methods, focusing on aggregation or regularization, fail to address this root cause of local training bias. To this end, we propose Geometry-Guided Text Prompt Calibration (GGTPC), a novel framework that directly corrects this bias by providing clients with a global geometric prior. This prior, representing the shape of the global data distribution derived from the covariance matrix, is reconstructed on the server in a privacy-preserving manner. Clients then use a novel Geometry-Prior Calibration Layer (GPCL) to align their local feature distributions with this global prior during training. Extensive experiments show GGTPC's effectiveness. On the label-skewed CIFAR-100 dataset ($Î²$=0.1), it outperforms the state-of-the-art by 2.15\%. Under extreme skew ($Î²$=0.01), it improves upon the baseline by 9.17\%. Furthermore, as a plug-and-play module on the domain-skewed Office-Home dataset, it boosts FedAvg's performance by 4.60\%. These results demonstrate that GGTPC effectively mitigates data heterogeneity by correcting the fundamental local training bias, serving as a versatile module to enhance various FL algorithms.",0,arxiv,AI,CC-BY/arXiv,Geometric Prior-Guided Federated Prompt Calibration
"Multi-agent role-playing has recently shown promise for studying social behavior with language agents, but existing simulations are mostly monolingual and fail to model cross-lingual interaction, an essential property of real societies. We introduce MASim, the first multilingual agent-based simulation framework that supports multi-turn interaction among generative agents with diverse sociolinguistic profiles. MASim offers two key analyses: (i) global public opinion modeling, by simulating how attitudes toward open-domain hypotheses evolve across languages and cultures, and (ii) media influence and information diffusion, via autonomous news agents that dynamically generate content and shape user behavior. To instantiate simulations, we construct the MAPS benchmark, which combines survey questions and demographic personas drawn from global population distributions. Experiments on calibration, sensitivity, consistency, and cultural case studies show that MASim reproduces sociocultural phenomena and highlights the importance of multilingual simulation for scalable, controlled computational social science.",0,arxiv,AI,CC-BY/arXiv,MASim: Multilingual Agent-Based Simulation for Social Science
"Chart understanding is crucial for deploying multimodal large language models (MLLMs) in real-world scenarios such as analyzing scientific papers and technical reports. Unlike natural images, charts pair a structured visual layout (spatial property) with an underlying data representation (textual property) -- grasping both is essential for precise, fine-grained chart reasoning. Motivated by this observation, we propose START, the Spatial and Textual learning for chART understanding. Specifically, we introduce (i) chart-element grounding and (ii) chart-to-code generation to strengthen an MLLM's understanding of both chart visual layout and data details. To facilitate spatial and textual learning, we propose the START-Dataset generated with a novel data-generation pipeline that first leverages an MLLM to translate real chart images into executable chart code, recovering the underlying data representation while preserving the visual distribution of real-world charts. We then evolve the code with a Large Language Model (LLM) to ascertain the positions of chart elements that capture the chart's visual structure, addressing challenges that existing methods cannot handle. To evaluate a model's ability to understand chart spatial structures, we propose the Chart Spatial understanding Benchmark (CS-Bench), filling a critical gap in comprehensive chart understanding evaluation. Leveraging spatial and textual learning, START delivers consistent gains across model sizes and benchmarks over the base models and surpasses prior state-of-the-art by a clear margin. Code, data and models will be publicly available.",0,arxiv,AI,CC-BY/arXiv,START: Spatial and Textual Learning for Chart Understanding
"With the recent surge in personalized learning, Intelligent Tutoring Systems (ITS) that can accurately track students' individual knowledge states and provide tailored learning paths based on this information are in demand as an essential task. This paper focuses on the core technology of Knowledge Tracing (KT) models that analyze students' sequences of interactions to predict their knowledge acquisition levels. However, existing KT models suffer from limitations such as restricted input data formats, cold start problems arising with new student enrollment or new question addition, and insufficient stability in real-world service environments. To overcome these limitations, a Practical Interlinked Concept Knowledge Tracing (PICKT) model that can effectively process multiple types of input data is proposed. Specifically, a knowledge map structures the relationships among concepts considering the question and concept text information, thereby enabling effective knowledge tracing even in cold start situations. Experiments reflecting real operational environments demonstrated the model's excellent performance and practicality. The main contributions of this research are as follows. First, a model architecture that effectively utilizes diverse data formats is presented. Second, significant performance improvements are achieved over existing models for two core cold start challenges: new student enrollment and new question addition. Third, the model's stability and practicality are validated through delicate experimental design, enhancing its applicability in real-world product environments. This provides a crucial theoretical and technical foundation for the practical implementation of next-generation ITS.",0,arxiv,AI,CC-BY/arXiv,PICKT: Practical Interlinked Concept Knowledge Tracing for Personalized Learning using Knowledge Map Concept Relations
"Explainable Artificial Intelligence (XAI) has become an increasingly important area of research, particularly as machine learning models are deployed in high-stakes domains. Among various XAI approaches, SHAP (SHapley Additive exPlanations) has gained prominence due to its ability to provide both global and local explanations across different machine learning models. While SHAP effectively visualizes feature importance, it often lacks contextual explanations that are meaningful for end-users, especially those without technical backgrounds. To address this gap, we propose a Python package that extends SHAP by integrating it with a large language model (LLM), specifically OpenAI's GPT, to generate contextualized textual explanations. This integration is guided by user-defined parameters (such as feature aliases, descriptions, and additional background) to tailor the explanation to both the model context and the user perspective. We hypothesize that this enhancement can improve the perceived understandability of SHAP explanations. To evaluate the effectiveness of the proposed package, we applied it in a healthcare-related case study and conducted user evaluations involving real end-users. The results, based on Likert-scale surveys and follow-up interviews, indicate that the generated explanations were perceived as more understandable and contextually appropriate compared to visual-only outputs. While the findings are preliminary, they suggest that combining visualization with contextualized text may support more user-friendly and trustworthy model explanations.",0,arxiv,AI,CC-BY/arXiv,ContextualSHAP : Enhancing SHAP Explanations Through Contextual Language Generation
"Image fusion aims to blend complementary information from multiple sensing modalities, yet existing approaches remain limited in robustness, adaptability, and controllability. Most current fusion networks are tailored to specific tasks and lack the ability to flexibly incorporate user intent, especially in complex scenarios involving low-light degradation, color shifts, or exposure imbalance. Moreover, the absence of ground-truth fused images and the small scale of existing datasets make it difficult to train an end-to-end model that simultaneously understands high-level semantics and performs fine-grained multimodal alignment. We therefore present DiTFuse, instruction-driven Diffusion-Transformer (DiT) framework that performs end-to-end, semantics-aware fusion within a single model. By jointly encoding two images and natural-language instructions in a shared latent space, DiTFuse enables hierarchical and fine-grained control over fusion dynamics, overcoming the limitations of pre-fusion and post-fusion pipelines that struggle to inject high-level semantics. The training phase employs a multi-degradation masked-image modeling strategy, so the network jointly learns cross-modal alignment, modality-invariant restoration, and task-aware feature selection without relying on ground truth images. A curated, multi-granularity instruction dataset further equips the model with interactive fusion capabilities. DiTFuse unifies infrared-visible, multi-focus, and multi-exposure fusion-as well as text-controlled refinement and downstream tasks-within a single architecture. Experiments on public IVIF, MFF, and MEF benchmarks confirm superior quantitative and qualitative performance, sharper textures, and better semantic retention. The model also supports multi-level user control and zero-shot generalization to other multi-image fusion scenarios, including instruction-conditioned segmentation.",0,arxiv,AI,CC-BY/arXiv,Towards Unified Semantic and Controllable Image Fusion: A Diffusion Transformer Approach
"The interpretation of Chest X-ray is an important diagnostic issue in clinical practice and especially in the resource-limited setting where the shortage of radiologists plays a role in delayed diagnosis and poor patient outcomes. Although the original CheXNet architecture has shown potential in automated analysis of chest radiographs, DenseNet-121 backbone is computationally inefficient and poorly single-label classifier. To eliminate such shortcomings, we suggest a better classification framework of chest disease that relies on EfficientNetV2-M and incorporates superior training approaches such as Automatic Mixed Precision training, AdamW, Cosine Annealing learning rate scheduling, and Exponential Moving Average regularization. We prepared a dataset of 18,080 chest X-ray images of three source materials of high authority and representing five key clinically significant disease categories which included Cardiomegaly, COVID-19, Normal, Pneumonia, and Tuberculosis. To achieve statistical reliability and reproducibility, nine independent experimental runs were run. The suggested architecture showed significant gains with mean test accuracy of 96.45 percent compared to 95.30 percent at baseline (p less than 0.001) and macro-averaged F1-score increased to 91.08 percent (p less than 0.001). Critical infectious diseases showed near-perfect classification performance with COVID-19 detection having 99.95 percent accuracy and Tuberculosis detection having 99.97 percent accuracy. Although 6.8 times more parameters are included, the training time was reduced by 11.4 percent and performance stability was increased by 22.7 percent. This framework presents itself as a decision-support tool that can be used to respond to a pandemic, screen tuberculosis, and assess thoracic disease regularly in various healthcare facilities.",0,arxiv,AI,CC-BY/arXiv,Enhanced Chest Disease Classification Using an Improved CheXNet Framework with EfficientNetV2-M and Optimization-Driven Learning
"We introduce a two-stage self-supervised framework that combines the Joint-Embedding Predictive Architecture (JEPA) with a Density Adaptive Attention Mechanism (DAAM) for learning robust speech representations. Stage~1 uses JEPA with DAAM to learn semantic audio features via masked prediction in latent space, fully decoupled from waveform reconstruction. Stage~2 leverages these representations for efficient tokenization using Finite Scalar Quantization (FSQ) and a mixed-radix packing scheme, followed by high-fidelity waveform reconstruction with a HiFi-GAN decoder. By integrating Gaussian mixture-based density-adaptive gating into the JEPA encoder, the model performs adaptive temporal feature selection and discovers hierarchical speech structure at a low frame rate of 2.5~Hz. The resulting tokens (47.5 tokens/sec) provide a reversible, highly compressed, and language-model-friendly representation that is competitive with, and often more efficient than, existing neural audio codecs.",0,arxiv,AI,CC-BY/arXiv,JEPA as a Neural Tokenizer: Learning Robust Speech Representations with Density Adaptive Attention
"Deep generative models have become powerful priors for solving inverse problems, and various training-free methods have been developed. However, when applied to latent flow models, existing methods often fail to converge to the posterior mode or suffer from manifold deviation within latent spaces. To mitigate this, here we introduce a novel training-free framework, FlowLPS, that solves inverse problems with pretrained flow models via a Langevin Proximal Sampling (LPS) strategy. Our method integrates Langevin dynamics for manifold-consistent exploration with proximal optimization for precise mode seeking, achieving a superior balance between reconstruction fidelity and perceptual quality across multiple inverse tasks on FFHQ and DIV2K, outperforming state of the art inverse solvers.",0,arxiv,AI,CC-BY/arXiv,FlowLPS: Langevin-Proximal Sampling for Flow-based Inverse Problem Solvers
"The Lottery Ticket Hypothesis asserts the existence of highly sparse, trainable subnetworks ('winning tickets') within dense, randomly initialized neural networks. However, state-of-the-art methods of drawing these tickets, like Lottery Ticket Rewinding (LTR), are computationally prohibitive, while more efficient saliency-based Pruning-at-Initialization (PaI) techniques suffer from a significant accuracy-sparsity trade-off and fail basic sanity checks. In this work, we argue that PaI's reliance on first-order saliency metrics, which ignore inter-weight dependencies, contributes substantially to this performance gap, especially in the sparse regime. To address this, we introduce Concrete Ticket Search (CTS), an algorithm that frames subnetwork discovery as a holistic combinatorial optimization problem. By leveraging a Concrete relaxation of the discrete search space and a novel gradient balancing scheme (GRADBALANCE) to control sparsity, CTS efficiently identifies high-performing subnetworks near initialization without requiring sensitive hyperparameter tuning. Motivated by recent works on lottery ticket training dynamics, we further propose a knowledge distillation-inspired family of pruning objectives, finding that minimizing the reverse Kullback-Leibler divergence between sparse and dense network outputs (CTS-KL) is particularly effective. Experiments on varying image classification tasks show that CTS produces subnetworks that robustly pass sanity checks and achieve accuracy comparable to or exceeding LTR, while requiring only a small fraction of the computation. For example, on ResNet-20 on CIFAR10, it reaches 99.3% sparsity with 74.0% accuracy in 7.9 minutes, while LTR attains the same sparsity with 68.3% accuracy in 95.2 minutes. CTS's subnetworks outperform saliency-based methods across all sparsities, but its advantage over LTR is most pronounced in the highly sparse regime.",0,arxiv,AI,CC-BY/arXiv,Winning the Lottery by Preserving Network Training Dynamics with Concrete Ticket Search
"Multimodal human action recognition (HAR) leverages complementary sensors for activity classification. Beyond recognition, recent advances in large language models (LLMs) enable detailed descriptions and causal reasoning, motivating new tasks: human action understanding (HAU) and human action reasoning (HARn). However, most LLMs, especially large vision language models (LVLMs), struggle with non-RGB modalities such as depth, IMU, and mmWave due to the lack of large-scale data-caption resources. Existing HAR datasets mainly provide coarse data-label annotations, which are insufficient to capture fine-grained action dynamics needed for HAU and HARn. We consider two ground-truth pair types: (1) data label (discrete category) and (2) data caption (textual description). Naively generating captions from labels often lacks logical and spatiotemporal consistency. We introduce CUHK-X, a large-scale multimodal dataset and benchmark suite for HAR, HAU, and HARn. CUHK-X contains 58,445 samples covering 40 actions performed by 30 participants across two indoor environments. To improve caption consistency, we propose a prompt-based scene creation method that leverages LLMs to generate logically connected activity sequences, followed by human validation. CUHK-X includes three benchmarks with six evaluation tasks. Experiments report average accuracies of 76.52% (HAR), 40.76% (HAU), and 70.25% (HARn). CUHK-X aims to enable the community to apply and develop data-intensive learning methods for robust, multimodal human activity analysis. Project page and code: https://openaiotlab.github.io/CUHK-X/ and https://github.com/openaiotlab/CUHK-X.",0,arxiv,AI,CC-BY/arXiv,A Large-Scale Multimodal Dataset and Benchmarks for Human Activity Scene Understanding and Reasoning
"Current autonomous driving systems often favor end-to-end frameworks, which take sensor inputs like images and learn to map them into trajectory space via neural networks. Previous work has demonstrated that models can achieve better planning performance when provided with a prior distribution of possible trajectories. However, these approaches often overlook two critical aspects: 1) The appropriate trajectory prior can vary significantly across different driving scenarios. 2) Their trajectory evaluation mechanism lacks policy-driven refinement, remaining constrained by the limitations of one-stage supervised training. To address these issues, we explore improvements in two key areas. For problem 1, we employ MoE to apply different trajectory priors tailored to different scenarios. For problem 2, we utilize Reinforcement Learning to fine-tune the trajectory scoring mechanism. Additionally, we integrate models with different perception backbones to enhance perceptual features. Our integrated model achieved a score of 51.08 on the navsim ICCV benchmark, securing third place.",0,arxiv,AI,CC-BY/arXiv,TrajMoE: Scene-Adaptive Trajectory Planning with Mixture of Experts and Reinforcement Learning
"Specialized visual tools can augment large language models or vision language models with expert knowledge (e.g., grounding, spatial reasoning, medical knowledge, etc.), but knowing which tools to call (and when to call them) can be challenging. We introduce DART, a multi-agent framework that uses disagreements between multiple debating visual agents to identify useful visual tools (e.g., object detection, OCR, spatial reasoning, etc.) that can resolve inter-agent disagreement. These tools allow for fruitful multi-agent discussion by introducing new information, and by providing tool-aligned agreement scores that highlight agents in agreement with expert tools, thereby facilitating discussion. We utilize an aggregator agent to select the best answer by providing the agent outputs and tool information. We test DART on four diverse benchmarks and show that our approach improves over multi-agent debate as well as over single agent tool-calling frameworks, beating the next-strongest baseline (multi-agent debate with a judge model) by 3.4% and 2.4% on A-OKVQA and MMMU respectively. We also find that DART adapts well to new tools in applied domains, with a 1.3% improvement on the M3D medical dataset over other strong tool-calling, single agent, and multi-agent baselines. Additionally, we measure text overlap across rounds to highlight the rich discussion in DART compared to existing multi-agent methods. Finally, we study the tool call distribution, finding that diverse tools are reliably used to help resolve disagreement.",0,arxiv,AI,CC-BY/arXiv,DART: Leveraging Multi-Agent Disagreement for Tool Recruitment in Multimodal Reasoning
"Flight control software is typically designed with numerous configurable parameters governing multiple functionalities, enabling flexible adaptation to mission diversity and environmental uncertainty. Although developers and manufacturers usually provide recommendations for these parameters to ensure safe and stable operations, certain combinations of parameters with recommended values may still lead to unstable flight behaviors, thereby degrading the drone's robustness. To this end, we propose a Large Language Model (LLM) based approach for real-time repair of risk-prone configurations (named RisConFix) that degrade drone robustness. RisConFix continuously monitors the drone's operational state and automatically triggers a repair mechanism once abnormal flight behaviors are detected. The repair mechanism leverages an LLM to analyze relationships between configuration parameters and flight states, and then generates corrective parameter updates to restore flight stability. To ensure the validity of the updated configuration, RisConFix operates as an iterative process; it continuously monitors the drone's flight state and, if an anomaly persists after applying an update, automatically triggers the next repair cycle. We evaluated RisConFix through a case study of ArduPilot (with 1,421 groups of misconfigurations). Experimental results show that RisConFix achieved a best repair success rate of 97% and an optimal average number of repairs of 1.17, demonstrating its capability to effectively and efficiently repair risk-prone configurations in real time.",0,arxiv,AI,CC-BY/arXiv,RisConFix: LLM-based Automated Repair of Risk-Prone Drone Configurations
"Large language models (LLMs) have demonstrated remarkable performance due to their large parameter counts and extensive training data. However, their scale leads to significant memory bottlenecks during training, especially when using memory-intensive optimizers like Adam. Existing memory-efficient approaches often rely on techniques such as singular value decomposition (SVD), projections, or weight freezing, which can introduce substantial computational overhead, require additional memory for projections, or degrade model performance. In this paper, we propose Folded Optimizer with Approximate Moment (FOAM), a method that compresses optimizer states by computing block-wise gradient means and incorporates a residual correction to recover lost information. Theoretically, FOAM achieves convergence rates equivalent to vanilla Adam under standard non-convex optimization settings. Empirically, FOAM reduces total training memory by approximately 50\%, eliminates up to 90\% of optimizer state memory overhead, and accelerates convergence. Furthermore, FOAM is compatible with other memory-efficient optimizers, delivering performance and throughput that match or surpass both full-rank and existing memory-efficient baselines.",0,arxiv,AI,CC-BY/arXiv,FOAM: Blocked State Folding for Memory-Efficient LLM Training
"Responding to Hodel et al.'s (2024) call for a formal definition of task relatedness in re-arc, we present the first 9-category taxonomy of all 400 tasks, validated at 97.5% accuracy via rule-based code analysis. We prove the taxonomy's visual coherence by training a CNN on raw grid pixels (95.24% accuracy on S3, 36.25% overall, 3.3x chance), then apply the taxonomy diagnostically to the original ARC-AGI-2 test set. Our curriculum analysis reveals 35.3% of tasks exhibit low neural affinity for Transformers--a distributional bias mirroring ARC-AGI-2. To probe this misalignment, we fine-tuned a 1.7M-parameter Transformer across 302 tasks, revealing a profound Compositional Gap: 210 of 302 tasks (69.5%) achieve >80% cell accuracy (local patterns) but <10% grid accuracy (global synthesis). This provides direct evidence for a Neural Affinity Ceiling Effect, where performance is bounded by architectural suitability, not curriculum. Applying our framework to Li et al.'s independent ViTARC study (400 specialists, 1M examples each) confirms its predictive power: Very Low affinity tasks achieve 51.9% versus 77.7% for High affinity (p<0.001), with a task at 0% despite massive data. The taxonomy enables precise diagnosis: low-affinity tasks (A2) hit hard ceilings, while high-affinity tasks (C1) reach 99.8%. These findings indicate that progress requires hybrid architectures with affinity-aligned modules. We release our validated taxonomy,",0,arxiv,AI,CC-BY/arXiv,A Neural Affinity Framework for Abstract Reasoning: Diagnosing the Compositional Gap in Transformer Architectures via Procedural Task Taxonomy
"Agentic LLM frameworks promise autonomous behavior via task decomposition, tool use, and iterative planning, but most deployed systems remain brittle. They lack runtime introspection, cannot diagnose their own failure modes, and do not improve over time without human intervention. In practice, many agent stacks degrade into decorated chains of LLM calls with no structural mechanisms for reliability. We present VIGIL (Verifiable Inspection and Guarded Iterative Learning), a reflective runtime that supervises a sibling agent and performs autonomous maintenance rather than task execution. VIGIL ingests behavioral logs, appraises each event into a structured emotional representation, maintains a persistent EmoBank with decay and contextual policies, and derives an RBT diagnosis that sorts recent behavior into strengths, opportunities, and failures. From this analysis, VIGIL generates both guarded prompt updates that preserve core identity semantics and read only code proposals produced by a strategy engine that operates on log evidence and code hotspots. VIGIL functions as a state gated pipeline. Illegal transitions produce explicit errors rather than allowing the LLM to improvise. In a reminder latency case study, VIGIL identified elevated lag, proposed prompt and code repairs, and when its own diagnostic tool failed due to a schema conflict, it surfaced the internal error, produced a fallback diagnosis, and emitted a repair plan. This demonstrates meta level self repair in a deployed agent runtime.",0,arxiv,AI,CC-BY/arXiv,VIGIL: A Reflective Runtime for Self-Healing Agents
"Background: The deployment of personalized Large Language Models (LLMs) is currently constrained by the stability-plasticity dilemma. Prevailing alignment methods, such as Supervised Fine-Tuning (SFT), rely on stochastic weight updates that often incur an ""alignment tax"" -- degrading general reasoning capabilities.   Methods: We propose the Soul Engine, a framework based on the Linear Representation Hypothesis, which posits that personality traits exist as orthogonal linear subspaces. We introduce SoulBench, a dataset constructed via dynamic contextual sampling. Using a dual-head architecture on a frozen Qwen-2.5 base, we extract disentangled personality vectors without modifying the backbone weights.   Results: Our experiments demonstrate three breakthroughs. First, High-Precision Profiling: The model achieves a Mean Squared Error (MSE) of 0.011 against psychological ground truth. Second, Geometric Orthogonality: T-SNE visualization confirms that personality manifolds are distinct and continuous, allowing for ""Zero-Shot Personality Injection"" that maintains original model intelligence. Third, Deterministic Steering: We achieve robust control over behavior via vector arithmetic, validated through extensive ablation studies.   Conclusion: This work challenges the necessity of fine-tuning for personalization. By transitioning from probabilistic prompting to deterministic latent intervention, we provide a mathematically rigorous foundation for safe, controllable AI personalization.",0,arxiv,AI,CC-BY/arXiv,The Geometry of Persona: Disentangling Personality from Reasoning in Large Language Models
"Pruning has emerged as a promising direction for accelerating large language model (LLM) inference, yet existing approaches often suffer from instability because they rely on offline calibration data that may not generalize across inputs. In this work, we introduce Token Filtering, a lightweight online structured pruning technique that makes pruning decisions directly during inference without any calibration data. The key idea is to measure token redundancy via joint key-value similarity and skip redundant attention computations, thereby reducing inference cost while preserving critical information. To further enhance stability, we design a variance-aware fusion strategy that adaptively weights key and value similarity across heads, ensuring that informative tokens are retained even under high pruning ratios. This design introduces no additional memory overhead and provides a more reliable criterion for token importance. Extensive experiments on LLaMA-2 (7B/13B), LLaMA-3 (8B), and Mistral (7B) demonstrate that Token Filtering consistently outperforms prior structured pruning methods, preserving accuracy on commonsense reasoning benchmarks and maintaining strong performance on challenging tasks such as MMLU, even with 50% pruning.",0,arxiv,AI,CC-BY/arXiv,Leveraging KV Similarity for Online Structured Pruning in LLMs
"Large Language Models (LLMs) have become foundational components in a wide range of applications, including natural language understanding and generation, embodied intelligence, and scientific discovery. As their computational requirements continue to grow, these models are increasingly deployed as cloud-based services, allowing users to access powerful LLMs via the Internet. However, this deployment model introduces a new class of threat: denial-of-service (DoS) attacks via unbounded reasoning, where adversaries craft specially designed inputs that cause the model to enter excessively long or infinite generation loops. These attacks can exhaust backend compute resources, degrading or denying service to legitimate users. To mitigate such risks, many LLM providers adopt a closed-source, black-box setting to obscure model internals. In this paper, we propose ThinkTrap, a novel input-space optimization framework for DoS attacks against LLM services even in black-box environments. The core idea of ThinkTrap is to first map discrete tokens into a continuous embedding space, then undertake efficient black-box optimization in a low-dimensional subspace exploiting input sparsity. The goal of this optimization is to identify adversarial prompts that induce extended or non-terminating generation across several state-of-the-art LLMs, achieving DoS with minimal token overhead. We evaluate the proposed attack across multiple commercial, closed-source LLM services. Our results demonstrate that, even far under the restrictive request frequency limits commonly enforced by these platforms, typically capped at ten requests per minute (10 RPM), the attack can degrade service throughput to as low as 1% of its original capacity, and in some cases, induce complete service failure.",0,arxiv,AI,CC-BY/arXiv,ThinkTrap: Denial-of-Service Attacks against Black-box LLM Services via Infinite Thinking
"Heart failure (HF) is one of the leading causes of rehospitalization among older adults in the United States. Although clinical notes contain rich, detailed patient information and make up a large portion of electronic health records (EHRs), they remain underutilized for HF readmission risk analysis. Traditional computational models for HF readmission often rely on expert-crafted rules, medical thesauri, and ontologies to interpret clinical notes, which are typically written under time pressure and may contain misspellings, abbreviations, and domain-specific jargon. We present ClinNoteAgents, an LLM-based multi-agent framework that transforms free-text clinical notes into (1) structured representations of clinical and social risk factors for association analysis and (2) clinician-style abstractions for HF 30-day readmission prediction. We evaluate ClinNoteAgents on 3,544 notes from 2,065 patients (readmission rate=35.16%), demonstrating strong performance in extracting risk factors from free-text, identifying key contributing factors, and predicting readmission risk. By reducing reliance on structured fields and minimizing manual annotation and model training, ClinNoteAgents provides a scalable and interpretable approach to note-based HF readmission risk modeling in data-limited healthcare systems.",0,arxiv,AI,CC-BY/arXiv,ClinNoteAgents: An LLM Multi-Agent System for Predicting and Interpreting Heart Failure 30-Day Readmission from Clinical Notes
"Progress in computer-aided synthesis planning (CASP) is obscured by the lack of standardized evaluation infrastructure and the reliance on metrics that prioritize topological completion over chemical validity. We introduce RetroCast, a unified evaluation suite that standardizes heterogeneous model outputs into a common schema to enable statistically rigorous, apples-to-apples comparison. The framework includes a reproducible benchmarking pipeline with stratified sampling and bootstrapped confidence intervals, accompanied by SynthArena, an interactive platform for qualitative route inspection. We utilize this infrastructure to evaluate leading search-based and sequence-based algorithms on a new suite of standardized benchmarks. Our analysis reveals a divergence between ""solvability"" (stock-termination rate) and route quality; high solvability scores often mask chemical invalidity or fail to correlate with the reproduction of experimental ground truths. Furthermore, we identify a ""complexity cliff"" in which search-based methods, despite high solvability rates, exhibit a sharp performance decay in reconstructing long-range synthetic plans compared to sequence-based approaches. We release the full framework, benchmark definitions, and a standardized database of model predictions to support transparent and reproducible development in the field.",0,arxiv,AI,CC-BY/arXiv,Procrustean Bed for AI-Driven Retrosynthesis: A Unified Framework for Reproducible Evaluation
"Self-supervised learning (SSL) plays a central role in molecular representation learning. Yet, many recent innovations in masking-based pretraining are introduced as heuristics and lack principled evaluation, obscuring which design choices are genuinely effective. This work cast the entire pretrain-finetune workflow into a unified probabilistic framework, enabling a transparent comparison and deeper understanding of masking strategies. Building on this formalism, we conduct a controlled study of three core design dimensions: masking distribution, prediction target, and encoder architecture, under rigorously controlled settings. We further employ information-theoretic measures to assess the informativeness of pretraining signals and connect them to empirically benchmarked downstream performance. Our findings reveal a surprising insight: sophisticated masking distributions offer no consistent benefit over uniform sampling for common node-level prediction tasks. Instead, the choice of prediction target and its synergy with the encoder architecture are far more critical. Specifically, shifting to semantically richer targets yields substantial downstream improvements, particularly when paired with expressive Graph Transformer encoders. These insights offer practical guidance for developing more effective SSL methods for molecular graphs.",0,arxiv,AI,CC-BY/arXiv,Self-Supervised Learning on Molecular Graphs: A Systematic Investigation of Masking Design
"Although diffusion models with strong visual priors have emerged as powerful dense prediction backboens, they overlook a core limitation: the stochastic noise at the core of diffusion sampling is inherently misaligned with dense prediction that requires a deterministic mapping from image to geometry. In this paper, we show that this stochastic noise corrupts fine-grained spatial cues and pushes the model toward timestep-specific noise objectives, consequently destroying meaningful geometric structure mappings. To address this, we introduce $\mathrm{D}^{\mathrm{3}}$-Predictor, a noise-free deterministic framework built by reformulating a pretrained diffusion model without stochasticity noise. Instead of relying on noisy inputs to leverage diffusion priors, $\mathrm{D}^{\mathrm{3}}$-Predictor views the pretrained diffusion network as an ensemble of timestep-dependent visual experts and self-supervisedly aggregates their heterogeneous priors into a single, clean, and complete geometric prior. Meanwhile, we utilize task-specific supervision to seamlessly adapt this noise-free prior to dense prediction tasks. Extensive experiments on various dense prediction tasks demonstrate that $\mathrm{D}^{\mathrm{3}}$-Predictor achieves competitive or state-of-the-art performance in diverse scenarios. In addition, it requires less than half the training data previously used and efficiently performs inference in a single step. Our code, data, and checkpoints are publicly available at https://x-gengroup.github.io/HomePage_D3-Predictor/.",0,arxiv,AI,CC-BY/arXiv,$\mathrm{D}^{\mathrm{3}}$-Predictor: Noise-Free Deterministic Diffusion for Dense Prediction
"Medical image segmentation plays a pivotal role in automated diagnostic and treatment planning systems. In this work, we present DAUNet, a novel lightweight UNet variant that integrates Deformable V2 Convolutions and Parameter-Free Attention (SimAM) to improve spatial adaptability and context-aware feature fusion without increasing model complexity. DAUNet's bottleneck employs dynamic deformable kernels to handle geometric variations, while the decoder and skip pathways are enhanced using SimAM attention modules for saliency-aware refinement. Extensive evaluations on two challenging datasets, FH-PS-AoP (fetal head and pubic symphysis ultrasound) and FUMPE (CT-based pulmonary embolism detection), demonstrate that DAUNet outperforms state-of-the-art models in Dice score, HD95, and ASD, while maintaining superior parameter efficiency. Ablation studies highlight the individual contributions of deformable convolutions and SimAM attention. DAUNet's robustness to missing context and low-contrast regions establishes its suitability for deployment in real-time and resource-constrained clinical environments.",0,arxiv,AI,CC-BY/arXiv,DAUNet: A Lightweight UNet Variant with Deformable Convolutions and Parameter-Free Attention for Medical Image Segmentation
"Glass is a prevalent material among solid objects in everyday life, yet segmentation methods struggle to distinguish it from opaque materials due to its transparency and reflection. While it is known that human perception relies on boundary and reflective-object features to distinguish glass objects, the existing literature has not yet sufficiently captured both properties when handling transparent objects. Hence, we propose incorporating both of these powerful visual cues via the Boundary Feature Enhancement and Reflection Feature Enhancement modules in a mutually beneficial way. Our proposed framework, TransCues, is a pyramidal transformer encoder-decoder architecture to segment transparent objects. We empirically show that these two modules can be used together effectively, improving overall performance across various benchmark datasets, including glass object semantic segmentation, mirror object semantic segmentation, and generic segmentation datasets. Our method outperforms the state-of-the-art by a large margin, achieving +4.2% mIoU on Trans10K-v2, +5.6% mIoU on MSD, +10.1% mIoU on RGBD-Mirror, +13.1% mIoU on TROSD, and +8.3% mIoU on Stanford2D3D, showing the effectiveness of our method against glass objects.",0,arxiv,AI,CC-BY/arXiv,Power of Boundary and Reflection: Semantic Transparent Object Segmentation using Pyramid Vision Transformer with Transparent Cues
"Among the various types of cyberattacks, identifying zero-day attacks is problematic because they are unknown to security systems as their pattern and characteristics do not match known blacklisted attacks. There are many Machine Learning (ML) models designed to analyze and detect network attacks, especially using supervised models. However, these models are designed to classify samples (normal and attacks) based on the patterns they learn during the training phase, so they perform inefficiently on unseen attacks. This research addresses this issue by evaluating five different supervised models to assess their performance and execution time in predicting zero-day attacks and find out which model performs accurately and quickly. The goal is to improve the performance of these supervised models by not only proposing a framework that applies grid search, dimensionality reduction and oversampling methods to overcome the imbalance problem, but also comparing the effectiveness of oversampling on ml model metrics, in particular the accuracy. To emulate attack detection in real life, this research applies a highly imbalanced data set and only exposes the classifiers to zero-day attacks during the testing phase, so the models are not trained to flag the zero-day attacks. Our results show that Random Forest (RF) performs best under both oversampling and non-oversampling conditions, this increased effectiveness comes at the cost of longer processing times. Therefore, we selected XG Boost (XGB) as the top model due to its fast and highly accurate performance in detecting zero-day attacks.",0,arxiv,AI,CC-BY/arXiv,A Comprehensive Study of Supervised Machine Learning Models for Zero-Day Attack Detection: Analyzing Performance on Imbalanced Data
"Bug localization remains a critical yet time-consuming challenge in large-scale software repositories. Traditional information retrieval-based bug localization (IRBL) methods rely on unchanged bug descriptions, which often contain noisy information, leading to poor retrieval accuracy. Recent advances in large language models (LLMs) have improved bug localization through query reformulation, yet the effect on agent performance remains unexplored. In this study, we investigate how an LLM-powered agent can improve file-level bug localization via lightweight query reformulation and summarization. We first employ an open-source, non-fine-tuned LLM to extract key information from bug reports, such as identifiers and code snippets, and reformulate queries pre-retrieval. Our agent then orchestrates BM25 retrieval using these preprocessed queries, automating localization workflow at scale. Using the best-performing query reformulation technique, our agent achieves 35% better ranking in first-file retrieval than our BM25 baseline and up to +22% file retrieval performance over SWE-agent.",0,arxiv,AI,CC-BY/arXiv,"Reformulate, Retrieve, Localize: Agents for Repository-Level Bug Localization"
"Deep learning models have shown high accuracy in classifying electrocardiograms (ECGs), but their black box nature hinders clinical adoption due to a lack of trust and interpretability. To address this, we propose a novel three-stage training paradigm that transfers knowledge from multimodal clinical data (laboratory exams, vitals, biometrics) into a powerful, yet unimodal, ECG encoder. We employ a self-supervised, joint-embedding pre-training stage to create an ECG representation that is enriched with contextual clinical information, while only requiring the ECG signal at inference time. Furthermore, as an indirect way to explain the model's output we train it to also predict associated laboratory abnormalities directly from the ECG embedding. Evaluated on the MIMIC-IV-ECG dataset, our model outperforms a standard signal-only baseline in multi-label diagnosis classification and successfully bridges a substantial portion of the performance gap to a fully multimodal model that requires all data at inference. Our work demonstrates a practical and effective method for creating more accurate and trustworthy ECG classification models. By converting abstract predictions into physiologically grounded \emph{explanations}, our approach offers a promising path toward the safer integration of AI into clinical workflows.",0,arxiv,AI,CC-BY/arXiv,Transferring Clinical Knowledge into ECGs Representation
"The proliferation of Large Language Models (LLMs) necessitates valid evaluation methods to provide guidance for both downstream applications and actionable future improvements. The Item Response Theory (IRT) model with Computerized Adaptive Testing has recently emerged as a promising framework for evaluating LLMs via their response accuracy. Beyond simple response accuracy, LLMs' chain of thought (CoT) lengths serve as a vital indicator of their reasoning ability. To leverage the CoT length information to assist the evaluation of LLMs, we propose the Latency-Response Theory (LaRT) model, which jointly models both the response accuracy and CoT length by introducing a key correlation parameter between the latent ability and the latent speed. We derive an efficient stochastic approximation Expectation-Maximization algorithm for parameter estimation. We establish rigorous identifiability results for the latent ability and latent speed parameters to ensure the statistical validity of their estimation. Through both theoretical asymptotic analyses and simulation studies, we demonstrate LaRT's advantages over IRT in terms of superior estimation accuracy and shorter confidence intervals for latent trait estimation. To evaluate LaRT in real data, we collect responses from diverse LLMs on popular benchmark datasets. We find that LaRT yields different LLM rankings than IRT and outperforms IRT across multiple key evaluation metrics including predictive power, item efficiency, ranking validity, and LLM evaluation efficiency. Code and data are available at https://github.com/Toby-X/Latency-Response-Theory-Model.",0,arxiv,AI,CC-BY/arXiv,Latency-Response Theory Model: Evaluating Large Language Models via Response Accuracy and Chain-of-Thought Length
"Retrieval-Augmented Generation (RAG) systems have significantly reduced hallucinations in Large Language Models (LLMs) by grounding responses in external context. However, standard RAG architectures suffer from a critical vulnerability: Retrieval Sycophancy. When presented with a query based on a false premise or a common misconception, vector-based retrievers tend to fetch documents that align with the user's bias rather than objective truth, leading the model to ""hallucinate with citations.""   In this work, we introduce Falsification-Verification Alignment RAG (FVA-RAG), a framework that shifts the retrieval paradigm from Inductive Verification (seeking support) to Deductive Falsification (seeking disproof). Unlike existing ""Self-Correction"" methods that rely on internal consistency, FVA-RAG deploys a distinct Adversarial Retrieval Policy that actively generates ""Kill Queries""-targeted search terms designed to surface contradictory evidence. We introduce a dual-verification mechanism that explicitly weighs the draft answer against this ""Anti-Context."" Preliminary experiments on a dataset of common misconceptions demonstrate that FVA-RAG significantly improves robustness against sycophantic hallucinations compared to standard RAG baselines, effectively acting as an inference-time ""Red Team"" for factual generation.",0,arxiv,AI,CC-BY/arXiv,FVA-RAG: Falsification-Verification Alignment for Mitigating Sycophantic Hallucinations
"Cost-effective and scalable video analytics are essential for precision livestock monitoring, where high-resolution footage and near-real-time monitoring needs from commercial farms generates substantial computational workloads. This paper presents a comprehensive case study on optimizing a poultry welfare monitoring system through system-level improvements across detection, tracking, clustering, and behavioral analysis modules. We introduce a set of optimizations, including multi-level parallelization, Optimizing code with substituting CPU code with GPU-accelerated code, vectorized clustering, and memory-efficient post-processing. Evaluated on real-world farm video footage, these changes deliver up to a 2x speedup across pipelines without compromising model accuracy. Our findings highlight practical strategies for building high-throughput, low-latency video inference systems that reduce infrastructure demands in agricultural and smart sensing deployments as well as other large-scale video analytics applications.",0,arxiv,AI,CC-BY/arXiv,Optimizing video analytics inference pipelines: a case study
"Singing accent research is underexplored compared to speech accent studies, primarily due to the scarcity of suitable datasets. Existing singing datasets often suffer from detail loss, frequently resulting from the vocal-instrumental separation process. Additionally, they often lack regional accent annotations. To address this, we introduce the Multi-Accent Mandarin Dry-Vocal Singing Dataset (MADVSD). MADVSD comprises over 670 hours of dry vocal recordings from 4,206 native Mandarin speakers across nine distinct Chinese regions. In addition to each participant recording audio of three popular songs in their native accent, they also recorded phonetic exercises covering all Mandarin vowels and a full octave range. We validated MADVSD through benchmark experiments in singing accent recognition, demonstrating its utility for evaluating state-of-the-art speech models in singing contexts. Furthermore, we explored dialectal influences on singing accent and analyzed the role of vowels in accentual variations, leveraging MADVSD's unique phonetic exercises.",0,arxiv,AI,CC-BY/arXiv,Multi-Accent Mandarin Dry-Vocal Singing Dataset: Benchmark for Singing Accent Recognition
"This paper examines the deployment of seven different neural network architectures CNN, RNN, GNN, Autoencoder, Transformer, NCF, and Siamese Networks on three distinct datasets: Retail E-commerce, Amazon Products, and Netflix Prize. It evaluates their effectiveness through metrics such as accuracy, recall, F1-score, and diversity in recommendations. The results demonstrate that GNNs are particularly adept at managing complex item relationships in e-commerce environments, whereas RNNs are effective in capturing the temporal dynamics that are essential for platforms such as Netflix.. Siamese Networks are emphasized for their contribution to the diversification of recommendations, particularly in retail settings. Despite their benefits, issues like computational demands, reliance on extensive data, and the challenge of balancing accurate and diverse recommendations are addressed. The study seeks to inform the advancement of recommendation systems by suggesting hybrid methods that merge the strengths of various models to better satisfy user preferences and accommodate the evolving demands of contemporary digital platforms.",0,arxiv,AI,CC-BY/arXiv,Benchmarking Deep Neural Networks for Modern Recommendation Systems
"Automated singing assessment is crucial for education and entertainment. However, existing systems face two fundamental limitations: reliance on reference tracks, which stifles creative expression, and the simplification of complex performances into non-diagnostic scores based solely on pitch and rhythm. We advocate for a shift from discriminative to descriptive evaluation, creating a complete ecosystem for reference-free, multi-dimensional assessment. First, we introduce Sing-MD, a large-scale dataset annotated by experts across four dimensions: breath control, timbre quality, emotional expression, and vocal technique. Our analysis reveals significant annotation inconsistencies among experts, challenging the validity of traditional accuracy-based metrics. Second, addressing the memory limitations of Multimodal Large Language Models (MLLMs) in analyzing full-length songs, we propose VocalVerse. This efficient hybrid architecture leverages a lightweight acoustic encoder to model global performance features and long-term dependencies. Third, to address automated metric shortcomings, we establish the H-TPR (Human-in-the-loop Tiered Perceptual Ranking) benchmark, which evaluates a model's ability to generate perceptually valid rankings rather than predicting noisy ground-truth scores.",0,arxiv,AI,CC-BY/arXiv,Singing Timbre Popularity Assessment Based on Multimodal Large Foundation Model
"Large Language Models (LLMs) have demonstrated remarkable capabilities across various natural language processing tasks. This research introduces a novel ""Prompting-in-a-Series"" algorithm, termed PICEPR (Psychology-Informed Contents Embeddings for Personality Recognition), featuring two pipelines: (a) Contents and (b) Embeddings. The approach demonstrates how a modularised decoder-only LLM can summarize or generate content, which can aid in classifying or enhancing personality recognition functions as a personality feature extractor and a generator for personality-rich content. We conducted various experiments to provide evidence to justify the rationale behind the PICEPR algorithm. Meanwhile, we also explored closed-source models such as \textit{gpt4o} from OpenAI and \textit{gemini} from Google, along with open-source models like \textit{mistral} from Mistral AI, to compare the quality of the generated content. The PICEPR algorithm has achieved a new state-of-the-art performance for personality recognition by 5-15\% improvement. The work repository and models' weight can be found at https://research.jingjietan.com/?q=PICEPR.",0,arxiv,AI,CC-BY/arXiv,Prompting-in-a-Series: Psychology-Informed Contents and Embeddings for Personality Recognition With Decoder-Only Models
"Currently, there is a noticeable lack of AI in the medical field to support doctors in treating heterogenous brain tumors such as Glioblastoma Multiforme (GBM), the deadliest human cancer in the world with a five-year survival rate of just 5.1%. This project develops an AI system offering the only end-to-end solution by aiding doctors with both diagnosis and treatment planning. In the diagnosis phase, a sequential decision-making framework consisting of 4 classification models (Convolutional Neural Networks and Support Vector Machine) are used. Each model progressively classifies the patient's brain into increasingly specific categories, with the final step being named diagnosis. For treatment planning, an RL system consisting of 3 generative models is used. First, the resection model (diffusion model) analyzes the diagnosed GBM MRI and predicts a possible resection outcome. Second, the radiotherapy model (Spatio-Temporal Vision Transformer) generates an MRI of the brain's progression after a user-defined number of weeks. Third, the chemotherapy model (Diffusion Model) produces the post-treatment MRI. A survival rate calculator (Convolutional Neural Network) then checks if the generated post treatment MRI has a survival rate within 15% of the user defined target. If not, a feedback loop using proximal policy optimization iterates over this system until an optimal resection location is identified. When compared to existing solutions, this project found 3 key findings: (1) Using a sequential decision-making framework consisting of 4 small diagnostic models reduced computing costs by 22.28x, (2) Transformers regression capabilities decreased tumor progression inference time by 113 hours, and (3) Applying Augmentations resembling Real-life situations improved overall DICE scores by 2.9%. These results project to increase survival rates by 0.9%, potentially saving approximately 2,250 lives.",0,arxiv,AI,CC-BY/arXiv,Utilizing Multi-Agent Reinforcement Learning with Encoder-Decoder Architecture Agents to Identify Optimal Resection Location in Glioblastoma Multiforme Patients
"We explore Multi-Head FFN (MH-FFN) as a replacement of FFN in the Transformer architecture, motivated by the structural similarity between single-head attention and FFN. While multi-head mechanisms enhance expressivity in attention, naively applying them to FFNs faces two challenges: memory consumption scaling with the head count, and an imbalanced ratio between the growing intermediate size and the fixed head dimension as models scale, which degrades scalability and expressive power. To address these challenges, we propose Flash Multi-Head FFN (FlashMHF), with two key innovations: an I/O-aware fused kernel computing outputs online in SRAM akin to FlashAttention, and a design using dynamically weighted parallel sub-networks to maintain a balanced ratio between intermediate and head dimensions. Validated on models from 128M to 1.3B parameters, FlashMHF consistently improves perplexity and downstream task accuracy over SwiGLU FFNs, while reducing peak memory usage by 3-5x and accelerating inference by up to 1.08x. Our work establishes the multi-head design as a superior architectural principle for FFNs, presenting FlashMHF as a powerful, efficient, and scalable alternative to FFNs in Transformers.",0,arxiv,AI,CC-BY/arXiv,Flash Multi-Head Feed-Forward Network
"World models enable agents to plan within imagined environments by predicting future states conditioned on past observations and actions. However, their ability to plan over long horizons is limited by the effective memory span of the backbone architecture. This limitation leads to perceptual drift in long rollouts, hindering the model's capacity to perform loop closures within imagined trajectories. In this work, we investigate the effective memory span of transformer-based world models through an analysis of several memory augmentation mechanisms. We introduce a taxonomy that distinguishes between memory encoding and memory injection mechanisms, motivating their roles in extending the world model's memory through the lens of residual stream dynamics. Using a state recall evaluation task, we measure the memory recall of each mechanism and analyze its respective trade-offs. Our findings show that memory mechanisms improve the effective memory span in vision transformers and provide a path to completing loop closures within a world model's imagination.",0,arxiv,AI,CC-BY/arXiv,On Memory: A comparison of memory mechanisms in world models
"Estimating the Hessian matrix, especially for neural network training, is a challenging problem due to high dimensionality and cost. In this work, we compare the classical Sherman-Morrison update used in the popular BFGS method (Broy-den-Fletcher-Goldfarb-Shanno), which maintains a positive definite Hessian approximation under a convexity assumption, with a novel approach called Online Gradient Regression (OGR). OGR performs regression of gradients against positions using an exponential moving average to estimate second derivatives online, without requiring Hessian inversion. Unlike BFGS, OGR allows estimation of a general (not necessarily positive definite) Hessian and can thus handle non-convex structures. We evaluate both methods across standard test functions and demonstrate that OGR achieves faster convergence and improved loss, particularly in non-convex settings.",0,arxiv,AI,CC-BY/arXiv,Comparing BFGS and OGR for Second-Order Optimization
"Generalization in robot manipulation is essential for deploying robots in open-world environments and advancing toward artificial general intelligence. While recent Vision-Language-Action (VLA) models leverage large pre-trained understanding models for perception and instruction following, their ability to generalize to novel tasks, objects, and settings remains limited. In this work, we present VideoVLA, a simple approach that explores the potential of transforming large video generation models into robotic VLA manipulators. Given a language instruction and an image, VideoVLA predicts an action sequence as well as the future visual outcomes. Built on a multi-modal Diffusion Transformer, VideoVLA jointly models video, language, and action modalities, using pre-trained video generative models for joint visual and action forecasting. Our experiments show that high-quality imagined futures correlate with reliable action predictions and task success, highlighting the importance of visual imagination in manipulation. VideoVLA demonstrates strong generalization, including imitating other embodiments' skills and handling novel objects. This dual-prediction strategy - forecasting both actions and their visual consequences - explores a paradigm shift in robot learning and unlocks generalization capabilities in manipulation systems.",0,arxiv,AI,CC-BY/arXiv,VideoVLA: Video Generators Can Be Generalizable Robot Manipulators
"We present a vision-action policy that won 1st place in the 2025 BEHAVIOR Challenge - a large-scale benchmark featuring 50 diverse long-horizon household tasks in photo-realistic simulation, requiring bimanual manipulation, navigation, and context-aware decision making.   Building on the Pi0.5 architecture, we introduce several innovations. Our primary contribution is correlated noise for flow matching, which improves training efficiency and enables correlation-aware inpainting for smooth action sequences. We also apply learnable mixed-layer attention and System 2 stage tracking for ambiguity resolution. Training employs multi-sample flow matching to reduce variance, while inference uses action compression and challenge-specific correction rules.   Our approach achieves 26% q-score across all 50 tasks on both public and private leaderboards.",0,arxiv,AI,CC-BY/arXiv,Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge
"The increasing use of Artificial Intelligence (AI) in critical societal domains has amplified concerns about fairness, particularly regarding unequal treatment across sensitive attributes such as race, gender, and socioeconomic status. While there has been substantial work on ensuring AI fairness, navigating trade-offs between competing notions of fairness as well as predictive accuracy remains challenging, creating barriers to the practical deployment of fair AI systems. To address this, we introduce a unifying human-centered fairness framework that systematically covers eight distinct fairness metrics, formed by combining individual and group fairness, infra-marginal and intersectional assumptions, and outcome-based and equality-of-opportunity (EOO) perspectives. This structure allows stakeholders to align fairness interventions with their values and contextual considerations. The framework uses a consistent and easy-to-understand formulation for all metrics to reduce the learning curve for non-experts. Rather than privileging a single fairness notion, the framework enables stakeholders to assign weights across multiple fairness objectives, reflecting their priorities and facilitating multi-stakeholder compromises. We apply this approach to four real-world datasets: the UCI Adult census dataset for income prediction, the COMPAS dataset for criminal recidivism, the German Credit dataset for credit risk assessment, and the MEPS dataset for healthcare utilization. We show that adjusting weights reveals nuanced trade-offs between different fairness metrics. Finally, through case studies in judicial decision-making and healthcare, we demonstrate how the framework can inform practical and value-sensitive deployment of fair AI systems.",0,arxiv,AI,CC-BY/arXiv,A Unifying Human-Centered AI Fairness Framework
"Deep learning models, particularly Long Short-Term Memory (LSTM) networks, are widely used in time series forecasting due to their ability to capture complex temporal dependencies. However, evaluation integrity is often compromised by data leakage, a methodological flaw in which input-output sequences are constructed before dataset partitioning, allowing future information to unintentionally influence training. This study investigates the impact of data leakage on performance, focusing on how validation design mediates leakage sensitivity. Three widely used validation techniques (2-way split, 3-way split, and 10-fold cross-validation) are evaluated under both leaky (pre-split sequence generation) and clean conditions, with the latter mitigating leakage risk by enforcing temporal separation during data splitting prior to sequence construction. The effect of leakage is assessed using RMSE Gain, which measures the relative increase in RMSE caused by leakage, computed as the percentage difference between leaky and clean setups. Empirical results show that 10-fold cross-validation exhibits RMSE Gain values of up to 20.5% at extended lag steps. In contrast, 2-way and 3-way splits demonstrate greater robustness, typically maintaining RMSE Gain below 5% across diverse configurations. Moreover, input window size and lag step significantly influence leakage sensitivity: smaller windows and longer lags increase the risk of leakage, whereas larger windows help reduce it. These findings underscore the need for configuration-aware, leakage-resistant evaluation pipelines to ensure reliable performance estimation.",0,arxiv,AI,CC-BY/arXiv,Hidden Leaks in Time Series Forecasting: How Data Leakage Affects LSTM Evaluation Across Configurations and Validation Strategies
"Time series forecasting in real world environments faces significant challenges non stationarity, multi scale temporal patterns, and distributional shifts that degrade model stability and accuracy. This study propose AdaMamba, a unified forecasting architecture that integrates adaptive normalization, multi scale trend extraction, and contextual sequence modeling to address these challenges. AdaMamba begins with an Adaptive Normalization Block that removes non stationary components through multi scale convolutional trend extraction and channel wise recalibration, enabling consistent detrending and variance stabilization. The normalized sequence is then processed by a Context Encoder that combines patch wise embeddings, positional encoding, and a Mamba enhanced Transformer layer with a mixture of experts feed forward module, allowing efficient modeling of both long range dependencies and local temporal dynamics. A lightweight prediction head generates multi horizon forecasts, and a denormalization mechanism reconstructs outputs by reintegrating local trends to ensure robustness under varying temporal conditions. AdaMamba provides strong representational capacity with modular extensibility, supporting deterministic prediction and compatibility with probabilistic extensions. Its design effectively mitigates covariate shift and enhances predictive reliability across heterogeneous datasets. Experimental evaluations demonstrate that AdaMamba's combination of adaptive normalization and expert augmented contextual modeling yields consistent improvements in stability and accuracy over conventional Transformer based baselines.",0,arxiv,AI,CC-BY/arXiv,Adaptive Normalization Mamba with Multi Scale Trend Decomposition and Patch MoE Encoding
"Deep learning (DL) models, a specialized class of multilayer neural networks, have become central to time-series forecasting in critical domains such as environmental monitoring and the Internet of Things (IoT). Among these, Bidirectional Long Short-Term Memory (BiLSTM) architectures are particularly effective in capturing complex temporal dependencies. However, the robustness and generalization of such models are highly sensitive to input data characteristics - an aspect that remains underexplored in existing literature. This study presents a systematic empirical analysis of two key data-centric factors: input sequence length and additive noise. To support this investigation, a modular and reproducible forecasting pipeline is developed, incorporating standardized preprocessing, sequence generation, model training, validation, and evaluation. Controlled experiments are conducted on three real-world datasets with varying sampling frequencies to assess BiLSTM performance under different input conditions. The results yield three key findings: (1) longer input sequences significantly increase the risk of overfitting and data leakage, particularly in data-constrained environments; (2) additive noise consistently degrades predictive accuracy across sampling frequencies; and (3) the simultaneous presence of both factors results in the most substantial decline in model stability. While datasets with higher observation frequencies exhibit greater robustness, they remain vulnerable when both input challenges are present. These findings highlight important limitations in current DL-based forecasting pipelines and underscore the need for data-aware design strategies. This work contributes to a deeper understanding of DL model behavior in dynamic time-series environments and provides practical insights for developing more reliable and generalizable forecasting systems.",0,arxiv,AI,CC-BY/arXiv,Evaluating the Sensitivity of BiLSTM Forecasting Models to Sequence Length and Input Noise
"Phishing is a cybercrime in which individuals are deceived into revealing personal information, often resulting in financial loss. These attacks commonly occur through fraudulent messages, misleading advertisements, and compromised legitimate websites. This study proposes a Quantile Regression Deep Q-Network (QR-DQN) approach that integrates RoBERTa semantic embeddings with handcrafted lexical features to enhance phishing detection while accounting for uncertainties. Unlike traditional DQN methods that estimate single scalar Q-values, QR-DQN leverages quantile regression to model the distribution of returns, improving stability and generalization on unseen phishing data. A diverse dataset of 105,000 URLs was curated from PhishTank, OpenPhish, Cloudflare, and other sources, and the model was evaluated using an 80/20 train-test split. The QR-DQN framework achieved a test accuracy of 99.86%, precision of 99.75%, recall of 99.96%, and F1-score of 99.85%, demonstrating high effectiveness. Compared to standard DQN with lexical features, the hybrid QR-DQN with lexical and semantic features reduced the generalization gap from 1.66% to 0.04%, indicating significant improvement in robustness. Five-fold cross-validation confirmed model reliability, yielding a mean accuracy of 99.90% with a standard deviation of 0.04%. These results suggest that the proposed hybrid approach effectively identifies phishing threats, adapts to evolving attack strategies, and generalizes well to unseen data.",0,arxiv,AI,CC-BY/arXiv,Deep Reinforcement Learning for Phishing Detection with Transformer-Based Semantic Features
"Multimodal Large Language Models (MLLMs) have shown significant potential in surgical video understanding. With improved zero-shot performance and more effective human-machine interaction, they provide a strong foundation for advancing surgical education and assistance. However, existing research and datasets primarily focus on understanding surgical procedures and workflows, while paying limited attention to the critical role of anatomical comprehension. In clinical practice, surgeons rely heavily on precise anatomical understanding to interpret, review, and learn from surgical videos. To fill this gap, we introduce the Neurosurgical Anatomy Benchmark (NeuroABench), the first multimodal benchmark explicitly created to evaluate anatomical understanding in the neurosurgical domain. NeuroABench consists of 9 hours of annotated neurosurgical videos covering 89 distinct procedures and is developed using a novel multimodal annotation pipeline with multiple review cycles. The benchmark evaluates the identification of 68 clinical anatomical structures, providing a rigorous and standardized framework for assessing model performance. Experiments on over 10 state-of-the-art MLLMs reveal significant limitations, with the best-performing model achieving only 40.87% accuracy in anatomical identification tasks. To further evaluate the benchmark, we extract a subset of the dataset and conduct an informative test with four neurosurgical trainees. The results show that the best-performing student achieves 56% accuracy, with the lowest scores of 28% and an average score of 46.5%. While the best MLLM performs comparably to the lowest-scoring student, it still lags significantly behind the group's average performance. This comparison underscores both the progress of MLLMs in anatomical understanding and the substantial gap that remains in achieving human-level performance.",0,arxiv,AI,CC-BY/arXiv,NeuroABench: A Multimodal Evaluation Benchmark for Neurosurgical Anatomy Identification
"Large Language Models (LLMs) are rapidly evolving into autonomous agents capable of interacting with the external world, significantly expanding their capabilities through standardized interaction protocols. However, this paradigm revives the classic cybersecurity challenges of agency and authorization in a novel and volatile context. As decision-making shifts from deterministic code logic to probabilistic inference driven by natural language, traditional security mechanisms designed for deterministic behavior fail. It is fundamentally challenging to establish trust for unpredictable AI agents and to enforce the Principle of Least Privilege (PoLP) when instructions are ambiguous. Despite the escalating threat landscape, the academic community's understanding of this emerging domain remains fragmented, lacking a systematic framework to analyze its root causes. This paper provides a unifying formal lens for agent-interaction security.   We observed that most security threats in this domain stem from a fundamental mismatch between trust evaluation and authorization policies. We introduce a novel risk analysis model centered on this trust-authorization gap. Using this model as a unifying lens, we survey and classify the implementation paths of existing, often seemingly isolated, attacks and defenses. This new framework not only unifies the field but also allows us to identify critical research gaps. Finally, we leverage our analysis to suggest a systematic research direction toward building robust, trusted agents and dynamic authorization mechanisms.",0,arxiv,AI,CC-BY/arXiv,SoK: Trust-Authorization Mismatch in LLM Agent Interactions
"As software systems evolve, developers increasingly work across multiple programming languages and often face the need to migrate code from one language to another. While automatic code translation offers a promising solution, it has long remained a challenging task. Recent advancements in Large Language Models (LLMs) have shown potential for this task, yet existing approaches remain limited in accuracy and fail to effectively leverage contextual and structural cues within the code. Prior work has explored translation and repair mechanisms, but lacks a structured, agentic framework where multiple specialized agents collaboratively improve translation quality. In this work, we introduce BabelCoder, an agentic framework that performs code translation by decomposing the task into specialized agents for translation, testing, and refinement, each responsible for a specific aspect such as generating code, validating correctness, or repairing errors. We evaluate BabelCoder on four benchmark datasets and compare it against four state-of-the-art baselines. BabelCoder outperforms existing methods by 0.5%-13.5% in 94% of cases, achieving an average accuracy of 94.16%.",0,arxiv,AI,CC-BY/arXiv,BabelCoder: Agentic Code Translation with Specification Alignment
"Panorama generation has recently attracted growing interest in the research community, with two core tasks, text-to-panorama and view-to-panorama generation. However, existing methods still face two major challenges: their U-Net-based architectures constrain the visual quality of the generated panoramas, and they usually treat the two core tasks independently, which leads to modeling redundancy and inefficiency. To overcome these challenges, we propose a joint-face panorama (JoPano) generation approach that unifies the two core tasks within a DiT-based model. To transfer the rich generative capabilities of existing DiT backbones learned from natural images to the panorama domain, we propose a Joint-Face Adapter built on the cubemap representation of panoramas, which enables a pretrained DiT to jointly model and generate different views of a panorama. We further apply Poisson Blending to reduce seam inconsistencies that often appear at the boundaries between cube faces. Correspondingly, we introduce Seam-SSIM and Seam-Sobel metrics to quantitatively evaluate the seam consistency. Moreover, we propose a condition switching mechanism that unifies text-to-panorama and view-to-panorama tasks within a single model. Comprehensive experiments show that JoPano can generate high-quality panoramas for both text-to-panorama and view-to-panorama generation tasks, achieving state-of-the-art performance on FID, CLIP-FID, IS, and CLIP-Score metrics.",0,arxiv,AI,CC-BY/arXiv,JoPano: Unified Panorama Generation via Joint Modeling
"Researchers struggle to efficiently locate and manage relevant literature within the exponentially growing body of scientific publications. We present \textsc{WisPaper}, an intelligent academic retrieval and literature management platform that addresses this challenge through three integrated capabilities: (1) \textit{Scholar Search}, featuring both quick keyword-based and deep agentic search modes for efficient paper discovery; (2) \textit{Library}, a customizable knowledge base for systematic literature organization; and (3) \textit{AI Feeds}, an intelligent recommendation system that automatically delivers relevant new publications based on user interests. Unlike existing academic tools, \textsc{WisPaper} provides a closed-loop workflow that seamlessly connects literature discovery, management, and continuous tracking of research frontiers. Our multilingual and multidisciplinary system significantly reduces the time researchers from diverse backgrounds spend on paper screening and management, enabling them to focus on their core research activities. The platform is publicly accessible and serves researchers across academia and industry.",0,arxiv,AI,CC-BY/arXiv,WisPaper: Your AI Scholar Search Engine
"Although persona prompting in large language models appears to trigger different styles of generated text, it is unclear whether these translate into measurable behavioral differences, much less whether they affect decision-making in an adversarial strategic environment that we provide as open-source. We investigate the impact of persona prompting on strategic performance in PERIL, a world-domination board game. Specifically, we compare the effectiveness of persona-derived heuristic strategies to those chosen manually. Our findings reveal that certain personas associated with strategic thinking improve game performance, but only when a mediator is used to translate personas into heuristic values. We introduce this mediator as a structured translation process, inspired by exploratory factor analysis, that maps LLM-generated inventory responses into heuristics. Results indicate our method enhances heuristic reliability and face validity compared to directly inferred heuristics, allowing us to better study the effect of persona types on decision making. These insights advance our understanding of how persona prompting influences LLM-based decision-making and propose a heuristic generation method that applies psychometric principles to LLMs.",0,arxiv,AI,CC-BY/arXiv,Do Persona-Infused LLMs Affect Performance in a Strategic Reasoning Game?
"Recent advances in Video Large Language Models (VLLMs) have achieved remarkable video understanding capabilities, yet face critical efficiency bottlenecks due to quadratic computational growth with lengthy visual token sequences of long videos. While existing keyframe sampling methods can improve temporal modeling efficiency, additional computational cost is introduced before feature encoding, and the binary frame selection paradigm is found suboptimal. Therefore, in this work, we propose Dynamic Token compression via LLM-guided Keyframe prior (DyToK), a training-free paradigm that enables dynamic token compression by harnessing VLLMs' inherent attention mechanisms. Our analysis reveals that VLLM attention layers naturally encoding query-conditioned keyframe priors, by which DyToK dynamically adjusts per-frame token retention ratios, prioritizing semantically rich frames while suppressing redundancies. Extensive experiments demonstrate that DyToK achieves state-of-the-art efficiency-accuracy tradeoffs. DyToK shows plug-and-play compatibility with existing compression methods, such as VisionZip and FastV, attaining 4.3x faster inference while preserving accuracy across multiple VLLMs, such as LLaVA-OneVision and Qwen2.5-VL. Code is available at https://github.com/yu-lin-li/DyToK .",0,arxiv,AI,CC-BY/arXiv,"Less Is More, but Where? Dynamic Token Compression via LLM-Guided Keyframe Prior"
"In this work, we present JT-DA-8B (JiuTian Data Analyst 8B), a specialized large language model designed for complex table reasoning tasks across diverse real-world scenarios. To address the lack of high-quality supervision in tabular reasoning scenarios, we construct a comprehensive and diverse training corpus with 34 well-defined table reasoning tasks, by aggregating 29 public table QA datasets and 3 million tables. An automatic pipeline is proposed to generate realistic multi-step analytical tasks involving reasoning patterns. The model is trained upon open-source JT-Coder-8B model, an 8B-parameter decoder-only foundation model trained from scratch. In the training stage, we leverage LLM-based scoring and workflow-aligned filtering to distill high-quality, table-centric data. Both supervised fine-tuning (SFT) and Reinforcement learning (RL) are adopted to optimize our model. Afterwards, a four-stage table reasoning workflow is proposed, including table preprocessing, table sensing, tool-integrated reasoning, and prompt engineering, to improve model interpretability and execution accuracy. Experimental results show that JT-DA-8B achieves strong performance in various table reasoning tasks, demonstrating the effectiveness of data-centric generation and workflow-driven optimization.",0,arxiv,AI,CC-BY/arXiv,JT-DA: Enhancing Data Analysis with Tool-Integrated Table Reasoning Large Language Models
"Power is the primary design objective of large-scale integrated circuits (ICs), especially for complex modern processors (i.e., CPUs). Accurate CPU power evaluation requires designers to go through the whole time-consuming IC implementation process, easily taking months. At the early design stage (e.g., architecture-level), classical power models are notoriously inaccurate. Recently, ML-based architecture-level power models have been proposed to boost accuracy, but the data availability is a severe challenge. Currently, there is no open-source dataset for this important ML application. A typical dataset generation process involves correct CPU design implementation and repetitive execution of power simulation flows, requiring significant design expertise, engineering effort, and execution time. Even private in-house datasets often fail to reflect realistic CPU design scenarios. In this work, we propose ArchPower, the first open-source dataset for architecture-level processor power modeling. We go through complex and realistic design flows to collect the CPU architectural information as features and the ground-truth simulated power as labels. Our dataset includes 200 CPU data samples, collected from 25 different CPU configurations when executing 8 different workloads. There are more than 100 architectural features in each data sample, including both hardware and event parameters. The label of each sample provides fine-grained power information, including the total design power and the power for each of the 11 components. Each power value is further decomposed into four fine-grained power groups: combinational logic power, sequential logic power, memory power, and clock power. ArchPower is available at https://github.com/hkust-zhiyao/ArchPower.",0,arxiv,AI,CC-BY/arXiv,ArchPower: Dataset for Architecture-Level Power Modeling of Modern CPU Design
"Formal verification of floating-point arithmetic remains challenging due to non-linear arithmetic behavior and the tight coupling between control and datapath logic. Existing approaches often rely on high-level C models for equivalence checking against Register Transfer Level (RTL) designs, but this introduces abstraction gaps, translation overhead, and limits scalability at the RTL level. To address these challenges, this paper presents a scalable methodology for verifying floating-point arithmetic using direct RTL-to-RTL model checking against a golden reference model. The approach adopts a divide-and conquer strategy that decomposes verification into modular stages, each captured by helper assertions and lemmas that collectively prove a main correctness theorem. Counterexample (CEX)-guided refinement is used to iteratively localize and resolve implementation defects, while targeted fault injection validates the robustness of the verification process against precision-critical datapath errors. To assess scalability and practicality, the methodology is extended with agentic AI-based formal property generation, integrating large language model (LLM)-driven automation with Human-in-the-Loop (HITL) refinement. Coverage analysis evaluates the effectiveness of the approach by comparing handwritten and AI-generated properties in both RTL-to-RTL model checking and standalone RTL verification settings. Results show that direct RTL-to-RTL model checking achieves higher coverage efficiency and requires fewer assertions than standalone verification, especially when combined with AI-generated properties refined through HITL guidance.",0,arxiv,AI,CC-BY/arXiv,"Formal that ""Floats"" High: Formal Verification of Floating Point Arithmetic"
"Software languages evolve over time for various reasons, such as the addition of new features. When the language's grammar definition evolves, textual instances that originally conformed to the grammar become outdated. For DSLs in a model-driven engineering context, there exists a plethora of techniques to co-evolve models with the evolving metamodel. However, these techniques are not geared to support DSLs with a textual syntax -- applying them to textual language definitions and instances may lead to the loss of information from the original instances, such as comments and layout information, which are valuable for software comprehension and maintenance. This study explores the potential of Large Language Model (LLM)-based solutions in achieving grammar and instance co-evolution, with attention to their ability to preserve auxiliary information when directly processing textual instances. By applying two advanced language models, Claude-3.5 and GPT-4o, and conducting experiments across seven case languages, we evaluated the feasibility and limitations of this approach. Our results indicate a good ability of the considered LLMs for migrating textual instances in small-scale cases with limited instance size, which are representative of a subset of cases encountered in practice. In addition, we observe significant challenges with the scalability of LLM-based solutions to larger instances, leading to insights that are useful for informing future research.",0,arxiv,AI,CC-BY/arXiv,Leveraging LLMs to support co-evolution between definitions and instances of textual DSLs
"Recent vision-language models (VLMs) achieve remarkable reasoning through reinforcement learning (RL), which provides a feasible solution for realizing continuous self-evolving large vision-language models (LVLMs) in the era of experience. However, RL for VLMs requires abundant high-quality multimodal data, especially challenging in specialized domains like chemistry, earth sciences, and multimodal mathematics. Existing strategies such as synthetic data and self-rewarding mechanisms suffer from limited distributions and alignment difficulties, ultimately causing reward hacking: models exploit high-reward patterns, collapsing policy entropy and destabilizing training. We propose DoGe (Decouple to Generalize), a dual-decoupling framework that guides models to first learn from context rather than problem solving by refocusing on the problem context scenarios overlooked by synthetic data methods. By decoupling learning process into dual components (Thinker and Solver), we reasonably quantify the reward signals of this process and propose a two-stage RL post-training approach from freely exploring context to practically solving tasks. Second, to increase the diversity of training data, DoGe constructs an evolving curriculum learning pipeline: an expanded native domain knowledge corpus and an iteratively evolving seed problems pool. Experiments show that our method consistently outperforms the baseline across various benchmarks, providing a scalable pathway for realizing self-evolving LVLMs.",0,arxiv,AI,CC-BY/arXiv,Decouple to Generalize: Context-First Self-Evolving Learning for Data-Scarce Vision-Language Reasoning
"Multimodal classifiers function as opaque black box models. While several techniques exist to interpret their predictions, very few of them are as intuitive and accessible as natural language explanations (NLEs). To build trust, such explanations must faithfully capture the classifier's internal decision making behavior, a property known as faithfulness. In this paper, we propose CAuSE (Causal Abstraction under Simulated Explanations), a novel framework to generate faithful NLEs for any pretrained multimodal classifier. We demonstrate that CAuSE generalizes across datasets and models through extensive empirical evaluations. Theoretically, we show that CAuSE, trained via interchange intervention, forms a causal abstraction of the underlying classifier. We further validate this through a redesigned metric for measuring causal faithfulness in multimodal settings. CAuSE surpasses other methods on this metric, with qualitative analysis reinforcing its advantages. We perform detailed error analysis to pinpoint the failure cases of CAuSE. For replicability, we make the codes available at https://github.com/newcodevelop/CAuSE",0,arxiv,AI,CC-BY/arXiv,CAuSE: Decoding Multimodal Classifiers using Faithful Natural Language Explanation
"High-performance concrete requires complex mix design decisions involving interdependent variables and practical constraints. While data-driven methods have improved predictive modeling for forward design in concrete engineering, inverse design remains limited, especially when some variables are fixed and only the remaining ones must be inferred. This study proposes a cooperative neural network framework for the partial inverse design of high-performance concrete. The framework integrates an imputation model with a surrogate strength predictor and learns through cooperative training. Once trained, it generates valid and performance-consistent mix designs in a single forward pass without retraining for different constraint scenarios. Compared with baseline models, including autoencoder models and Bayesian inference with Gaussian process surrogates, the proposed method achieves R-squared values of 0.87 to 0.92 and substantially reduces mean squared error by approximately 50% and 70%, respectively. The results show that the framework provides an accurate and computationally efficient foundation for constraint-aware, data-driven mix proportioning.",0,arxiv,AI,CC-BY/arXiv,Partial Inverse Design of High-Performance Concrete Using Cooperative Neural Networks for Constraint-Aware Mix Generation
"Pre-trained Vision-Language Models (VLMs), \textit{e.g.} CLIP, have become essential tools in multimodal transfer learning. However, fine-tuning VLMs in few-shot scenarios poses significant challenges in balancing task-specific adaptation and generalization in the obtained model. Meanwhile, current researches have predominantly focused on prompt-based adaptation methods, leaving adapter-based approaches underexplored and revealing notable performance gaps. To address these challenges, we introduce a novel Reconstruction-based Multimodal Adapter (RMAdapter), which leverages a dual-branch architecture. Unlike conventional single-branch adapters, RMAdapter consists of: (1) an adaptation branch that injects task-specific knowledge through parameter-efficient fine-tuning, and (2) a reconstruction branch that preserves general knowledge by reconstructing latent space features back into the original feature space. This design facilitates a dynamic balance between general and task-specific knowledge. Importantly, although RMAdapter introduces an additional reconstruction branch, it is carefully optimized to remain lightweight. By computing reconstruction loss locally at each layer and sharing projection modules, the overall computational overhead is kept minimal. A consistency constraint is also incorporated to better regulate the trade-off between discriminability and generalization. We comprehensively evaluate the effectiveness of RMAdapter on three representative tasks: generalization to new categories, generalization to new target datasets, and domain generalization. Without relying on data augmentation or duplicate prompt designs, our RMAdapter consistently outperforms state-of-the-art approaches across all evaluation metrics.",0,arxiv,AI,CC-BY/arXiv,RMAdapter: Reconstruction-based Multi-Modal Adapter for Vision-Language Models
"Several problems in machine learning are naturally expressed as the design and analysis of time-evolving probability distributions. This includes sampling via diffusion methods, optimizing the weights of neural networks, and analyzing the evolution of token distributions across layers of large language models. While the targeted applications differ (samples, weights, tokens), their mathematical descriptions share a common structure. A key idea is to switch from the Eulerian representation of densities to their Lagrangian counterpart through vector fields that advect particles. This dual view introduces challenges, notably the non-uniqueness of Lagrangian vector fields, but also opportunities to craft density evolutions and flows with favorable properties in terms of regularity, stability, and computational tractability. This survey presents an overview of these methods, with emphasis on two complementary approaches: diffusion methods, which rely on stochastic interpolation processes and underpin modern generative AI, and optimal transport, which defines interpolation by minimizing displacement cost. We illustrate how both approaches appear in applications ranging from sampling, neural network optimization, to modeling the dynamics of transformers for large language models.",0,arxiv,AI,CC-BY/arXiv,Optimal and Diffusion Transports in Machine Learning
"Positive-Unlabeled (PU) learning addresses classification problems where only a subset of positive examples is labeled and the remaining data is unlabeled, making explicit negative supervision unavailable. Existing PU methods often rely on negative-risk estimation or pseudo-labeling, which either require strong distributional assumptions or can collapse in high-dimensional settings. We propose AngularPU, a novel PU framework that operates on the unit hypersphere using cosine similarity and angular margin. In our formulation, the positive class is represented by a learnable prototype vector, and classification reduces to thresholding the cosine similarity between an embedding and this prototype-eliminating the need for explicit negative modeling. To counteract the tendency of unlabeled embeddings to cluster near the positive prototype, we introduce an angular regularizer that encourages dispersion of the unlabeled set over the hypersphere, improving separation. We provide theoretical guarantees on the Bayes-optimality of the angular decision rule, consistency of the learned prototype, and the effect of the regularizer on the unlabeled distribution. Experiments on benchmark datasets demonstrate that AngularPU achieves competitive or superior performance compared to state-of-the-art PU methods, particularly in settings with scarce positives and high-dimensional embeddings, while offering geometric interpretability and scalability.",0,arxiv,AI,CC-BY/arXiv,Angular Regularization for Positive-Unlabeled Learning on the Hypersphere
"Manual vulnerability scoring, such as assigning Common Vulnerability Scoring System (CVSS) scores, is a resource-intensive process that is often influenced by subjective interpretation. This study investigates the potential of general-purpose large language models (LLMs), namely ChatGPT, Llama, Grok, DeepSeek, and Gemini, to automate this process by analyzing over 31{,}000 recent Common Vulnerabilities and Exposures (CVE) entries. The results show that LLMs substantially outperform the baseline on certain metrics (e.g., \textit{Availability Impact}), while offering more modest gains on others (e.g., \textit{Attack Complexity}). Moreover, model performance varies across both LLM families and individual CVSS metrics, with ChatGPT-5 attaining the highest precision. Our analysis reveals that LLMs tend to misclassify many of the same CVEs, and ensemble-based meta-classifiers only marginally improve performance. Further examination shows that CVE descriptions often lack critical context or contain ambiguous phrasing, which contributes to systematic misclassifications. These findings underscore the importance of enhancing vulnerability descriptions and incorporating richer contextual details to support more reliable automated reasoning and alleviate the growing backlog of CVEs awaiting triage.",0,arxiv,AI,CC-BY/arXiv,From Description to Score: Can LLMs Quantify Vulnerabilities?
"Large language models (LLMs) excel at generation but dominant autoregressive (AR) decoding is inherently sequential, creating a throughput bottleneck. Diffusion Language Models (DLMs)--especially block-wise variants--enable parallel generation and intra-block bidirectional reasoning, yet training large DLMs from scratch is costly and wastes the knowledge in mature AR checkpoints. Prior ""adaptation"" attempts either modify logits or randomly grow attention masks to full-sequence diffusion, or simply transplant AR weights into a block-diffusion recipe, leaving a fundamental mismatch between AR causality and block-wise bidirectionality unaddressed. We reframe adaptation as a intra-paradigm path from AR to Block-Diffusion by viewing AR as Block-Diffusion with blocksize=1. Concretely, we design the pathway of adaptation as follows: we use a context-causal attention mask (causal in context, bidirectional only within the active block), an efficient parallel adaptation procedure, an auxiliary AR loss to maximize data utilization and retain pretrained knowledge, and gradual increment of the generation block size. The recipe integrates cleanly with masked block-diffusion and maintains train-inference consistency. Built on these components, NBDiff-7B (Base and Instruct) could inherit the long-context modeling and reasoning capabilities, and achieve state-of-the-art performance among the 7B-class DLMs, delivering strong gains on general-knowledge, math, and code benchmarks over strong baselines. These results demonstrate that principled AR-to-block-diffusion adaptation is an effective and compute-efficient alternative to training DLMs from scratch. Codes: https://github.com/YuchuanTian/NBDiff.",0,arxiv,AI,CC-BY/arXiv,From Next-Token to Next-Block: A Principled Adaptation Path for Diffusion LLMs
"3D Gaussian Splatting (3DGS) has enabled the creation of digital assets and downstream applications, underscoring the need for robust copyright protection via digital watermarking. However, existing 3DGS watermarking methods remain highly vulnerable to diffusion-based editing, which can easily erase embedded provenance. This challenge highlights the urgent need for 3DGS watermarking techniques that are intrinsically resilient to diffusion-based editing. In this paper, we introduce RDSplat, a Robust watermarking paradigm against Diffusion editing for 3D Gaussian Splatting. RDSplat embeds watermarks into 3DGS components that diffusion-based editing inherently preserve, achieved through (i) proactively targeting low-frequency Gaussians and (ii) adversarial training with a diffusion proxy. Specifically, we introduce a multi-domain framework that operates natively in 3DGS space and embeds watermarks into diffusion-editing-preserved low-frequency Gaussians via coordinated covariance regularization and 2D filtering. In addition, we exploit the low-pass filtering behavior of diffusion-based editing by using Gaussian blur as an efficient training surrogate, enabling adversarial fine-tuning that further enhances watermark robustness against diffusion-based editing. Empirically, comprehensive quantitative and qualitative evaluations on three benchmark datasets demonstrate that RDSplat not only maintains superior robustness under diffusion-based editing, but also preserves watermark invisibility, achieving state-of-the-art performance.",0,arxiv,AI,CC-BY/arXiv,RDSplat: Robust Watermarking Against Diffusion Editing for 3D Gaussian Splatting
"Existing vision-language models often suffer from spatial hallucinations, i.e., generating incorrect descriptions about the relative positions of objects in an image. We argue that this problem mainly stems from the asymmetric properties between images and text. To enrich the spatial understanding ability of vision-language models, we propose a simple, annotation-free, plug-and-play method named $\text{Stitch and Tell}$ (abbreviated as SiTe), which injects structured spatial supervision into data. It constructs stitched image-text pairs by stitching images along a spatial axis and generating spatially-aware captions or question answer pairs based on the layout of stitched image, without relying on costly advanced models or human involvement. We evaluate SiTe across three architectures including LLaVA-v1.5-7B, LLaVA-Qwen2-1.5B and HALVA-7B, two training datasets, and eight benchmarks. Experiments show that SiTe improves spatial understanding tasks such as $\text{MME}_{\text{Position}}$ (+5.50%) and Spatial-MM (+4.19%), while maintaining or improving performance on general vision-language benchmarks including COCO-QA (+1.02%) and MMBench (+4.76%). Our findings suggest that explicitly injecting spatially-aware structure into training data offers an effective way to mitigate spatial hallucinations and improve spatial understanding, while preserving general vision-language capabilities.",0,arxiv,AI,CC-BY/arXiv,Stitch and Tell: A Structured Multimodal Data Augmentation Method for Spatial Understanding
"Understanding multi-image, multi-turn scenarios is a critical yet underexplored capability for Large Vision-Language Models (LVLMs). Existing benchmarks predominantly focus on static or horizontal comparisons -- e.g., spotting visual differences or assessing appropriateness -- while relying heavily on language cues. Such settings overlook progressive, context-dependent reasoning and the challenge of visual-to-visual inference. To bridge this gap, we present VisChainBench, a large-scale benchmark designed to rigorously evaluate LVLMs' ability to perform multi-step visual reasoning across sequential, interdependent tasks with minimal language guidance. VisChainBench contains 1,457 tasks spanning over 20,000 images across three diverse domains (e.g., daily scenarios, engineering troubleshooting), structured to mimic real-world decision-making processes. Uniquely, the benchmark is constructed using a multi-agent generation pipeline, ensuring high visual diversity and controlled language bias. All the benchmark data and code for benchmark construction are available for viewing and download via following Link: https://huggingface.co/datasets/eyehole/VisChainBench",0,arxiv,AI,CC-BY/arXiv,"VisChainBench: A Benchmark for Multi-Turn, Multi-Image Visual Reasoning Beyond Language Priors"
"Automatic evaluation with large language models, commonly known as LLM-as-a-judge, is now standard across reasoning and alignment tasks. Despite evaluating many samples in deployment, these evaluators typically (i) treat each case independently, missing the opportunity to accumulate experience, and (ii) rely on a single fixed prompt for all cases, neglecting the need for sample-specific evaluation criteria. We introduce Learning While Evaluating (LWE), a framework that allows evaluators to improve sequentially at inference time without requiring training or validation sets. LWE maintains an evolving meta-prompt that (i) produces sample-specific evaluation instructions and (ii) refines itself through self-generated feedback. Furthermore, we propose Selective LWE, which updates the meta-prompt only on self-inconsistent cases, focusing computation where it matters most. This selective approach retains the benefits of sequential learning while being far more cost-effective. Across two pairwise comparison benchmarks, Selective LWE outperforms strong baselines, empirically demonstrating that evaluators can improve during sequential testing with a simple selective update, learning most from the cases they struggle with.",0,arxiv,AI,CC-BY/arXiv,Becoming Experienced Judges: Selective Test-Time Learning for Evaluators
"Large language model (LLM)-based multi-agent systems are challenging to debug because failures often arise from long, branching interaction traces. The prevailing practice is to leverage LLMs for log-based failure localization, attributing errors to a specific agent and step. However, this paradigm has two key limitations: (i) log-only debugging lacks validation, producing untested hypotheses, and (ii) single-step or single-agent attribution is often ill-posed, as we find that multiple distinct interventions can independently repair the failed task. To address the first limitation, we introduce DoVer, an intervention-driven debugging framework, which augments hypothesis generation with active verification through targeted interventions (e.g., editing messages, altering plans). For the second limitation, rather than evaluating on attribution accuracy, we focus on measuring whether the system resolves the failure or makes quantifiable progress toward task success, reflecting a more outcome-oriented view of debugging. Within the Magnetic-One agent framework, on the datasets derived from GAIA and AssistantBench, DoVer flips 18-28% of failed trials into successes, achieves up to 16% milestone progress, and validates or refutes 30-60% of failure hypotheses. DoVer also performs effectively on a different dataset (GSMPlus) and agent framework (AG2), where it recovers 49% of failed trials. These results highlight intervention as a practical mechanism for improving reliability in agentic systems and open opportunities for more robust, scalable debugging methods for LLM-based multi-agent systems. Project website and code will be available at https://aka.ms/DoVer.",0,arxiv,AI,CC-BY/arXiv,DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems
"Large Language Models (LLMs) are emerging as powerful enablers for autonomous reasoning and natural-language coordination in unmanned aerial vehicle (UAV) swarms operating within Internet of Things (IoT) environments. However, existing LLM-driven UAV systems process sensitive operational data in plaintext, exposing them to privacy and security risks. This work introduces PrivLLMSwarm, a privacy-preserving framework that performs secure LLM inference for UAV swarm coordination through Secure Multi-Party Computation (MPC). The framework incorporates MPC-optimized transformer components with efficient approximations of nonlinear activations, enabling practical encrypted inference on resource-constrained aerial platforms. A fine-tuned GPT-based command generator, enhanced through reinforcement learning in simulation, provides reliable instructions while maintaining confidentiality. Experimental evaluation in urban-scale simulations demonstrates that PrivLLMSwarm achieves high semantic accuracy, low encrypted inference latency, and robust formation control under privacy constraints. Comparative analysis shows PrivLLMSwarm offers a superior privacy-utility balance compared to differential privacy, federated learning, and plaintext baselines. To support reproducibility, the full implementation including source code, MPC components, and a synthetic dataset is publicly available. PrivLLMSwarm establishes a practical foundation for secure, LLM-enabled UAV swarms in privacy-sensitive IoT applications including smart-city monitoring and emergency response.",0,arxiv,AI,CC-BY/arXiv,PrivLLMSwarm: Privacy-Preserving LLM-Driven UAV Swarms for Secure IoT Surveillance
"Vision Language Models (VLMs) are increasingly adopted for AI-generated images (AIGI) detection, yet converting VLMs into detectors requires substantial resource, while the resulting models still exhibit severe hallucinations. To probe the core issue, we conduct an empirical analysis and observe two characteristic behaviors: (i) fine-tuning VLMs on high-level semantic supervision strengthens semantic discrimination and well generalize to unseen data; (ii) fine-tuning VLMs on low-level pixel-artifact supervision yields poor transfer. We attribute VLMs' underperformance to task-model misalignment: semantics-oriented VLMs inherently lack sensitivity to fine-grained pixel artifacts, and semantically non-discriminative pixel artifacts thus exceeds their inductive biases. In contrast, we observe that conventional pixel-artifact detectors capture low-level pixel artifacts yet exhibit limited semantic awareness relative to VLMs, highlighting that distinct models are better matched to distinct tasks. In this paper, we formalize AIGI detection as two complementary tasks--semantic consistency checking and pixel-artifact detection--and show that neglecting either induces systematic blind spots. Guided by this view, we introduce the Task-Model Alignment principle and instantiate it as a two-branch detector, AlignGemini, comprising a VLM fine-tuned exclusively with pure semantic supervision and a pixel-artifact expert trained exclusively with pure pixel-artifact supervision. By enforcing orthogonal supervision on two simplified datasets, each branch trains to its strengths, producing complementary discrimination over semantic and pixel cues. On five in-the-wild benchmarks, AlignGemini delivers a +9.5 gain in average accuracy, supporting task-model alignment as an effective path to generalizable AIGI detection.",0,arxiv,AI,CC-BY/arXiv,Task-Model Alignment: A Simple Path to Generalizable AI-Generated Image Detection
"The paper presents the formulation, implementation, and evaluation of the ArcGD optimiser. The evaluation is conducted initially on a non-convex benchmark function and subsequently on a real-world ML dataset. The initial comparative study using the Adam optimiser is conducted on a stochastic variant of the highly non-convex and notoriously challenging Rosenbrock function, renowned for its narrow, curved valley, across dimensions ranging from 2D to 1000D and an extreme case of 50,000D. Two configurations were evaluated to eliminate learning-rate bias: (i) both using ArcGD's effective learning rate and (ii) both using Adam's default learning rate. ArcGD consistently outperformed Adam under the first setting and, although slower under the second, achieved super ior final solutions in most cases. In the second evaluation, ArcGD is evaluated against state-of-the-art optimizers (Adam, AdamW, Lion, SGD) on the CIFAR-10 image classification dataset across 8 diverse MLP architectures ranging from 1 to 5 hidden layers. ArcGD achieved the highest average test accuracy (50.7%) at 20,000 iterations, outperforming AdamW (46.6%), Adam (46.8%), SGD (49.6%), and Lion (43.4%), winning or tying on 6 of 8 architectures. Notably, while Adam and AdamW showed strong early convergence at 5,000 iterations, but regressed with extended training, whereas ArcGD continued improving, demonstrating generalization and resistance to overfitting without requiring early stopping tuning. Strong performance on geometric stress tests and standard deep-learning benchmarks indicates broad applicability, highlighting the need for further exploration. Moreover, it is also shown that a variant of ArcGD can be interpreted as a special case of the Lion optimiser, highlighting connections between the inherent mechanisms of such optimisation methods.",0,arxiv,AI,CC-BY/arXiv,"Arc Gradient Descent: A Mathematically Derived Reformulation of Gradient Descent with Phase-Aware, User-Controlled Step Dynamics"
"Transfer Learning (TL) has accelerated the rapid development and availability of large language models (LLMs) for mainstream natural language processing (NLP) use cases. However, training and deploying such gigantic LLMs in resource-constrained, real-world healthcare situations remains challenging. This study addresses the limited support available to visually impaired users and speakers of low-resource languages such as Hindi who require medical assistance in rural environments. We propose PDFTEMRA (Performant Distilled Frequency Transformer Ensemble Model with Random Activations), a compact transformer-based architecture that integrates model distillation, frequency-domain modulation, ensemble learning, and randomized activation patterns to reduce computational cost while preserving language understanding performance. The model is trained and evaluated on medical question-answering and consultation datasets tailored to Hindi and accessibility scenarios, and its performance is compared against standard NLP state-of-the-art model baselines. Results demonstrate that PDFTEMRA achieves comparable performance with substantially lower computational requirements, indicating its suitability for accessible, inclusive, low-resource medical NLP applications.",0,arxiv,AI,CC-BY/arXiv,A Patient-Doctor-NLP-System to contest inequality for less privileged
"Existing benchmarks evaluating biases in large language models (LLMs) primarily rely on explicit cues, declaring protected attributes like religion, race, gender by name. However, real-world interactions often contain implicit biases, inferred subtly through names, cultural cues, or traits. This critical oversight creates a significant blind spot in fairness evaluation. We introduce ImplicitBBQ, a benchmark extending the Bias Benchmark for QA (BBQ) with implicitly cued protected attributes across 6 categories. Our evaluation of GPT-4o on ImplicitBBQ illustrates troubling performance disparity from explicit BBQ prompts, with accuracy declining up to 7% in the ""sexual orientation"" subcategory and consistent decline located across most other categories. This indicates that current LLMs contain implicit biases undetected by explicit benchmarks. ImplicitBBQ offers a crucial tool for nuanced fairness evaluation in NLP.",0,arxiv,AI,CC-BY/arXiv,"""The Dentist is an involved parent, the bartender is not"": Revealing Implicit Biases in QA with Implicit BBQ"
"Recent advances in fine-tuning multimodal large language models (MLLMs) using reinforcement learning have achieved remarkable progress, particularly with the introduction of various entropy control techniques. However, the role and characteristics of entropy in perception-oriented tasks like visual grounding, as well as effective strategies for controlling it, remain largely unexplored. To address this issue, we focus on the visual grounding task and analyze the role and characteristics of entropy in comparison to reasoning tasks. Building on these findings, we introduce ECVGPO (Entropy Control Visual Grounding Policy Optimization), an interpretable algorithm designed for effective entropy regulation. Through entropy control, the trade-off between exploration and exploitation is better balanced. Experiments show that ECVGPO achieves broad improvements across various benchmarks and models.",0,arxiv,AI,CC-BY/arXiv,The Role of Entropy in Visual Grounding: Analysis and Optimization
"Large Language Model (LLM) agents are emerging to transform daily life. However, existing LLM agents primarily follow a reactive paradigm, relying on explicit user instructions to initiate services, which increases both physical and cognitive workload. In this paper, we propose ProAgent, the first end-to-end proactive agent system that harnesses massive sensory contexts and LLM reasoning to deliver proactive assistance. ProAgent first employs a proactive-oriented context extraction approach with on-demand tiered perception to continuously sense the environment and derive hierarchical contexts that incorporate both sensory and persona cues. ProAgent then adopts a context-aware proactive reasoner to map these contexts to user needs and tool calls, providing proactive assistance. We implement ProAgent on Augmented Reality (AR) glasses with an edge server and extensively evaluate it on a real-world testbed, a public dataset, and through a user study. Results show that ProAgent achieves up to 33.4% higher proactive prediction accuracy, 16.8% higher tool-calling F1 score, and notable improvements in user satisfaction over state-of-the-art baselines, marking a significant step toward proactive assistants. A video demonstration of ProAgent is available at https://youtu.be/pRXZuzvrcVs.",0,arxiv,AI,CC-BY/arXiv,ProAgent: Harnessing On-Demand Sensory Contexts for Proactive LLM Agent Systems
"Autonomous Large Language Model (LLM) agents exhibit significant vulnerability to Indirect Prompt Injection (IPI) attacks. These attacks hijack agent behavior by polluting external information sources, exploiting fundamental trade-offs between security and functionality in existing defense mechanisms. This leads to malicious and unauthorized tool invocations, diverting agents from their original objectives. The success of complex IPIs reveals a deeper systemic fragility: while current defenses demonstrate some effectiveness, most defense architectures are inherently fragmented. Consequently, they fail to provide full integrity assurance across the entire task execution pipeline, forcing unacceptable multi-dimensional compromises among security, functionality, and efficiency. Our method is predicated on a core insight: no matter how subtle an IPI attack, its pursuit of a malicious objective will ultimately manifest as a detectable deviation in the action trajectory, distinct from the expected legitimate plan. Based on this, we propose the Cognitive Control Architecture (CCA), a holistic framework achieving full-lifecycle cognitive supervision. CCA constructs an efficient, dual-layered defense system through two synergistic pillars: (i) proactive and preemptive control-flow and data-flow integrity enforcement via a pre-generated ""Intent Graph""; and (ii) an innovative ""Tiered Adjudicator"" that, upon deviation detection, initiates deep reasoning based on multi-dimensional scoring, specifically designed to counter complex conditional attacks. Experiments on the AgentDojo benchmark substantiate that CCA not only effectively withstands sophisticated attacks that challenge other advanced defense methods but also achieves uncompromised security with notable efficiency and robustness, thereby reconciling the aforementioned multi-dimensional trade-off.",0,arxiv,AI,CC-BY/arXiv,Cognitive Control Architecture (CCA): A Lifecycle Supervision Framework for Robustly Aligned AI Agents
"Short-term water demand forecasting (StWDF) is the foundation stone in the derivation of an optimal plan for controlling water supply systems. Deep learning (DL) approaches provide the most accurate solutions for this purpose. However, they suffer from complexity problem due to the massive number of parameters, in addition to the high forecasting error at the extreme points. In this work, an effective method to alleviate the error at these points is proposed. It is based on extending the data by inserting virtual data within the actual data to relieve the nonlinearity around them. To our knowledge, this is the first work that considers the problem related to the extreme points. Moreover, the water demand forecasting model proposed in this work is a novel DL model with relatively low complexity. The basic model uses the gated recurrent unit (GRU) to handle the sequential relationship in the historical demand data, while an unsupervised classification method, K-means, is introduced for the creation of new features to enhance the prediction accuracy with less number of parameters. Real data obtained from two different water plants in China are used to train and verify the model proposed. The prediction results and the comparison with the state-of-the-art illustrate that the method proposed reduces the complexity of the model six times of what achieved in the literature while conserving the same accuracy. Furthermore, it is found that extending the data set significantly reduces the error by about 30%. However, it increases the training time.",0,arxiv,AI,CC-BY/arXiv,A Novel Deep Neural Network Architecture for Real-Time Water Demand Forecasting
"As large language models become components of larger agentic systems, evaluation reliability becomes critical: unreliable sub-agents introduce brittleness into downstream system behavior. Yet current evaluation practice, reporting a single accuracy number from a single run, obscures the variance underlying these results, making it impossible to distinguish genuine capability improvements from lucky sampling. We propose adopting Intraclass Correlation Coefficient (ICC), a metric from measurement science, to characterize this variance. ICC decomposes observed variance into between-query variance (task difficulty) and within-query variance (agent inconsistency), highlighting whether reported results reflect true capability or measurement noise. We evaluated on GAIA (Levels 1-3, measuring agentic capabilities across varying reasoning complexity) and FRAMES (measuring retrieval and factuality across multiple documents). We found that ICC varies dramatically with task structure, with reasoning and retrieval tasks (FRAMES) exhibit ICC=0.4955-0.7118 across models, and agentic tasks (GAIA) exhibiting ICC=0.304-0.774 across models. For sub-agent replacement decisions in agentic systems, accuracy improvements are only trustworthy if ICC also improves. We demonstrate that ICC converges by n=8-16 trials for structured tasks and n>=32 for complex reasoning, enabling practitioners to set evidence-based resampling budgets. We recommend reporting accuracy alongside ICC and within-query variance as standard practice, and propose updated Evaluation Cards capturing these metrics. By making evaluation stability visible, we aim to transform agentic benchmarking from opaque leaderboard competition to trustworthy experimental science. Our code is open-sourced at https://github.com/youdotcom-oss/stochastic-agent-evals.",0,arxiv,AI,CC-BY/arXiv,Stochasticity in Agentic Evaluations: Quantifying Inconsistency with Intraclass Correlation
"Estimating the Remaining Useful Life (RUL) of mechanical systems is pivotal in Prognostics and Health Management (PHM). Rolling-element bearings are among the most frequent causes of machinery failure, highlighting the need for robust RUL estimation methods. Existing approaches often suffer from poor generalization, lack of robustness, high data demands, and limited interpretability. This paper proposes a novel multimodal-RUL framework that jointly leverages image representations (ImR) and time-frequency representations (TFR) of multichannel, nonstationary vibration signals. The architecture comprises three branches: (1) an ImR branch and (2) a TFR branch, both employing multiple dilated convolutional blocks with residual connections to extract spatial degradation features; and (3) a fusion branch that concatenates these features and feeds them into an LSTM to model temporal degradation patterns. A multi-head attention mechanism subsequently emphasizes salient features, followed by linear layers for final RUL regression. To enable effective multimodal learning, vibration signals are converted into ImR via the Bresenham line algorithm and into TFR using Continuous Wavelet Transform. We also introduce multimodal Layer-wise Relevance Propagation (multimodal-LRP), a tailored explainability technique that significantly enhances model transparency. The approach is validated on the XJTU-SY and PRONOSTIA benchmark datasets. Results show that our method matches or surpasses state-of-the-art baselines under both seen and unseen operating conditions, while requiring ~28 % less training data on XJTU-SY and ~48 % less on PRONOSTIA. The model exhibits strong noise resilience, and multimodal-LRP visualizations confirm the interpretability and trustworthiness of predictions, making the framework highly suitable for real-world industrial deployment.",0,arxiv,AI,CC-BY/arXiv,A Novel Multimodal RUL Framework for Remaining Useful Life Estimation with Layer-wise Explanations
"The rapid integration of generative AI into academic writing has prompted widespread policy responses from journals and publishers. However, the effectiveness of these policies remains unclear. Here, we analyze 5,114 journals and over 5.2 million papers to evaluate the real-world impact of AI usage guidelines. We show that despite 70% of journals adopting AI policies (primarily requiring disclosure), researchers' use of AI writing tools has increased dramatically across disciplines, with no significant difference between journals with or without policies. Non-English-speaking countries, physical sciences, and high-OA journals exhibit the highest growth rates. Crucially, full-text analysis on 164k scientific publications reveals a striking transparency gap: Of the 75k papers published since 2023, only 76 (0.1%) explicitly disclosed AI use. Our findings suggest that current policies have largely failed to promote transparency or restrain AI adoption. We urge a re-evaluation of ethical frameworks to foster responsible AI integration in science.",0,arxiv,AI,CC-BY/arXiv,Academic journals' AI policies fail to curb the surge in AI-assisted academic writing
"Modern machine learning training is increasingly bottlenecked by data I/O rather than compute. GPUs often sit idle at below 50% utilization waiting for data. This paper presents a machine learning approach to predict I/O performance and recommend optimal storage configurations for ML training pipelines. We collected 141 observations through systematic benchmarking across different storage backends (NVMe SSD, network-attached storage, in-memory filesystems), data formats, and access patterns, covering both low-level I/O operations and full training pipelines. After evaluating seven regression models and three classification approaches, XGBoost achieved the best performance with R-squared of 0.991, predicting I/O throughput within 11.8% error on average. Feature importance analysis revealed that throughput metrics and batch size are the primary performance drivers. This data-driven approach can reduce configuration time from days of trial-and-error to minutes of predictive recommendation. The methodology is reproducible and extensible to other resource management problems in ML systems. Code and data are available at https://github.com/knkarthik01/gpu_storage_ml_project",0,arxiv,AI,CC-BY/arXiv,Predictive Modeling of I/O Performance for Machine Learning Training Pipelines: A Data-Driven Approach to Storage Optimization
"We present a mechanistic interpretability study of GPT-2 that causally examines how sentiment information is processed across its transformer layers. Using systematic activation patching across all 12 layers, we test the hypothesized two-stage sentiment architecture comprising early lexical detection and mid-layer contextual integration. Our experiments confirm that early layers (0-3) act as lexical sentiment detectors, encoding stable, position specific polarity signals that are largely independent of context. However, all three contextual integration hypotheses: Middle Layer Concentration, Phenomenon Specificity, and Distributed Processing are falsified. Instead of mid-layer specialization, we find that contextual phenomena such as negation, sarcasm, domain shifts etc. are integrated primarily in late layers (8-11) through a unified, non-modular mechanism. These experimental findings provide causal evidence that GPT-2's sentiment computation differs from the predicted hierarchical pattern, highlighting the need for further empirical characterization of contextual integration in large language models.",0,arxiv,AI,CC-BY/arXiv,Mechanistic Interpretability of GPT-2: Lexical and Contextual Layers in Sentiment Analysis
"Instruction tuning is one of the key steps required for adapting large language models (LLMs) to a broad spectrum of downstream applications. However, this procedure is difficult because real-world datasets are rarely homogeneous; they consist of a mixture of diverse information, causing gradient interference, where conflicting gradients pull the model in opposing directions, degrading performance. A common strategy to mitigate this issue is to group data based on semantic or embedding similarity. However, this fails to capture how data influences model parameters during learning. While recent works have attempted to cluster gradients directly, they randomly project gradients into lower dimensions to manage memory, which leads to accuracy loss. Moreover, these methods rely on expert ensembles which necessitates multiple inference passes and expensive on-the-fly gradient computations during inference. To address these limitations, we propose GradientSpace, a framework that clusters samples directly in full-dimensional gradient space. We introduce an online SVD-based algorithm that operates on LoRA gradients to identify latent skills without the infeasible cost of storing all sample gradients. Each cluster is used to train a specialized LoRA expert along with a lightweight router trained to select the best expert during inference. We show that routing to a single, appropriate expert outperforms expert ensembles used in prior work, while significantly reducing inference latency. Our experiments across mathematical reasoning, code generation, finance, and creative writing tasks demonstrate that GradientSpace leads to coherent expert specialization and consistent accuracy gains over state-of-the-art clustering methods and finetuning techniques.",0,arxiv,AI,CC-BY/arXiv,GradientSpace: Unsupervised Data Clustering for Improved Instruction Tuning
"This paper studies the robustness of feature attribution methods for deep neural networks. It challenges the current notion of attributional robustness that largely ignores the difference in the model's outputs and introduces a new way of evaluating the robustness of attribution methods. Specifically, we propose a new definition of similar inputs, a new robustness metric, and a novel method based on generative adversarial networks to generate these inputs. In addition, we present a comprehensive evaluation with existing metrics and state-of-the-art attribution methods. Our findings highlight the need for a more objective metric that reveals the weaknesses of an attribution method rather than that of the neural network, thus providing a more accurate evaluation of the robustness of attribution methods.",0,arxiv,AI,CC-BY/arXiv,Rethinking Robustness: A New Approach to Evaluating Feature Attribution Methods
"Analysts in Security Operations Centers routinely query massive telemetry streams using Kusto Query Language (KQL). Writing correct KQL requires specialized expertise, and this dependency creates a bottleneck as security teams scale. This paper investigates whether Small Language Models (SLMs) can enable accurate, cost-effective natural-language-to-KQL translation for enterprise security. We propose a three-knob framework targeting prompting, fine-tuning, and architecture design. First, we adapt existing NL2KQL framework for SLMs with lightweight retrieval and introduce error-aware prompting that addresses common parser failures without increasing token count. Second, we apply LoRA fine-tuning with rationale distillation, augmenting each NLQ-KQL pair with a brief chain-of-thought explanation to transfer reasoning from a teacher model while keeping the SLM compact. Third, we propose a two-stage architecture that uses an SLM for candidate generation and a low-cost LLM judge for schema-aware refinement and selection. We evaluate nine models (five SLMs and four LLMs) across syntax correctness, semantic accuracy, table selection, and filter precision, alongside latency and token cost. On Microsoft's NL2KQL Defender Evaluation dataset, our two-stage approach achieves 0.987 syntax and 0.906 semantic accuracy. We further demonstrate generalizability on Microsoft Sentinel data, reaching 0.964 syntax and 0.831 semantic accuracy. These results come at up to 10x lower token cost than GPT-5, establishing SLMs as a practical, scalable foundation for natural-language querying in security operations.",0,arxiv,AI,CC-BY/arXiv,Towards Small Language Models for Security Query Generation in SOC Workflows
"In scene text detection, Transformer-based methods have addressed the global feature extraction limitations inherent in traditional convolution neural network-based methods. However, most directly rely on native Transformer attention layers as encoders without evaluating their cross-domain limitations and inherent shortcomings: forgetting important information or focusing on irrelevant representations when modeling long-range dependencies for text detection. The recently proposed state space model Mamba has demonstrated better long-range dependencies modeling through a linear complexity selection mechanism. Therefore, we propose a novel scene text detector based on Mamba that integrates the selection mechanism with attention layers, enhancing the encoder's ability to extract relevant information from long sequences. We adopt the Top\_k algorithm to explicitly select key information and reduce the interference of irrelevant information in Mamba modeling. Additionally, we design a dual-scale feed-forward network and an embedding pyramid enhancement module to facilitate high-dimensional hidden state interactions and multi-scale feature fusion. Our method achieves state-of-the-art or competitive performance on various benchmarks, with F-measures of 89.7\%, 89.2\%, and 78.5\% on CTW1500, TotalText, and ICDAR19ArT, respectively. Codes will be available.",0,arxiv,AI,CC-BY/arXiv,TextMamba: Scene Text Detector with Mamba
"Large language models (LLMs) face critical safety challenges, as they can be manipulated to generate harmful content through adversarial prompts and jailbreak attacks. Many defenses are typically either black-box guardrails that filter outputs, or internals-based methods that steer hidden activations by operationalizing safety as a single latent feature or dimension. While effective for simple concepts, this assumption is limiting, as recent evidence shows that abstract concepts such as refusal and temporality are distributed across multiple features rather than isolated in one. To address this limitation, we introduce Graph-Regularized Sparse Autoencoders (GSAEs), which extends SAEs with a Laplacian smoothness penalty on the neuron co-activation graph. Unlike standard SAEs that assign each concept to a single latent feature, GSAEs recover smooth, distributed safety representations as coherent patterns spanning multiple features. We empirically demonstrate that GSAE enables effective runtime safety steering, assembling features into a weighted set of safety-relevant directions and controlling them with a two-stage gating mechanism that activates interventions only when harmful prompts or continuations are detected during generation. This approach enforces refusals adaptively while preserving utility on benign queries. Across safety and QA benchmarks, GSAE steering achieves an average 82% selective refusal rate, substantially outperforming standard SAE steering (42%), while maintaining strong task accuracy (70% on TriviaQA, 65% on TruthfulQA, 74% on GSM8K). Robustness experiments further show generalization across LLaMA-3, Mistral, Qwen, and Phi families and resilience against jailbreak attacks (GCG, AutoDAN), consistently maintaining >= 90% refusal of harmful content.",0,arxiv,AI,CC-BY/arXiv,GSAE: Graph-Regularized Sparse Autoencoders for Robust LLM Safety Steering
"DeepSearch paradigms have become a core enabler for deep reasoning models, allowing them to invoke external search tools to access up-to-date, domain-specific knowledge beyond parametric boundaries, thereby enhancing the depth and factual reliability of reasoning. Building upon this foundation, recent advances in reinforcement learning (RL) have further empowered models to autonomously and strategically control search tool usage, optimizing when and how to query external knowledge sources. Yet, these RL-driven DeepSearch systems often reveal a see-saw trade-off between accuracy and efficiency-frequent tool invocations can improve factual correctness but lead to unnecessary computational overhead and diminished efficiency. To address this challenge, we propose LightSearcher, an efficient RL framework that incorporates textual experiential memory by learning contrastive reasoning trajectories to generate interpretable summaries of successful reasoning patterns. In addition, it employs an adaptive reward shaping mechanism that penalizes redundant tool calls only in correct-answer scenarios. This design effectively balances the inherent accuracy-efficiency trade-off in DeepSearch paradigms. Experiments on four multi-hop QA benchmarks show that LightSearcher maintains accuracy comparable to SOTA baseline ReSearch, while reducing search tool invocations by 39.6%, inference time by 48.6%, and token consumption by 21.2%, demonstrating its superior efficiency.",0,arxiv,AI,CC-BY/arXiv,LightSearcher: Efficient DeepSearch via Experiential Memory
"Accurate prediction of the need for invasive mechanical ventilation (IMV) in intensive care units (ICUs) patients is crucial for timely interventions and resource allocation. However, variability in patient populations, clinical practices, and electronic health record (EHR) systems across institutions introduces domain shifts that degrade the generalization performance of predictive models during deployment. Test-Time Training (TTT) has emerged as a promising approach to mitigate such shifts by adapting models dynamically during inference without requiring labeled target-domain data. In this work, we introduce Adaptive Test-Time Training (AdaTTT), an enhanced TTT framework tailored for EHR-based IMV prediction in ICU settings. We begin by deriving information-theoretic bounds on the test-time prediction error and demonstrate that it is constrained by the uncertainty between the main and auxiliary tasks. To enhance their alignment, we introduce a self-supervised learning framework with pretext tasks: reconstruction and masked feature modeling optimized through a dynamic masking strategy that emphasizes features critical to the main task. Additionally, to improve robustness against domain shifts, we incorporate prototype learning and employ Partial Optimal Transport (POT) for flexible, partial feature alignment while maintaining clinically meaningful patient representations. Experiments across multi-center ICU cohorts demonstrate competitive classification performance on different test-time adaptation benchmarks.",0,arxiv,AI,CC-BY/arXiv,Adaptive Test-Time Training for Predicting Need for Invasive Mechanical Ventilation in Multi-Center Cohorts
"Since the emergence of joint-stock companies, financial fraud by listed firms has repeatedly undermined capital markets. Fraud is difficult to detect because of covert tactics and the high labor and time costs of audits. Traditional statistical models are interpretable but struggle with nonlinear feature interactions, while machine learning models are powerful but often opaque. In addition, most existing methods judge fraud only for the current year based on current year data, limiting timeliness.   This paper proposes a financial fraud detection framework for Chinese A-share listed companies based on convolutional neural networks (CNNs). We design a feature engineering scheme that transforms firm-year panel data into image like representations, enabling the CNN to capture cross-sectional and temporal patterns and to predict fraud in advance. Experiments show that the CNN outperforms logistic regression and LightGBM in accuracy, robustness, and early-warning performance, and that proper tuning of the classification threshold is crucial in high-risk settings.   To address interpretability, we analyze the model along the dimensions of entity, feature, and time using local explanation techniques. We find that solvency, ratio structure, governance structure, and internal control are general predictors of fraud, while environmental indicators matter mainly in high-pollution industries. Non-fraud firms share stable feature patterns, whereas fraud firms exhibit heterogeneous patterns concentrated in short time windows. A case study of Guanong Shares in 2022 shows that cash flow analysis, social responsibility, governance structure, and per-share indicators are the main drivers of the model's fraud prediction, consistent with the company's documented misconduct.",0,arxiv,AI,CC-BY/arXiv,Financial Fraud Identification and Interpretability Study for Listed Companies Based on Convolutional Neural Network
"Strong gravitational lensing can reveal the influence of dark-matter substructure in galaxies, but analyzing these effects from noisy, low-resolution images poses a significant challenge. In this work, we propose a masked autoencoder (MAE) pretraining strategy on simulated strong-lensing images from the DeepLense ML4SCI benchmark to learn generalizable representations for two downstream tasks: (i) classifying the underlying dark matter model (cold dark matter, axion-like, or no substructure) and (ii) enhancing low-resolution lensed images via super-resolution. We pretrain a Vision Transformer encoder using a masked image modeling objective, then fine-tune the encoder separately for each task. Our results show that MAE pretraining, when combined with appropriate mask ratio tuning, yields a shared encoder that matches or exceeds a ViT trained from scratch. Specifically, at a 90% mask ratio, the fine-tuned classifier achieves macro AUC of 0.968 and accuracy of 88.65%, compared to the scratch baseline (AUC 0.957, accuracy 82.46%). For super-resolution (16x16 to 64x64), the MAE-pretrained model reconstructs images with PSNR ~33 dB and SSIM 0.961, modestly improving over scratch training. We ablate the MAE mask ratio, revealing a consistent trade-off: higher mask ratios improve classification but slightly degrade reconstruction fidelity. Our findings demonstrate that MAE pretraining on physics-rich simulations provides a flexible, reusable encoder for multiple strong-lensing analysis tasks.",0,arxiv,AI,CC-BY/arXiv,Masked Autoencoder Pretraining on Strong-Lensing Images for Joint Dark-Matter Model Classification and Super-Resolution
"Knowledge Tracing (KT) models face a critical ``Performance-Complexity Trap'': capturing complex cognitive dynamics like learning sessions and memory decay typically requires deep hierarchical architectures, which incur prohibitive computational costs for real-time deployment. To resolve this, we propose FlatFormer, a streamlined architecture based on the novel design paradigm of ``Information Injection over Structural Stacking.'' Unlike parameter-heavy hierarchical models, FlatFormer leverages a standard flat Transformer augmented with two lightweight injection mechanisms: (i) a hybrid input encoding strategy combining learnable session identifiers with fixed sinusoidal step embeddings; and (ii) a pre-computed power-law bias integrated directly into attention logits to explicitly model the forgetting curve. Extensive experiments on four large-scale datasets (e.g., EdNet, Junyi) show that FlatFormer achieves state-of-the-art performance. For example, on the EdNet dataset, compared to the strongest hierarchical baseline (HiTSKT), its absolute AUC increased by 8.3%, while using less than 15% of parameters, and inference speed was about three times faster. These results validate that high cognitive fidelity does not necessitate architectural complexity.",0,arxiv,AI,CC-BY/arXiv,FlatFormer: A Flat Transformer Knowledge Tracing Model Based on Cognitive Bias Injection
"As artificial intelligence (AI) becomes embedded in personal and professional relationships, a new kind of power imbalance emerges from asymmetric memory capabilities. Human relationships have historically relied on mutual forgetting, the natural tendency for both parties to forget details over time, as a foundation for psychological safety, forgiveness, and identity change. By contrast, AI systems can record, store, and recombine interaction histories at scale, often indefinitely. We introduce Memory Power Asymmetry (MPA): a structural power imbalance that arises when one relationship partner (typically an AI-enabled firm) possesses a substantially superior capacity to record, retain, retrieve, and integrate the shared history of the relationship, and can selectively deploy that history in ways the other partner (the human) cannot. Drawing on research in human memory, power-dependence theory, AI architecture, and consumer vulnerability, we develop a conceptual framework with four dimensions of MPA (persistence, accuracy, accessibility, integration) and four mechanisms by which memory asymmetry is translated into power (strategic memory deployment, narrative control, dependence asymmetry, vulnerability accumulation). We theorize downstream consequences at individual, relational/firm, and societal levels, formulate boundary-conditioned propositions, and articulate six design principles for restoring a healthier balance of memory in human-AI relationships (e.g., forgetting by design, contextual containment, symmetric access to records). Our analysis positions MPA as a distinct construct relative to information asymmetry, privacy, surveillance, and customer relationship management, and argues that protecting mutual forgetting, or at least mutual control over memory, should become a central design and policy goal in the AI age.",0,arxiv,AI,CC-BY/arXiv,Memory Power Asymmetry in Human-AI Relationships: Preserving Mutual Forgetting in the Digital Age
"Automated negotiation has emerged as a critical area of research in multiagent systems, with applications spanning e-commerce, resource allocation, and autonomous decision-making. This paper presents ChargingBoul, a negotiating agent that competed in the 2022 Automated Negotiating Agents Competition (ANAC) and placed second in individual utility by an exceptionally narrow margin. ChargingBoul employs a lightweight yet effective strategy that balances concession and opponent modeling to achieve high negotiation outcomes. The agent classifies opponents based on bid patterns, dynamically adjusts its bidding strategy, and applies a concession policy in later negotiation stages to maximize utility while fostering agreements. We evaluate ChargingBoul's performance using competition results and subsequent studies that have utilized the agent in negotiation research. Our analysis highlights ChargingBoul's effectiveness across diverse opponent strategies and its contributions to advancing automated negotiation techniques. We also discuss potential enhancements, including more sophisticated opponent modeling and adaptive bidding heuristics, to improve its performance further.",0,arxiv,AI,CC-BY/arXiv,ChargingBoul: A Competitive Negotiating Agent with Novel Opponent Modeling
"Explainable AI (XAI) presents useful tools to facilitate transparency and trustworthiness in machine learning systems. However, current evaluations of system explainability often rely heavily on subjective user surveys, which may not adequately capture the effectiveness of explanations. This paper critiques the overreliance on user satisfaction metrics and explores whether these can differentiate between meaningful (actionable) and vacuous (placebic) explanations. In experiments involving optimal Social Security filing age selection tasks, participants used one of three protocols: no explanations, placebic explanations, and actionable explanations. Participants who received actionable explanations significantly outperformed the other groups in objective measures of their mental model, but users rated placebic and actionable explanations as equally satisfying. This suggests that subjective surveys alone fail to capture whether explanations truly support users in building useful domain understanding. We propose that future evaluations of agent explanation capabilities should integrate objective task performance metrics alongside subjective assessments to more accurately measure explanation quality. The code for this study can be found at https://github.com/Shymkis/social-security-explainer.",0,arxiv,AI,CC-BY/arXiv,Beyond Satisfaction: From Placebic to Actionable Explanations For Enhanced Understandability
"Recommender Systems (RSs) have become the cornerstone of various applications such as e-commerce and social media platforms. The evolution of RSs is paramount in the digital era, in which personalised user experience is tailored to the user's preferences. Large Language Models (LLMs) have sparked a new paradigm - generative retrieval and recommendation. Despite their potential, generative RS methods face issues such as hallucination, which degrades the recommendation performance, and high computational cost in practical scenarios. To address these issues, we introduce HGLMRec, a novel Multi-LLM agent-based RS that incorporates a hypergraph encoder designed to capture complex, multi-behaviour relationships between users and items. The HGLMRec model retrieves only the relevant tokens during inference, reducing computational overhead while enriching the retrieval context. Experimental results show performance improvement by HGLMRec against state-of-the-art baselines at lower computational cost.",0,arxiv,AI,CC-BY/arXiv,Towards Efficient Hypergraph and Multi-LLM Agent Recommender Systems
"Recurrent neural architectures such as LSTM and GRU remain widely used in sequence modeling, but they continue to face two core limitations: redundant gate-specific parameters and reduced ability to retain information across long temporal distances. This paper introduces the Quantum-Leap LSTM (QL-LSTM), a recurrent architecture designed to address both challenges through two independent components. The Parameter-Shared Unified Gating mechanism replaces all gate-specific transformations with a single shared weight matrix, reducing parameters by approximately 48 percent while preserving full gating behavior. The Hierarchical Gated Recurrence with Additive Skip Connections component adds a multiplication-free pathway that improves long-range information flow and reduces forget-gate degradation. We evaluate QL-LSTM on sentiment classification using the IMDB dataset with extended document lengths, comparing it to LSTM, GRU, and BiLSTM reference models. QL-LSTM achieves competitive accuracy while using substantially fewer parameters. Although the PSUG and HGR-ASC components are more efficient per time step, the current prototype remains limited by the inherent sequential nature of recurrent models and therefore does not yet yield wall-clock speed improvements without further kernel-level optimization.",0,arxiv,AI,CC-BY/arXiv,QL-LSTM: A Parameter-Efficient LSTM for Stable Long-Sequence Modeling
"As multi-agent systems are increasingly utilized for reasoning and decision-making applications, there is a greater need for LLM-based agents to have something resembling propositional beliefs. One simple method for doing so is to include statements describing beliefs maintained in the prompt space (in what we'll call their belief boxes). But when agents have such statements in belief boxes, how does it actually affect their behaviors and dispositions towards those beliefs? And does it significantly affect agents' ability to be persuasive in multi-agent scenarios? Likewise, if the agents are given instructions to be open-minded, how does that affect their behaviors? We explore these and related questions in a series of experiments. Our findings confirm that instructing agents to be open-minded affects how amenable they are to belief change. We show that incorporating belief statements and their strengths influences an agent's resistance to (and persuasiveness against) opposing viewpoints. Furthermore, it affects the likelihood of belief change, particularly when the agent is outnumbered in a debate by opposing viewpoints, i.e., peer pressure scenarios. The results demonstrate the feasibility and validity of the belief box technique in reasoning and decision-making tasks.",0,arxiv,AI,CC-BY/arXiv,The Effect of Belief Boxes and Open-mindedness on Persuasion
"This work develops the global equations of neural networks through stacked piecewise manifolds, fixed-point theory, and boundary-conditioned iteration. Once fixed coordinates and operators are removed, a neural network appears as a learnable numerical computation shaped by manifold complexity, high-order nonlinearity, and boundary conditions. Real-world data impose strong data complexity, near-infinite scope, scale, and minibatch fragmentation, while training dynamics produce learning complexity through shifting node covers, curvature accumulation, and the rise and decay of plasticity. These forces constrain learnability and explain why capability emerges only when fixed-point regions stabilize. Neural networks do not begin with fixed points; they construct them through residual-driven iteration. This perspective clarifies the limits of monolithic models under geometric and data-induced plasticity and motivates architectures and federated systems that distribute manifold complexity across many elastic models, forming a coherent world-modeling framework grounded in geometry, algebra, fixed points, and real-data complexity.",0,arxiv,AI,CC-BY/arXiv,Deep Manifold Part 2: Neural Network Mathematics
"Recent advances in 3D-aware generative models have enabled high-fidelity image synthesis of human identities. However, this progress raises urgent questions around user consent and the ability to remove specific individuals from a model's output space. We address this by introducing SUGAR, a framework for scalable generative unlearning that enables the removal of many identities (simultaneously or sequentially) without retraining the entire model. Rather than projecting unwanted identities to unrealistic outputs or relying on static template faces, SUGAR learns a personalized surrogate latent for each identity, diverting reconstructions to visually coherent alternatives while preserving the model's quality and diversity. We further introduce a continual utility preservation objective that guards against degradation as more identities are forgotten. SUGAR achieves state-of-the-art performance in removing up to 200 identities, while delivering up to a 700% improvement in retention utility compared to existing baselines. Our code is publicly available at https://github.com/judydnguyen/SUGAR-Generative-Unlearn.",0,arxiv,AI,CC-BY/arXiv,SUGAR: A Sweeter Spot for Generative Unlearning of Many Identities
"The Model Context Protocol (MCP) enables Large Language Models to integrate external tools through structured descriptors, increasing autonomy in decision-making, task execution, and multi-agent workflows. However, this autonomy creates a largely overlooked security gap. Existing defenses focus on prompt-injection attacks and fail to address threats embedded in tool metadata, leaving MCP-based systems exposed to semantic manipulation. This work analyzes three classes of semantic attacks on MCP-integrated systems: (1) Tool Poisoning, where adversarial instructions are hidden in tool descriptors; (2) Shadowing, where trusted tools are indirectly compromised through contaminated shared context; and (3) Rug Pulls, where descriptors are altered after approval to subvert behavior. To counter these threats, we introduce a layered security framework with three components: RSA-based manifest signing to enforce descriptor integrity, LLM-on-LLM semantic vetting to detect suspicious tool definitions, and lightweight heuristic guardrails that block anomalous tool behavior at runtime. Through evaluation of GPT-4, DeepSeek, and Llama-3.5 across eight prompting strategies, we find that security performance varies widely by model architecture and reasoning method. GPT-4 blocks about 71 percent of unsafe tool calls, balancing latency and safety. DeepSeek shows the highest resilience to Shadowing attacks but with greater latency, while Llama-3.5 is fastest but least robust. Our results show that the proposed framework reduces unsafe tool invocation rates without model fine-tuning or internal modification.",0,arxiv,AI,CC-BY/arXiv,Securing the Model Context Protocol: Defending LLMs Against Tool Poisoning and Adversarial Attacks
"Cybercrime increasingly exploits human cognitive biases in addition to technical vulnerabilities, yet most existing analytical frameworks focus primarily on operational aspects and overlook psychological manipulation. This paper proposes BEACON, a unified dual-dimension framework that integrates behavioral psychology with the tactical lifecycle of cybercrime to enable structured, interpretable, and scalable analysis of cybercrime. We formalize six psychologically grounded manipulation categories derived from Prospect Theory and Cialdini's principles of persuasion, alongside a fourteen-stage cybercrime tactical lifecycle spanning reconnaissance to final impact. A single large language model is fine-tuned using parameter-efficient learning to perform joint multi-label classification across both psychological and tactical dimensions while simultaneously generating human-interpretable explanations. Experiments conducted on a curated dataset of real-world and synthetically augmented cybercrime narratives demonstrate a 20 percent improvement in overall classification accuracy over the base model, along with substantial gains in reasoning quality measured using ROUGE and BERTScore. The proposed system enables automated decomposition of unstructured victim narratives into structured behavioral and operational intelligence, supporting improved cybercrime investigation, case linkage, and proactive scam detection.",0,arxiv,AI,CC-BY/arXiv,BEACON: A Unified Behavioral-Tactical Framework for Explainable Cybercrime Analysis with Large Language Models
"Decoupled loss has been a successful reinforcement learning (RL) algorithm to deal with the high data staleness under the asynchronous RL setting. Decoupled loss improves coupled-loss style of algorithms' (e.g., PPO, GRPO) learning stability by introducing a proximal policy to decouple the off-policy corrections (importance weight) from the controlling policy updates (trust region). However, the proximal policy requires an extra forward pass through the network at each training step, creating a computational bottleneck for large language models. We observe that since the proximal policy only serves as a trust region anchor between the behavior and target policies, we can approximate it through simple interpolation without explicit computation. We call this approach A-3PO (APproximated Proximal Policy Optimization). A-3PO eliminates this overhead, reducing training time by 18% while maintaining comparable performance. Code & off-the-shelf example are available at: https://github.com/inclusionAI/AReaL/blob/main/docs/algorithms/prox_approx.md",0,arxiv,AI,CC-BY/arXiv,A-3PO: Accelerating Asynchronous LLM Training with Staleness-aware Proximal Policy Approximation
"Decoding-based regression, which reformulates regression as a sequence generation task, has emerged as a promising paradigm of applying large language models for numerical prediction. However, its progress is hindered by the misalignment between discrete token-level objectives (e.g., cross-entropy) and continuous numerical values. Existing approaches relying on token-level constraints often fail to capture the global magnitude of the target value, limiting their precision and generalization. In this paper, we propose to unlock the potential of decoding-based regression via Reinforcement Learning (RL). We formulate the generation process as a Markov Decision Process, utilizing sequence-level rewards to enforce global numerical coherence. Extensive experiments on tabular regression and code metric regression demonstrate that our method (specifically with ReMax and GRPO) consistently outperforms both state-of-the-art token-level baselines and traditional regression heads, showing the superiority of introducing sequence-level signals. Our analysis further reveals that RL significantly enhances sampling efficiency and predictive precision, establishing decoding-based regression as a robust and accurate paradigm for general-purpose numerical prediction.",0,arxiv,AI,CC-BY/arXiv,Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning
"Brain tumors pose a significant threat to human life, therefore it is very much necessary to detect them accurately in the early stages for better diagnosis and treatment. Brain tumors can be detected by the radiologist manually from the MRI scan images of the patients. However, the incidence of brain tumors has risen amongst children and adolescents in recent years, resulting in a substantial volume of data, as a result, it is time-consuming and difficult to detect manually. With the emergence of Artificial intelligence in the modern world and its vast application in the medical field, we can make an approach to the CAD (Computer Aided Diagnosis) system for the early detection of Brain tumors automatically. All the existing models for this task are not completely generalized and perform poorly on the validation data. So, we have proposed two novel Deep Learning Architectures - (a) SAETCN (Self-Attention Enhancement Tumor Classification Network) for the classification of different kinds of brain tumors. We have achieved an accuracy of 99.38% on the validation dataset making it one of the few Novel Deep learning-based architecture that is capable of detecting brain tumors accurately. We have trained the model on the dataset, which contains images of 3 types of tumors (glioma, meningioma, and pituitary tumors) and non-tumor cases. and (b) SAS-Net (Self-Attentive Segmentation Network) for the accurate segmentation of brain tumors. We have achieved an overall pixel accuracy of 99.23%.",0,arxiv,AI,CC-BY/arXiv,Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images
"The continuous growth of the global human population is leading to the expansion of human habitats, resulting in decreasing wildlife spaces and increasing human-wildlife interactions. These interactions can range from minor disturbances, such as raccoons in urban waste bins, to more severe consequences, including species extinction. As a result, the monitoring of wildlife is gaining significance in various contexts. Artificial intelligence (AI) offers a solution by automating the recognition of animals in images and videos, thereby reducing the manual effort required for wildlife monitoring. Traditional AI training involves three main stages: image collection, labelling, and model training. However, the variability, for example, in the landscape (e.g., mountains, open fields, forests), weather (e.g., rain, fog, sunshine), lighting (e.g., day, night), and camera-animal distances presents significant challenges to model robustness and adaptability in real-world scenarios.   In this work, we propose a unified framework, called ShadowWolf, designed to address these challenges by integrating and optimizing the stages of AI model training and evaluation. The proposed framework enables dynamic model retraining to adjust to changes in environmental conditions and application requirements, thereby reducing labelling efforts and allowing for on-site model adaptation. This adaptive and unified approach enhances the accuracy and efficiency of wildlife monitoring systems, promoting more effective and scalable conservation efforts.",0,arxiv,AI,CC-BY/arXiv,"ShadowWolf -- Automatic Labelling, Evaluation and Model Training Optimised for Camera Trap Wildlife Images"
"This paper studies whether, how, and for whom generative artificial intelligence (GenAI) facilitates firm creation. Our identification strategy exploits the November 2022 release of ChatGPT as a global shock that lowered start-up costs and leverages variations across geo-coded grids with differential pre-existing AI-specific human capital. Using high-resolution and universal data on Chinese firm registrations by the end of 2024, we find that grids with stronger AI-specific human capital experienced a sharp surge in new firm formation$\unicode{x2013}$driven entirely by small firms, contributing to 6.0% of overall national firm entry. Large-firm entry declines, consistent with a shift toward leaner ventures. New firms are smaller in capital, shareholder number, and founding team size, especially among small firms. The effects are strongest among firms with potential AI applications, weaker financing needs, and among first-time entrepreneurs. Overall, our results highlight that GenAI serves as a pro-competitive force by disproportionately boosting small-firm entry.",0,arxiv,AI,CC-BY/arXiv,"AI as ""Co-founder"": GenAI for Entrepreneurship"
"The subject of this research is the development of an intelligent, integrated framework for the automated inspection of photovoltaic (PV) infrastructure that addresses the critical shortcomings of conventional methods, including thermal palette bias, data redundancy, and high communication bandwidth requirements. The goal of this study is to design, develop, and validate a comprehensive, multi-modal system that fully automates the monitoring workflow, from data acquisition to the generation of actionable, geo-located maintenance alerts, thereby enhancing plant safety and operational efficiency. The methods employed involve a synergistic architecture that begins with a palette-invariant thermal embedding, learned by enforcing representational consistency, which is fused with a contrast-normalized RGB stream via a gated mechanism. This is supplemented by a closed-loop, adaptive re-acquisition controller that uses Rodrigues-based updates for targeted confirmation of ambiguous anomalies and a geospatial deduplication module that clusters redundant alerts using DBSCAN over the haversine distance. In conclusion, this study establishes a powerful new paradigm for proactive PV inspection, with the proposed system achieving a mean Average Precision (mAP@0.5) of 0.903 on the public PVF-10 benchmark, a significant 12-15% improvement over single-modality baselines. Field validation confirmed the system's readiness, achieving 96% recall, while the de-duplication process reduced duplicate-induced false positives by 15-20%, and relevance-only telemetry cut airborne data transmission by 60-70%.",0,arxiv,AI,CC-BY/arXiv,Method of UAV Inspection of Photovoltaic Modules Using Thermal and RGB Data Fusion
"Assessing language proficiency is essential for education, as it enables instruction tailored to learners needs. This paper investigates the use of Large Language Models (LLMs) for automatically classifying German texts according to the Common European Framework of Reference for Languages (CEFR) into different proficiency levels. To support robust training and evaluation, we construct a diverse dataset by combining multiple existing CEFR-annotated corpora with synthetic data. We then evaluate prompt-engineering strategies, fine-tuning of a LLaMA-3-8B-Instruct model and a probing-based approach that utilizes the internal neural state of the LLM for classification. Our results show a consistent performance improvement over prior methods, highlighting the potential of LLMs for reliable and scalable CEFR classification.",0,arxiv,AI,CC-BY/arXiv,Classifying German Language Proficiency Levels Using Large Language Models
"Goal-conditioned reinforcement learning (RL) concerns the problem of training an agent to maximize the probability of reaching target goal states. This paper presents an analysis of the goal-conditioned setting based on optimal control. In particular, we derive an optimality gap between more classical, often quadratic, objectives and the goal-conditioned reward, elucidating the success of goal-conditioned RL and why classical ``dense'' rewards can falter. We then consider the partially observed Markov decision setting and connect state estimation to our probabilistic reward, further making the goal-conditioned reward well suited to dual control problems. The advantages of goal-conditioned policies are validated on nonlinear and uncertain environments using both RL and predictive control techniques.",0,arxiv,AI,CC-BY/arXiv,Why Goal-Conditioned Reinforcement Learning Works: Relation to Dual Control
"Sampling algorithms play a pivotal role in probabilistic AI. However, verifying if a sampler program indeed samples from the claimed distribution is a notoriously hard problem. Provably correct testers like Barbarik, Teq, Flash, CubeProbe for testing of different kinds of samplers were proposed only in the last few years. All these testers focus on the worst-case efficiency, and do not support verification of samplers over infinite domains, a case occurring frequently in Astronomy, Finance, Network Security, etc.   In this work, we design the first tester of samplers with instance-dependent efficiency, allowing us to test samplers over natural numbers. Our tests are developed via a novel distance estimation algorithm between an unknown and a known probability distribution using an interval conditioning framework. The core technical contribution is a new connection with probability mass estimation of a continuous distribution. The practical gains are also substantial: our experiments establish up to 1000x speedup over state-of-the-art testers.",0,arxiv,AI,CC-BY/arXiv,Instance Dependent Testing of Samplers using Interval Conditioning
"Large language models (LLMs) are increasingly deployed on edge devices. To meet strict resource constraints, real-world deployment has pushed LLM quantization from 8-bit to 4-bit, 2-bit, and now 1.58-bit. Combined with lookup table (LUT)-based inference, CPUs run these ultra-low-bit LLMs even faster than NPUs, opening new opportunities for ubiquitous on-device intelligence.   However, this paper identifies that LUT-based inference underutilizes memory bandwidth during parallel inference, which is required for prefilling, test-time scaling, and other multi-token scenarios. The root cause is the scalar LUT paradigm, which performs repetitive and non-contiguous memory accesses for each token.   To solve the issue, we propose vector LUT, a new lookup paradigm that constructs a unified LUT across parallel tokens, and performs a single $1 \rightarrow N$ lookup per index. To realize it efficiently, we further introduce (1) Vector LUT-Centric Tensor Layout, and (2) Cache-Aware Streamed Lookup techniques. Evaluations on 5 edge devices across 3 LLMs show that Vec-LUT outperforms state-of-the-art baselines by up to $4.2\times$. Our implementation is integrated into llama.cpp. The code is available at https://github.com/Cipherxzc/vlut.cpp.",0,arxiv,AI,CC-BY/arXiv,Vec-LUT: Vector Table Lookup for Parallel Ultra-Low-Bit LLM Inference on Edge Devices
"National planning standards for public services in Egypt often fail to align with unique local characteristics. Addressing this gap, this study develops a tailored planning model for Qena City. Using a hybrid methodology (descriptive, analytical, and experimental), the research utilizes Python programming to generate an intelligent spatial analysis algorithm based on Voronoi Diagrams. This approach creates city-specific planning criteria and evaluates the current coverage of public facilities. The primary contribution of this study is the successful derivation of a localized planning standards model and the deployment of an automated algorithm to assess service efficiency. Application of this model reveals a general service coverage average of 81.3%. Ambulance stations demonstrated the highest efficiency (99.8%) due to recent upgrades, while parks and open spaces recorded the lowest coverage (10%) caused by limited land availability. Spatial analysis indicates a high service density in midtown (>45 services/km^2), which diminishes significantly towards the outskirts (<5 services/km^2). Consequently, the Hajer Qena district contains the highest volume of unserved areas, while the First District (Qesm 1) exhibits the highest level of service coverage. This model offers a replicable framework for data-driven urban planning in Egyptian cities.",0,arxiv,AI,CC-BY/arXiv,Smart Spatial Planning in Egypt: An Algorithm-Driven Approach to Public Service Evaluation in Qena City
"Accurate gender recognition from extreme long-range imagery remains a challenging problem due to limited spatial resolution, viewpoint variability, and loss of facial cues. For such purpose, we present a dual-path transformer framework that leverages CLIP to jointly model visual and attribute-driven cues for gender recognition at a distance. The framework integrates two complementary streams: (1) a direct visual path that refines a pre-trained CLIP image encoder through selective fine-tuning of its upper layers, and (2) an attribute-mediated path that infers gender from a set of soft-biometric prompts (e.g., hairstyle, clothing, accessories) aligned in the CLIP text-image space. Spatial channel attention modules further enhance discriminative localization under occlusion and low resolution. To support large-scale evaluation, we construct U-DetAGReID, a unified long-range gender dataset derived from DetReIDx and AG-ReID.v2, harmonized under a consistent ternary labeling scheme (Male, Female, Unknown). Extensive experiments suggest that the proposed solution surpasses state-of-the-art person-attribute and re-identification baselines across multiple metrics (macro-F1, accuracy, AUC), with consistent robustness to distance, angle, and height variations. Qualitative attention visualizations confirm interpretable attribute localization and responsible abstention behavior. Our results show that language-guided dual-path learning offers a principled, extensible foundation for responsible gender recognition in unconstrained long-range scenarios.",0,arxiv,AI,CC-BY/arXiv,When Gender is Hard to See: Multi-Attribute Support for Long-Range Recognition
"Inverse design aims to design the input variables of a physical system to optimize a specified objective function, typically formulated as a search or optimization problem. However, in 3D domains, the design space grows exponentially, rendering exhaustive grid-based searches infeasible. Recent advances in deep learning have accelerated inverse design by providing powerful generative priors and differentiable surrogate models. Nevertheless, current methods tend to approximate the 3D design space using 2D projections or fine-tune existing 3D shapes. These approaches sacrifice volumetric detail and constrain design exploration, preventing true 3D design from scratch. In this paper, we propose a 3D Inverse Design (3DID) framework that directly navigates the 3D design space by coupling a continuous latent representation with a physics-aware optimization strategy. We first learn a unified physics-geometry embedding that compactly captures shape and physical field data in a continuous latent space. Then, we introduce a two-stage strategy to perform physics-aware optimization. In the first stage, a gradient-guided diffusion sampler explores the global latent manifold. In the second stage, an objective-driven, topology-preserving refinement further sculpts each candidate toward the target objective. This enables 3DID to generate high-fidelity 3D geometries, outperforming existing methods in both solution quality and design versatility.",0,arxiv,AI,CC-BY/arXiv,3DID: Direct 3D Inverse Design for Aerodynamics with Physics-Aware Optimization
"Recent advances in autoregressive (AR) generative models have produced increasingly powerful systems for media synthesis. Among them, next-scale prediction has emerged as a popular paradigm, where models generate images in a coarse-to-fine manner. However, scale-wise AR models suffer from exposure bias, which undermines generation quality. We identify two primary causes of this issue: (1) train-test mismatch, where the model must rely on its own imperfect predictions during inference, and (2) imbalance in scale-wise learning difficulty, where certain scales exhibit disproportionately higher optimization complexity. Through a comprehensive analysis of training dynamics, we propose Self-Autoregressive Refinement (SAR) to address these limitations. SAR introduces a Stagger-Scale Rollout (SSR) mechanism that performs lightweight autoregressive rollouts to expose the model to its own intermediate predictions, thereby aligning train-test patterns, and a complementary Contrastive Student-Forcing Loss (CSFL) that provides adequate supervision for self-generated contexts to ensure stable training. Experimental results show that applying SAR to pretrained AR models consistently improves generation quality with minimal computational overhead. For instance, SAR yields a 5.2% FID reduction on FlexVAR-d16 trained on ImageNet 256 within 10 epochs (5 hours on 32xA100 GPUs). Given its efficiency, scalability, and effectiveness, we expect SAR to serve as a reliable post-training method for visual autoregressive generation.",0,arxiv,AI,CC-BY/arXiv,Rethinking Training Dynamics in Scale-wise Autoregressive Generation
"Large language models(LLMs) are increasingly expanding their real-world applications across domains, e.g., question answering, autonomous driving, and automatic software development. Despite this achievement, LLMs, as data-driven systems, often make incorrect predictions, which can lead to potential losses in safety-critical scenarios. To address this issue and measure the confidence of model outputs, multiple uncertainty quantification(UQ) criteria have been proposed. However, even though important, there are limited tools to integrate these methods, hindering the practical usage of UQ methods and future research in this domain. To bridge this gap, in this paper, we introduce UncertaintyZoo, a unified toolkit that integrates 29 uncertainty quantification methods, covering five major categories under a standardized interface. Using UncertaintyZoo, we evaluate the usefulness of existing uncertainty quantification methods under the code vulnerability detection task on CodeBERT and ChatGLM3 models. The results demonstrate that UncertaintyZoo effectively reveals prediction uncertainty. The tool with a demonstration video is available on the project site https://github.com/Paddingbuta/UncertaintyZoo.",0,arxiv,AI,CC-BY/arXiv,UncertaintyZoo: A Unified Toolkit for Quantifying Predictive Uncertainty in Deep Learning Systems
"Diabetic Retinopathy (DR) affects individuals with long-term diabetes. Without early diagnosis, DR can lead to vision loss. Fundus photography captures the structure of the retina along with abnormalities indicative of the stage of the disease. Artificial Intelligence (AI) can support clinicians in identifying these lesions, reducing manual workload, but models require high-quality annotated datasets. Due to the complexity of retinal structures, errors in image acquisition and lesion interpretation of manual annotators can occur. We proposed a quality-control framework, ensuring only high-standard data is used for evaluation and AI training. First, an explainable feature-based classifier is used to filter inadequate images. The features are extracted both using image processing and contrastive learning. Then, the images are enhanced and put subject to annotation, using deep-learning-based assistance. Lastly, the agreement between annotators calculated using derived formulas determines the usability of the annotations.",0,arxiv,AI,CC-BY/arXiv,Explainable Fundus Image Curation and Lesion Detection in Diabetic Retinopathy
"Predictive atomistic simulations have propelled materials discovery, yet routine setup and debugging still demand computer specialists. This know-how gap limits Integrated Computational Materials Engineering (ICME), where state-of-the-art codes exist but remain cumbersome for non-experts. We address this bottleneck with GENIUS, an AI-agentic workflow that fuses a smart Quantum ESPRESSO knowledge graph with a tiered hierarchy of large language models supervised by a finite-state error-recovery machine. Here we show that GENIUS translates free-form human-generated prompts into validated input files that run to completion on $\approx$80% of 295 diverse benchmarks, where 76% are autonomously repaired, with success decaying exponentially to a 7% baseline. Compared with LLM-only baselines, GENIUS halves inference costs and virtually eliminates hallucinations. The framework democratizes electronic-structure DFT simulations by intelligently automating protocol generation, validation, and repair, opening large-scale screening and accelerating ICME design loops across academia and industry worldwide.",0,arxiv,AI,CC-BY/arXiv,GENIUS: An Agentic AI Framework for Autonomous Design and Execution of Simulation Protocols
"The increasing complexity of cyber threats in distributed environments demands advanced frameworks for real-time detection and response across multimodal data streams. This paper introduces AgenticCyber, a generative AI powered multi-agent system that orchestrates specialized agents to monitor cloud logs, surveillance videos, and environmental audio concurrently. The solution achieves 96.2% F1-score in threat detection, reduces response latency to 420 ms, and enables adaptive security posture management using multimodal language models like Google's Gemini coupled with LangChain for agent orchestration. Benchmark datasets, such as AWS CloudTrail logs, UCF-Crime video frames, and UrbanSound8K audio clips, show greater performance over standard intrusion detection systems, reducing mean time to respond (MTTR) by 65% and improving situational awareness. This work introduces a scalable, modular proactive cybersecurity architecture for enterprise networks and IoT ecosystems that overcomes siloed security technologies with cross-modal reasoning and automated remediation.",0,arxiv,AI,CC-BY/arXiv,AgenticCyber: A GenAI-Powered Multi-Agent System for Multimodal Threat Detection and Adaptive Response in Cybersecurity
"Large language models (LLMs) excel across many natural language tasks, yet their generalisation to structural perturbations in logical contexts remains poorly understood. We introduce a controlled evaluation framework that probes reasoning reliability through four targeted stress tests: (1) rule deletion, removing either redundant or essential rules from a multi-step inference chain; (2) contradictory evidence injection; (3) logic-preserving rewrites generated through several families of equivalence laws (contrapositive, double negation, implication, De Morgan, identity, and commutativity); and (4) multi-law equivalence stacking that introduces 2-5 simultaneous logical transformations.   Across three representative model families: BERT, Qwen2, and LLaMA-like models. Our experiments reveal a strikingly consistent pattern: all models achieve perfect accuracy on the base tasks and remain fully generalise to redundant rule deletion and all equivalence-based rewrites (single or multi-law), but fail sharply under essential rule deletion (dropping to 25% accuracy) and collapse completely in the presence of explicit contradictions (0% accuracy). These results demonstrate that LLMs possess stable invariance to semantic-preserving logical transformations, yet remain fundamentally brittle to missing or conflicting evidence. Our framework provides a clean diagnostic tool for isolating such reasoning failure modes and highlights persistent gaps in the logical generalisation abilities of current LLMs.",0,arxiv,AI,CC-BY/arXiv,"Less Is More for Multi-Step Logical Reasoning of LLM Generalisation Under Rule Removal, Paraphrasing, and Compression"
"Reinforcement learning (RL) has emerged as the de-facto paradigm for improving the reasoning capabilities of large language models (LLMs). We have developed RLAX, a scalable RL framework on TPUs. RLAX employs a parameter-server architecture. A master trainer periodically pushes updated model weights to the parameter server while a fleet of inference workers pull the latest weights and generates new rollouts. We introduce a suite of system techniques to enable scalable and preemptible RL for a diverse set of state-of-art RL algorithms. To accelerate convergence and improve model quality, we have devised new dataset curation and alignment techniques. Large-scale evaluations show that RLAX improves QwQ-32B's pass@8 accuracy by 12.8% in just 12 hours 48 minutes on 1024 v5p TPUs, while remaining robust to preemptions during training.",0,arxiv,AI,CC-BY/arXiv,"RLAX: Large-Scale, Distributed Reinforcement Learning for Large Language Models on TPUs"
"The modern web stack, which is dominated by browser-based applications and API-first backends, now operates under an adversarial equilibrium where automated, AI-assisted attacks evolve continuously. Content Delivery Networks (CDNs) and edge computing place programmable defenses closest to users and bots, making them natural enforcement points for machine-learning (ML) driven inspection, throttling, and isolation. This survey synthesizes the landscape of AI-enhanced defenses deployed at the edge: (i) anomaly- and behavior-based Web Application Firewalls (WAFs) within broader Web Application and API Protection (WAAP), (ii) adaptive DDoS detection and mitigation, (iii) bot management that resists human-mimicry, and (iv) API discovery, positive security modeling, and encrypted-traffic anomaly analysis. We add a systematic survey method, a threat taxonomy mapped to edge-observable signals, evaluation metrics, deployment playbooks, and governance guidance. We conclude with a research agenda spanning XAI, adversarial robustness, and autonomous multi-agent defense. Our findings indicate that edge-centric AI measurably improves time-to-detect and time-to-mitigate while reducing data movement and enhancing compliance, yet introduces new risks around model abuse, poisoning, and governance.",0,arxiv,AI,CC-BY/arXiv,Web Technologies Security in the AI Era: A Survey of CDN-Enhanced Defenses
"Large audio language models (LALMs) are increasingly deployed in real-world settings where they inevitably capture speech from unintended nearby bystanders, raising privacy risks that existing benchmarks and defences largely overlook. We introduce SH-Bench, the first benchmark designed to evaluate selective hearing: a model's ability to attend to an intended main speaker while refusing to process or reveal information about incidental bystander speech. SH-Bench contains 3,968 multi-speaker audio mixtures spanning both real-world and synthetic scenarios, paired with 77k multiple-choice questions that probe models under general and selective operating modes. We propose Selective Efficacy (SE), a unified metric capturing both multi-speaker comprehension and bystander-privacy protection. Our evaluation of state-of-the-art open-source and proprietary LALMs reveals substantial privacy leakage, with strong audio understanding failing to translate into selective protection of bystander privacy. To mitigate this gap, we introduce Bystander Privacy Fine-Tuning (BPFT), a training pipeline that teaches models to refuse bystander-related queries without degrading main-speaker comprehension. BPFT yields substantial gains which improve SE by up to 15.9% over Gemini 2.5 Pro, demonstrating that selective hearing is learnable but far from achieved in current LALMs. SH-Bench and BPFT provide the first systematic framework for measuring and improving bystander privacy in audio foundation models.",0,arxiv,AI,CC-BY/arXiv,Protecting Bystander Privacy via Selective Hearing in LALMs
"Multi-step time-series prediction is an essential supportive step for decision-makers in several industrial areas. Artificial intelligence techniques, which use a neural network component in various forms, have recently frequently been used to accomplish this step. However, the complexity of the neural network structure still stands up as a critical problem against prediction accuracy. In this paper, a method inspired by the proportional-integral-derivative (PID) control approach is investigated to enhance the performance of neural network models used for multi-step ahead prediction of periodic time-series information while maintaining a negligible impact on the complexity of the system. The PID-based method is applied to the predicted value at each time step to bring that value closer to the real value. The water demand forecasting problem is considered as a case study, where two deep neural network models from the literature are used to prove the effectiveness of the proposed boosting method. Furthermore, to prove the applicability of this PID-based booster to other types of periodic time-series prediction problems, it is applied to enhance the accuracy of a neural network model used for multi-step forecasting of hourly energy consumption. The comparison between the results of the original prediction models and the results after using the proposed technique demonstrates the superiority of the proposed method in terms of prediction accuracy and system complexity.",0,arxiv,AI,CC-BY/arXiv,Proportional integral derivative booster for neural networks-based time-series prediction: Case of water demand prediction
"The emergence of transformative technologies often surfaces deep societal divisions, nowhere more evident than in contemporary debates about artificial intelligence (AI). A striking feature of these divisions is that they persist despite shared interests in ensuring that AI benefits humanity and avoiding catastrophic outcomes. This paper analyzes contemporary debates about AI risk, parsing the differences between the ""doomer"" and ""boomer"" perspectives into definitional, factual, causal, and moral premises to identify key points of contention. We find that differences in perspectives about existential risk (""X-risk"") arise fundamentally from differences in causal premises about design vs. emergence in complex systems, while differences in perspectives about employment risks (""E-risks"") pertain to different causal premises about the applicability of past theories (evolution) vs their inapplicability (revolution). Disagreements about these two forms of AI risk appear to share two properties: neither involves significant disagreements on moral values and both can be described in terms of differing views on the extent of boundedness of human rationality. Our approach to analyzing reasoning chains at scale, using an ensemble of LLMs to parse textual data, can be applied to identify key points of contention in debates about risk to the public in any arena.",0,arxiv,AI,CC-BY/arXiv,Why They Disagree: Decoding Differences in Opinions about AI Risk on the Lex Fridman Podcast
"Reward models are central to Large Language Model (LLM) alignment within the framework of RLHF. The standard objective used in reward modeling is the Bradley-Terry (BT) loss, which learns from pairwise data consisting of a pair of chosen and rejected responses. In this work, we analyze the per-sample gradient of BT-loss and show that its norm scales with two distinct components: (1) the difference in predicted rewards between chosen and rejected responses, which reflects the prediction error, and critically, (2) representation distance between the pair measured in the output space of the final layer. While the first term captures the intended training signal, we show that the second term can significantly impact the update magnitude and misalign learning. Specifically, pairs with small representation distance often receive vanishingly weak updates, even when misranked, while pairs with large distance receive disproportionately strong updates. This leads to gradients from large-distance pairs to overshadow those from small-distance pairs, where fine-grained distinctions are especially important. To overcome this limitation, we propose NormBT, an adaptive pair-wise normalization scheme that balances representation-driven effects and focuses learning signals on prediction error. NormBT is a lightweight, drop-in integration to BT loss with negligible overhead. Across various LLM backbones and datasets, NormBT improves reward model performance consistently, with notable gains of over 5% on the Reasoning category of RewardBench, which contains numerous small-distance pairs. This work reveals a key limitation in the widely used BT objective and provides a simple, effective correction.",0,arxiv,AI,CC-BY/arXiv,When Distance Distracts: Representation Distance Bias in BT-Loss for Reward Models
"The evolution of Large Language Models (LLMs) has catalyzed a paradigm shift from superficial instruction following to rigorous long-horizon reasoning. While Group Relative Policy Optimization (GRPO) has emerged as a pivotal mechanism for eliciting such post-training reasoning capabilities due to its exceptional performance, it remains plagued by significant training instability and poor sample efficiency. We theoretically identify the root cause of these issues as the lack of distinctiveness within on-policy rollouts: for routine queries, highly homogeneous samples induce destructive gradient conflicts; whereas for hard queries, the scarcity of valid positive samples results in ineffective optimization. To bridge this gap, we propose Distinctiveness-aware Group Relative Policy Optimization (DaGRPO). DaGRPO incorporates two core mechanisms: (1) Sequence-level Gradient Rectification, which utilizes fine-grained scoring to dynamically mask sample pairs with low distinctiveness, thereby eradicating gradient conflicts at the source; and (2) Off-policy Data Augmentation, which introduces high-quality anchors to recover training signals for challenging tasks. Extensive experiments across 9 mathematical reasoning and out-of-distribution (OOD) generalization benchmarks demonstrate that DaGRPO significantly surpasses existing SFT, GRPO, and hybrid baselines, achieving new state-of-the-art performance (e.g., a +4.7% average accuracy gain on math benchmarks). Furthermore, in-depth analysis confirms that DaGRPO effectively mitigates gradient explosion and accelerates the emergence of long-chain reasoning capabilities.",0,arxiv,AI,CC-BY/arXiv,DaGRPO: Rectifying Gradient Conflict in Reasoning via Distinctiveness-Aware Group Relative Policy Optimization
"Human pose estimation focuses on predicting body keypoints to analyze human motion. Event cameras provide high temporal resolution and low latency, enabling robust estimation under challenging conditions. However, most existing methods convert event streams into dense event frames, which adds extra computation and sacrifices the high temporal resolution of the event signal. In this work, we aim to exploit the spatiotemporal properties of event streams based on point cloud-based framework, designed to enhance human pose estimation performance. We design Event Temporal Slicing Convolution module to capture short-term dependencies across event slices, and combine it with Event Slice Sequencing module for structured temporal modeling. We also apply edge enhancement in point cloud-based event representation to enhance spatial edge information under sparse event conditions to further improve performance. Experiments on the DHP19 dataset show our proposed method consistently improves performance across three representative point cloud backbones: PointNet, DGCNN, and Point Transformer.",0,arxiv,AI,CC-BY/arXiv,Exploiting Spatiotemporal Properties for Efficient Event-Driven Human Pose Estimation
"Identity, accent, style, and emotions are essential components of human speech. Voice conversion (VC) techniques process the speech signals of two input speakers and other modalities of auxiliary information such as prompts and emotion tags. It changes para-linguistic features from one to another, while maintaining linguistic contents. Recently, VC models have made rapid advancements in both generation quality and personalization capabilities. These developments have attracted considerable attention for diverse applications, including privacy preservation, voice-print reproduction for the deceased, and dysarthric speech recovery. However, these models only learn non-robust features due to the clean training data. Subsequently, it results in unsatisfactory performances when dealing with degraded input speech in real-world scenarios, including additional noise, reverberation, adversarial attacks, or even minor perturbation. Hence, it demands robust deployments, especially in real-world settings. Although latest researches attempt to find potential attacks and countermeasures for VC systems, there remains a significant gap in the comprehensive understanding of how robust the VC model is under input manipulation. here also raises many questions: For instance, to what extent do different forms of input degradation attacks alter the expected output of VC models? Is there potential for optimizing these attack and defense strategies? To answer these questions, we classify existing attack and defense methods from the perspective of input manipulation and evaluate the impact of degraded input speech across four dimensions, including intelligibility, naturalness, timbre similarity, and subjective perception. Finally, we outline open issues and future directions.",0,arxiv,AI,CC-BY/arXiv,Degrading Voice: A Comprehensive Overview of Robust Voice Conversion Through Input Manipulation
"\citet{farrell2021deep} establish non-asymptotic high-probability bounds for general deep feedforward neural network (with rectified linear unit activation function) estimators, with \citet[Theorem 1]{farrell2021deep} achieving a suboptimal convergence rate for fully connected feedforward networks. The authors suggest that improved approximation of fully connected networks could yield sharper versions of \citet[Theorem 1]{farrell2021deep} without altering the theoretical framework. By deriving approximation bounds specifically for a narrower fully connected deep neural network, this note demonstrates that \citet[Theorem 1]{farrell2021deep} can be improved to achieve an optimal rate (up to a logarithmic factor). Furthermore, this note briefly shows that deep neural network estimators can mitigate the curse of dimensionality for functions with compositional structure and functions defined on manifolds.",0,arxiv,Ä°statistik,CC-BY/arXiv,New Approximation Results and Optimal Estimation for Fully Connected Deep Neural Networks
"Consistency-based methods have emerged as an effective approach to uncertainty quantification (UQ) in large language models. These methods typically rely on several generations obtained via multinomial sampling, measuring their agreement level. However, in short-form QA, multinomial sampling is prone to producing duplicates due to peaked distributions, and its stochasticity introduces considerable variance in uncertainty estimates across runs. We introduce a new family of methods that employ beam search to generate candidates for consistency-based UQ, yielding improved performance and reduced variance compared to multinomial sampling. We also provide a theoretical lower bound on the beam set probability mass under which beam search achieves a smaller error than multinomial sampling. We empirically evaluate our approach on six QA datasets and find that its consistent improvements over multinomial sampling lead to state-of-the-art UQ performance.",0,arxiv,Ä°statistik,CC-BY/arXiv,Don't Throw Away Your Beams: Improving Consistency-based Uncertainties in LLMs via Beam Search
"This thesis examines self-attention training through the lens of Optimal Transport (OT) and develops an OT-based alternative for tabular classification. The study tracks intermediate projections of the self-attention layer during training and evaluates their evolution using discrete OT metrics, including Wasserstein distance, Monge gap, optimality, and efficiency. Experiments are conducted on classification tasks with two and three classes, as well as on a biomedical dataset.   Results indicate that the final self-attention mapping often approximates the OT optimal coupling, yet the training trajectory remains inefficient. Pretraining the MLP section on synthetic data partially improves convergence but is sensitive to their initialization. To address these limitations, an OT-based algorithm is introduced: it generates class-specific dummy Gaussian distributions, computes an OT alignment with the data, and trains an MLP to generalize this mapping. The method achieves accuracy comparable to Transformers while reducing computational cost and scaling more efficiently under standardized inputs, though its performance depends on careful dummy-geometry design. All experiments and implementations are conducted in R.",0,arxiv,Ä°statistik,CC-BY/arXiv,Transformers for Tabular Data: A Training Perspective of Self-Attention via Optimal Transport
"The optimal transport (OT) map is a geometry-driven transformation between high-dimensional probability distributions which underpins a wide range of tasks in statistics, applied probability, and machine learning. However, existing statistical theory for OT map estimation is quite restricted, hinging on Brenier's theorem (quadratic cost, absolutely continuous source) to guarantee existence and uniqueness of a deterministic OT map, on which various additional regularity assumptions are imposed to obtain quantitative error bounds. In many real-world problems these conditions fail or cannot be certified, in which case optimal transportation is possible only via stochastic maps that can split mass. To broaden the scope of map estimation theory to such settings, this work introduces a novel metric for evaluating the transportation quality of stochastic maps. Under this metric, we develop computationally efficient map estimators with near-optimal finite-sample risk bounds, subject to easy-to-verify minimal assumptions. Our analysis further accommodates common forms of adversarial sample contamination, yielding estimators with robust estimation guarantees. Empirical experiments are provided which validate our theory and demonstrate the utility of the proposed framework in settings where existing theory fails. These contributions constitute the first general-purpose theory for map estimation, compatible with a wide spectrum of real-world applications where optimal transport may be intrinsically stochastic.",0,arxiv,Ä°statistik,CC-BY/arXiv,Estimation of Stochastic Optimal Transport Maps
"We propose geodesic-based optimization methods on dually flat spaces, where the geometric structure of the parameter manifold is closely related to the form of the objective function. A primary application is maximum likelihood estimation in statistical models, especially exponential families, whose model manifolds are dually flat. We show that an m-geodesic update, which directly optimizes the log-likelihood, can theoretically reach the maximum likelihood estimator in a single step. In contrast, an e-geodesic update has a practical advantage in cases where the parameter space is geodesically complete, allowing optimization without explicitly handling parameter constraints. We establish the theoretical properties of the proposed methods and validate their effectiveness through numerical experiments.",0,arxiv,Ä°statistik,CC-BY/arXiv,Minimization of Functions on Dually Flat Spaces Using Geodesic Descent Based on Dual Connections
"We revisit the signal denoising problem through the lens of optimal transport: the goal is to recover an unknown scalar signal distribution $X \sim P$ from noisy observations $Y = X + ÏƒZ$, with $Z$ being standard Gaussian independent of $X$ and $Ïƒ>0$ a known noise level. Let $Q$ denote the distribution of $Y$. We introduce a hierarchy of denoisers $T_0, T_1, \ldots, T_\infty : \mathbb{R} \to \mathbb{R}$ that are agnostic to the signal distribution $P$, depending only on higher-order score functions of $Q$. Each denoiser $T_K$ is progressively refined using the $(2K-1)$-th order score function of $Q$ at noise resolution $Ïƒ^{2K}$, achieving better denoising quality measured by the Wasserstein metric $W(T_K \sharp Q, P)$. The limiting denoiser $T_\infty$ identifies the optimal transport map with $T_\infty \sharp Q = P$.   We provide a complete characterization of the combinatorial structure underlying this hierarchy through Bell polynomial recursions, revealing how higher-order score functions encode the optimal transport map for signal denoising. We study two estimation strategies with convergence rates for higher-order scores from i.i.d. samples drawn from $Q$: (i) plug-in estimation via Gaussian kernel smoothing, and (ii) direct estimation via higher-order score matching. This hierarchy of agnostic denoisers opens new perspectives in signal denoising and empirical Bayes.",0,arxiv,Ä°statistik,CC-BY/arXiv,Distributional Shrinkage II: Optimal Transport Denoisers with Higher-Order Scores
"Positional encoding (PE) is a core architectural component of Transformers, yet its impact on the Transformer's generalization and robustness remains unclear. In this work, we provide the first generalization analysis for a single-layer Transformer under in-context regression that explicitly accounts for a completely trainable PE module. Our result shows that PE systematically enlarges the generalization gap. Extending to the adversarial setting, we derive the adversarial Rademacher generalization bound. We find that the gap between models with and without PE is magnified under attack, demonstrating that PE amplifies the vulnerability of models. Our bounds are empirically validated by a simulation study. Together, this work establishes a new framework for understanding the clean and adversarial generalization in ICL with PE.",0,arxiv,Ä°statistik,CC-BY/arXiv,Impact of Positional Encoding: Clean and Adversarial Rademacher Complexity for Transformers under In-Context Regression
"We examine the non-asymptotic properties of robust density ratio estimation (DRE) in contaminated settings. Weighted DRE is the most promising among existing methods, exhibiting doubly strong robustness from an asymptotic perspective. This study demonstrates that Weighted DRE achieves sparse consistency even under heavy contamination within a non-asymptotic framework. This method addresses two significant challenges in density ratio estimation and robust estimation. For density ratio estimation, we provide the non-asymptotic properties of estimating unbounded density ratios under the assumption that the weighted density ratio function is bounded. For robust estimation, we introduce a non-asymptotic framework for doubly strong robustness under heavy contamination, assuming that at least one of the following conditions holds: (i) contamination ratios are small, and (ii) outliers have small weighted values. This work provides the first non-asymptotic analysis of strong robustness under heavy contamination.",0,arxiv,Ä°statistik,CC-BY/arXiv,Robust and Sparse Estimation of Unbounded Density Ratio under Heavy Contamination
"There has been significant progress in Bayesian inference based on sparsity-inducing (e.g., spike-and-slab and horseshoe-type) priors for high-dimensional regression models. The resulting posteriors, however, in general do not possess desirable frequentist properties, and the credible sets thus cannot serve as valid confidence sets even asymptotically. We introduce a novel debiasing approach that corrects the bias for the entire Bayesian posterior distribution. We establish a new Bernstein-von Mises theorem that guarantees the frequentist validity of the debiased posterior. We demonstrate the practical performance of our proposal through Monte Carlo simulations and two empirical applications in economics.",0,arxiv,Ä°statistik,CC-BY/arXiv,Debiased Bayesian Inference for High-dimensional Regression Models
"Revisiting the continuous-time Mean-Variance (MV) Portfolio Optimization problem, we model the market dynamics with a jump-diffusion process and apply Reinforcement Learning (RL) techniques to facilitate informed exploration within the control space. We recognize the time-inconsistency of the MV problem and adopt the time-inconsistent control (TIC) approach to analytically solve for an exploratory equilibrium investment policy, which is a Gaussian distribution centered on the equilibrium control of the classical MV problem. Our approach accounts for time-inconsistent preferences and actions, and our equilibrium policy is the best option an investor can take at any given time during the investment period. Moreover, we leverage the martingale properties of the equilibrium policy, design a RL model, and propose an Actor-Critic RL algorithm. All of our RL model parameters converge to the corresponding true values in a simulation study. Our numerical study on 24 years of real market data shows that the proposed RL model is profitable in 13 out of 14 tests, demonstrating its practical applicability in real world investment.",0,arxiv,Ä°statistik,CC-BY/arXiv,Exploratory Mean-Variance with Jumps: An Equilibrium Approach
"The Weibull distribution is a commonly adopted choice for modeling the survival of systems subject to maintenance over time. When only proxy indicators and censored observations are available, it becomes necessary to express the distribution's parameters as functions of time-dependent covariates. Deep neural networks provide the flexibility needed to learn complex relationships between these covariates and operational lifetime, thereby extending the capabilities of traditional regression-based models. Motivated by the analysis of a fleet of military vehicles operating in highly variable and demanding environments, as well as by the limitations observed in existing methodologies, this paper introduces WTNN, a new neural network-based modeling framework specifically designed for Weibull survival studies. The proposed architecture is specifically designed to incorporate qualitative prior knowledge regarding the most influential covariates, in a manner consistent with the shape and structure of the Weibull distribution. Through numerical experiments, we show that this approach can be reliably trained on proxy and right-censored data, and is capable of producing robust and interpretable survival predictions that can improve existing approaches.",0,arxiv,Ä°statistik,CC-BY/arXiv,WTNN: Weibull-Tailored Neural Networks for survival analysis
"Accurate and efficient surrogate modeling is essential for modern computational science, and there are a staggering number of emulation methods to choose from. With new methods being developed all the time, comparing the relative strengths and weaknesses of different methods remains a challenge due to inconsistent benchmarking practices and (sometimes) limited reproducibility and transparency. In this work, we present a large-scale, fully reproducible comparison of $29$ distinct emulators across $60$ canonical test functions and $40$ real emulation datasets. To facilitate rigorous, apples-to-apples comparisons, we introduce the R package \texttt{duqling}, which streamlines reproducible simulation studies using a consistent, simple syntax, and automatic internal scaling of inputs. This framework allows researchers to compare emulators in a unified environment and makes it possible to replicate or extend previous studies with minimal effort, even across different publications. Our results provide detailed empirical insight into the strengths and weaknesses of state-of-the-art emulators and offer guidance for both method developers and practitioners selecting a surrogate for new data. We discuss best practices for emulator comparison and highlight how \texttt{duqling} can accelerate research in emulator design and application.",0,arxiv,Ä°statistik,CC-BY/arXiv,"All Emulators are Wrong, Many are Useful, and Some are More Useful Than Others: A Reproducible Comparison of Computer Model Surrogates"
"Kernel density estimation is a key component of a wide variety of algorithms in machine learning, Bayesian inference, stochastic dynamics and signal processing. However, the unsupervised density estimation technique requires tuning a crucial hyperparameter: the kernel bandwidth. The choice of bandwidth is critical as it controls the bias-variance trade-off by over- or under-smoothing the topological features. Topological data analysis provides methods to mathematically quantify topological characteristics, such as connected components, loops, voids et cetera, even in high dimensions where visualization of density estimates is impossible. In this paper, we propose an unsupervised learning approach using a topology-based loss function for the automated and unsupervised selection of the optimal bandwidth and benchmark it against classical techniques -- demonstrating its potential across different dimensions.",0,arxiv,Ä°statistik,CC-BY/arXiv,Unsupervised Learning of Density Estimates with Topological Optimization
"Accurately quantifying uncertainty of individual treatment effects (ITEs) across multiple decision points is crucial for personalized decision-making in fields such as healthcare, finance, education, and online marketplaces. Previous work has focused on predicting non-causal longitudinal estimands or constructing prediction bands for ITEs using cross-sectional data based on exchangeability assumptions. We propose a novel method for constructing prediction intervals using conformal inference techniques for time-varying ITEs with weaker assumptions than prior literature. We guarantee a lower bound for coverage, which is dependent on the degree of non-exchangeability in the data. Although our method is broadly applicable across decision-making contexts, we support our theoretical claims with simulations emulating micro-randomized trials (MRTs) -- a sequential experimental design for mobile health (mHealth) studies. We demonstrate the practical utility of our method by applying it to a real-world MRT - the Intern Health Study (IHS).",0,arxiv,Ä°statistik,CC-BY/arXiv,Prediction Intervals for Individual Treatment Effects in a Multiple Decision Point Framework using Conformal Inference
"Recent work \cite{arifgroup} introduced Federated Proximal Gradient \textbf{(\texttt{FedProxGrad})} for solving non-convex composite optimization problems in group fair federated learning. However, the original analysis established convergence only to a \textit{noise-dominated neighborhood of stationarity}, with explicit dependence on a variance-induced noise floor. In this work, we provide an improved asymptotic convergence analysis for a generalized \texttt{FedProxGrad}-type analytical framework with inexact local proximal solutions and explicit fairness regularization. We call this extended analytical framework \textbf{DS \texttt{FedProxGrad}} (Decay Step Size \texttt{FedProxGrad}). Under a Robbins-Monro step-size schedule \cite{robbins1951stochastic} and a mild decay condition on local inexactness, we prove that $\liminf_{r\to\infty} \mathbb{E}[\|\nabla F(\mathbf{x}^r)\|^2] = 0$, i.e., the algorithm is asymptotically stationary and the convergence rate does not depend on a variance-induced noise floor.",0,arxiv,Ä°statistik,CC-BY/arXiv,DS FedProxGrad: Asymptotic Stationarity Without Noise Floor in Fair Federated Learning
"Since the 1990s, considerable empirical work has been carried out to train statistical models, such as neural networks (NNs), as learned heuristics for combinatorial optimization (CO) problems. When successful, such an approach eliminates the need for experts to design heuristics per problem type. Due to their structure, many hard CO problems are amenable to treatment through reinforcement learning (RL). Indeed, we find a wealth of literature training NNs using value-based, policy gradient, or actor-critic approaches, with promising results, both in terms of empirical optimality gaps and inference runtimes. Nevertheless, there has been a paucity of theoretical work undergirding the use of RL for CO problems. To this end, we introduce a unified framework to model CO problems through Markov decision processes (MDPs) and solve them using RL techniques. We provide easy-to-test assumptions under which CO problems can be formulated as equivalent undiscounted MDPs that provide optimal solutions to the original CO problems. Moreover, we establish conditions under which value-based RL techniques converge to approximate solutions of the CO problem with a guarantee on the associated optimality gap. Our convergence analysis provides: (1) a sufficient rate of increase in batch size and projected gradient descent steps at each RL iteration; (2) the resulting optimality gap in terms of problem parameters and targeted RL accuracy; and (3) the importance of a choice of state-space embedding. Together, our analysis illuminates the success (and limitations) of the celebrated deep Q-learning algorithm in this problem context.",0,arxiv,Ä°statistik,CC-BY/arXiv,Heuristics for Combinatorial Optimization via Value-based Reinforcement Learning: A Unified Framework and Analysis
"We consider an adaptive experiment for treatment choice and design a minimax and Bayes optimal adaptive experiment with respect to regret. Given binary treatments, the experimenter's goal is to choose the treatment with the highest expected outcome through an adaptive experiment, in order to maximize welfare. We consider adaptive experiments that consist of two phases, the treatment allocation phase and the treatment choice phase. The experiment starts with the treatment allocation phase, where the experimenter allocates treatments to experimental subjects to gather observations. During this phase, the experimenter can adaptively update the allocation probabilities using the observations obtained in the experiment. After the allocation phase, the experimenter proceeds to the treatment choice phase, where one of the treatments is selected as the best. For this adaptive experimental procedure, we propose an adaptive experiment that splits the treatment allocation phase into two stages, where we first estimate the standard deviations and then allocate each treatment proportionally to its standard deviation. We show that this experiment, often referred to as Neyman allocation, is minimax and Bayes optimal in the sense that its regret upper bounds exactly match the lower bounds that we derive. To show this optimality, we derive minimax and Bayes lower bounds for the regret using change-of-measure arguments. Then, we evaluate the corresponding upper bounds using the central limit theorem and large deviation bounds.",0,arxiv,Ä°statistik,CC-BY/arXiv,Minimax and Bayes Optimal Adaptive Experimental Design for Treatment Choice
"We study the following distribution clustering problem: Given a hidden partition of $k$ distributions into two groups, such that the distributions within each group are the same, and the two distributions associated with the two clusters are $\varepsilon$-far in total variation, the goal is to recover the partition. We establish upper and lower bounds on the sample complexity for two fundamental cases: (1) when one of the cluster's distributions is known, and (2) when both are unknown. Our upper and lower bounds characterize the sample complexity's dependence on the domain size $n$, number of distributions $k$, size $r$ of one of the clusters, and distance $\varepsilon$. In particular, we achieve tightness with respect to $(n,k,r,\varepsilon)$ (up to an $O(\log k)$ factor) for all regimes.",0,arxiv,Ä°statistik,CC-BY/arXiv,A Distribution Testing Approach to Clustering Distributions
"Datasets may contain observations with multiple labels. If the labels are not mutually exclusive, and if the labels vary greatly in frequency, obtaining a sample that includes sufficient observations with scarcer labels to make inferences about those labels, and which deviates from the population frequencies in a known manner, creates challenges. In this paper, we consider a multivariate Bernoulli distribution as our underlying distribution of a multi-label problem. We present a novel sampling algorithm that takes label dependencies into account. It uses observed label frequencies to estimate multivariate Bernoulli distribution parameters and calculate weights for each label combination. This approach ensures the weighted sampling acquires target distribution characteristics while accounting for label dependencies. We applied this approach to a sample of research articles from Web of Science labeled with 64 biomedical topic categories. We aimed to preserve category frequency order, reduce frequency differences between most and least common categories, and account for category dependencies. This approach produced a more balanced sub-sample, enhancing the representation of minority categories.",0,arxiv,Ä°statistik,CC-BY/arXiv,A Multivariate Bernoulli-Based Sampling Method for Multi-Label Data with Application to Meta-Research
"Input features are conventionally represented as vectors, matrices, or third order tensors in the real field, for color image classification. Inspired by the success of quaternion data modeling for color images in image recovery and denoising tasks, we propose a novel classification method for color image classification, named as the Low-rank Support Quaternion Matrix Machine (LSQMM), in which the RGB channels are treated as pure quaternions to effectively preserve the intrinsic coupling relationships among channels via the quaternion algebra. For the purpose of promoting low-rank structures resulting from strongly correlated color channels, a quaternion nuclear norm regularization term, serving as a natural extension of the conventional matrix nuclear norm to the quaternion domain, is added to the hinge loss in our LSQMM model. An Alternating Direction Method of Multipliers (ADMM)-based iterative algorithm is designed to effectively resolve the proposed quaternion optimization model. Experimental results on multiple color image classification datasets demonstrate that our proposed classification approach exhibits advantages in classification accuracy, robustness and computational efficiency, compared to several state-of-the-art methods using support vector machines, support matrix machines, and support tensor machines.",0,arxiv,Ä°statistik,CC-BY/arXiv,Low Rank Support Quaternion Matrix Machine
"We study estimation of the conditional law $P(Y|X=\mathbf{x})$ and continuous functionals $Î¨(P(Y|X=\mathbf{x}))$ when $Y$ takes values in a locally compact Polish space, $X \in \mathbb{R}^p$, and the observations arise from a complex survey design. We propose a survey-calibrated distributional random forest (SDRF) that incorporates complex-design features via a pseudo-population bootstrap, PSU-level honesty, and a Maximum Mean Discrepancy (MMD) split criterion computed from kernel mean embeddings of HÃ¡jek-type (design-weighted) node distributions. We provide a framework for analyzing forest-style estimators under survey designs; establish design consistency for the finite-population target and model consistency for the super-population target under explicit conditions on the design, kernel, resampling multipliers, and tree partitions. As far as we are aware, these are the first results on model-free estimation of conditional distributions under survey designs. Simulations under a stratified two-stage cluster design provide finite sample performance and demonstrate the statistical error price of ignoring the survey design. The broad applicability of SDRF is demonstrated using NHANES: We estimate the tolerance regions of the conditional joint distribution of two diabetes biomarkers, illustrating how distributional heterogeneity can support subgroup-specific risk profiling for diabetes mellitus in the U.S. population.",0,arxiv,Ä°statistik,CC-BY/arXiv,Distributional Random Forests for Complex Survey Designs on Reproducing Kernel Hilbert Spaces
"Worst-case generation plays a critical role in evaluating robustness and stress-testing systems under distribution shifts, in applications ranging from machine learning models to power grids and medical prediction systems. We develop a generative modeling framework for worst-case generation for a pre-specified risk, based on min-max optimization over continuous probability distributions, namely the Wasserstein space. Unlike traditional discrete distributionally robust optimization approaches, which often suffer from scalability issues, limited generalization, and costly worst-case inference, our framework exploits the Brenier theorem to characterize the least favorable (worst-case) distribution as the pushforward of a transport map from a continuous reference measure, enabling a continuous and expressive notion of risk-induced generation beyond classical discrete DRO formulations. Based on the min-max formulation, we propose a Gradient Descent Ascent (GDA)-type scheme that updates the decision model and the transport map in a single loop, establishing global convergence guarantees under mild regularity assumptions and possibly without convexity-concavity. We also propose to parameterize the transport map using a neural network that can be trained simultaneously with the GDA iterations by matching the transported training samples, thereby achieving a simulation-free approach. The efficiency of the proposed method as a risk-induced worst-case generator is validated by numerical experiments on synthetic and image data.",0,arxiv,Ä°statistik,CC-BY/arXiv,Worst-case generation via minimax optimization in Wasserstein space
"In recent years, mixture cure models have gained increasing popularity in survival analysis as an alternative to the Cox proportional hazards model, particularly in settings where a subset of patients is considered cured. The proportional hazards mixture cure model is especially advantageous when the presence of a cured fraction can be reasonably assumed, providing a more accurate representation of long-term survival dynamics. In this study, we propose a novel hierarchical Bayesian framework for the semiparametric mixture cure model, which accommodates both the inclusion and exclusion of a frailty component, allowing for greater flexibility in capturing unobserved heterogeneity among patients. Samples from the posterior distribution are obtained using a Markov chain Monte Carlo method, leveraging a hierarchical structure inspired by Bayesian Lasso. Comprehensive simulation studies are conducted across diverse scenarios to evaluate the performance and robustness of the proposed models. Bayesian model comparison and assessment are performed using various criteria. Finally, the proposed approaches are applied to two well-known datasets in the cure model literature: the E1690 melanoma trial and a colon cancer clinical trial.",0,arxiv,Ä°statistik,CC-BY/arXiv,Bayesian Semiparametric Mixture Cure (Frailty) Models
"We study the expressivity of one-dimensional (1D) ReLU deep neural networks through the lens of their linear regions. For randomly initialized, fully connected 1D ReLU networks (He scaling with nonzero bias) in the infinite-width limit, we prove that the expected number of linear regions grows as $\sum_{i = 1}^L n_i + \mathop{o}\left(\sum_{i = 1}^L{n_i}\right) + 1$, where $n_\ell$ denotes the number of neurons in the $\ell$-th hidden layer. We also propose a function-adaptive notion of sparsity that compares the expected regions used by the network to the minimal number needed to approximate a target within a fixed tolerance.",0,arxiv,Ä°statistik,CC-BY/arXiv,Complexity of One-Dimensional ReLU DNNs
"Scaling attention faces a critical bottleneck: the $\mathcal{O}(n^2)$ quadratic computational cost of softmax attention, which limits its application in long-sequence domains. While linear attention mechanisms reduce this cost to $\mathcal{O}(n)$, they typically rely on fixed random feature maps, such as random Fourier features or hand-crafted functions. This reliance on static, data-agnostic kernels creates a fundamental trade-off, forcing practitioners to sacrifice significant model accuracy for computational efficiency. We introduce \textsc{LUNA}, a kernelized linear attention mechanism that eliminates this trade-off, retaining linear cost while matching and surpassing the accuracy of quadratic attention. \textsc{LUNA} is built on the key insight that the kernel feature map itself should be learned rather than fixed a priori. By parameterizing the kernel, \textsc{LUNA} learns a feature basis tailored to the specific data and task, overcoming the expressive limitations of fixed-feature methods. \textsc{Luna} implements this with a learnable feature map that induces a positive-definite kernel and admits a streaming form, yielding linear time and memory scaling in the sequence length. Empirical evaluations validate our approach across diverse settings. On the Long Range Arena (LRA), \textsc{Luna} achieves state-of-the-art average accuracy among efficient Transformers under compute parity, using the same parameter count, training steps, and approximate FLOPs. \textsc{Luna} also excels at post-hoc conversion: replacing softmax in fine-tuned BERT and ViT-B/16 checkpoints and briefly fine-tuning recovers most of the original performance, substantially outperforming fixed linearizations.",0,arxiv,Ä°statistik,CC-BY/arXiv,LUNA: Linear Universal Neural Attention with Generalization Guarantees
"This paper proposes a novel diffusion-based posterior sampling method within a plug-and-play (PnP) framework. Our approach constructs a probability transport from an easy-to-sample terminal distribution to the target posterior, using a warm-start strategy to initialize the particles. To approximate the posterior score, we develop a Monte Carlo estimator in which particles are generated using Langevin dynamics, avoiding the heuristic approximations commonly used in prior work. The score governing the Langevin dynamics is learned from data, enabling the model to capture rich structural features of the underlying prior distribution. On the theoretical side, we provide non-asymptotic error bounds, showing that the method converges even for complex, multi-modal target posterior distributions. These bounds explicitly quantify the errors arising from posterior score estimation, the warm-start initialization, and the posterior sampling procedure. Our analysis further clarifies how the prior score-matching error and the condition number of the Bayesian inverse problem influence overall performance. Finally, we present numerical experiments demonstrating the effectiveness of the proposed method across a range of inverse problems.",0,arxiv,Ä°statistik,CC-BY/arXiv,Provable Diffusion Posterior Sampling for Bayesian Inversion
"Conformal prediction provides a pivotal and flexible technique for uncertainty quantification by constructing prediction sets with a predefined coverage rate. Many online conformal prediction methods have been developed to address data distribution shifts in fully adversarial environments, resulting in overly conservative prediction sets. We propose Conformal Optimistic Prediction (COP), an online conformal prediction algorithm incorporating underlying data pattern into the update rule. Through estimated cumulative distribution function of non-conformity scores, COP produces tighter prediction sets when predictable pattern exists, while retaining valid coverage guarantees even when estimates are inaccurate. We establish a joint bound on coverage and regret, which further confirms the validity of our approach. We also prove that COP achieves distribution-free, finite-sample coverage under arbitrary learning rates and can converge when scores are $i.i.d.$. The experimental results also show that COP can achieve valid coverage and construct shorter prediction intervals than other baselines.",0,arxiv,Ä°statistik,CC-BY/arXiv,Distribution-informed Online Conformal Prediction
"Recent studies have shown the success of deep learning in solving forward and inverse problems in engineering and scientific computing domains, such as physics-informed neural networks (PINNs). In the fields of atmospheric science and environmental monitoring, estimating emission source locations is a central task that further relies on multiple model parameters that dictate velocity profiles and diffusion parameters. Estimating these parameters at the same time as emission sources from scarce data is a difficult task. In this work, we achieve this by leveraging the flexibility and generality of PINNs. We use a weighted adaptive method based on the neural tangent kernels to solve a source inversion problem with parameter estimation on the 2D and 3D advection-diffusion equations with unknown velocity and diffusion coefficients that may vary in space and time. Our proposed weighted adaptive method is presented as an extension of PINNs for forward PDE problems to a highly ill-posed source inversion and parameter estimation problem. The key idea behind our methodology is to attempt the joint recovery of the solution, the sources along with the unknown parameters, thereby using the underlying partial differential equation as a constraint that couples multiple unknown functional parameters, leading to more efficient use of the limited information in the measurements. We present various numerical experiments, using different types of measurements that model practical engineering systems, to show that our proposed method is indeed successful and robust to additional noise in the measurements.",0,arxiv,Ä°statistik,CC-BY/arXiv,Physics-Informed Neural Networks for Source Inversion and Parameters Estimation in Atmospheric Dispersion
"Machine learning models trained with \emph{stochastic} gradient descent (SGD) can generalize better than those trained with deterministic gradient descent (GD). In this work, we study SGD's impact on generalization through the lens of the statistical bootstrap: SGD uses gradient variability under batch sampling as a proxy for solution variability under the randomness of the data collection process. We use empirical results and theoretical analysis to substantiate this claim. In idealized experiments on empirical risk minimization, we show that SGD is drawn to parameter choices that are robust under resampling and thus avoids spurious solutions even if they lie in wider and deeper minima of the training loss. We prove rigorously that by implicitly regularizing the trace of the gradient covariance matrix, SGD controls the algorithmic variability. This regularization leads to solutions that are less sensitive to sampling noise, thereby improving generalization. Numerical experiments on neural network training show that explicitly incorporating the estimate of the algorithmic variability as a regularizer improves test performance. This fact supports our claim that bootstrap estimation underpins SGD's generalization advantages.",0,arxiv,Ä°statistik,CC-BY/arXiv,A Bootstrap Perspective on Stochastic Gradient Descent
"We propose $Ï†$-test, a global feature-selection and significance procedure for black-box predictors that combines Shapley attributions with selective inference. Given a trained model and an evaluation dataset, $Ï†$-test performs SHAP-guided screening and fits a linear surrogate on the screened features via a selection rule with a tractable selective-inference form. For each retained feature, it outputs a Shapley-based global score, a surrogate coefficient, and post-selection $p$-values and confidence intervals in a global feature-importance table. Experiments on real tabular regression tasks with tree-based and neural backbones suggest that $Ï†$-test can retain much of the predictive ability of the original model while using only a few features and producing feature sets that remain fairly stable across resamples and backbone classes. In these settings, $Ï†$-test acts as a practical global explanation layer linking Shapley-based importance summaries with classical statistical inference.",0,arxiv,Ä°statistik,CC-BY/arXiv,$Ï†$-test: Global Feature Selection and Inference for Shapley Additive Explanations
"Estimation of the conditional independence graph (CIG) of high-dimensional multivariate Gaussian time series from multi-attribute data is considered. Existing methods for graph estimation for such data are based on single-attribute models where one associates a scalar time series with each node. In multi-attribute graphical models, each node represents a random vector or vector time series. In this paper we provide a unified theoretical analysis of multi-attribute graph learning for dependent time series using a penalized log-likelihood objective function formulated in the frequency domain using the discrete Fourier transform of the time-domain data. We consider both convex (sparse-group lasso) and non-convex (log-sum and SCAD group penalties) penalty/regularization functions. We establish sufficient conditions in a high-dimensional setting for consistency (convergence of the inverse power spectral density to true value in the Frobenius norm), local convexity when using non-convex penalties, and graph recovery. We do not impose any incoherence or irrepresentability condition for our convergence results. We also empirically investigate selection of the tuning parameters based on the Bayesian information criterion, and illustrate our approach using numerical examples utilizing both synthetic and real data.",0,arxiv,Ä°statistik,CC-BY/arXiv,On Conditional Independence Graph Learning From Multi-Attribute Gaussian Dependent Time Series
"Inspired by graph-based methodologies, we introduce a novel graph-spanning algorithm designed to identify changes in both offline and online data across low to high dimensions. This versatile approach is applicable to Euclidean and graph-structured data with unknown distributions, while maintaining control over error probabilities. Theoretically, we demonstrate that the algorithm achieves high detection power when the magnitude of the change surpasses the lower bound of the minimax separation rate, which scales on the order of $\sqrt{nd}$. Our method outperforms other techniques in terms of accuracy for both Gaussian and non-Gaussian data. Notably, it maintains strong detection power even with small observation windows, making it particularly effective for online environments where timely and precise change detection is critical.",0,arxiv,Ä°statistik,CC-BY/arXiv,High-Dimensional Change Point Detection using Graph Spanning Ratio
"Decision making often occurs in the presence of incomplete information, leading to the under- or overestimation of risk. Leveraging the observable information to learn the complete information is called nowcasting. In practice, incomplete information is often a consequence of reporting or observation delays. In this paper, we propose an expectation-maximisation (EM) framework for nowcasting that uses machine learning techniques to model both the occurrence as well as the reporting process of events. We allow for the inclusion of covariate information specific to the occurrence and reporting periods as well as characteristics related to the entity for which events occurred. We demonstrate how the maximisation step and the information flow between EM iterations can be tailored to leverage the predictive power of neural networks and (extreme) gradient boosting machines (XGBoost). With simulation experiments, we show that we can effectively model both the occurrence and reporting of events when dealing with high-dimensional covariate information. In the presence of non-linear effects, we show that our methodology outperforms existing EM-based nowcasting frameworks that use generalised linear models in the maximisation step. Finally, we apply the framework to the reporting of Argentinian Covid-19 cases, where the XGBoost-based approach again is most performant.",0,arxiv,Ä°statistik,CC-BY/arXiv,Machine learning in an expectation-maximisation framework for nowcasting
"Real-time calibration of stochastic volatility models (SVMs) is computationally bottlenecked by the need to repeatedly solve coupled partial differential equations (PDEs). In this work, we propose DeepSVM, a physics-informed Deep Operator Network (PI-DeepONet) designed to learn the solution operator of the Heston model across its entire parameter space. Unlike standard data-driven deep learning (DL) approaches, DeepSVM requires no labelled training data. Rather, we employ a hard-constrained ansatz that enforces terminal payoffs and static no-arbitrage conditions by design. Furthermore, we use Residual-based Adaptive Refinement (RAR) to stabilize training in difficult regions subject to high gradients. Overall, DeepSVM achieves a final training loss of $10^{-5}$ and predicts highly accurate option prices across a range of typical market dynamics. While pricing accuracy is high, we find that the model's derivatives (Greeks) exhibit noise in the at-the-money (ATM) regime, highlighting the specific need for higher-order regularization in physics-informed operator learning.",0,arxiv,Ä°statistik,CC-BY/arXiv,DeepSVM: Learning Stochastic Volatility Models with Physics-Informed Deep Operator Networks
"Statistically correcting measured cross sections for detector effects is an important step across many applications. In particle physics, this inverse problem is known as \textit{unfolding}. In cases with complex instruments, the distortions they introduce are often known only implicitly through simulations of the detector. Modern machine learning has enabled efficient simulation-based approaches for unfolding high-dimensional data. Among these, one of the first methods successfully deployed on experimental data is the \textsc{OmniFold} algorithm, a classifier-based Expectation-Maximization procedure. In practice, however, the forward model is only approximately specified, and the corresponding uncertainty is encoded through nuisance parameters. Building on the well-studied \textsc{OmniFold} algorithm, we show how to extend machine learning-based unfolding to incorporate nuisance parameters. Our new algorithm, called Profile \textsc{OmniFold}, is demonstrated using a Gaussian example as well as a particle physics case study using simulated data from the CMS Experiment at the Large Hadron Collider.",0,arxiv,Ä°statistik,CC-BY/arXiv,Machine Learning-based Unfolding for Cross Section Measurements in the Presence of Nuisance Parameters
"We introduce ideal attribution mechanisms, a formal abstraction for reasoning about attribution decisions over strings. At the core of this abstraction lies the ledger, an append-only log of the prompt-response interaction history between a model and its user. Each mechanism produces deterministic decisions based on the ledger and an explicit selection criterion, making it well-suited to serve as a ground truth for attribution. We frame the design goal of watermarking schemes as faithful representation of ideal attribution mechanisms. This novel perspective brings conceptual clarity, replacing piecemeal probabilistic statements with a unified language for stating the guarantees of each scheme. It also enables precise reasoning about desiderata for future watermarking schemes, even when no current construction achieves them, since the ideal functionalities are specified first. In this way, the framework provides a roadmap that clarifies which guarantees are attainable in an idealized setting and worth pursuing in practice.",0,arxiv,Ä°statistik,CC-BY/arXiv,Ideal Attribution and Faithful Watermarks for Language Models
"We study the classic problem of prediction with expert advice under the constraint of local differential privacy (LDP). In this context, we first show that a classical algorithm naturally satisfies LDP and then design two new algorithms that improve it: RW-AdaBatch and RW-Meta. For RW-AdaBatch, we exploit the limited-switching behavior induced by LDP to provide a novel form of privacy amplification that grows stronger on easier data, analogous to the shuffle model in offline learning. Drawing on the theory of random walks, we prove that this improvement carries essentially no utility cost. For RW-Meta, we develop a general method for privately selecting between experts that are themselves non-trivial learning algorithms, and we show that in the context of LDP this carries no extra privacy cost. In contrast, prior work has only considered data-independent experts. We also derive formal regret bounds that scale inversely with the degree of independence between experts. Our analysis is supplemented by evaluation on real-world data reported by hospitals during the COVID-19 pandemic; RW-Meta outperforms both the classical baseline and a state-of-the-art \textit{central} DP algorithm by 1.5-3$\times$ on the task of predicting which hospital will report the highest density of COVID patients each week.",0,arxiv,Ä°statistik,CC-BY/arXiv,Prediction with Expert Advice under Local Differential Privacy
"Estimation of differences in conditional independence graphs (CIGs) of two time series Gaussian graphical models (TSGGMs) is investigated where the two TSGGMs are known to have similar structure. The TSGGM structure is encoded in the inverse power spectral density (IPSD) of the time series. In several existing works, one is interested in estimating the difference in two precision matrices to characterize underlying changes in conditional dependencies of two sets of data consisting of independent and identically distributed (i.i.d.) observations. In this paper we consider estimation of the difference in two IPSDs to characterize the underlying changes in conditional dependencies of two sets of time-dependent data. Our approach accounts for data time dependencies unlike past work. We analyze a penalized D-trace loss function approach in the frequency domain for differential graph learning, using Wirtinger calculus. We consider both convex (group lasso) and non-convex (log-sum and SCAD group penalties) penalty/regularization functions. An alternating direction method of multipliers (ADMM) algorithm is presented to optimize the objective function. We establish sufficient conditions in a high-dimensional setting for consistency (convergence of the inverse power spectral density to true value in the Frobenius norm) and graph recovery. Both synthetic and real data examples are presented in support of the proposed approaches. In synthetic data examples, our log-sum-penalized differential time-series graph estimator significantly outperformed our lasso based differential time-series graph estimator which, in turn, significantly outperformed an existing lasso-penalized i.i.d. modeling approach, with $F_1$ score as the performance metric.",0,arxiv,Ä°statistik,CC-BY/arXiv,Learning Conditional Independence Differential Graphs From Time-Dependent Data
"Inverse reinforcement learning aims to infer the reward function that explains expert behavior observed through trajectories of state--action pairs. A long-standing difficulty in classical IRL is the non-uniqueness of the recovered reward: many reward functions can induce the same optimal policy, rendering the inverse problem ill-posed. In this paper, we develop a statistical framework for Inverse Entropy-regularized Reinforcement Learning that resolves this ambiguity by combining entropy regularization with a least-squares reconstruction of the reward from the soft Bellman residual. This combination yields a unique and well-defined so-called least-squares reward consistent with the expert policy. We model the expert demonstrations as a Markov chain with the invariant distribution defined by an unknown expert policy $Ï€^\star$ and estimate the policy by a penalized maximum-likelihood procedure over a class of conditional distributions on the action space. We establish high-probability bounds for the excess Kullback--Leibler divergence between the estimated policy and the expert policy, accounting for statistical complexity through covering numbers of the policy class. These results lead to non-asymptotic minimax optimal convergence rates for the least-squares reward function, revealing the interplay between smoothing (entropy regularization), model complexity, and sample size. Our analysis bridges the gap between behavior cloning, inverse reinforcement learning, and modern statistical learning theory.",0,arxiv,Ä°statistik,CC-BY/arXiv,Statistical analysis of Inverse Entropy-regularized Reinforcement Learning
"The challenge of \textbf{imbalanced regression} arises when standard Empirical Risk Minimization (ERM) biases models toward high-frequency regions of the data distribution, causing severe degradation on rare but high-impact ``tail'' events. Existing strategies uch as loss re-weighting or synthetic over-sampling often introduce noise, distort the underlying distribution, or add substantial algorithmic complexity.   We introduce \textbf{PARIS} (Pruning Algorithm via the Representer theorem for Imbalanced Scenarios), a principled framework that mitigates imbalance by \emph{optimizing the training set itself}. PARIS leverages the representer theorem for neural networks to compute a \textbf{closed-form representer deletion residual}, which quantifies the exact change in validation loss caused by removing a single training point \emph{without retraining}. Combined with an efficient Cholesky rank-one downdating scheme, PARIS performs fast, iterative pruning that eliminates uninformative or performance-degrading samples.   We use a real-world space weather example, where PARIS reduces the training set by up to 75\% while preserving or improving overall RMSE, outperforming re-weighting, synthetic oversampling, and boosting baselines. Our results demonstrate that representer-guided dataset pruning is a powerful, interpretable, and computationally efficient approach to rare-event regression.",0,arxiv,Ä°statistik,CC-BY/arXiv,PARIS: Pruning Algorithm via the Representer theorem for Imbalanced Scenarios
"Access to multiple predictive models trained for the same task, whether in regression or classification, is increasingly common in many applications. Aggregating their predictive uncertainties to produce reliable and efficient uncertainty quantification is therefore a critical but still underexplored challenge, especially within the framework of conformal prediction (CP). While CP methods can generate individual prediction sets from each model, combining them into a single, more informative set remains a challenging problem. To address this, we propose SACP (Symmetric Aggregated Conformal Prediction), a novel method that aggregates nonconformity scores from multiple predictors. SACP transforms these scores into e-values and combines them using any symmetric aggregation function. This flexible design enables a robust, data-driven framework for selecting aggregation strategies that yield sharper prediction sets. We also provide theoretical insights that help justify the validity and performance of the SACP approach. Extensive experiments on diverse datasets show that SACP consistently improves efficiency and often outperforms state-of-the-art model aggregation baselines.",0,arxiv,Ä°statistik,CC-BY/arXiv,Symmetric Aggregation of Conformity Scores for Efficient Uncertainty Sets
"Adam is a widely used optimizer in neural network training due to its adaptive learning rate. However, because different data samples influence model updates to varying degrees, treating them equally can lead to inefficient convergence. To address this, a prior work proposed adapting the sampling distribution using a bandit framework to select samples adaptively. While promising, the bandit-based variant of Adam suffers from limited theoretical guarantees. In this paper, we introduce Adam with Combinatorial Bandit Sampling (AdamCB), which integrates combinatorial bandit techniques into Adam to resolve these issues. AdamCB is able to fully utilize feedback from multiple samples at once, enhancing both theoretical guarantees and practical performance. Our regret analysis shows that AdamCB achieves faster convergence than Adam-based methods including the previous bandit-based variant. Numerical experiments demonstrate that AdamCB consistently outperforms existing methods.",0,arxiv,Ä°statistik,CC-BY/arXiv,ADAM Optimization with Adaptive Batch Selection
"We present latent nonlinear denoising score matching (LNDSM), a novel training objective for score-based generative models that integrates nonlinear forward dynamics with the VAE-based latent SGM framework. This combination is achieved by reformulating the cross-entropy term using the approximate Gaussian transition induced by the Euler-Maruyama scheme. To ensure numerical stability, we identify and remove two zero-mean but variance exploding terms arising from small time steps. Experiments on variants of the MNIST dataset demonstrate that the proposed method achieves faster synthesis and enhanced learning of inherently structured distributions. Compared to benchmark structure-agnostic latent SGMs, LNDSM consistently attains superior sample quality and variability.",0,arxiv,Ä°statistik,CC-BY/arXiv,Latent Nonlinear Denoising Score Matching for Enhanced Learning of Structured Distributions
"Agglomerative hierarchical clustering is one of the most widely used approaches for exploring how observations in a dataset relate to each other. However, its greedy nature makes it highly sensitive to small perturbations in the data, often producing different clustering results and making it difficult to separate genuine structure from spurious patterns. In this paper, we show how randomizing hierarchical clustering can be useful not just for measuring stability but also for designing valid hypothesis testing procedures based on the clustering results.   We propose a simple randomization scheme together with a method for constructing a valid p-value at each node of the hierarchical clustering dendrogram that quantifies evidence against performing the greedy merge. Our test controls the Type I error rate, works with any hierarchical linkage without case-specific derivations, and simulations show it is substantially more powerful than existing selective inference approaches. To demonstrate the practical utility of our p-values, we develop an adaptive $Î±$-spending procedure that estimates the number of clusters, with a probabilistic guarantee on overestimation. Experiments on simulated and real data show that this estimate yields powerful clustering and can be used, for example, to assess clustering stability across multiple runs of the randomized algorithm.",0,arxiv,Ä°statistik,CC-BY/arXiv,Hierarchical Clustering With Confidence
"Heterogeneous data are now ubiquitous in many applications in which correctly identifying the subgroups from a heterogeneous population is critical. Although there is an increasing body of literature on subgroup detection, existing methods mainly focus on the univariate response setting. In this paper, we propose a joint heterogeneity and reduced-rank learning framework to simultaneously identify the subgroup structure and estimate the covariate effects for heterogeneous multivariate response regression. In particular, our approach uses rank-constrained pairwise fusion penalization and conducts the subgroup analysis without requiring prior knowledge regarding the individual subgroup memberships. We implement the proposed approach by an alternating direction method of multipliers (ADMM) algorithm and show its convergence. We also establish the asymptotic properties for the resulting estimators under mild and interpretable conditions. A predictive information criterion is proposed to select the rank of the coefficient matrix with theoretical support. The effectiveness of the proposed approach is demonstrated through simulation studies and a real data application.",0,arxiv,Ä°statistik,CC-BY/arXiv,Simultaneous Heterogeneity and Reduced-rank Learning for Multivariate Response Regression
"We develop a novel characterization of extremal dependence between two cortical regions of the brain when its signals display extremely large amplitudes. We show that connectivity in the tails of the distribution reveals unique features of extreme events (e.g., seizures) that can help to identify their occurrence. Numerous studies have established that connectivity-based features are effective for discriminating brain states. Here, we demonstrate the advantage of the proposed approach: that tail connectivity provides additional discriminatory power, enabling more accurate identification of extreme-related events and improved seizure risk management. Common approaches in tail dependence modeling use pairwise summary measures or parametric models. However, these approaches do not identify channels that drive the maximal tail dependence between two groups of signals -- an information that is useful when analyzing electroencephalography of epileptic patients where specific channels are responsible for seizure occurrences. A familiar approach in traditional signal processing is canonical correlation, which we extend to the tails to develop a visualization of extremal channel-contributions. Through the tail pairwise dependence matrix (TPDM), we develop a computationally-efficient estimator for our canonical tail dependence measure. Our method is then used for accurate frequency-based soft clustering of neonates, distinguishing those with seizures from those without.",0,arxiv,Ä°statistik,CC-BY/arXiv,Canonical Tail Dependence for Soft Extremal Clustering of Multichannel Brain Signals
"We lay the theoretical foundation for automating optimizer design in gradient-based learning. Based on the greedy principle, we formulate the problem of designing optimizers as maximizing the instantaneous decrease in loss. By treating an optimizer as a function that translates loss gradient signals into parameter motions, the problem reduces to a family of convex optimization problems over the space of optimizers. Solving these problems under various constraints not only recovers a wide range of popular optimizers as closed-form solutions, but also produces the optimal hyperparameters of these optimizers with respect to the problems at hand. This enables a systematic approach to design optimizers and tune their hyperparameters according to the gradient statistics that are collected during the training process. Furthermore, this optimization of optimization can be performed dynamically during training.",0,arxiv,Ä°statistik,CC-BY/arXiv,Optimizing Optimizers for Fast Gradient-Based Learning
"Extreme weather events are widely studied in fields such as agriculture, ecology, and meteorology. The spatio-temporal co-occurrence of extreme events can strengthen or weaken under changing climate conditions. In this paper, we propose a novel approach to model spatio-temporal extremes by integrating climate indices via a conditional variational autoencoder (cXVAE). A convolutional neural network (CNN) is embedded in the decoder to convolve climatological indices with the spatial dependence within the latent space, thereby allowing the decoder to be dependent on the climate variables. There are three main contributions here. First, we demonstrate through extensive simulations that the proposed conditional XVAE accurately emulates spatial fields and recovers spatially and temporally varying extremal dependence with very low computational cost post training. Second, we provide a simple, scalable approach to detecting condition-driven shifts and whether the dependence structure is invariant to the conditioning variable. Third, when dependence is found to be condition-sensitive, the conditional XVAE supports counterfactual experiments allowing intervention on the climate covariate and propagating the associated change through the learned decoder to quantify differences in joint tail risk, co-occurrence ranges, and return metrics. To demonstrate the practical utility and performance of the model in real-world scenarios, we apply our method to analyze the monthly maximum Fire Weather Index (FWI) over eastern Australia from 2014 to 2024 conditioned on the El NiÃ±o/Southern Oscillation (ENSO) index.",0,arxiv,Ä°statistik,CC-BY/arXiv,Modeling Spatio-temporal Extremes via Conditional Variational Autoencoders
"We theoretically demonstrate that the generalization error of interpolators for machine learning models under teacher-student settings becomes 0 once the number of training samples exceeds a certain threshold. Understanding the high generalization ability of large-scale models such as deep neural networks (DNNs) remains one of the central open problems in machine learning theory. While recent theoretical studies have attributed this phenomenon to the implicit bias of stochastic gradient descent (SGD) toward well-generalizing solutions, empirical evidences indicate that it primarily stems from properties of the model itself. Specifically, even randomly sampled interpolators, which are parameters that achieve zero training error, have been observed to generalize effectively. In this study, under a teacher-student framework, we prove that the generalization error of randomly sampled interpolators becomes exactly zero once the number of training samples exceeds a threshold determined by the geometric structure of the interpolator set in parameter space. As a proof technique, we leverage tools from algebraic geometry to mathematically characterize this geometric structure.",0,arxiv,Ä°statistik,CC-BY/arXiv,Zero Generalization Error Theorem for Random Interpolators via Algebraic Geometry
"Modern neural networks exhibit a striking property: basins of attraction in the loss landscape are often connected by low-loss paths, yet optimization dynamics generally remain confined to a single convex basin and rarely explore intermediate points. We resolve this paradox by identifying entropic barriers arising from the interplay between curvature variations along these paths and noise in optimization dynamics. Empirically, we find that curvature systematically rises away from minima, producing effective forces that bias noisy dynamics back toward the endpoints - even when the loss remains nearly flat. These barriers persist longer than energetic barriers, shaping the late-time localization of solutions in parameter space. Our results highlight the role of curvature-induced entropic forces in governing both connectivity and confinement in deep learning landscapes.",0,arxiv,Ä°statistik,CC-BY/arXiv,Entropic Confinement and Mode Connectivity in Overparameterized Neural Networks
"Stochastic Reaction Networks (SRNs) are a fundamental modeling framework for systems ranging from chemical kinetics and epidemiology to ecological and synthetic biological processes. A central computational challenge is the estimation of expected outputs across initial conditions and times, a task that is rarely solvable analytically and becomes computationally prohibitive with current methods such as Finite State Projection or the Stochastic Simulation Algorithm. Existing deep learning approaches offer empirical scalability, but provide neither interpretability nor reliability guarantees, limiting their use in scientific analysis and in applications where model outputs inform real-world decisions. Here we introduce DeepSKA, a neural framework that jointly achieves interpretability, guaranteed reliability, and substantial computational gains. DeepSKA yields mathematically transparent representations that generalise across states, times, and output functions, and it integrates this structure with a small number of stochastic simulations to produce unbiased, provably convergent, and dramatically lower-variance estimates than classical Monte Carlo. We demonstrate these capabilities across nine SRNs, including nonlinear and non-mass-action models with up to ten species, where DeepSKA delivers accurate predictions and orders-of-magnitude efficiency improvements. This interpretable and reliable neural framework offers a principled foundation for developing analogous methods for other Markovian systems, including stochastic differential equations.",0,arxiv,Ä°statistik,CC-BY/arXiv,Interpretable Neural Approximation of Stochastic Reaction Dynamics with Guaranteed Reliability
"In this work, we study contextual strongly convex simulation optimization and adopt an ""optimize then predict"" (OTP) approach for real-time decision making. In the offline stage, simulation optimization is conducted across a set of covariates to approximate the optimal-solution function; in the online stage, decisions are obtained by evaluating this approximation at the observed covariate. The central theoretical challenge is to understand how the inexactness of solutions generated by simulation-optimization algorithms affects the optimality gap, which is overlooked in existing studies. To address this, we develop a unified analysis framework that explicitly accounts for both solution bias and variance. Using Polyak-Ruppert averaging SGD as an illustrative simulation-optimization algorithm, we analyze the optimality gap of OTP under four representative smoothing techniques: $k$ nearest neighbor, kernel smoothing, linear regression, and kernel ridge regression. We establish convergence rates, derive the optimal allocation of the computational budget $Î“$ between the number of design covariates and the per-covariate simulation effort, and demonstrate the convergence rate can approximately achieve $Î“^{-1}$ under appropriate smoothing technique and sample-allocation rule. Finally, through a numerical study, we validate the theoretical findings and demonstrate the effectiveness and practical value of the proposed approach.",0,arxiv,Ä°statistik,CC-BY/arXiv,Contextual Strongly Convex Simulation Optimization: Optimize then Predict with Inexact Solutions
"Semi-implicit variational inference (SIVI) constructs approximate posteriors of the form $q(Î¸) = \int k(Î¸| z) r(dz)$, where the conditional kernel is parameterized and the mixing base is fixed and tractable. This paper develops a unified ""approximation-optimization-statistics'' theory for such families.   On the approximation side, we show that under compact L1-universality and a mild tail-dominance condition, semi-implicit families are dense in L1 and can achieve arbitrarily small forward Kullback-Leibler (KL) error. We also identify two sharp obstructions to global approximation: (i) an Orlicz tail-mismatch condition that induces a strictly positive forward-KL gap, and (ii) structural restrictions, such as non-autoregressive Gaussian kernels, that force ""branch collapse'' in conditional distributions. For each obstruction we give a minimal structural modification that restores approximability.   On the optimization side, we establish finite-sample oracle inequalities and prove that the empirical SIVI objectives L(K,n) $Î“$-converge to their population limit as n and K tend to infinity. These results give consistency of empirical maximizers, quantitative control of finite-K surrogate bias, and stability of the resulting variational posteriors.   Combining the approximation and optimization analyses yields the first general end-to-end statistical theory for SIVI: we characterize precisely when SIVI can recover the target distribution, when it cannot, and how architectural and algorithmic choices govern the attainable asymptotic behavior.",0,arxiv,Ä°statistik,CC-BY/arXiv,From Tail Universality to Bernstein-von Mises: A Unified Statistical Theory of Semi-Implicit Variational Inference
"In this work we investigate the relationship between kernel regularity and algorithmic performance in the bandit optimization of RKHS functions. While reproducing kernel Hilbert space (RKHS) methods traditionally rely on global kernel regressors, it is also common to use a smoothness-based approach that exploits local approximations. We show that these perspectives are deeply connected through the spectral properties of isotropic kernels. In particular, we characterize the Fourier spectra of the MatÃ©rn, square-exponential, rational-quadratic, $Î³$-exponential, piecewise-polynomial, and Dirichlet kernels, and show that the decay rate determines asymptotic regret from both viewpoints. For kernelized bandit algorithms, spectral decay yields upper bounds on the maximum information gain, governing worst-case regret, while for smoothness-based methods, the same decay rates establish HÃ¶lder space embeddings and Besov space norm-equivalences, enabling local continuity analysis. These connections show that kernel-based and locally adaptive algorithms can be analyzed within a unified framework. This allows us to derive explicit regret bounds for each kernel family, obtaining novel results in several cases and providing improved analysis for others. Furthermore, we analyze LP-GP-UCB, an algorithm that combines both approaches, augmenting global Gaussian process surrogates with local polynomial estimators. While the hybrid approach does not uniformly dominate specialized methods, it achieves order-optimality across multiple kernel families.",0,arxiv,Ä°statistik,CC-BY/arXiv,Consequences of Kernel Regularity for Bandit Optimization
"Optimal experimental design is a classic topic in statistics, with many well-studied problems, applications, and solutions. The design problem we study is the placement of sensors to monitor spatiotemporal processes, explicitly accounting for the temporal dimension in our modeling and optimization. We observe that recent advancements in computational sciences often yield large datasets based on physics-based simulations, which are rarely leveraged in experimental design. We introduce a novel model-based sensor placement criterion, along with a highly-efficient optimization algorithm, which integrates physics-based simulations and Bayesian experimental design principles to identify sensor networks that ""minimize information loss"" from simulated data. Our technique relies on sparse variational inference and (separable) Gauss-Markov priors, and thus may adapt many techniques from Bayesian experimental design. We validate our method through a case study monitoring air temperature in Phoenix, Arizona, using state-of-the-art physics-based simulations. Our results show our framework to be superior to random or quasi-random sampling, particularly with a limited number of sensors. We conclude by discussing practical considerations and implications of our framework, including more complex modeling tools and real-world deployments.",0,arxiv,Ä°statistik,CC-BY/arXiv,Designing an Optimal Sensor Network via Minimizing Information Loss
"Deep neural networks often fail when deployed in real-world contexts due to distribution shift, a critical barrier to building safe and reliable systems. An emerging approach to address this problem relies on \emph{disagreement discrepancy} -- a measure of how the disagreement between two models changes under a shifting distribution. The process of maximizing this measure has seen applications in bounding error under shifts, testing for harmful shifts, and training more robust models. However, this optimization involves the non-differentiable zero-one loss, necessitating the use of practical surrogate losses. We prove that existing surrogates for disagreement discrepancy are not Bayes consistent, revealing a fundamental flaw: maximizing these surrogates can fail to maximize the true disagreement discrepancy. To address this, we introduce new theoretical results providing both upper and lower bounds on the optimality gap for such surrogates. Guided by this theory, we propose a novel disagreement loss that, when paired with cross-entropy, yields a provably consistent surrogate for disagreement discrepancy. Empirical evaluations across diverse benchmarks demonstrate that our method provides more accurate and robust estimates of disagreement discrepancy than existing approaches, particularly under challenging adversarial conditions.",0,arxiv,Ä°statistik,CC-BY/arXiv,On the Bayes Inconsistency of Disagreement Discrepancy Surrogates
"We consider the fundamental problem of balanced $k$-means clustering. In particular, we introduce an optimal transport approach to alternating minimization called BalLOT, and we show that it delivers a fast and effective solution to this problem. We establish this with a variety of numerical experiments before proving several theoretical guarantees. First, we prove that for generic data, BalLOT produces integral couplings at each step. Next, we perform a landscape analysis to provide theoretical guarantees for both exact and partial recoveries of planted clusters under the stochastic ball model. Finally, we propose initialization schemes that achieve one-step recovery of planted clusters.",0,arxiv,Ä°statistik,CC-BY/arXiv,BalLOT: Balanced $k$-means clustering with optimal transport
"Accurate real-time waypoints estimation for the UAV-based online Terrain Following during wildfire patrol missions is critical to ensuring flight safety and enabling wildfire detection. However, existing real-time filtering algorithms struggle to maintain accurate waypoints under measurement noise in nonlinear and time-varying systems, posing risks of flight instability and missed wildfire detections during UAV-based terrain following. To address this issue, a Residual Variance Matching Recursive Least Squares (RVM-RLS) filter, guided by a Residual Variance Matching Estimation (RVME) criterion, is proposed to adaptively estimate the real-time waypoints of nonlinear, time-varying UAV-based terrain following systems. The proposed method is validated using a UAV-based online terrain following system within a simulated terrain environment. Experimental results show that the RVM-RLS filter improves waypoints estimation accuracy by approximately 88$\%$ compared with benchmark algorithms across multiple evaluation metrics. These findings demonstrate both the methodological advances in real-time filtering and the practical potential of the RVM-RLS filter for UAV-based online wildfire patrol.",0,arxiv,Ä°statistik,CC-BY/arXiv,A Residual Variance Matching Recursive Least Squares Filter for Real-time UAV Terrain Following
"In this paper, we propose a recurrent neural network (RNN)-based framework for estimating the parameters of the fractional Poisson process (FPP), which models event arrivals with memory and long-range dependence. The Long Short-Term Memory (LSTM) network estimates the key parameters $Î¼>0$ and $Î²\in(0,1)$ from sequences of inter-arrival times, effectively modeling their temporal dependencies. Our experiments on synthetic data show that the proposed approach reduces the mean squared error (MSE) by about 55.3\% compared to the traditional method of moments (MOM) and performs reliably across different training conditions. We tested the method on two real-world high-frequency datasets: emergency call records from Montgomery County, PA, and AAPL stock trading data. The results show that the LSTM can effectively track daily patterns and parameter changes, indicating its effectiveness on real-world data with complex time dependencies.",0,arxiv,Ä°statistik,CC-BY/arXiv,NeuroMemFPP: A recurrent neural approach for memory-aware parameter estimation in fractional Poisson process
"Analyzing decision problems under uncertainty commonly relies on idealizing assumptions about the describability of the world, with the most prominent examples being the closed world and the small world assumption. Most assumptions are operationalized by introducing states of the world, conditional on which the decision situation can be analyzed without any remaining uncertainty. Conversely, most classical decision-theoretic approaches are not applicable if the states of the world are inaccessible. We propose a decision model that retains the appeal and simplicity of the original theory, but completely overcomes the need to specify the states of the world explicitly. The main idea of our approach is to address decision problems in a radically empirical way: instead of specifying states and consequences prior to the decision analysis, we only assume a protocol of observed act--consequence pairs as model primitives. We show how optimality in such empirical decision problems can be addressed by using protocol-based empirical choice functions and discuss three approaches for deriving inferential guarantees: (I) consistent statistical estimation of choice sets, (II) consistent statistical testing of choice functions with robustness guarantees, and (III) direct inference for empirical choice functions using credal sets. We illustrate our theory with a proof-of-concept application comparing different prompting strategies in generative AI models.",0,arxiv,Ä°statistik,CC-BY/arXiv,Empirical Decision Theory
"Semantic communication systems aim to transmit task-relevant information between devices capable of artificial intelligence, but their performance can degrade when heterogeneous transmitter-receiver models produce misaligned latent representations. Existing semantic alignment methods typically rely on additional digital processing at the transmitter or receiver, increasing overall device complexity. In this work, we introduce the first over-the-air semantic alignment framework based on stacked intelligent metasurfaces (SIM), which enables latent-space alignment directly in the wave domain, reducing substantially the computational burden at the device level. We model SIMs as trainable linear operators capable of emulating both supervised linear aligners and zero-shot Parseval-frame-based equalizers. To realize these operators physically, we develop a gradient-based optimization procedure that tailors the metasurface transfer function to a desired semantic mapping. Experiments with heterogeneous vision transformer (ViT) encoders show that SIMs can accurately reproduce both supervised and zero-shot semantic equalizers, achieving up to 90% task accuracy in regimes with high signal-to-noise ratio (SNR), while maintaining strong robustness even at low SNR values.",0,arxiv,Ä°statistik,CC-BY/arXiv,Over-the-Air Semantic Alignment with Stacked Intelligent Metasurfaces
"Classical supervised learning evaluates models primarily via predictive risk on hold-out data. Such evaluations quantify how well a function behaves on a distribution, but they do not address whether the internal decomposition of a model is uniquely determined by the data and evaluation design. In this paper, we introduce \emph{Modular Jets} for regression and classification pipelines. Given a task manifold (input space), a modular decomposition, and access to module-level representations, we estimate empirical jets, which are local linear response maps that describe how each module reacts to small structured perturbations of the input. We propose an empirical notion of \emph{mirage} regimes, where multiple distinct modular decompositions induce indistinguishable jets and thus remain observationally equivalent, and contrast this with an \emph{identifiable} regime, where the observed jets single out a decomposition up to natural symmetries. In the setting of two-module linear regression pipelines we prove a jet-identifiability theorem. Under mild rank assumptions and access to module-level jets, the internal factorisation is uniquely determined, whereas risk-only evaluation admits a large family of mirage decompositions that implement the same input-to-output map. We then present an algorithm (MoJet) for empirical jet estimation and mirage diagnostics, and illustrate the framework using linear and deep regression as well as pipeline classification.",0,arxiv,Ä°statistik,CC-BY/arXiv,Modular Jets for Supervised Pipelines: Diagnosing Mirage vs Identifiability
"We study the calibration of Gaussian process (GP) predictive distributions in the interpolation setting from a design-marginal perspective. Conditioning on the data and averaging over a design measure Î¼, we formalize Î¼-coverage for central intervals and Î¼-probabilistic calibration through randomized probability integral transforms. We introduce two methods. cps-gp adapts conformal predictive systems to GP interpolation using standardized leave-one-out residuals, yielding stepwise predictive distributions with finite-sample marginal calibration. bcr-gp retains the GP posterior mean and replaces the Gaussian residual by a generalized normal model fitted to cross-validated standardized residuals. A Bayesian selection rule-based either on a posterior upper quantile of the variance for conservative prediction or on a cross-posterior Kolmogorov-Smirnov criterion for probabilistic calibration-controls dispersion and tail behavior while producing smooth predictive distributions suitable for sequential design. Numerical experiments on benchmark functions compare cps-gp, bcr-gp, Jackknife+ for GPs, and the full conformal Gaussian process, using calibration metrics (coverage, Kolmogorov-Smirnov, integral absolute error) and accuracy or sharpness through the scaled continuous ranked probability score.",0,arxiv,Ä°statistik,CC-BY/arXiv,Design-marginal calibration of Gaussian process predictive distributions: Bayesian and conformal approaches
"The main objective of this study is to propose an optimal transport based semi-supervised approach to learn from scarce labelled image data using deep convolutional networks. The principle lies in implicit graph-based transductive semi-supervised learning where the similarity metric between image samples is the Wasserstein distance. This metric is used in the label propagation mechanism during learning. We apply and demonstrate the effectiveness of the method on a GNSS real life application. More specifically, we address the problem of multi-path interference detection. Experiments are conducted under various signal conditions. The results show that for specific choices of hyperparameters controlling the amount of semi-supervision and the level of sensitivity to the metric, the classification accuracy can be significantly improved over the fully supervised training method.",0,arxiv,Ä°statistik,CC-BY/arXiv,Wasserstein distance based semi-supervised manifold learning and application to GNSS multi-path detection
"As artificial intelligence and machine learning tools become more accessible, and scientists face new obstacles to data collection (e.g., rising costs, declining survey response rates), researchers increasingly use predictions from pre-trained algorithms as substitutes for missing or unobserved data. Though appealing for financial and logistical reasons, using standard tools for inference can misrepresent the association between independent variables and the outcome of interest when the true, unobserved outcome is replaced by a predicted value. In this paper, we characterize the statistical challenges inherent to drawing inference with predicted data (IPD) and show that high predictive accuracy does not guarantee valid downstream inference. We show that all such failures reduce to statistical notions of (i) bias, when predictions systematically shift the estimand or distort relationships among variables, and (ii) variance, when uncertainty from the prediction model and the intrinsic variability of the true data are ignored. We then review recent methods for conducting IPD and discuss how this framework is deeply rooted in classical statistical theory. We then comment on some open questions and interesting avenues for future work in this area, and end with some comments on how to use predicted data in scientific studies that is both transparent and statistically principled.",0,arxiv,Ä°statistik,CC-BY/arXiv,Do We Really Even Need Data? A Modern Look at Drawing Inference with Predicted Data
"Recent advances in natural language processing have enabled the increasing use of text data in causal inference, particularly for adjusting confounding factors in treatment effect estimation. Although high-dimensional text can encode rich contextual information, it also poses unique challenges for causal identification and estimation. In particular, the positivity assumption, which requires sufficient treatment overlap across confounder values, is often violated at the observational level, when massive text is represented in feature spaces. Redundant or spurious textual features inflate dimensionality, producing extreme propensity scores, unstable weights, and inflated variance in effect estimates. We address these challenges with Confounding-Aware Token Rationalization (CATR), a framework that selects a sparse necessary subset of tokens using a residual-independence diagnostic designed to preserve confounding information sufficient for unconfoundedness. By discarding irrelevant texts while retaining key signals, CATR mitigates observational-level positivity violations and stabilizes downstream causal effect estimators. Experiments on synthetic data and a real-world study using the MIMIC-III database demonstrate that CATR yields more accurate, stable, and interpretable causal effect estimates than existing baselines.",0,arxiv,Ä°statistik,CC-BY/arXiv,Text Rationalization for Robust Causal Effect Estimation
"We consider the problem of learning the parameters of a $N$-dimensional stochastic linear dynamics under both full and partial observations from a single trajectory of time $T$. We introduce and analyze a new estimator that achieves a small maximum element-wise error on the recovery of symmetric dynamic matrices using only $T=\mathcal{O}(\log N)$ observations, irrespective of whether the matrix is sparse or dense. This estimator is based on the method of moments and does not rely on problem-specific regularization. This is especially important for applications such as structure discovery.",0,arxiv,Ä°statistik,CC-BY/arXiv,Symmetric Linear Dynamical Systems are Learnable from Few Observations
"Understanding the robustness of a weather forecasting model with respect to input noise or different uncertainties is important in assessing its output reliability, particularly for extreme weather events like hurricanes. In this paper, we test sensitivity and robustness of an artificial intelligence (AI) weather forecasting model: NVIDIAs FourCastNetv2 (FCNv2). We conduct two experiments designed to assess model output under different levels of injected noise in the models initial condition. First, we perturb the initial condition of Hurricane Florence from the European Centre for Medium-Range Weather Forecasts (ECMWF) Reanalysis v5 (ERA5) dataset (September 13-16, 2018) with varying amounts of Gaussian noise and examine the impact on predicted trajectories and forecasted storm intensity. Second, we start FCNv2 with fully random initial conditions and observe how the model responds to nonsensical inputs. Our results indicate that FCNv2 accurately preserves hurricane features under low to moderate noise injection. Even under high levels of noise, the model maintains the general storm trajectory and structure, although positional accuracy begins to degrade. FCNv2 consistently underestimates storm intensity and persistence across all levels of injected noise. With full random initial conditions, the model generates smooth and cohesive forecasts after a few timesteps, implying the models tendency towards stable, smoothed outputs. Our approach is simple and portable to other data-driven AI weather forecasting models.",0,arxiv,Ä°statistik,CC-BY/arXiv,Robustness Test for AI Forecasting of Hurricane Florence Using FourCastNetv2 and Random Perturbations of the Initial Condition
"Kolmogorov-Arnold Networks have emerged as interpretable alternatives to traditional multi-layer perceptrons. However, standard implementations lack principled uncertainty quantification capabilities essential for many scientific applications. We present a framework integrating sparse variational Gaussian process inference with the Kolmogorov-Arnold topology, enabling scalable Bayesian inference with computational complexity quasi-linear in sample size. Through analytic moment matching, we propagate uncertainty through deep additive structures while maintaining interpretability. We use three example studies to demonstrate the framework's ability to distinguish aleatoric from epistemic uncertainty: calibration of heteroscedastic measurement noise in fluid flow reconstruction, quantification of prediction confidence degradation in multi-step forecasting of advection-diffusion dynamics, and out-of-distribution detection in convolutional autoencoders. These results suggest Sparse Variational Gaussian Process Kolmogorov-Arnold Networks (SVGP KANs) is a promising architecture for uncertainty-aware learning in scientific machine learning.",0,arxiv,Ä°statistik,CC-BY/arXiv,Uncertainty Quantification for Scientific Machine Learning using Sparse Variational Gaussian Process Kolmogorov-Arnold Networks (SVGP KAN)
"Sampling from unnormalized target distributions is a fundamental yet challenging task in machine learning and statistics. Existing sampling algorithms typically require many iterative steps to produce high-quality samples, leading to high computational costs. We introduce one-step diffusion samplers which learn a step-conditioned ODE so that one large step reproduces the trajectory of many small ones via a state-space consistency loss. We further show that standard ELBO estimates in diffusion samplers degrade in the few-step regime because common discrete integrators yield mismatched forward/backward transition kernels. Motivated by this analysis, we derive a deterministic-flow (DF) importance weight for ELBO estimation without a backward kernel. To calibrate DF, we introduce a volume-consistency regularization that aligns the accumulated volume change along the flow across step resolutions. Our proposed sampler therefore achieves both sampling and stable evidence estimate in only one or few steps. Across challenging synthetic and Bayesian benchmarks, it achieves competitive sample quality with orders-of-magnitude fewer network evaluations while maintaining robust ELBO estimates.",0,arxiv,Ä°statistik,CC-BY/arXiv,One-Step Diffusion Samplers via Self-Distillation and Deterministic Flow
"Although diffusion models now occupy a central place in generative modeling, introductory treatments commonly assume Euclidean data and seldom clarify their connection to discrete-state analogues. This article is a self-contained primer on diffusion over general state spaces, unifying continuous domains and discrete/categorical structures under one lens. We develop the discrete-time view (forward noising via Markov kernels and learned reverse dynamics) alongside its continuous-time limits -- stochastic differential equations (SDEs) in $\mathbb{R}^d$ and continuous-time Markov chains (CTMCs) on finite alphabets -- and derive the associated Fokker--Planck and master equations. A common variational treatment yields the ELBO that underpins standard training losses. We make explicit how forward corruption choices -- Gaussian processes in continuous spaces and structured categorical transition kernels (uniform, masking/absorbing and more) in discrete spaces -- shape reverse dynamics and the ELBO. The presentation is layered for three audiences: newcomers seeking a self-contained intuitive introduction; diffusion practitioners wanting a global theoretical synthesis; and continuous-diffusion experts looking for an analogy-first path into discrete diffusion. The result is a unified roadmap to modern diffusion methodology across continuous domains and discrete sequences, highlighting a compact set of reusable proofs, identities, and core theoretical principles.",0,arxiv,Ä°statistik,CC-BY/arXiv,Foundations of Diffusion Models in General State Spaces: A Self-Contained Introduction
"Simulating the conditioned dynamics of diffusion processes, given their initial and terminal states, is an important but challenging problem in the sciences. The difficulty is particularly pronounced for rare events, for which the unconditioned dynamics rarely reach the terminal state. In this work, we leverage a self-consistency property of the conditioned dynamics to learn the diffusion bridge in an iterative online manner, and demonstrate promising empirical results in a range of settings.",0,arxiv,Ä°statistik,CC-BY/arXiv,Control Consistency Losses for Diffusion Bridges
"Guided or controlled data generation with diffusion models\blfootnote{Partial preliminary results of this work appeared in International Conference on Machine Learning 2025 \citep{li2025provable}.} has become a cornerstone of modern generative modeling. Despite substantial advances in diffusion model theory, the theoretical understanding of guided diffusion samplers remains severely limited. We make progress by developing a unified algorithmic and theoretical framework that accommodates both diffusion guidance and reward-guided diffusion. Aimed at fine-tuning diffusion models to improve certain rewards, we propose injecting a reward guidance term -- constructed from the difference between the original and reward-reweighted scores -- into the backward diffusion process, and rigorously quantify the resulting reward improvement over the unguided counterpart. As a key application, our framework shows that classifier-free guidance (CFG) decreases the expected reciprocal of the classifier probability, providing the first theoretical characterization of the specific performance metric that CFG improves for general target distributions. When applied to reward-guided diffusion, our framework yields a new sampler that is easy-to-train and requires no full diffusion trajectories during training. Numerical experiments further corroborate our theoretical findings.",0,arxiv,Ä°statistik,CC-BY/arXiv,Towards a unified framework for guided diffusion models
"This thesis develops methods for causal inference and causal representation learning (CRL) in high-dimensional, time-varying data.   The first contribution introduces the Causal Dynamic Variational Autoencoder (CDVAE), a model for estimating Individual Treatment Effects (ITEs) by capturing unobserved heterogeneity in treatment response driven by latent risk factors that affect only outcomes. CDVAE comes with theoretical guarantees on valid latent adjustment and generalization bounds for ITE error. Experiments on synthetic and real datasets show that CDVAE outperforms baselines, and that state-of-the-art models greatly improve when augmented with its latent substitutes, approaching oracle performance without access to true adjustment variables.   The second contribution proposes an efficient framework for long-term counterfactual regression based on RNNs enhanced with Contrastive Predictive Coding (CPC) and InfoMax. It captures long-range dependencies under time-varying confounding while avoiding the computational cost of transformers, achieving state-of-the-art results and introducing CPC into causal inference.   The third contribution advances CRL by addressing how latent causes manifest in observed variables. We introduce a model-agnostic interpretability layer based on the geometry of the decoder Jacobian. A sparse self-expression prior induces modular, possibly overlapping groups of observed features aligned with shared latent influences. We provide recovery guarantees in both disjoint and overlapping settings and show that meaningful latent-to-observed structure can be recovered without anchor features or single-parent assumptions. Scalable Jacobian-based regularization techniques are also developed.",0,arxiv,Ä°statistik,CC-BY/arXiv,Learning Causality for Longitudinal Data
"We prove finite-sample concentration and anti-concentration bounds for dimension estimation using Gaussian kernel sums. Our bounds provide explicit dependence on sample size, bandwidth, and local geometric and distributional parameters, characterizing precisely how regularity conditions govern statistical performance. We also propose a bandwidth selection heuristic using derivative information, which shows promise in numerical experiments.",0,arxiv,Ä°statistik,CC-BY/arXiv,Concentration bounds for intrinsic dimension estimation using Gaussian kernels
"We develop a flexible feature selection framework based on deep neural networks that approximately controls the false discovery rate (FDR), a measure of Type-I error. The method applies to architectures whose first layer is fully connected. From the second layer onward, it accommodates multilayer perceptrons (MLPs) of arbitrary width and depth, convolutional and recurrent networks, attention mechanisms, residual connections, and dropout. The procedure also accommodates stochastic gradient descent with data-independent initializations and learning rates. To the best of our knowledge, this is the first work to provide a theoretical guarantee of FDR control for feature selection within such a general deep learning setting.   Our analysis is built upon a multi-index data-generating model and an asymptotic regime in which the feature dimension $n$ diverges faster than the latent dimension $q^{*}$, while the sample size, the number of training iterations, the network depth, and hidden layer widths are left unrestricted. Under this setting, we show that each coordinate of the gradient-based feature-importance vector admits a marginal normal approximation, thereby supporting the validity of asymptotic FDR control. As a theoretical limitation, we assume $\mathbf{B}$-right orthogonal invariance of the design matrix, and we discuss broader generalizations. We also present numerical experiments that underscore the theoretical findings.",0,arxiv,Ä°statistik,CC-BY/arXiv,Provable FDR Control for Deep Feature Selection: Deep MLPs and Beyond
"We present a novel recurrent neural network architecture designed explicitly for day-ahead electricity price forecasting, aimed at improving short-term decision-making and operational management in energy systems. Our combined forecasting model embeds linear structures, such as expert models and Kalman filters, into recurrent networks, enabling efficient computation and enhanced interpretability. The design leverages the strengths of both linear and non-linear model structures, allowing it to capture all relevant stylised price characteristics in power markets, including calendar and autoregressive effects, as well as influences from load, renewable energy, and related fuel and carbon markets. For empirical testing, we use hourly data from the largest European electricity market spanning 2018 to 2025 in a comprehensive forecasting study, comparing our model against state-of-the-art approaches, particularly high-dimensional linear and neural network models. The proposed model achieves approximately 12% higher accuracy than leading benchmarks. We evaluate the contributions of the interpretable model components and conclude on the impact of combining linear and non-linear structures.",0,arxiv,Ä°statistik,CC-BY/arXiv,Recurrent Neural Networks with Linear Structures for Electricity Price Forecasting
"We develop a general theory of semantic dynamics for large language models by formalizing them as Continuous State Machines (CSMs): smooth dynamical systems whose latent manifolds evolve under probabilistic transition operators. The associated transfer operator $P: L^2(M,Î¼) \to L^2(M,Î¼)$ encodes the propagation of semantic mass. Under mild regularity assumptions (compactness, ergodicity, bounded Jacobian), $P$ is compact with discrete spectrum. Within this setting, we prove the Semantic Characterization Theorem (SCT): the leading eigenfunctions of $P$ induce finitely many spectral basins of invariant meaning, each definable in an o-minimal structure over $\mathbb{R}$. Thus spectral lumpability and logical tameness coincide. This explains how discrete symbolic semantics can emerge from continuous computation: the continuous activation manifold collapses into a finite, logically interpretable ontology. We further extend the SCT to stochastic and adiabatic (time-inhomogeneous) settings, showing that slowly drifting kernels preserve compactness, spectral coherence, and basin structure.",0,arxiv,Ä°statistik,CC-BY/arXiv,How to Tame Your LLM: Semantic Collapse in Continuous Systems
"Surrogate models (including deep neural networks and other machine learning algorithms in supervised learning) are capable of approximating arbitrarily complex, high-dimensional input-output problems in science and engineering, but require a thorough data-agnostic uncertainty quantification analysis before these can be deployed for any safety-critical application. The standard approach for data-agnostic uncertainty quantification is to use conformal prediction (CP), a well-established framework to build uncertainty models with proven statistical guarantees that do not assume any shape for the error distribution of the surrogate model. However, since the classic statistical guarantee offered by CP is given in terms of bounds for the marginal coverage, for small calibration set sizes (which are frequent in realistic surrogate modelling that aims to quantify error at different regions), the potentially strong dispersion of the coverage distribution around its average negatively impacts the reliability of the uncertainty model, often obtaining coverages below the expected value, resulting in a less applicable framework. After providing a gentle presentation of uncertainty quantification for surrogate models for machine learning practitioners, in this paper we bridge the gap by proposing a new statistical guarantee that offers probabilistic information for the coverage of a single conformal predictor. We show that the proposed framework converges to the standard solution offered by CP for large calibration set sizes and, unlike the classic guarantee, still offers reliable information about the coverage of a conformal predictor for small data sizes. We illustrate and validate the methodology in a suite of examples, and implement an open access software solution that can be used alongside common conformal prediction libraries to obtain uncertainty models that fulfil the new guarantee.",0,arxiv,Ä°statistik,CC-BY/arXiv,Reliable Statistical Guarantees for Conformal Predictors with Small Datasets
"We formulate the Fast-Weights Homeostatic Reentry Network (FHRN) as a continuous-time neural-ODE system, revealing its role as a norm-regulated reentrant dynamical process. Starting from the discrete reentry rule $x_t = x_t^{(\mathrm{ex})} + Î³\, W_r\, g(\|y_{t-1}\|)\, y_{t-1}$, we derive the coupled system $\dot{y}=-y+f(W_ry;\,x,\,A)+g_{\mathrm{h}}(y)$ showing that the network couples fast associative memory with global radial homeostasis. The dynamics admit bounded attractors governed by an energy functional, yielding a ring-like manifold. A Jacobian spectral analysis identifies a \emph{reflective regime} in which reentry induces stable oscillatory trajectories rather than divergence or collapse. Unlike continuous-time recurrent neural networks or liquid neural networks, FHRN achieves stability through population-level gain modulation rather than fixed recurrence or neuron-local time adaptation. These results establish the reentry network as a distinct class of self-referential neural dynamics supporting recursive yet bounded computation.",0,arxiv,Ä°statistik,CC-BY/arXiv,Continuous-Time Homeostatic Dynamics for Reentrant Inference Models
"Machine learning on graphs has recently achieved impressive progress in various domains, including molecular property prediction and chip design. However, benchmarking practices remain fragmented, often relying on narrow, task-specific datasets and inconsistent evaluation protocols, which hampers reproducibility and broader progress. To address this, we introduce GraphBench, a comprehensive benchmarking suite that spans diverse domains and prediction tasks, including node-level, edge-level, graph-level, and generative settings. GraphBench provides standardized evaluation protocols -- with consistent dataset splits and performance metrics that account for out-of-distribution generalization -- as well as a unified hyperparameter tuning framework. Additionally, we benchmark GraphBench using message-passing neural networks and graph transformer models, providing principled baselines and establishing a reference performance. See www.graphbench.io for further details.",0,arxiv,Ä°statistik,CC-BY/arXiv,GraphBench: Next-generation graph learning benchmarking
"Semi-supervised learning (SSL) constructs classifiers using both labelled and unlabelled data. It leverages information from labelled samples, whose acquisition is often costly or labour-intensive, together with unlabelled data to enhance prediction performance. This defines an incomplete-data problem, which statistically can be formulated within the likelihood framework for finite mixture models that can be fitted using the expectation-maximisation (EM) algorithm. Ideally, one would prefer a completely labelled sample, as one would anticipate that a labelled observation provides more information than an unlabelled one. However, when the mechanism governing label absence depends on the observed features or the class labels or both, the missingness indicators themselves contain useful information. In certain situations, the information gained from modelling the missing-label mechanism can even outweigh the loss due to missing labels, yielding a classifier with a smaller expected error than one based on a completely labelled sample analysed. This improvement arises particularly when class overlap is moderate, labelled data are sparse, and the missingness is informative. Modelling such informative missingness thus offers a coherent statistical framework that unifies likelihood-based inference with the behaviour of empirical SSL methods.",0,arxiv,Ä°statistik,CC-BY/arXiv,Informative missingness and its implications in semi-supervised learning
"A classical result of Carleman, based on the theory of quasianalytic functions, shows that polynomials are dense in $L^2(Î¼)$ for any $Î¼$ such that the moments $\int x^k dÎ¼$ do not grow too rapidly as $k \to \infty$. In this work, we develop a fairly tight quantitative analogue of the underlying Denjoy-Carleman theorem via complex analysis, and show that this allows for nonasymptotic control of the rate of approximation by polynomials for any smooth function with polynomial growth at infinity. In many cases, this allows us to establish $L^2$ approximation-theoretic results for functions over general classes of distributions (e.g., multivariate sub-Gaussian or sub-exponential distributions) which were previously known only in special cases. As one application, we show that the Paley--Wiener class of functions bandlimited to $[-Î©,Î©]$ admits superexponential rates of approximation over all strictly sub-exponential distributions, which leads to a new characterization of the class. As another application, we solve an open problem recently posed by Chandrasekaran, Klivans, Kontonis, Meka and Stavropoulos on the smoothed analysis of learning, and also obtain quantitative improvements to their main results and applications.",0,arxiv,Ä°statistik,CC-BY/arXiv,"Constructive Approximation under Carleman's Condition, with Applications to Smoothed Analysis"
"Spectral gradient methods, such as the recently popularized Muon optimizer, are a promising alternative to standard Euclidean gradient descent for training deep neural networks and transformers, but it is still unclear in which regimes they are expected to perform better. We propose a simple layerwise condition that predicts when a spectral update yields a larger decrease in the loss than a Euclidean gradient step. This condition compares, for each parameter block, the squared nuclear-to-Frobenius ratio of the gradient to the stable rank of the incoming activations. To understand when this condition may be satisfied, we first prove that post-activation matrices have low stable rank at Gaussian initialization in random feature regression, feedforward networks, and transformer blocks. In spiked random feature models we then show that, after a short burn-in, the Euclidean gradient's nuclear-to-Frobenius ratio grows with the data dimension while the stable rank of the activations remains bounded, so the predicted advantage of spectral updates scales with dimension. We validate these predictions in synthetic regression experiments and in NanoGPT-scale language model training, where we find that intermediate activations have low-stable-rank throughout training and the corresponding gradients maintain large nuclear-to-Frobenius ratios. Together, these results identify conditions for spectral gradient methods, such as Muon, to be effective in training deep networks and transformers.",0,arxiv,Ä°statistik,CC-BY/arXiv,When do spectral gradient updates help in deep learning?
"Two pressing topics in the theory of deep learning are the interpretation of feature learning mechanisms and the determination of implicit bias of networks in the rich regime. Current theories of rich feature learning, often appear in the form of high-dimensional non-linear equations, which require computationally intensive numerical solutions. Given the many details that go into defining a deep learning problem, this complexity is a significant and often unavoidable challenge. Here, we propose a powerful heuristic route for predicting the data and width scales at which various patterns of feature learning emerge. This form of scale analysis is considerably simpler than exact theories and reproduces the scaling exponents of various known results. In addition, we make novel predictions on complex toy architectures, such as three-layer non-linear networks and attention heads, thus extending the scope of first-principle theories of deep learning.",0,arxiv,Ä°statistik,CC-BY/arXiv,Mitigating the Curse of Detail: Scaling Arguments for Feature Learning and Sample Complexity
"Cross-entropy (CE) training loss dominates deep learning practice, yet existing theory often relies on simplifications, either replacing it with squared loss or restricting to convex models, that miss essential behavior. CE and squared loss generate fundamentally different dynamics, and convex linear models cannot capture the complexities of non-convex optimization. We provide an in-depth characterization of multi-class CE optimization dynamics beyond the convex regime by analyzing a canonical two-layer linear neural network with standard-basis vectors as inputs: the simplest non-convex extension for which the implicit bias remained unknown. This model coincides with the unconstrained features model used to study neural collapse, making our work the first to prove that gradient flow on CE converges to the neural collapse geometry. We construct an explicit Lyapunov function that establishes global convergence, despite the presence of spurious critical points in the non-convex landscape. A key insight underlying our analysis is an inconspicuous finding: Hadamard Initialization diagonalizes the softmax operator, freezing the singular vectors of the weight matrices and reducing the dynamics entirely to their singular values. This technique opens a pathway for analyzing CE training dynamics well beyond our specific setting considered here.",0,arxiv,Ä°statistik,CC-BY/arXiv,Diagonalizing the Softmax: Hadamard Initialization for Tractable Cross-Entropy Dynamics
"Fuzzy simplicial sets have become an object of interest in dimensionality reduction and manifold learning, most prominently through their role in UMAP. However, their definition through tools from algebraic topology without a clear probabilistic interpretation detaches them from commonly used theoretical frameworks in those areas. In this work we introduce a framework that explains fuzzy simplicial sets as marginals of probability measures on simplicial sets. In particular, this perspective shows that the fuzzy weights of UMAP arise from a generative model that samples Vietoris-Rips filtrations at random scales, yielding cumulative distribution functions of pairwise distances. More generally, the framework connects fuzzy simplicial sets to probabilistic models on the face poset, clarifies the relation between Kullback-Leibler divergence and fuzzy cross-entropy in this setting, and recovers standard t-norms and t-conorms via Boolean operations on the underlying simplicial sets. We then show how new embedding methods may be derived from this framework and illustrate this on an example where we generalize UMAP using ÄŒech filtrations with triplet sampling. In summary, this probabilistic viewpoint provides a unified probabilistic theoretical foundation for fuzzy simplicial sets, clarifies the role of UMAP within this framework, and enables the systematic derivation of new dimensionality reduction methods.",0,arxiv,Ä°statistik,CC-BY/arXiv,Probabilistic Foundations of Fuzzy Simplicial Sets for Nonlinear Dimensionality Reduction
"Neural networks have become a widely adopted tool for modeling nonlinear dynamical systems from data. However, the choice of training strategy remains a key design decision, particularly for simulation tasks. This paper compares two predominant strategies: parallel and series-parallel training. The conducted empirical analysis spans five neural network architectures and two examples: a pneumatic valve test bench and an industrial robot benchmark. The study reveals that, even though series-parallel training dominates current practice, parallel training consistently yields better long-term prediction accuracy. Additionally, this work clarifies the often inconsistent terminology in the literature and relate both strategies to concepts from system identification. The findings suggest that parallel training should be considered the default training strategy for neural network-based simulation of dynamical systems.",0,arxiv,Ä°statistik,CC-BY/arXiv,Comparison of neural network training strategies for the simulation of dynamical systems
"Boolean matrix factorization (BMF) approximates a given binary input matrix as the product of two smaller binary factors. Unlike binary matrix factorization based on standard arithmetic, BMF employs the Boolean OR and AND operations for the matrix product, which improves interpretability and reduces the approximation error. It is also used in role mining and computer vision. In this paper, we first propose algorithms for BMF that perform alternating optimization (AO) of the factor matrices, where each subproblem is solved via integer programming (IP). We then design different approaches to further enhance AO-based algorithms by selecting an optimal subset of rank-one factors from multiple runs. To address the scalability limits of IP-based methods, we introduce new greedy and local-search heuristics. We also construct a new C++ data structure for Boolean vectors and matrices that is significantly faster than existing ones and is of independent interest, allowing our heuristics to scale to large datasets. We illustrate the performance of all our proposed methods and compare them with the state of the art on various real datasets, both with and without missing data, including applications in topic modeling and imaging.",0,arxiv,Ä°statistik,CC-BY/arXiv,Algorithms for Boolean Matrix Factorization using Integer Programming and Heuristics
"Infinite hidden Markov models provide a flexible framework for modelling time series with structural changes and complex dynamics, without requiring the number of latent states to be specified in advance. This flexibility is achieved through the hierarchical Dirichlet process prior, while efficient Bayesian inference is enabled by the beam sampler, which combines dynamic programming with slice sampling to truncate the infinite state space adaptively. Despite extensive methodological developments, the role of initialization in this framework has received limited attention. This study addresses this gap by systematically evaluating initialization strategies commonly used for finite hidden Markov models and assessing their suitability in the infinite setting. Results from both simulated and real datasets show that distance-based clustering initializations consistently outperform model-based and uniform alternatives, the latter being the most widely adopted in the existing literature.",0,arxiv,Ä°statistik,CC-BY/arXiv,A comparison between initialization strategies for the infinite hidden Markov model
"Probabilistic Graphical Models (PGMs) encode conditional dependencies among random variables using a graph -nodes for variables, links for dependencies- and factorize the joint distribution into lower-dimensional components. This makes PGMs well-suited for analyzing complex systems and supporting decision-making. Recent advances in topological signal processing highlight the importance of variables defined on topological spaces in several application domains. In such cases, the underlying topology shapes statistical relationships, limiting the expressiveness of canonical PGMs. To overcome this limitation, we introduce Colored Markov Random Fields (CMRFs), which model both conditional and marginal dependencies among Gaussian edge variables on topological spaces, with a theoretical foundation in Hodge theory. CMRFs extend classical Gaussian Markov Random Fields by including link coloring: connectivity encodes conditional independence, while color encodes marginal independence. We quantify the benefits of CMRFs through a distributed estimation case study over a physical network, comparing it with baselines with different levels of topological prior.",0,arxiv,Ä°statistik,CC-BY/arXiv,Colored Markov Random Fields for Probabilistic Topological Modeling
"Transformer-based audio SSL (self-supervised learning) models often treat spectrograms as images, applying convolutional patchification with heavy temporal downsampling. This lowers the effective Nyquist frequency and introduces aliasing, while naÃ¯ve low-pass filtering removes task-relevant high-frequency cues. In this study, we present Aliasing-aware Patch Embedding (AaPE), a drop-in patch stem that mitigates aliasing while preserving high-frequency information. AaPE augments standard patch tokens with features produced by a band-limited complex sinusoidal kernel using a two-sided exponential window that dynamically targets alias-prone bands. Frequency and decay parameters of the kernel are estimated from the input, enabling parallel, adaptive subband analysis whose outputs are fused with the standard patch tokens. AaPE integrates seamlessly into the masked teacher-student self-supervised learning. In addition, we combine a multi-mask strategy with a contrastive objective to enforce consistency across diverse mask patterns, stabilizing training. Pre-training on AudioSet followed by fine-tuning evaluation across diverse downstream benchmarks, which spanned categories, such as environmental sounds and other common audio domains. This approach yields state-of-the-art performance on a subset of tasks and competitive results across the remainder. Complementary linear probing evaluation mirrors this pattern, yielding clear gains on several benchmarks and strong performance elsewhere. The collective analysis of these results indicates that AaPE serves to mitigate the effects of aliasing without discarding of informative high-frequency content.",0,arxiv,Ä°statistik,CC-BY/arXiv,AaPE: Aliasing-aware Patch Embedding for Self-Supervised Audio Representation Learning
"Existing class-incremental learning (CIL) approaches based on replay or knowledge distillation are often constrained by forgetting or the stability-plasticity dilemma. Some expansion-based approaches could achieve higher accuracy. However, they always require significant parameter increases. In this paper, we propose a plugin extension paradigm termed the Deployment of extra LoRA Components (DLC) for non-pre-trained CIL scenarios.We treat the feature extractor trained through replay or distillation as a base model with rich knowledge. For each task, we use Low-Rank Adaptation (LoRA) to inject task-specific residuals into the base model's deep layers. During inference, representations with task-specific residuals are aggregated to produce classification predictions. To mitigate interference from non-target LoRA plugins, we introduce a lightweight weighting unit. This unit learns to assign importance scores to different LoRA-tuned representations. Like downloadable contents in software, our method serves as a plug-and-play enhancement that efficiently extends the base methods. Remarkably, on the large-scale ImageNet-100, with merely 4 % of the parameters of a standard ResNet-18, our DLC model achieves a significant 8 % improvement in accuracy, demonstrating exceptional efficiency. Moreover, it could surpass state-of-the-art methods under the fixed memory budget.",0,arxiv,Ä°statistik,CC-BY/arXiv,Parameter-Efficient Augment Plugin for Class-Incremental Learning
"We propose GaussDetect-LiNGAM, a novel approach for bivariate causal discovery that eliminates the need for explicit Gaussianity tests by leveraging a fundamental equivalence between noise Gaussianity and residual independence in the reverse regression. Under the standard LiNGAM assumptions of linearity, acyclicity, and exogeneity, we prove that the Gaussianity of the forward-model noise is equivalent to the independence between the regressor and residual in the reverse model. This theoretical insight allows us to replace fragile and sample-sensitive Gaussianity tests with robust kernel-based independence tests. Experimental results validate the equivalence and demonstrate that GaussDetect-LiNGAM maintains high consistency across diverse noise types and sample sizes, while reducing the number of tests per decision (TPD). Our method enhances both the efficiency and practical applicability of causal inference, making LiNGAM more accessible and reliable in real-world scenarios.",0,arxiv,Ä°statistik,CC-BY/arXiv,GaussDetect-LiNGAM:Causal Direction Identification without Gaussianity test
"Recovering jointly sparse signals in the multiple measurement vectors (MMV) setting is a fundamental problem in machine learning, but traditional methods like multiple measurement vectors orthogonal matching pursuit (M-OMP) and multiple measurement vectors FOCal Underdetermined System Solver (M-FOCUSS) often require careful parameter tuning or prior knowledge of the sparsity of the signal and/or noise variance. We introduce a novel tuning-free framework that leverages Implicit Regularization (IR) from overparameterization to overcome this limitation. Our approach reparameterizes the estimation matrix into factors that decouple the shared row-support from individual vector entries. We show that the optimization dynamics inherently promote the desired row-sparse structure by applying gradient descent to a standard least-squares objective on these factors. We prove that with a sufficiently small and balanced initialization, the optimization dynamics exhibit a ""momentum-like"" effect, causing the norms of rows in the true support to grow significantly faster than others. This formally guarantees that the solution trajectory converges towards an idealized row-sparse solution. Additionally, empirical results demonstrate that our approach achieves performance comparable to established methods without requiring any prior information or tuning.",0,arxiv,Ä°statistik,CC-BY/arXiv,Tuning-Free Structured Sparse Recovery of Multiple Measurement Vectors using Implicit Regularization
"Online A/B testing, the gold standard for evaluating new advertising policies, consumes substantial engineering resources and risks significant revenue loss from deploying underperforming variations. This motivates the use of Off-Policy Evaluation (OPE) for rapid, offline assessment. However, applying OPE to ad auctions is fundamentally more challenging than in domains like recommender systems, where stochastic policies are common. In online ad auctions, it is common for the highest-bidding ad to win the impression, resulting in a deterministic, winner-takes-all setting. This results in zero probability of exposure for non-winning ads, rendering standard OPE estimators inapplicable. We introduce the first principled framework for OPE in deterministic auctions by repurposing the bid landscape model to approximate the propensity score. This model allows us to derive robust approximate propensity scores, enabling the use of stable estimators like Self-Normalized Inverse Propensity Scoring (SNIPS) for counterfactual evaluation. We validate our approach on the AuctionNet simulation benchmark and against 2-weeks online A/B test from a large-scale industrial platform. Our method shows remarkable alignment with online results, achieving a 92\% Mean Directional Accuracy (MDA) in CTR prediction, significantly outperforming the parametric baseline. MDA is the most critical metric for guiding deployment decisions, as it reflects the ability to correctly predict whether a new model will improve or harm performance. This work contributes the first practical and validated framework for reliable OPE in deterministic auction environments, offering an efficient alternative to costly and risky online experiments.",0,arxiv,Ä°statistik,CC-BY/arXiv,Breaking Determinism: Stochastic Modeling for Reliable Off-Policy Evaluation in Ad Auctions
"Federated Learning (FL) is plagued by two key challenges: high communication overhead and performance collapse on heterogeneous (non-IID) data. Analytic FL (AFL) provides a single-round, data distribution invariant solution, but is limited to linear models. Subsequent non-linear approaches, like DeepAFL, regain accuracy but sacrifice the single-round benefit. In this work, we break this trade-off. We propose SAFLe, a framework that achieves scalable non-linear expressivity by introducing a structured head of bucketed features and sparse, grouped embeddings. We prove this non-linear architecture is mathematically equivalent to a high-dimensional linear regression. This key equivalence allows SAFLe to be solved with AFL's single-shot, invariant aggregation law. Empirically, SAFLe establishes a new state-of-the-art for analytic FL, significantly outperforming both linear AFL and multi-round DeepAFL in accuracy across all benchmarks, demonstrating a highly efficient and scalable solution for federated vision.",0,arxiv,Ä°statistik,CC-BY/arXiv,Single-Round Scalable Analytic Federated Learning
"We introduce Sketch Tomography, an efficient procedure for quantum state tomography based on the classical shadow protocol used for quantum observable estimations. The procedure applies to the case where the ground truth quantum state is a matrix product state (MPS). The density matrix of the ground truth state admits a tensor train ansatz as a result of the MPS assumption, and we estimate the tensor components of the ansatz through a series of observable estimations, thus outputting an approximation of the density matrix. The procedure is provably convergent with a sample complexity that scales quadratically in the system size. We conduct extensive numerical experiments to show that the procedure outputs an accurate approximation to the quantum state. For observable estimation tasks involving moderately large subsystems, we show that our procedure gives rise to a more accurate estimation than the classical shadow protocol. We also show that sketch tomography is more accurate in observable estimation than quantum states trained from the maximum likelihood estimation formulation.",0,arxiv,Ä°statistik,CC-BY/arXiv,Sketch Tomography: Hybridizing Classical Shadow and Matrix Product State
"A major effort in modern high-dimensional statistics has been devoted to the analysis of linear predictors trained on nonlinear feature embeddings via empirical risk minimization (ERM). Gaussian equivalence theory (GET) has emerged as a powerful universality principle in this context: it states that the behavior of high-dimensional, complex features can be captured by Gaussian surrogates, which are more amenable to analysis. Despite its remarkable successes, numerical experiments show that this equivalence can fail even for simple embeddings -- such as polynomial maps -- under general scaling regimes.   We investigate this breakdown in the setting of random feature (RF) models in the quadratic scaling regime, where both the number of features and the sample size grow quadratically with the data dimension. We show that when the target function depends on a low-dimensional projection of the data, such as generalized linear models, GET yields incorrect predictions. To capture the correct asymptotics, we introduce a Conditional Gaussian Equivalent (CGE) model, which can be viewed as appending a low-dimensional non-Gaussian component to an otherwise high-dimensional Gaussian model. This hybrid model retains the tractability of the Gaussian framework and accurately describes RF models in the quadratic scaling regime. We derive sharp asymptotics for the training and test errors in this setting, which continue to agree with numerical simulations even when GET fails.   Our analysis combines general results on CLT for Wiener chaos expansions and a careful two-phase Lindeberg swapping argument. Beyond RF models and quadratic scaling, our work hints at a rich landscape of universality phenomena in high-dimensional ERM.",0,arxiv,Ä°statistik,CC-BY/arXiv,When does Gaussian equivalence fail and how to fix it: Non-universal behavior of random features with quadratic scaling
"Semi-supervised learning (SSL) constructs classifiers from datasets in which only a subset of observations is labelled, a situation that naturally arises because obtaining labels often requires expert judgement or costly manual effort. This motivates methods that integrate labelled and unlabelled data within a learning framework. Most SSL approaches assume that label absence is harmless, typically treated as missing completely at random or ignored, but in practice, the missingness process can be informative, as the chances of an observation being unlabelled may depend on the ambiguity of its feature vector. In such cases, the missingness indicators themselves provide additional information that, if properly modelled, may improve estimation efficiency. The \textbf{SSLfmm} package for R is designed to capture this behaviour by estimating the Bayes' classifier under a finite mixture model in which each component corresponding to a class follows a multivariate normal distribution. It incorporates a mixed-missingness mechanism that combines a missing completely at random (MCAR) component with a (non-ignorable) missing at random (MAR) component, the latter modelling the probability of label missingness as a logistic function of the entropy based on the features. Parameters are estimated via an Expectation--Conditional Maximisation algorithm. In the two-class Gaussian setting with arbitrary covariance matrices, the resulting classifier trained on partially labelled data may, in some cases, achieve a lower misclassification rate than the supervised version in the case where all the labels are known. The package includes a practical tool for modelling and illustrates its performance through simulated examples.",0,arxiv,Ä°statistik,CC-BY/arXiv,SSLfmm: An R Package for Semi-Supervised Learning with a Mixed-Missingness Mechanism in Finite Mixture Models
"The emergence of large language models (LLMs) has sparked much interest in creating LLM-based digital populations that can be applied to many applications such as social simulation, crowdsourcing, marketing, and recommendation systems. A digital population can reduce the cost of recruiting human participants and alleviate many concerns related to human subject study. However, research has found that most of the existing works rely solely on LLMs and could not sufficiently capture the accuracy and diversity of a real human population. To address this limitation, we propose CrowdLLM that integrates pretrained LLMs and generative models to enhance the diversity and fidelity of the digital population. We conduct theoretical analysis of CrowdLLM regarding its great potential in creating cost-effective, sufficiently representative, scalable digital populations that can match the quality of a real crowd. Comprehensive experiments are also conducted across multiple domains (e.g., crowdsourcing, voting, user rating) and simulation studies which demonstrate that CrowdLLM achieves promising performance in both accuracy and distributional fidelity to human data.",0,arxiv,Ä°statistik,CC-BY/arXiv,CrowdLLM: Building LLM-Based Digital Populations Augmented with Generative Models
"We frame novelty detection on path space as a hypothesis testing problem with signature-based test statistics. Using transportation-cost inequalities of Gasteratos and Jacquier (2023), we obtain tail bounds for false positive rates that extend beyond Gaussian measures to laws of RDE solutions with smooth bounded vector fields, yielding estimates of quantiles and p-values. Exploiting the shuffle product, we derive exact formulae for smooth surrogates of conditional value-at-risk (CVaR) in terms of expected signatures, leading to new one-class SVM algorithms optimising smooth CVaR objectives. We then establish lower bounds on type-$\mathrm{II}$ error for alternatives with finite first moment, giving general power bounds when the reference measure and the alternative are absolutely continuous with respect to each other. Finally, we evaluate numerically the type-$\mathrm{I}$ error and statistical power of signature-based test statistic, using synthetic anomalous diffusion data and real-world molecular biology data.",0,arxiv,Ä°statistik,CC-BY/arXiv,Novelty detection on path space
"High quality data is needed to unlock the full potential of AI for end users. However finding new sources of such data is getting harder: most publicly-available human generated data will soon have been used. Additionally, publicly available data often is not representative of users of a particular system -- for example, a research speech dataset of contractors interacting with an AI assistant will likely be more homogeneous, well articulated and self-censored than real world commands that end users will issue. Therefore unlocking high-quality data grounded in real user interactions is of vital interest. However, the direct use of user data comes with significant privacy risks. Differential Privacy (DP) is a well established framework for reasoning about and limiting information leakage, and is a gold standard for protecting user privacy. The focus of this work, \emph{Differentially Private Synthetic data}, refers to synthetic data that preserves the overall trends of source data,, while providing strong privacy guarantees to individuals that contributed to the source dataset. DP synthetic data can unlock the value of datasets that have previously been inaccessible due to privacy concerns and can replace the use of sensitive datasets that previously have only had rudimentary protections like ad-hoc rule-based anonymization.   In this paper we explore the full suite of techniques surrounding DP synthetic data, the types of privacy protections they offer and the state-of-the-art for various modalities (image, tabular, text and decentralized). We outline all the components needed in a system that generates DP synthetic data, from sensitive data handling and preparation, to tracking the use and empirical privacy testing. We hope that work will result in increased adoption of DP synthetic data, spur additional research and increase trust in DP synthetic data approaches.",0,arxiv,Ä°statistik,CC-BY/arXiv,How to DP-fy Your Data: A Practical Guide to Generating Synthetic Data With Differential Privacy
"We introduce iterative tilting, a gradient-free method for fine-tuning diffusion models toward reward-tilted distributions. The method decomposes a large reward tilt $\exp(Î»r)$ into $N$ sequential smaller tilts, each admitting a tractable score update via first-order Taylor expansion. This requires only forward evaluations of the reward function and avoids backpropagating through sampling chains. We validate on a two-dimensional Gaussian mixture with linear reward, where the exact tilted distribution is available in closed form.",0,arxiv,Ä°statistik,CC-BY/arXiv,Iterative Tilting for Diffusion Fine-Tuning
"We investigate the convergence properties of a class of iterative algorithms designed to minimize a potentially non-smooth and noisy objective function, which may be algebraically intractable and whose values may be obtained as the output of a black box. The algorithms considered can be cast under the umbrella of a generalised gradient descent recursion, where the gradient is that of a smooth approximation of the objective function. The framework we develop includes as special cases model-based and mollification methods, two classical approaches to zero-th order optimisation. The convergence results are obtained under very weak assumptions on the regularity of the objective function and involve a trade-off between the degree of smoothing and size of the steps taken in the parameter updates. As expected, additional assumptions are required in the stochastic case. We illustrate the relevance of these algorithms and our convergence results through a challenging classification example from machine learning.",0,arxiv,Ä°statistik,CC-BY/arXiv,"Convergence of a class of gradient-free optimisation schemes when the objective function is noisy, irregular, or both"
"We study estimation and statistical inference for reward models used in aligning large language models (LLMs). A key component of LLM alignment is reinforcement learning from human feedback (RLHF), where humans compare pairs of model-generated answers and their preferences are used to train a reward model. However, human feedback is inherently heterogeneous, creating significant challenges for reliable reward learning. To address this, we adopt a heterogeneous preference framework that jointly models the latent reward of answers and human rationality. This leads to a challenging biconvex optimization problem, which we solve via an alternating gradient descent algorithm. We establish theoretical guarantees for the resulting estimator, including its convergence and asymptotic distribution. These results enable the construction of confidence intervals for reward estimates. Leveraging these uncertainty quantification results, we conduct valid statistical comparisons between rewards and incorporate uncertainty into the best-of-$N$ (BoN) policy framework. Extensive simulations demonstrate the effectiveness of our method, and applications to real LLM data highlight the practical value of accounting for uncertainty in reward modeling for LLM alignment.",0,arxiv,Ä°statistik,CC-BY/arXiv,Uncertainty Quantification for Large Language Model Reward Learning under Heterogeneous Human Feedback
"This paper develops new identification results for multidimensional continuous measurement-error models where all observed measurements are contaminated by potentially correlated errors and none provides an injective mapping of the latent distribution. Using third order cross moments, the paper constructs a three way tensor whose unique decomposition, guaranteed by Kruskal theorem, identifies the factor loading matrices. Starting with a linear structure, the paper recovers the full distribution of latent factors by constructing suitable measurements and applying scalar or multivariate versions of Kotlarski identity. As a result, the joint distribution of the latent vector and measurement errors is fully identified without requiring injective measurements, showing that multivariate latent structure can be recovered in broader settings than previously believed. Under injectivity, the paper also provides user-friendly testable conditions for identification. Finally, this paper provides general identification results for nonlinear models using a newly-defined generalized Kruskal rank - signal rank - of intergral operators. These results have wide applicability in empirical work involving noisy or indirect measurements, including factor models, survey data with reporting errors, mismeasured regressors in econometrics, and multidimensional latent-trait models in psychology and marketing, potentially enabling more robust estimation and interpretation when clean measurements are unavailable.",0,arxiv,Ä°statistik,CC-BY/arXiv,Identification of Multivariate Measurement Error Models
"This paper is concerned with the problem of how to speed up computation for Gaussian process models trained on autocorrelated data. The Gaussian process model is a powerful tool commonly used in nonlinear regression applications. Standard regression modeling assumes random samples and an independently, identically distributed noise. Various fast approximations that speed up Gaussian process regression work under this standard setting. But for autocorrelated data, failing to account for autocorrelation leads to a phenomenon known as temporal overfitting that deteriorates model performance on new test instances. To handle autocorrelated data, existing fast Gaussian process approximations have to be modified; one such approach is to segment the originally correlated data points into blocks in which the blocked data are de-correlated. This work explains how to make some of the existing Gaussian process approximations work with blocked data. Numerical experiments across diverse application datasets demonstrate that the proposed approaches can remarkably accelerate computation for Gaussian process regression on autocorrelated data without compromising model prediction performance.",0,arxiv,Ä°statistik,CC-BY/arXiv,Fast Gaussian Process Approximations for Autocorrelated Data
"In this work, we develop a hypothesis testing framework to determine whether pairwise comparison data is generated by an underlying \emph{generalized Thurstone model} $\mathcal{T}_F$ for a given choice function $F$. While prior work has predominantly focused on parameter estimation and uncertainty quantification for such models, we address the fundamental problem of minimax hypothesis testing for $\mathcal{T}_F$ models. We formulate this testing problem by introducing a notion of separation distance between general pairwise comparison models and the class of $\mathcal{T}_F$ models. We then derive upper and lower bounds on the critical threshold for testing that depend on the topology of the observation graph. For the special case of complete observation graphs, this threshold scales as $Î˜((nk)^{-1/2})$, where $n$ is the number of agents and $k$ is the number of comparisons per pair. Furthermore, we propose a hypothesis test based on our separation distance, construct confidence intervals, establish time-uniform bounds on the probabilities of type I and II errors using reverse martingale techniques, and derive minimax lower bounds using information-theoretic methods. Finally, we validate our results through experiments on synthetic and real-world datasets.",0,arxiv,Ä°statistik,CC-BY/arXiv,Hypothesis Testing for Generalized Thurstone Models
"Many modern datasets consist of multiple related matrices measured on a common set of units, where the goal is to recover the shared low-dimensional subspace. While the Angle-based Joint and Individual Variation Explained (AJIVE) framework provides a solution, it relies on equal-weight aggregation, which can be strictly suboptimal when views exhibit significant statistical heterogeneity (arising from varying SNR and dimensions) and structural heterogeneity (arising from individual components). In this paper, we propose HeteroJIVE, a weighted two-stage spectral algorithm tailored to such heterogeneity. Theoretically, we first revisit the ``non-diminishing"" error barrier with respect to the number of views $K$ identified in recent literature for the equal-weight case. We demonstrate that this barrier is not universal: under generic geometric conditions, the bias term vanishes and our estimator achieves the $O(K^{-1/2})$ rate without the need for iterative refinement. Extending this to the general-weight case, we establish error bounds that explicitly disentangle the two layers of heterogeneity. Based on this, we derive an oracle-optimal weighting scheme implemented via a data-driven procedure. Extensive simulations corroborate our theoretical findings, and an application to TCGA-BRCA multi-omics data validates the superiority of HeteroJIVE in practice.",0,arxiv,Ä°statistik,CC-BY/arXiv,HeteroJIVE: Joint Subspace Estimation for Heterogeneous Multi-View Data
"Contrastive learning is among the most popular and powerful approaches for self-supervised representation learning, where the goal is to map semantically similar samples close together while separating dissimilar ones in the latent space. Existing theoretical methods assume that downstream task classes are drawn from the same latent class distribution used during the pretraining phase. However, in real-world settings, downstream tasks may not only exhibit distributional shifts within the same label space but also introduce new or broader label spaces, leading to domain generalization challenges. In this work, we introduce novel generalization bounds that explicitly account for both types of mismatch: domain shift and domain generalization. Specifically, we analyze scenarios where downstream tasks either (i) draw classes from the same latent class space but with shifted distributions, or (ii) involve new label spaces beyond those seen during pretraining. Our analysis reveals how the performance of contrastively learned representations depends on the statistical discrepancy between pretraining and downstream distributions. This extended perspective allows us to derive provable guarantees on the performance of learned representations on average classification tasks involving class distributions outside the pretraining latent class set.",0,arxiv,Ä°statistik,CC-BY/arXiv,Revisiting Theory of Contrastive Learning for Domain Generalization
"Model-based filtering is often carried out while subject to an imperfect model, as learning partially-observable stochastic systems remains a challenge. Recent work on Bayesian inference found that tempering the likelihood or full posterior of an imperfect model can improve predictive accuracy, as measured by expected negative log likelihood. In this paper, we develop the tempered Bayes filter, improving estimation performance through both of the aforementioned, and one newly introduced, modalities. The result admits a recursive implementation with a computational complexity no higher than that of the original Bayes filter. Our analysis reveals that -- besides the well-known fact in the field of Bayesian inference that likelihood tempering affects the balance between prior and likelihood -- full-posterior tempering tunes the level of entropy in the final belief distribution. We further find that a region of the tempering space can be understood as interpolating between the Bayes- and MAP filters, recovering these as special cases. Analytical results further establish conditions under which a tempered Bayes filter achieves improved predictive performance. Specializing the results to the linear Gaussian case, we obtain the tempered Kalman filter. In this context, we interpret how the parameters affect the Kalman state estimate and covariance propagation. Empirical results confirm that our method consistently improves predictive accuracy over the Bayes filter baseline.",0,arxiv,Ä°statistik,CC-BY/arXiv,Tempering the Bayes Filter towards Improved Model-Based Estimation
"We present the winning strategy for the EVA2025 Data Challenge, which aimed to estimate the probability of extreme precipitation events. These events occurred at most once in the dataset making the challenge fundamentally one of extrapolating extreme values. Given the scarcity of extreme events, we argue that a simple, robust modeling approach is essential. We adopt univariate models instead of multivariate ones and model Peaks Over Thresholds using Extreme Value Theory. Specifically, we fit an exponential distribution to model exceedances of the target variable above a high quantile (after seasonal adjustment). The novelty of our approach lies in using martingale testing to evaluate the extrapolation power of the procedure and to agnostically select the level of the high quantile. While this method has several limitations, we believe that framing extrapolation as a game opens the door to other agnostic approaches in Extreme Value Analysis.",0,arxiv,Ä°statistik,CC-BY/arXiv,Assessing Extrapolation of Peaks Over Thresholds with Martingale Testing
"To address the scalability limitations of Gaussian process (GP) regression, several approximation techniques have been proposed. One such method is based on tensor networks, which utilizes an exponential number of basis functions without incurring exponential computational cost. However, extending this model to a fully probabilistic formulation introduces several design challenges. In particular, for tensor train (TT) models, it is unclear which TT-core should be treated in a Bayesian manner. We introduce a Bayesian tensor train kernel machine that applies Laplace approximation to estimate the posterior distribution over a selected TT-core and employs variational inference (VI) for precision hyperparameters. Experiments show that core selection is largely independent of TT-ranks and feature structure, and that VI replaces cross-validation while offering up to 65x faster training. The method's effectiveness is demonstrated on an inverse dynamics problem.",0,arxiv,Ä°statistik,CC-BY/arXiv,Laplace Approximation For Tensor Train Kernel Machines In System Identification
"Shapley values, a gold standard for feature attribution in Explainable AI, face two primary challenges. First, the canonical Shapley framework assumes that the worth function is additive, yet real-world payoff constructions--driven by non-Gaussian distributions, heavy tails, feature dependence, or domain-specific loss scales--often violate this assumption, leading to distorted attributions. Secondly, achieving sparse explanations in high dimensions by computing dense Shapley values and then applying ad hoc thresholding is prohibitively costly and risks inconsistency. We introduce Sparse Isotonic Shapley Regression (SISR), a unified nonlinear explanation framework. SISR simultaneously learns a monotonic transformation to restore additivity--obviating the need for a closed-form specification--and enforces an L0 sparsity constraint on the Shapley vector, enhancing computational efficiency in large feature spaces. Its optimization algorithm leverages Pool-Adjacent-Violators for efficient isotonic regression and normalized hard-thresholding for support selection, yielding implementation ease and global convergence guarantees. Analysis shows that SISR recovers the true transformation in a wide range of scenarios and achieves strong support recovery even in high noise. Moreover, we are the first to demonstrate that irrelevant features and inter-feature dependencies can induce a true payoff transformation that deviates substantially from linearity. Experiments in regression, logistic regression, and tree ensembles demonstrate that SISR stabilizes attributions across payoff schemes, correctly filters irrelevant features while standard Shapley values suffer severe rank and sign distortions. By unifying nonlinear transformation estimation with sparsity pursuit, SISR advances the frontier of nonlinear explainability, providing a theoretically grounded and practical attribution framework.",0,arxiv,Ä°statistik,CC-BY/arXiv,Beyond Additivity: Sparse Isotonic Shapley Regression toward Nonlinear Explainability
"Zero-shot anomaly classification and segmentation (AC/AS) aim to detect anomalous samples and regions without any training data, a capability increasingly crucial in industrial inspection and medical imaging. This dissertation aims to investigate the core challenges of zero-shot AC/AS and presents principled solutions rooted in theory and algorithmic design.   We first formalize the problem of consistent anomalies, a failure mode in which recurring similar anomalies systematically bias distance-based methods. By analyzing the statistical and geometric behavior of patch representations from pre-trained Vision Transformers, we identify two key phenomena - similarity scaling and neighbor-burnout - that describe how relationships among normal patches change with and without consistent anomalies in settings characterized by highly similar objects.   We then introduce CoDeGraph, a graph-based framework for filtering consistent anomalies built on the similarity scaling and neighbor-burnout phenomena. Through multi-stage graph construction, community detection, and structured refinement, CoDeGraph effectively suppresses the influence of consistent anomalies.   Next, we extend this framework to 3D medical imaging by proposing a training-free, computationally efficient volumetric tokenization strategy for MRI data. This enables a genuinely zero-shot 3D anomaly detection pipeline and shows that volumetric anomaly segmentation is achievable without any 3D training samples.   Finally, we bridge batch-based and text-based zero-shot methods by demonstrating that CoDeGraph-derived pseudo-masks can supervise prompt-driven vision-language models. Together, this dissertation provides theoretical understanding and practical solutions for the zero-shot AC/AS problem.",0,arxiv,Ä°statistik,CC-BY/arXiv,On the Problem of Consistent Anomalies in Zero-Shot Anomaly Detection
"Inverse problems arise across scientific and engineering domains, where the goal is to infer hidden parameters or physical fields from indirect and noisy observations. Classical approaches, such as variational regularization and Bayesian inference, provide well established theoretical foundations for handling ill posedness. However, these methods often become computationally restrictive in high dimensional settings or when the forward model is governed by complex physics. Physics Informed Neural Networks (PINNs) have recently emerged as a promising framework for solving inverse problems by embedding physical laws directly into the training process of neural networks. In this paper, we introduce a new perspective on the Bayesian Physics Informed Neural Network (BPINN) framework, extending classical PINNs by explicitly incorporating training data generation, modeling and measurement uncertainties through Bayesian prior modeling and doing inference with the posterior laws. Also, as we focus on the inverse problems, we call this method BPINN-IP, and we show that the standard PINN formulation naturally appears as its special case corresponding to the Maximum A Posteriori (MAP) estimate. This unified formulation allows simultaneous exploitation of physical constraints, prior knowledge, and data-driven inference, while enabling uncertainty quantification through posterior distributions. To demonstrate the effectiveness of the proposed framework, we consider inverse problems arising in infrared image processing, including deconvolution and super-resolution, and present results on both simulated and real industrial data.",0,arxiv,Ä°statistik,CC-BY/arXiv,Bayesian Physics-Informed Neural Networks for Inverse Problems (BPINN-IP): Application in Infrared Image Processing
"Agentic AI systems execute a sequence of actions, such as reasoning steps or tool calls, in response to a user prompt. To evaluate the success of their trajectories, researchers have developed verifiers, such as LLM judges and process-reward models, to score the quality of each action in an agent's trajectory. Although these heuristic scores can be informative, there are no guarantees of correctness when used to decide whether an agent will yield a successful output. Here, we introduce e-valuator, a method to convert any black-box verifier score into a decision rule with provable control of false alarm rates. We frame the problem of distinguishing successful trajectories (that is, a sequence of actions that will lead to a correct response to the user's prompt) and unsuccessful trajectories as a sequential hypothesis testing problem. E-valuator builds on tools from e-processes to develop a sequential hypothesis test that remains statistically valid at every step of an agent's trajectory, enabling online monitoring of agents over arbitrarily long sequences of actions. Empirically, we demonstrate that e-valuator provides greater statistical power and better false alarm rate control than other strategies across six datasets and three agents. We additionally show that e-valuator can be used for to quickly terminate problematic trajectories and save tokens. Together, e-valuator provides a lightweight, model-agnostic framework that converts verifier heuristics into decisions rules with statistical guarantees, enabling the deployment of more reliable agentic systems.",0,arxiv,Ä°statistik,CC-BY/arXiv,E-valuator: Reliable Agent Verifiers with Sequential Hypothesis Testing
"Large language models (LLMs) produce fluent but unsupported answers - hallucinations - limiting safe deployment in high-stakes domains. We propose ECLIPSE, a framework that treats hallucination as a mismatch between a model's semantic entropy and the capacity of available evidence. We combine entropy estimation via multi-sample clustering with a novel perplexity decomposition that measures how models use retrieved evidence. We prove that under mild conditions, the resulting entropy-capacity objective is strictly convex with a unique stable optimum. We evaluate on a controlled financial question answering dataset with GPT-3.5-turbo (n=200 balanced samples with synthetic hallucinations), where ECLIPSE achieves ROC AUC of 0.89 and average precision of 0.90, substantially outperforming a semantic entropy-only baseline (AUC 0.50). A controlled ablation with Claude-3-Haiku, which lacks token-level log probabilities, shows AUC dropping to 0.59 with coefficient magnitudes decreasing by 95% - demonstrating that ECLIPSE is a logprob-native mechanism whose effectiveness depends on calibrated token-level uncertainties. The perplexity decomposition features exhibit the largest learned coefficients, confirming that evidence utilization is central to hallucination detection. We position this work as a controlled mechanism study; broader validation across domains and naturally occurring hallucinations remains future work.",0,arxiv,Ä°statistik,CC-BY/arXiv,Detecting AI Hallucinations in Finance: An Information-Theoretic Method Cuts Hallucination Rate by 92%
"Classification of functional data where observations are curves or trajectories poses unique challenges, particularly under severe class imbalance. Traditional Random Forest algorithms, while robust for tabular data, often fail to capture the intrinsic structure of functional observations and struggle with minority class detection. This paper introduces Functional Random Forest with Adaptive Cost-Sensitive Splitting (FRF-ACS), a novel ensemble framework designed for imbalanced functional data classification. The proposed method leverages basis expansions and Functional Principal Component Analysis (FPCA) to represent curves efficiently, enabling trees to operate on low dimensional functional features. To address imbalance, we incorporate a dynamic cost sensitive splitting criterion that adjusts class weights locally at each node, combined with a hybrid sampling strategy integrating functional SMOTE and weighted bootstrapping. Additionally, curve specific similarity metrics replace traditional Euclidean measures to preserve functional characteristics during leaf assignment. Extensive experiments on synthetic and real world datasets including biomedical signals and sensor trajectories demonstrate that FRF-ACS significantly improves minority class recall and overall predictive performance compared to existing functional classifiers and imbalance handling techniques. This work provides a scalable, interpretable solution for high dimensional functional data analysis in domains where minority class detection is critical.",0,arxiv,Ä°statistik,CC-BY/arXiv,Functional Random Forest with Adaptive Cost-Sensitive Splitting for Imbalanced Functional Data Classification
"The stochastic Polyak step size (SPS) has proven to be a promising choice for stochastic gradient descent (SGD), delivering competitive performance relative to state-of-the-art methods on smooth convex and non-convex optimization problems, including deep neural network training. However, extensions of this approach to non-smooth settings remain in their early stages, often relying on interpolation assumptions or requiring knowledge of the optimal solution. In this work, we propose a novel SPS variant, Safeguarded SPS (SPS$_{safe}$), for the stochastic subgradient method, and provide rigorous convergence guarantees for non-smooth convex optimization with no need for strong assumptions. We further incorporate momentum into the update rule, yielding equally tight theoretical results. Comprehensive experiments on convex benchmarks and deep neural networks corroborate our theory: the proposed step size accelerates convergence, reduces variance, and consistently outperforms existing adaptive baselines. Finally, in the context of deep neural network training, our method demonstrates robust performance by addressing the vanishing gradient problem.",0,arxiv,Ä°statistik,CC-BY/arXiv,Safeguarded Stochastic Polyak Step Sizes for Non-smooth Optimization: Robust Performance Without Small (Sub)Gradients
"Boltzmann machines (BMs) are powerful energy-based generative models, but their heavy training cost has largely confined practical use to Restricted BMs (RBMs) trained with an efficient learning method called contrastive divergence. More accurate learning typically requires Markov chain Monte Carlo (MCMC) Boltzmann sampling, but it is time-consuming due to the difficulty of parallelization for more expressive models. To address this limitation, we first propose a new Boltzmann sampler inspired by a quantum-inspired combinatorial optimization called simulated bifurcation (SB). This SB-inspired approach, which we name Langevin SB (LSB), enables parallelized sampling while maintaining accuracy comparable to MCMC. Furthermore, this is applicable not only to RBMs but also to BMs with general couplings. However, LSB cannot control the inverse temperature of the output Boltzmann distribution, which hinders learning and degrades performance. To overcome this limitation, we also developed an efficient method for estimating the inverse temperature during the learning process, which we call conditional expectation matching (CEM). By combining LSB and CEM, we establish an efficient learning framework for BMs with greater expressive power than RBMs. We refer to this framework as sampler-adaptive learning (SAL). SAL opens new avenues for energy-based generative modeling beyond RBMs.",0,arxiv,Ä°statistik,CC-BY/arXiv,Unlocking the Power of Boltzmann Machines by Parallelizable Sampler and Efficient Temperature Estimation
"Generative models have the potential to transform the way we emulate Earth's changing climate. Previous generative approaches rely on weather-scale autoregression for climate emulation, but this is inherently slow for long climate horizons and has yet to demonstrate stable rollouts under nonstationary forcings. Here, we introduce Spatiotemporal Pyramid Flows (SPF), a new class of flow matching approaches that model data hierarchically across spatial and temporal scales. Inspired by cascaded video models, SPF partitions the generative trajectory into a spatiotemporal pyramid, progressively increasing spatial resolution to reduce computation and coupling each stage with an associated timescale to enable direct sampling at any temporal level in the pyramid. This design, together with conditioning each stage on prescribed physical forcings (e.g., greenhouse gases or aerosols), enables efficient, parallel climate emulation at multiple timescales. On ClimateBench, SPF outperforms strong flow matching baselines and pre-trained models at yearly and monthly timescales while offering fast sampling, especially at coarser temporal levels. To scale SPF, we curate ClimateSuite, the largest collection of Earth system simulations to date, comprising over 33,000 simulation-years across ten climate models and the first dataset to include simulations of climate interventions. We find that the scaled SPF model demonstrates good generalization to held-out scenarios across climate models. Together, SPF and ClimateSuite provide a foundation for accurate, efficient, probabilistic climate emulation across temporal scales and realistic future scenarios. Data and code is publicly available at https://github.com/stanfordmlgroup/spf .",0,arxiv,Ä°statistik,CC-BY/arXiv,Spatiotemporal Pyramid Flow Matching for Climate Emulation
"Increasing climate change and habitat loss are driving unprecedented shifts in species distributions. Conservation professionals urgently need timely, high-resolution predictions of biodiversity risks, especially in ecologically diverse regions like Africa. We propose EcoCast, a spatio-temporal model designed for continual biodiversity and climate risk forecasting. Utilizing multisource satellite imagery, climate data, and citizen science occurrence records, EcoCast predicts near-term (monthly to seasonal) shifts in species distributions through sequence-based transformers that model spatio-temporal environmental dependencies. The architecture is designed with support for continual learning to enable future operational deployment with new data streams. Our pilot study in Africa shows promising improvements in forecasting distributions of selected bird species compared to a Random Forest baseline, highlighting EcoCast's potential to inform targeted conservation policies. By demonstrating an end-to-end pipeline from multi-modal data ingestion to operational forecasting, EcoCast bridges the gap between cutting-edge machine learning and biodiversity management, ultimately guiding data-driven strategies for climate resilience and ecosystem conservation throughout Africa.",0,arxiv,Ä°statistik,CC-BY/arXiv,EcoCast: A Spatio-Temporal Model for Continual Biodiversity and Climate Risk Forecasting
"Diffusion models have achieved remarkable success in data-driven learning and in sampling from complex, unnormalized target distributions. Building on this progress, we reinterpret Maximum Entropy Reinforcement Learning (MaxEntRL) as a diffusion model-based sampling problem. We tackle this problem by minimizing the reverse Kullback-Leibler (KL) divergence between the diffusion policy and the optimal policy distribution using a tractable upper bound. By applying the policy gradient theorem to this objective, we derive a modified surrogate objective for MaxEntRL that incorporates diffusion dynamics in a principled way. This leads to simple diffusion-based variants of Soft Actor-Critic (SAC), Proximal Policy Optimization (PPO) and Wasserstein Policy Optimization (WPO), termed DiffSAC, DiffPPO and DiffWPO. All of these methods require only minor implementation changes to their base algorithm. We find that on standard continuous control benchmarks, DiffSAC, DiffPPO and DiffWPO achieve better returns and higher sample efficiency than SAC and PPO.",0,arxiv,Ä°statistik,CC-BY/arXiv,A Diffusion Model Framework for Maximum Entropy Reinforcement Learning
"Safety-critical environments are inherently dynamic. Distribution shifts, emerging vulnerabilities, and evolving requirements demand continuous updates to machine learning models. Yet even benign parameter updates can have unintended consequences, such as catastrophic forgetting in classical models or alignment drift in foundation models. Existing heuristic approaches (e.g., regularization, parameter isolation) can mitigate these effects but cannot certify that updated models continue to satisfy required performance specifications. We address this problem by introducing a framework for provably safe model updates. Our approach first formalizes the problem as computing the largest locally invariant domain (LID): a connected region in parameter space where all points are certified to satisfy a given specification. While exact maximal LID computation is intractable, we show that relaxing the problem to parameterized abstract domains (orthotopes, zonotopes) yields a tractable primal-dual formulation. This enables efficient certification of updates - independent of the data or algorithm used - by projecting them onto the safe domain. Our formulation further allows computation of multiple approximately optimal LIDs, incorporation of regularization-inspired biases, and use of lookahead data buffers. Across continual learning and foundation model fine-tuning benchmarks, our method matches or exceeds heuristic baselines for avoiding forgetting while providing formal safety guarantees.",0,arxiv,Ä°statistik,CC-BY/arXiv,Provably Safe Model Updates
"A central challenge in machine learning is to distinguish genuine structure from chance correlations in high-dimensional data. In this work, we address this issue for the perceptron, a foundational model of neural computation. Specifically, we investigate the relationship between the pattern load $Î±$ and the variable selection ratio $Ï$ for which a simple perceptron can perfectly classify $P = Î±N$ random patterns by optimally selecting $M = ÏN$ variables out of $N$ variables. While the Cover--Gardner theory establishes that a random subset of $ÏN$ dimensions can separate $Î±N$ random patterns if and only if $Î±< 2Ï$, we demonstrate that optimal variable selection can surpass this bound by developing a method, based on the replica method from statistical mechanics, for enumerating the combinations of variables that enable perfect pattern classification. This not only provides a quantitative criterion for distinguishing true structure in the data from spurious regularities, but also yields the storage capacity of associative memory models with sparse asymmetric couplings.",0,arxiv,Ä°statistik,CC-BY/arXiv,Storage capacity of perceptron with variable selection
"Diffusion generative models have emerged as powerful tools for producing synthetic data from an empirically observed distribution. A common approach involves simulating the time-reversal of an Ornstein-Uhlenbeck (OU) process initialized at the true data distribution. Since the score function associated with the OU process is typically unknown, it is approximated using a trained neural network. This approximation, along with finite time simulation, time discretization and statistical approximation, introduce several sources of error whose impact on the generated samples must be carefully understood. Previous analyses have quantified the error between the generated and the true data distributions in terms of Wasserstein distance or Kullback-Leibler (KL) divergence. However, both metrics present limitations: KL divergence requires absolute continuity between distributions, while Wasserstein distance, though more general, leads to error bounds that scale poorly with dimension, rendering them impractical in high-dimensional settings. In this work, we derive an explicit, dimension-free bound on the discrepancy between the generated and the true data distributions. The bound is expressed in terms of a smooth test functional with bounded first and second derivatives. The key novelty lies in the use of this weaker, functional metric to obtain dimension-independent guarantees, at the cost of higher regularity on the test functions. As an application, we formulate and solve a variational problem to minimize the time-discretization error, leading to the derivation of an optimal time-scheduling strategy for the reverse-time diffusion. Interestingly, this scheduler has appeared previously in the literature in a different context; our analysis provides a new justification for its optimality, now grounded in minimizing the discretization bias in generative sampling.",0,arxiv,Ä°statistik,CC-BY/arXiv,Dimension-free error estimate for diffusion model and optimal scheduling
"Decision trees and random forest remain highly competitive for classification on medium-sized, standard datasets due to their robustness, minimal preprocessing requirements, and interpretability. However, a single tree suffers from high estimation variance, while large ensembles reduce this variance at the cost of substantial computational overhead and diminished interpretability. In this paper, we propose Decision Tree Embedding (DTE), a fast and effective method that leverages the leaf partitions of a trained classification tree to construct an interpretable feature representation. By using the sample means within each leaf region as anchor points, DTE maps inputs into an embedding space defined by the tree's partition structure, effectively circumventing the high variance inherent in decision-tree splitting rules. We further introduce an ensemble extension based on additional bootstrap trees, and pair the resulting embedding with linear discriminant analysis for classification. We establish several population-level theoretical properties of DTE, including its preservation of conditional density under mild conditions and a characterization of the resulting classification error. Empirical studies on synthetic and real datasets demonstrate that DTE strikes a strong balance between accuracy and computational efficiency, outperforming or matching random forest and shallow neural networks while requiring only a fraction of their training time in most cases. Overall, the proposed DTE method can be viewed either as a scalable decision tree classifier that improves upon standard split rules, or as a neural network model whose weights are learned from tree-derived anchor points, achieving an intriguing integration of both paradigms.",0,arxiv,Ä°statistik,CC-BY/arXiv,Decision Tree Embedding by Leaf-Means
"In this paper, we investigate a second-order stochastic algorithm for solving large-scale binary classification problems. We propose to make use of a new hybrid stochastic Newton algorithm that includes two weighted components in the Hessian matrix estimation: the first one coming from the natural Hessian estimate and the second associated with the stochastic gradient information. Our motivation comes from the fact that both parts evaluated at the true parameter of logistic regression, are equal to the Hessian matrix. This new formulation has several advantages and it enables us to prove the almost sure convergence of our stochastic algorithm to the true parameter. Moreover, we significantly improve the almost sure rate of convergence to the Hessian matrix. Furthermore, we establish the central limit theorem for our hybrid stochastic Newton algorithm. Finally, we show a surprising result on the almost sure convergence of the cumulative excess risk.",0,arxiv,Ä°statistik,CC-BY/arXiv,An hybrid stochastic Newton algorithm for logistic regression
"Bipartite networks are widely used to encode the ecological interactions. Being able to compare the organization of bipartite networks is a first step toward a better understanding of how environmental factors shape community structure and resilience. Yet current methods for structure detection in bipartite networks overlook shared patterns across collections of networks. We introduce the \emph{colBiSBM}, a family of probabilistic models for collections of bipartite networks that extends the classical Latent Block Model (LBM). The proposed framework assumes that networks are independent realizations of a shared mesoscale structure, encoded through common inter-block connectivity parameters. We establish identifiability conditions for the different variants of \emph{colBiSBM} and develop a variational EM algorithm for parameter estimation, coupled with an adaptation of the Integrated Classification Likelihood (ICL) criterion for model selection. We demonstrate how our approach can be used to classify networks based on their topology or organization. Simulation studies highlight the ability of \emph{colBiSBM} to recover common structures, improve clustering performance, and enhance link prediction by borrowing strength across networks. An application to plant--pollinator networks highlights how the method uncovers shared ecological roles and partitions networks into sub-collections with similar connectivity patterns. These results illustrate the methodological and practical advantages of joint modeling over separate network analyses in the study of bipartite systems.",0,arxiv,Ä°statistik,CC-BY/arXiv,Common Structure Discovery in Collections of Bipartite Networks: Application to Pollination Systems
"Learning the structure of a Bayesian network from decentralized data poses two major challenges: (i) ensuring rigorous privacy guarantees for participants, and (ii) avoiding communication costs that scale poorly with dimensionality. In this work, we introduce Fed-Sparse-BNSL, a novel federated method for learning linear Gaussian Bayesian network structures that addresses both challenges. By combining differential privacy with greedy updates that target only a few relevant edges per participant, Fed-Sparse-BNSL efficiently uses the privacy budget while keeping communication costs low. Our careful algorithmic design preserves model identifiability and enables accurate structure estimation. Experiments on synthetic and real datasets demonstrate that Fed-Sparse-BNSL achieves utility close to non-private baselines while offering substantially stronger privacy and communication efficiency.",0,arxiv,Ä°statistik,CC-BY/arXiv,Differentially Private and Federated Structure Learning in Bayesian Networks
"Post-training quantization (PTQ) aims to preserve model-level behavior; however, most methods focus on individual linear layers. Even recent extensions, such as QEP and LoaQ, which mitigate error propagation or target specific submodules, still rely on layer-wise formulations and fail to capture the behavior of larger submodules. We introduce Layer-Projected Coordinate Descent (LPCD), a unified framework that extends PTQ beyond layers by optimizing relaxed objectives across arbitrary submodules and projecting the solutions with layer-wise quantizers. LPCD generalizes existing methods and provides a principled approach to quantizing complex submodules while maintaining the efficiency and compatibility of layer-wise PTQ pipelines. Across diverse LLM architectures and bit-widths, LPCD-based submodule quantization consistently enhances both layer-wise PTQ methods and existing submodule approaches.",0,arxiv,Ä°statistik,CC-BY/arXiv,LPCD: Unified Framework from Layer-Wise to Submodule Quantization
"This paper introduces a comprehensive unified framework for constructing multi-view diffusion geometries through intertwined multi-view diffusion trajectories (MDTs), a class of inhomogeneous diffusion processes that iteratively combine the random walk operators of multiple data views. Each MDT defines a trajectory-dependent diffusion operator with a clear probabilistic and geometric interpretation, capturing over time the interplay between data views. Our formulation encompasses existing multi-view diffusion models, while providing new degrees of freedom for view interaction and fusion. We establish theoretical properties under mild assumptions, including ergodicity of both the point-wise operator and the process in itself. We also derive MDT-based diffusion distances, and associated embeddings via singular value decompositions. Finally, we propose various strategies for learning MDT operators within the defined operator space, guided by internal quality measures. Beyond enabling flexible model design, MDTs also offer a neutral baseline for evaluating diffusion-based approaches through comparison with randomly selected MDTs. Experiments show the practical impact of the MDT operators in a manifold learning and data clustering context.",0,arxiv,Ä°statistik,CC-BY/arXiv,Multi-view diffusion geometry using intertwined diffusion trajectories
"We consider the problem of generalization of arbitrarily overparameterized two-layer ReLU Neural Networks with univariate input. Recent work showed that under square loss, flat solutions (motivated by flat / stable minima and Edge of Stability phenomenon) provably cannot overfit, but it remains unclear whether the same phenomenon holds for logistic loss. This is a puzzling open problem because existing work on logistic loss shows that gradient descent with increasing step size converges to interpolating solutions (at infinity, for the margin-separable cases). In this paper, we prove that the \emph{flatness implied generalization} is more delicate under logistic loss. On the positive side, we show that flat solutions enjoy near-optimal generalization bounds within a region between the left-most and right-most \emph{uncertain} sets determined by each candidate solution. On the negative side, we show that there exist arbitrarily flat yet overfitting solutions at infinity that are (falsely) certain everywhere, thus certifying that flatness alone is insufficient for generalization in general. We demonstrate the effects predicted by our theory in a well-controlled simulation study.",0,arxiv,Ä°statistik,CC-BY/arXiv,Does Flatness imply Generalization for Logistic Loss in Univariate Two-Layer ReLU Network?
"Discrete event systems are present both in observations of nature, socio economical sciences, and industrial systems. Standard analysis approaches do not usually exploit their dual event / state nature: signals are either modeled as transition event sequences, emphasizing event order alignment, or as categorical or ordinal state timeseries, usually resampled a distorting and costly operation as the observation period and number of events grow. In this work we define state transition event timeseries (STE-ts) and propose a new Selective Temporal Hamming distance (STH) leveraging both transition time and duration-in-state, avoiding costly and distorting resampling on large databases. STH generalizes both resampled Hamming and Jaccard metrics with better precision and computation time, and an ability to focus on multiple states of interest. We validate these benefits on simulated and real-world datasets.",0,arxiv,Ä°statistik,CC-BY/arXiv,"A Selective Temporal Hamming distance to find patterns in state transition event timeseries, at scale"
"We present CLAPS, a posterior-aware conformal regression method that pairs a Last-Layer Laplace Approximation with split-conformal calibration. From the resulting Gaussian posterior, CLAPS defines a simple two-sided posterior CDF score that aligns the conformity metric with the full predictive shape, not just a point estimate. This alignment yields narrower prediction intervals at the same target coverage, especially on small to medium tabular datasets where data are scarce and uncertainty modeling matters. We also provide a lightweight diagnostic suite that separates aleatoric and epistemic components and visualizes posterior behavior, helping practitioners understand why intervals shrink when they do. Across multiple benchmarks using the same MLP backbone, CLAPS consistently attains nominal coverage with improved efficiency and minimal overhead, offering a clear, practical upgrade to residual-based conformal baselines.",0,arxiv,Ä°statistik,CC-BY/arXiv,CLAPS: Posterior-Aware Conformal Intervals via Last-Layer Laplace
"Diffusion-based solvers for partial differential equations (PDEs) are often bottle-necked by slow gradient-based test-time optimization routines that use PDE residuals for loss guidance. They additionally suffer from optimization instabilities and are unable to dynamically adapt their inference scheme in the presence of noisy PDE residuals. To address these limitations, we introduce PRISMA (PDE Residual Informed Spectral Modulation with Attention), a conditional diffusion neural operator that embeds PDE residuals directly into the model's architecture via attention mechanisms in the spectral domain, enabling gradient-descent free inference. In contrast to previous methods that use PDE loss solely as external optimization targets, PRISMA integrates PDE residuals as integral architectural features, making it inherently fast, robust, accurate, and free from sensitive hyperparameter tuning. We show that PRISMA has competitive accuracy, at substantially lower inference costs, compared to previous methods across five benchmark PDEs, especially with noisy observations, while using 10x to 100x fewer denoising steps, leading to 15x to 250x faster inference.",0,arxiv,Ä°statistik,CC-BY/arXiv,Beyond Loss Guidance: Using PDE Residuals as Spectral Attention in Diffusion Neural Operators
"We present a transfer-learning generative downscaling framework to reconstruct fine resolution satellite images from coarse scale inputs. Our approach combines a lightweight U-Net transfer encoder with a diffusion-based generative model. The simpler U-Net is first pretrained on a long time series of coarse resolution data to learn spatiotemporal representations; its encoder is then frozen and transferred to a larger downscaling model as physically meaningful latent features. Our application uses NASA's MERRA-2 reanalysis as the low resolution source domain (50 km) and the GEOS-5 Nature Run (G5NR) as the high resolution target (7 km). Our study area included a large area in Asia, which was made computationally tractable by splitting into two subregions and four seasons. We conducted domain similarity analysis using Wasserstein distances confirmed minimal distributional shift between MERRA-2 and G5NR, validating the safety of parameter frozen transfer. Across seasonal regional splits, our model achieved excellent performance (R2 = 0.65 to 0.94), outperforming comparison models including deterministic U-Nets, variational autoencoders, and prior transfer learning baselines. Out of data evaluations using semivariograms, ACF/PACF, and lag-based RMSE/R2 demonstrated that the predicted downscaled images preserved physically consistent spatial variability and temporal autocorrelation, enabling stable autoregressive reconstruction beyond the G5NR record. These results show that transfer enhanced diffusion models provide a robust and physically coherent solution for downscaling a long time series of coarse resolution images with limited training periods. This advancement has significant implications for improving environmental exposure assessment and long term environmental monitoring.",0,arxiv,Ä°statistik,CC-BY/arXiv,Spatiotemporal Satellite Image Downscaling with Transfer Encoders and Autoregressive Generative Models
"Online learning is the cornerstone of applications like recommendation and advertising systems, where models continuously adapt to shifting data distributions. Model training for such systems is remarkably expensive, a cost that multiplies during hyperparameter search. We introduce a two-stage paradigm to reduce this cost: (1) efficiently identifying the most promising configurations, and then (2) training only these selected candidates to their full potential. Our core insight is that focusing on accurate identification in the first stage, rather than achieving peak performance, allows for aggressive cost-saving measures. We develop novel data reduction and prediction strategies that specifically overcome the challenges of sequential, non-stationary data not addressed by conventional hyperparameter optimization. We validate our framework's effectiveness through a dual evaluation: first on the Criteo 1TB dataset, the largest suitable public benchmark, and second on an industrial advertising system operating at a scale two orders of magnitude larger. Our methods reduce the total hyperparameter search cost by up to 10$\times$ on the public benchmark and deliver significant, validated efficiency gains in the industrial setting.",0,arxiv,Ä°statistik,CC-BY/arXiv,Efficient Hyperparameter Search for Non-Stationary Model Training
"Many online learning algorithms, including classical online PCA methods, enforce explicit normalization steps that discard the evolving norm of the parameter vector. We show that this norm can in fact encode meaningful information about the underlying statistical structure of the problem, and that exploiting this information leads to improved learning behavior. Motivated by this principle, we introduce Implicitly Normalized Online PCA (INO-PCA), an online PCA algorithm that removes the unit-norm constraint and instead allows the parameter norm to evolve dynamically through a simple regularized update. We prove that in the high-dimensional limit the joint empirical distribution of the estimate and the true component converges to a deterministic measure-valued process governed by a nonlinear PDE. This analysis reveals that the parameter norm obeys a closed-form ODE coupled with the cosine similarity, forming an internal state variable that regulates learning rate, stability, and sensitivity to signal-to-noise ratio (SNR). The resulting dynamics uncover a three-way relationship between the norm, SNR, and optimal step size, and expose a sharp phase transition in steady-state performance. Both theoretically and experimentally, we show that INO-PCA consistently outperforms Oja's algorithm and adapts rapidly in non-stationary environments. Overall, our results demonstrate that relaxing norm constraints can be a principled and effective way to encode and exploit problem-relevant information in online learning algorithms.",0,arxiv,Ä°statistik,CC-BY/arXiv,Implicitly Normalized Online PCA: A Regularized Algorithm with Exact High-Dimensional Dynamics
"Mean-field games (MFGs) study the Nash equilibrium of systems with a continuum of interacting agents, which can be formulated as the fixed-point of optimal control problems. They provide a unified framework for a variety of applications, including optimal transport (OT) and generative models. Despite their broad applicability, solving high-dimensional MFGs remains a significant challenge due to fundamental computational and analytical obstacles. In this work, we propose a particle-based deep Flow Matching (FM) method to tackle high-dimensional MFG computation. In each iteration of our proximal fixed-point scheme, particles are updated using first-order information, and a flow neural network is trained to match the velocity of the sample trajectories in a simulation-free manner. Theoretically, in the optimal control setting, we prove that our scheme converges to a stationary point sublinearly, and upgrade to linear (exponential) convergence under additional convexity assumptions. Our proof uses FM to induce an Eulerian coordinate (density-based) from a Lagrangian one (particle-based), and this also leads to certain equivalence results between the two formulations for MFGs when the Eulerian solution is sufficiently regular. Our method demonstrates promising performance on non-potential MFGs and high-dimensional OT problems cast as MFGs through a relaxed terminal-cost formulation.",0,arxiv,Ä°statistik,CC-BY/arXiv,High-dimensional Mean-Field Games by Particle-based Flow Matching
"Foundation models, and in particular large language models, can generate highly informative responses, prompting growing interest in using these ''synthetic'' outputs as data in empirical research and decision-making. This paper introduces the idea of a foundation prior, which shows that model-generated outputs are not as real observations, but draws from the foundation prior induced prior predictive distribution. As such synthetic data reflects both the model's learned patterns and the user's subjective priors, expectations, and biases. We model the subjectivity of the generative process by making explicit the dependence of synthetic outputs on the user's anticipated data distribution, the prompt-engineering process, and the trust placed in the foundation model.   We derive the foundation prior as an exponential-tilted, generalized Bayesian update of the user's primitive prior, where a trust parameter governs the weight assigned to synthetic data. We then show how synthetic data and the associated foundation prior can be incorporated into standard statistical and econometric workflows, and discuss their use in applications such as refining complex models, informing latent constructs, guiding experimental design, and augmenting random-coefficient and partially linear specifications. By treating generative outputs as structured, explicitly subjective priors rather than as empirical observations, the framework offers a principled way to harness foundation models in empirical work while avoiding the conflation of synthetic ''facts'' with real data.",0,arxiv,Ä°statistik,CC-BY/arXiv,Foundation Priors
"We introduce Smart Bayes, a new classification framework that bridges generative and discriminative modeling by integrating likelihood-ratio-based generative features into a logistic-regression-style discriminative classifier. From the generative perspective, Smart Bayes relaxes the fixed unit weights of Naive Bayes by allowing data-driven coefficients on density-ratio features. From a discriminative perspective, it constructs transformed inputs as marginal log-density ratios that explicitly quantify how much more likely each feature value is under one class than another, thereby providing predictors with stronger class separation than the raw covariates. To support this framework, we develop a spline-based estimator for univariate log-density ratios that is flexible, robust, and computationally efficient. Through extensive simulations and real-data studies, Smart Bayes often outperforms both logistic regression and Naive Bayes. Our results highlight the potential of hybrid approaches that exploit generative structure to enhance discriminative performance.",0,arxiv,Ä°statistik,CC-BY/arXiv,Discriminative classification with generative features: bridging Naive Bayes and logistic regression
"Multipurpose batch processes become increasingly popular in manufacturing industries since they adapt to low-volume, high-value products and shifting demands. These processes often operate in a dynamic environment, which faces disturbances such as processing delays and demand changes. To minimise long-term cost and system nervousness (i.e., disruptive changes to schedules), schedulers must design rescheduling strategies to address such disturbances effectively. Existing methods often assume complete look-ahead information over the scheduling horizon. This assumption contrasts with realistic situations where schedulers can only access incomplete look-ahead information. Sticking with existing methods may lead to suboptimal long-term costs and high-level system nervousness. In this work we propose a Bayesian dynamic scheduling method. Our method relies on learning a Bayesian Network from the probability distribution of disturbances. Specifically, the Bayesian Network represents how likely each operation will be impacted by disturbances. During the online execution, when new disturbances become observed, this method updates the posterior distribution and therefore guides the rescheduling strategy. We compare our method with the existing periodic rescheduling strategy (which generates new schedules from scratch at fixed intervals) on four benchmark problems. Computational results show that our method achieves statistically better long-term costs and system nervousness. In the theoretical aspect, we prove that if disturbances are mutually independent, the impact-quantifying variables inherently satisfy the independence assumptions required by Bayesian Networks. As an implication, practitioners can extend the method to other scheduling problems (such as job shop scheduling and continuous processes), given that they define the problem-specific dependencies between operations.",0,arxiv,Ä°statistik,CC-BY/arXiv,Bayesian dynamic scheduling of multipurpose batch processes under incomplete look-ahead information
"Principal Component Analysis (PCA) and K-means constitute fundamental techniques in multivariate analysis. Although they are frequently applied independently or sequentially to cluster observations, the relationship between them, especially when K-means is used to cluster variables rather than observations, has been scarcely explored. This study seeks to address this gap by proposing an innovative method that analyzes the relationship between clusters of variables obtained by applying K-means on transposed data and the principal components of PCA. Our approach involves applying PCA to the original data and K-means to the transposed data set, where the original variables are converted into observations. The contribution of each variable cluster to each principal component is then quantified using measures based on variable loadings. This process provides a tool to explore and understand the clustering of variables and how such clusters contribute to the principal dimensions of variation identified by PCA.",0,arxiv,Ä°statistik,CC-BY/arXiv,An Approach to Variable Clustering: K-means in Transposed Data and its Relationship with Principal Component Analysis
"We study the multi-objective linear contextual bandit problem, where multiple possible conflicting objectives must be optimized simultaneously. We propose \texttt{MOL-TS}, the \textit{first} Thompson Sampling algorithm with Pareto regret guarantees for this problem. Unlike standard approaches that compute an empirical Pareto front each round, \texttt{MOL-TS} samples parameters across objectives and efficiently selects an arm from a novel \emph{effective Pareto front}, which accounts for repeated selections over time. Our analysis shows that \texttt{MOL-TS} achieves a worst-case Pareto regret bound of $\widetilde{O}(d^{3/2}\sqrt{T})$, where $d$ is the dimension of the feature vectors, $T$ is the total number of rounds, matching the best known order for randomized linear bandit algorithms for single objective. Empirical results confirm the benefits of our proposed approach, demonstrating improved regret minimization and strong multi-objective performance.",0,arxiv,Ä°statistik,CC-BY/arXiv,Thompson Sampling for Multi-Objective Linear Contextual Bandit
"We address the problem of causal effect estimation in the presence of hidden confounders using nonparametric instrumental variable (IV) regression. An established approach is to use estimators based on learned spectral features, that is, features spanning the top singular subspaces of the operator linking treatments to instruments. While powerful, such features are agnostic to the outcome variable. Consequently, the method can fail when the true causal function is poorly represented by these dominant singular functions. To mitigate, we introduce Augmented Spectral Feature Learning, a framework that makes the feature learning process outcome-aware. Our method learns features by minimizing a novel contrastive loss derived from an augmented operator that incorporates information from the outcome. By learning these task-specific features, our approach remains effective even under spectral misalignment. We provide a theoretical analysis of this framework and validate our approach on challenging benchmarks.",0,arxiv,Ä°statistik,CC-BY/arXiv,Outcome-Aware Spectral Feature Learning for Instrumental Variable Regression
"Evaluating rare-event forecasts is challenging because standard metrics collapse as event prevalence declines. Measures such as F1-score, AUPRC, MCC, and accuracy induce degenerate thresholds -- converging to zero or one -- and their values become dominated by class imbalance rather than tail discrimination. We develop a family of rare-event-stable (RES) metrics whose optimal thresholds remain strictly interior as the event probability approaches zero, ensuring coherent decision rules under extreme rarity. Simulations spanning event probabilities from 0.01 down to one in a million show that RES metrics maintain stable thresholds, consistent model rankings, and near-complete prevalence invariance, whereas traditional metrics exhibit statistically significant threshold drift and structural collapse. A credit-default application confirms these results: RES metrics yield interpretable probability-of-default cutoffs (4-9%) and remain robust under subsampling, while classical metrics fail operationally. The RES framework provides a principled, prevalence-invariant basis for evaluating extreme-risk forecasts.",0,arxiv,Ä°statistik,CC-BY/arXiv,An Imbalance-Robust Evaluation Framework for Extreme Risk Forecasts
"Differential privacy is increasingly formalized through the lens of hypothesis testing via the robust and interpretable $f$-DP framework, where privacy guarantees are encoded by a baseline Blackwell trade-off function $f_{\infty} = T(P_{\infty}, Q_{\infty})$ involving a pair of distributions $(P_{\infty}, Q_{\infty})$. The problem of choosing the right privacy metric in practice leads to a central question: what is a statistically appropriate baseline $f_{\infty}$ given some prior modeling assumptions? The special case of Gaussian differential privacy (GDP) showed that, under compositions of nearly perfect mechanisms, these trade-off functions exhibit a central limit behavior with a Gaussian limit experiment. Inspired by Le Cam's theory of limits of statistical experiments, we answer this question in full generality in an infinitely divisible setting.   We show that suitable composition experiments $(P_n^{\otimes n}, Q_n^{\otimes n})$ converge to a binary limit experiment $(P_{\infty}, Q_{\infty})$ whose log-likelihood ratio $L = \log(dQ_{\infty} / dP_{\infty})$ is infinitely divisible under $P_{\infty}$. Thus any limiting trade-off function $f_{\infty}$ is determined by an infinitely divisible law $P_{\infty}$, characterized by its Levy--Khintchine triplet, and its Esscher tilt defined by $dQ_{\infty}(x) = e^{x} dP_{\infty}(x)$. This characterizes all limiting baseline trade-off functions $f_{\infty}$ arising from compositions of nearly perfect differentially private mechanisms. Our framework recovers GDP as the purely Gaussian case and yields explicit non-Gaussian limits, including Poisson examples. It also positively resolves the empirical $s^2 = 2k$ phenomenon observed in the GDP paper and provides an optimal mechanism for count statistics achieving asymmetric Poisson differential privacy.",0,arxiv,Ä°statistik,CC-BY/arXiv,Infinitely divisible privacy and beyond I: resolution of the $s^2=2k$ conjecture
"Synthetic data generation is an important tool for privacy-preserving data sharing. While diffusion models have set recent benchmarks, flow matching (FM) offers a promising alternative. This paper presents different ways to implement flow matching for tabular data synthesis. We provide a comprehensive empirical study that compares flow matching (FM and variational FM) with a state-of-the-art diffusion method (TabDDPM and TabSyn) in tabular data synthesis. We evaluate both the standard Optimal Transport (OT) and the Variance Preserving (VP) probability paths, and also compare deterministic and stochastic samplers -- something possible when learning to generate using \textit{variational} flow matching -- characterising the empirical relationship between data utility and privacy risk. Our key findings reveal that flow matching, particularly TabbyFlow, outperforms diffusion baselines. Flow matching methods also achieves better performance with remarkably low function evaluations ($\leq$ 100 steps), offering a substantial computational advantage. The choice of probability path is also crucial, as using the OT path demonstrates superior performance, while VP has potential for producing synthetic data with lower disclosure risk. Lastly, our results show that making flows stochastic not only preserves marginal distributions but, in some instances, enables the generation of high utility synthetic data with reduced disclosure risk.",0,arxiv,Ä°statistik,CC-BY/arXiv,Flow Matching for Tabular Data Synthesis
"Classical statistical inference and learning theory often fail to explain the success of modern neural networks. A key reason is that these models are non-identifiable (singular), violating core assumptions behind PAC bounds and asymptotic normality. Singular learning theory (SLT), a physics-inspired framework grounded in algebraic geometry, has gained popularity for its ability to close this theory-practice gap. In this paper, we empirically study SLT in toy settings relevant to interpretability and phase transitions. First, we understand the SLT free energy $\mathcal{F}_n$ by testing an Arrhenius-style rate hypothesis using both a grokking modulo-arithmetic model and Anthropic's Toy Models of Superposition. Second, we understand the local learning coefficient $Î»_Î±$ by measuring how it scales with problem difficulty across several controlled network families (polynomial regressors, low-rank linear networks, and low-rank autoencoders). Our experiments recover known scaling laws while others yield meaningful deviations from theoretical expectations. Overall, our paper illustrates the many merits of SLT for understanding neural network phase transitions, and poses open research questions for the field.",0,arxiv,Ä°statistik,CC-BY/arXiv,Using physics-inspired Singular Learning Theory to understand grokking & other phase transitions in modern neural networks
"We study a structured permutation scheme for two-sample testing that restricts permutations to single cross-swaps between block-selected representatives. Our analysis yields three main results. First, we provide an exact validity construction that applies to any fixed restricted permutation set. Second, for both the difference of sample means and the unbiased $\widehat{\mathrm{MMD}}^{2}$ estimator, we derive closed-form one-swap increment identities whose conditional variances scale as $O(h^{2})$, in contrast to the $Î˜(h)$ increment variability under full relabeling. This increment-level variance contraction sharpens the Bernstein--Freedman variance proxy and leads to substantially smaller permutation critical values. Third, we obtain explicit, data-dependent expressions for the resulting critical values and statistical power. Together, these results show that block-restricted one-swap permutations can achieve strictly higher power than classical full permutation tests while maintaining exact finite-sample validity, without relying on pessimistic worst-case Lipschitz bounds.",0,arxiv,Ä°statistik,CC-BY/arXiv,Restricted Block Permutation for Two-Sample Testing
"We study the problem of learning disentangled signals from data using non-linear Independent Component Analysis (ICA). Motivated by advances in self-supervised learning, we propose to learn self-sufficient signals: A recovered signal should be able to reconstruct a missing value of its own from all remaining components without relying on any other signals. We formulate this problem as the minimization of a conditional KL divergence. Compared to traditional maximum likelihood estimation, our algorithm is prior-free and likelihood-free, meaning that we do not need to impose any prior on the original signals or any observational model, which often restricts the model's flexibility. To tackle the KL divergence minimization problem, we propose a sequential algorithm that reduces the KL divergence and learns an optimal de-mixing flow model at each iteration. This approach completely avoids the unstable adversarial training, a common issue in minimizing the KL divergence. Experiments on toy and real-world datasets show the effectiveness of our method.",0,arxiv,Ä°statistik,CC-BY/arXiv,Self-sufficient Independent Component Analysis via KL Minimizing Flows
"We investigate the existence of a statistical-computational gap in multiple Gaussian graph alignment. We first generalize a previously established informational threshold from Vassaux and MassouliÃ© (2025) to regimes where the number of observed graphs $p$ may also grow with the number of nodes $n$: when $p \leq O(n/\log(n))$, we recover the results from Vassaux and MassouliÃ© (2025), and $p \geq Î©(n/\log(n))$ corresponds to a regime where the problem is as difficult as aligning one single graph with some unknown ""signal"" graph. Moreover, when $\log p = Ï‰(\log n)$, the informational thresholds for partial and exact recovery no longer coincide, in contrast to the all-or-nothing phenomenon observed when $\log p=O(\log n)$. Then, we provide the first computational barrier in the low-degree framework for (multiple) Gaussian graph alignment. We prove that when the correlation $Ï$ is less than $1$, up to logarithmic terms, low degree non-trivial estimation fails. Our results suggest that the task of aligning $p$ graphs in polynomial time is as hard as the problem of aligning two graphs in polynomial time, up to logarithmic factors. These results characterize the existence of a statistical-computational gap and provide another example in which polynomial-time algorithms cannot handle complex combinatorial bi-dimensional structures.",0,arxiv,Ä°statistik,CC-BY/arXiv,Statistical-computational gap in multiple Gaussian graph alignment
"Diffusion models for continuous state spaces based on Gaussian noising processes are now relatively well understood, as many works have focused on their theoretical analysis. In contrast, results for diffusion models on discrete state spaces remain limited and pose significant challenges, particularly due to their combinatorial structure and their more recent introduction in generative modelling. In this work, we establish new and sharp convergence guarantees for three popular discrete diffusion models (DDMs). Two of these models are designed for finite state spaces and are based respectively on the random walk and the masking process. The third DDM we consider is defined on the countably infinite space $\mathbb{N}^d$ and uses a drifted random walk as its forward process. For each of these models, the backward process can be characterized by a discrete score function that can, in principle, be estimated. However, even with perfect access to these scores, simulating the exact backward process is infeasible, and one must rely on approximations. In this work, we study Euler-type approximations and establish convergence bounds in both Kullback-Leibler divergence and total variation distance for the resulting models, under minimal assumptions on the data distribution. In particular, we show that the computational complexity of each method scales linearly in the dimension, up to logarithmic factors. Furthermore, to the best of our knowledge, this study provides the first non-asymptotic convergence guarantees for these noising processes that do not rely on boundedness assumptions on the estimated score.",0,arxiv,Ä°statistik,CC-BY/arXiv,Non-Asymptotic Convergence of Discrete Diffusion Models: Masked and Random Walk dynamics
"Replicability is a fundamental challenge in reinforcement learning (RL), as RL algorithms are empirically observed to be unstable and sensitive to variations in training conditions. To formally address this issue, we study \emph{list replicability} in the Probably Approximately Correct (PAC) RL framework, where an algorithm must return a near-optimal policy that lies in a \emph{small list} of policies across different runs, with high probability. The size of this list defines the \emph{list complexity}. We introduce both weak and strong forms of list replicability: the weak form ensures that the final learned policy belongs to a small list, while the strong form further requires that the entire sequence of executed policies remains constrained. These objectives are challenging, as existing RL algorithms exhibit exponential list complexity due to their instability. Our main theoretical contribution is a provably efficient tabular RL algorithm that guarantees list replicability by ensuring the list complexity remains polynomial in the number of states, actions, and the horizon length. We further extend our techniques to achieve strong list replicability, bounding the number of possible policy execution traces polynomially with high probability. Our theoretical result is made possible by key innovations including (i) a novel planning strategy that selects actions based on lexicographic order among near-optimal choices within a randomly chosen tolerance threshold, and (ii) a mechanism for testing state reachability in stochastic environments while preserving replicability. Finally, we demonstrate that our theoretical investigation sheds light on resolving the \emph{instability} issue of RL algorithms used in practice. In particular, we show that empirically, our new planning strategy can be incorporated into practical RL frameworks to enhance their stability.",0,arxiv,Ä°statistik,CC-BY/arXiv,List Replicable Reinforcement Learning
"The Influence Maximization (IM) problem aims to select a set of seed nodes within a given budget to maximize the spread of influence in a social network. However, real-world social networks have several structural inequalities, such as dominant majority groups and underrepresented minority groups. If these inequalities are not considered while designing IM algorithms, the outcomes might be biased, disproportionately benefiting majority groups while marginalizing minorities. In this work, we address this gap by designing a fairness-aware IM method using Reinforcement Learning (RL) that ensures equitable influence outreach across all communities, regardless of protected attributes. Fairness is incorporated using a maximin fairness objective, which prioritizes improving the outreach of the least-influenced group, pushing the solution toward an equitable influence distribution. We propose a novel fairness-aware deep RL method, called DQ4FairIM, that maximizes the expected number of influenced nodes by learning an RL policy. The learnt policy ensures that minority groups formulate the IM problem as a Markov Decision Process (MDP) and use deep Q-learning, combined with the Structure2Vec network embedding, earning together with Structure2Vec network embedding to solve the MDP. We perform extensive experiments on synthetic benchmarks and real-world networks to compare our method with fairness-agnostic and fairness-aware baselines. The results show that our method achieves a higher level of fairness while maintaining a better fairness-performance trade-off than baselines. Additionally, our approach learns effective seeding policies that generalize across problem instances without retraining, such as varying the network size or the number of seed nodes.",0,arxiv,Ä°statistik,CC-BY/arXiv,DQ4FairIM: Fairness-aware Influence Maximization using Deep Reinforcement Learning
"Given a training dataset, the goal of dataset distillation is to derive a synthetic dataset such that models trained on the latter perform as well as those trained on the training dataset. In this work, we develop and analyze an efficient dataset distillation algorithm for supervised learning, specifically regression in $\mathbb{R}^d$, based on matching the losses on the training and synthetic datasets with respect to a fixed set of randomly sampled regressors without any model training. Our first key contribution is a novel performance guarantee proving that our algorithm needs only $\tilde{O}(d^2)$ sampled regressors to derive a synthetic dataset on which the MSE loss of any bounded linear model is nearly the same as its MSE loss on the given training data. In particular, the model optimized on the synthetic data has close to minimum loss on the training data, thus performing nearly as well as the model optimized on the latter. Complementing this, we also prove a matching lower bound of $Î©(d^2)$ for the number of sampled regressors showing the tightness of our analysis.   Our second contribution is to extend our algorithm to offline RL dataset distillation by matching the Bellman loss, unlike previous works which used a behavioral cloning objective. This is the first such method which leverages both, the rewards and the next state information, available in offline RL datasets, without any policy model optimization. Our algorithm generates a synthetic dataset whose Bellman loss with respect to any linear action-value predictor is close to the latter's Bellman loss on the offline RL training dataset. Therefore, a policy associated with an action-value predictor optimized on the synthetic dataset performs nearly as well as that derived from the one optimized on the training data. We conduct experiments to validate our theoretical guarantees and observe performance gains.",0,arxiv,Ä°statistik,CC-BY/arXiv,Algorithmic Guarantees for Distilling Supervised and Offline RL Datasets
"Hierarchical clustering is a fundamental machine-learning technique for grouping data points into dendrograms. However, existing hierarchical clustering methods encounter two primary challenges: 1) Most methods specify dendrograms without a global objective. 2) Graph-based methods often neglect the significance of graph structure, optimizing objectives on complete or static predefined graphs. In this work, we propose Hyperbolic Continuous Structural Entropy neural networks, namely HypCSE, for structure-enhanced continuous hierarchical clustering. Our key idea is to map data points in the hyperbolic space and minimize the relaxed continuous structural entropy (SE) on structure-enhanced graphs. Specifically, we encode graph vertices in hyperbolic space using hyperbolic graph neural networks and minimize approximate SE defined on graph embeddings. To make the SE objective differentiable for optimization, we reformulate it into a function using the lowest common ancestor (LCA) on trees and then relax it into continuous SE (CSE) by the analogy of hyperbolic graph embeddings and partitioning trees. To ensure a graph structure that effectively captures the hierarchy of data points for CSE calculation, we employ a graph structure learning (GSL) strategy that updates the graph structure during training. Extensive experiments on seven datasets demonstrate the superior performance of HypCSE.",0,arxiv,Ä°statistik,CC-BY/arXiv,Hyperbolic Continuous Structural Entropy for Hierarchical Clustering
"Sequential optimization of black-box functions from noisy evaluations has been widely studied, with Gaussian Process bandit algorithms such as GP-UCB guaranteeing no-regret in stationary settings. However, for time-varying objectives, it is known that no-regret is unattainable under pure bandit feedback unless strong and often unrealistic assumptions are imposed.   In this article, we propose a novel method to optimize time-varying rewards in the frequentist setting, where the objective has bounded RKHS norm. Time variations are captured through uncertainty injection (UI), which enables heteroscedastic GP regression that adapts past observations to the current time step. As no-regret is unattainable in general in the strict bandit setting, we relax the latter allowing additional queries on previously observed points. Building on sparse inference and the effect of UI on regret, we propose W-SparQ-GP-UCB, an online algorithm that achieves no-regret with only a vanishing number of additional queries per iteration. To assess the theoretical limits of this approach, we establish a lower bound on the number of additional queries required for no-regret, proving the efficiency of our method. Finally, we provide a comprehensive analysis linking the degree of time-variation of the function to achievable regret rates, together with upper and lower bounds on the number of additional queries needed in each regime.",0,arxiv,Ä°statistik,CC-BY/arXiv,No-Regret Gaussian Process Optimization of Time-Varying Functions
"Large language model (LLM) reinforcement learning has increasingly relied on group-based policy optimization frameworks, such as GRPO and GSPO, to achieve stable fine-tuning at scale. However, a fundamental trade-off persists between optimization granularity and training stability. While GSPO improves robustness via sequence-level optimization, its monolithic treatment of sequences introduces severe inefficiencies: its conservative clipping mechanism indiscriminately discards valid training samples-a phenomenon we term gradient underutilization-and its uniform credit assignment fails to capture the heterogeneous contributions of critical reasoning steps. In this work, we propose Entropy Importance Sampling Policy Optimization (ESPO), a novel framework that reconciles fine-grained control with training stability. ESPO decomposes sequences into groups based on predictive entropy, enabling (1) Entropy-driven Importance Sampling to capture intra-sequence heterogeneity, and (2) Entropy-adaptive Clipping to dynamically allocate trust regions based on model uncertainty. Extensive experiments on mathematical reasoning benchmarks demonstrate that ESPO not only accelerates convergence but also achieves state-of-the-art performance, notably improving accuracy on the challenging HMMT benchmark from 4.4% to 13.13%.",0,arxiv,Ä°statistik,CC-BY/arXiv,ESPO: Entropy Importance Sampling Policy Optimization
"Data privacy is a critical challenge in modern medical workflows as the adoption of electronic patient records has grown rapidly. Stringent data protection regulations limit access to clinical records for training and integrating machine learning models that have shown promise in improving diagnostic accuracy and personalized care outcomes. Synthetic data offers a promising alternative; however, current generative models either struggle with time-series data or lack formal privacy guaranties. In this paper, we enhance a state-of-the-art time-series generative model to better handle longitudinal clinical data while incorporating quantifiable privacy safeguards. Using real data from chronic kidney disease and ICU patients, we evaluate our method through statistical tests, a Train-on-Synthetic-Test-on-Real (TSTR) setup, and expert clinical review. Our non-private model (Augmented TimeGAN) outperforms transformer- and flow-based models on statistical metrics in several datasets, while our private model (DP-TimeGAN) maintains a mean authenticity of 0.778 on the CKD dataset, outperforming existing state-of-the-art models on the privacy-utility frontier. Both models achieve performance comparable to real data in clinician evaluations, providing robust input data necessary for developing models for complex chronic conditions without compromising data privacy.",0,arxiv,Ä°statistik,CC-BY/arXiv,Privacy-Preserving Generative Modeling and Clinical Validation of Longitudinal Health Records for Chronic Disease
"Random Forests and Gradient Boosting are among the most effective algorithms for supervised learning on tabular data. Both belong to the class of tree-based ensemble methods, where predictions are obtained by aggregating many randomized regression trees. In this paper, we develop a theoretical framework for analyzing such methods through Reproducing Kernel Hilbert Spaces (RKHSs) constructed on tree ensembles -- more precisely, on the random partitions generated by randomized regression trees. We establish fundamental analytical properties of the resulting Random Forest kernel, including boundedness, continuity, and universality, and show that a Random Forest predictor can be characterized as the unique minimizer of a penalized empirical risk functional in this RKHS, providing a variational interpretation of ensemble learning. We further extend this perspective to the continuous-time formulation of Gradient Boosting introduced by Dombry and Duchamps, and demonstrate that it corresponds to a gradient flow on a Hilbert manifold induced by the Random Forest RKHS. A key feature of this framework is that both the kernel and the RKHS geometry are data-dependent, offering a theoretical explanation for the strong empirical performance of tree-based ensembles. Finally, we illustrate the practical potential of this approach by introducing a kernel principal component analysis built on the Random Forest kernel, which enhances the interpretability of ensemble models, as well as GVI, a new geometric variable importance criterion.",0,arxiv,Ä°statistik,CC-BY/arXiv,An RKHS Perspective on Tree Ensembles
"Many emerging applications - such as adversarial training, AI alignment, and robust optimization - can be framed as zero-sum games between neural nets, with von Neumann-Nash equilibria (NE) capturing the desirable system behavior. While such games often involve non-convex non-concave objectives, empirical evidence shows that simple gradient methods frequently converge, suggesting a hidden geometric structure. In this paper, we provide a theoretical framework that explains this phenomenon through the lens of hidden convexity and overparameterization. We identify sufficient conditions - spanning initialization, training dynamics, and network width - that guarantee global convergence to a NE in a broad class of non-convex min-max games. To our knowledge, this is the first such result for games that involve two-layer neural networks. Technically, our approach is twofold: (a) we derive a novel path-length bound for the alternating gradient descent-ascent scheme in min-max games; and (b) we show that the reduction from a hidden convex-concave geometry to two-sided Polyak-Åojasiewicz (PÅ) min-max condition hold with high probability under overparameterization, using tools from random matrix theory.",0,arxiv,Ä°statistik,CC-BY/arXiv,"Solving Neural Min-Max Games: The Role of Architecture, Initialization & Dynamics"
"We take the novel perspective of incorporating offline RL algorithms as subroutines of tabula rasa online RL. This is feasible because an online learning agent can repurpose its historical interactions as offline dataset. We formalize this idea into a framework that accommodates several variants of offline RL incorporation such as final policy recommendation and online fine-tuning. We further introduce convenient techniques to improve its effectiveness in enhancing online learning efficiency. Our extensive and systematic empirical analyses show that 1) the effectiveness of the proposed framework depends strongly on the nature of the task, 2) our proposed techniques greatly enhance its effectiveness, and 3) existing online fine-tuning methods are overall ineffective, calling for more research therein.",0,arxiv,Ä°statistik,CC-BY/arXiv,An Empirical Study on the Effectiveness of Incorporating Offline RL As Online RL Subroutines
"A critical challenge for reinforcement learning (RL) is making decisions based on incomplete and noisy observations, especially in perturbed and partially observable Markov decision processes (P$^2$OMDPs). Existing methods fail to mitigate perturbations while addressing partial observability. We propose \textit{Causal State Representation under Asynchronous Diffusion Model (CaDiff)}, a framework that enhances any RL algorithm by uncovering the underlying causal structure of P$^2$OMDPs. This is achieved by incorporating a novel asynchronous diffusion model (ADM) and a new bisimulation metric. ADM enables forward and reverse processes with different numbers of steps, thus interpreting the perturbation of P$^2$OMDP as part of the noise suppressed through diffusion. The bisimulation metric quantifies the similarity between partially observable environments and their causal counterparts. Moreover, we establish the theoretical guarantee of CaDiff by deriving an upper bound for the value function approximation errors between perturbed observations and denoised causal states, reflecting a principled trade-off between approximation errors of reward and transition-model. Experiments on Roboschool tasks show that CaDiff enhances returns by at least 14.18\% compared to baselines. CaDiff is the first framework that approximates causal states using diffusion models with both theoretical rigor and practicality.",0,arxiv,Ä°statistik,CC-BY/arXiv,Learning Causal States Under Partial Observability and Perturbation
"Multi-agent reinforcement learning (MARL), as a thriving field, explores how multiple agents independently make decisions in a shared dynamic environment. Due to environmental uncertainties, policies in MARL must remain robust to tackle the sim-to-real gap. We focus on robust two-player zero-sum Markov games (TZMGs) in offline settings, specifically on tabular robust TZMGs (RTZMGs). We propose a model-based algorithm (\textit{RTZ-VI-LCB}) for offline RTZMGs, which is optimistic robust value iteration combined with a data-driven Bernstein-style penalty term for robust value estimation. By accounting for distribution shifts in the historical dataset, the proposed algorithm establishes near-optimal sample complexity guarantees under partial coverage and environmental uncertainty. An information-theoretic lower bound is developed to confirm the tightness of our algorithm's sample complexity, which is optimal regarding both state and action spaces. To the best of our knowledge, RTZ-VI-LCB is the first to attain this optimality, sets a new benchmark for offline RTZMGs, and is validated experimentally.",0,arxiv,Ä°statistik,CC-BY/arXiv,Sample-Efficient Tabular Self-Play for Offline Robust Reinforcement Learning
"The thriving field of multi-agent reinforcement learning (MARL) studies how a group of interacting agents make decisions autonomously in a shared dynamic environment. Existing theoretical studies in this area suffer from at least two of the following obstacles: memory inefficiency, the heavy dependence of sample complexity on the long horizon and the large state space, the high computational complexity, non-Markov policy, non-Nash policy, and high burn-in cost. In this work, we take a step towards settling this problem by designing a model-free self-play algorithm \emph{Memory-Efficient Nash Q-Learning (ME-Nash-QL)} for two-player zero-sum Markov games, which is a specific setting of MARL. ME-Nash-QL is proven to enjoy the following merits. First, it can output an $\varepsilon$-approximate Nash policy with space complexity $O(SABH)$ and sample complexity $\widetilde{O}(H^4SAB/\varepsilon^2)$, where $S$ is the number of states, $\{A, B\}$ is the number of actions for two players, and $H$ is the horizon length. It outperforms existing algorithms in terms of space complexity for tabular cases, and in terms of sample complexity for long horizons, i.e., when $\min\{A, B\}\ll H^2$. Second, ME-Nash-QL achieves the lowest computational complexity $O(T\mathrm{poly}(AB))$ while preserving Markov policies, where $T$ is the number of samples. Third, ME-Nash-QL also achieves the best burn-in cost $O(SAB\,\mathrm{poly}(H))$, whereas previous algorithms have a burn-in cost of at least $O(S^3 AB\,\mathrm{poly}(H))$ to attain the same level of sample complexity with ours.",0,arxiv,Ä°statistik,CC-BY/arXiv,Provable Memory Efficient Self-Play Algorithm for Model-free Reinforcement Learning
"The analysis of non-real-valued data, such as binary time series, has attracted great interest in recent years. This manuscript proposes a post-selection estimator for estimating the coefficient matrices of a high-dimensional generalized binary vector autoregressive process and establishes a Gaussian approximation theorem for the proposed estimator. Furthermore, it introduces a second-order wild bootstrap algorithm to enable statistical inference on the coefficient matrices. Numerical studies and empirical applications demonstrate the good finite-sample performance of the proposed method.",0,arxiv,Ä°statistik,CC-BY/arXiv,On Statistical Inference for High-Dimensional Binary Time Series
"Kolmogorov-Arnold Networks (KANs) offer a promising alternative to Multi-Layer Perceptron (MLP) by placing learnable univariate functions on network edges, enhancing interpretability. However, standard KANs lack probabilistic outputs, limiting their utility in applications requiring uncertainty quantification. While recent Gaussian Process (GP) extensions to KANs address this, they utilize exact inference methods that scale cubically with data size N, restricting their application to smaller datasets. We introduce the Sparse Variational GP-KAN (SVGP-KAN), an architecture that integrates sparse variational inference with the KAN topology. By employing $M$ inducing points and analytic moment matching, our method reduces computational complexity from $O(N^3)$ to $O(NM^2)$ or linear in sample size, enabling the application of probabilistic KANs to larger scientific datasets. Furthermore, we demonstrate that integrating a permutation-based importance analysis enables the network to function as a framework for structural identification, identifying relevant inputs and classifying functional relationships.",0,arxiv,Ä°statistik,CC-BY/arXiv,Scalable and Interpretable Scientific Discovery via Sparse Variational Gaussian Process Kolmogorov-Arnold Networks (SVGP KAN)
"Data assimilation (DA) is a cornerstone of scientific and engineering applications, combining model forecasts with sparse and noisy observations to estimate latent system states. Classical DA methods, such as the ensemble Kalman filter, rely on Gaussian approximations and heuristic tuning (e.g., inflation and localization) to scale to high dimensions. While often successful, these approximations can make the methods unstable or inaccurate when the underlying distributions of states and observations depart significantly from Gaussianity. To address this limitation, we introduce DAISI, a scalable filtering algorithm built on flow-based generative models that enables flexible probabilistic inference using data-driven priors. The core idea is to use a stationary, pre-trained generative prior to assimilate observations via guidance-based conditional sampling while incorporating forecast information through a novel inverse-sampling step. This step maps the forecast ensemble into a latent space to provide initial conditions for the conditional sampling, allowing us to encode model dynamics into the DA pipeline without having to retrain or fine-tune the generative prior at each assimilation step. Experiments on challenging nonlinear systems show that DAISI achieves accurate filtering results in regimes with sparse, noisy, and nonlinear observations where traditional methods struggle.",0,arxiv,Ä°statistik,CC-BY/arXiv,DAISI: Data Assimilation with Inverse Sampling using Stochastic Interpolants
"Sheaf Neural Networks equip graph structures with a cellular sheaf: a geometric structure which assigns local vector spaces (stalks) and a linear learnable restriction/transport maps to nodes and edges, yielding an edge-aware inductive bias that handles heterophily and limits oversmoothing. However, common Neural Sheaf Diffusion implementations rely on SVD-based sheaf normalization and dense per-edge restriction maps, which scale with stalk dimension, require frequent Laplacian rebuilds, and yield brittle gradients. To address these limitations, we introduce Polynomial Neural Sheaf Diffusion (PolyNSD), a new sheaf diffusion approach whose propagation operator is a degree-K polynomial in a normalised sheaf Laplacian, evaluated via a stable three-term recurrence on a spectrally rescaled operator. This provides an explicit K-hop receptive field in a single layer (independently of the stalk dimension), with a trainable spectral response obtained as a convex mixture of K+1 orthogonal polynomial basis responses. PolyNSD enforces stability via convex mixtures, spectral rescaling, and residual/gated paths, reaching new state-of-the-art results on both homophilic and heterophilic benchmarks, inverting the Neural Sheaf Diffusion trend by obtaining these results with just diagonal restriction maps, decoupling performance from large stalk dimension, while reducing runtime and memory requirements.",0,arxiv,Ä°statistik,CC-BY/arXiv,Polynomial Neural Sheaf Diffusion: A Spectral Filtering Approach on Cellular Sheaves
"The effectiveness of self-supervised learning (SSL) for physiological time series depends on the ability of a pretraining objective to preserve information about the underlying physiological state while filtering out unrelated noise. However, existing strategies are limited due to reliance on heuristic principles or poorly constrained generative tasks. To address this limitation, we propose a pretraining framework that exploits the information structure of a dynamical systems generative model across multiple time-series. This framework reveals our key insight that class identity can be efficiently captured by extracting information about the generative variables related to the system parameters shared across similar time series samples, while noise unique to individual samples should be discarded. Building on this insight, we propose PULSE, a cross-reconstruction-based pretraining objective for physiological time series datasets that explicitly extracts system information while discarding non-transferrable sample-specific ones. We establish theory that provides sufficient conditions for the system information to be recovered, and empirically validate it using a synthetic dynamical systems experiment. Furthermore, we apply our method to diverse real-world datasets, demonstrating that PULSE learns representations that can broadly distinguish semantic classes, increase label efficiency, and improve transfer learning.",0,arxiv,Ä°statistik,CC-BY/arXiv,Self-Supervised Dynamical System Representations for Physiological Time-Series
"Deep neural networks often struggle to recognize when an input lies outside their training experience, leading to unreliable and overconfident predictions. Building dependable machine learning systems therefore requires methods that can both estimate predictive \textit{uncertainty} and detect \textit{out-of-distribution (OOD)} samples in a unified manner. In this paper, we propose \textbf{TIE: a Training--Inversion--Exclusion} framework for visually interpretable and uncertainty-guided anomaly detection that jointly addresses these challenges through iterative refinement. TIE extends a standard $n$-class classifier to an $(n+1)$-class model by introducing a garbage class initialized with Gaussian noise to represent outlier inputs. Within each epoch, TIE performs a closed-loop process of \textit{training, inversion, and exclusion}, where highly uncertain inverted samples reconstructed from the just-trained classifier are excluded into the garbage class. Over successive iterations, the inverted samples transition from noisy artifacts into visually coherent class prototypes, providing transparent insight into how the model organizes its learned manifolds. During inference, TIE rejects OOD inputs by either directly mapping them to the garbage class or producing low-confidence, uncertain misclassifications within the in-distribution classes that are easily separable, all without relying on external OOD datasets. A comprehensive threshold-based evaluation using multiple OOD metrics and performance measures such as \textit{AUROC}, \textit{AUPR}, and \textit{FPR@95\%TPR} demonstrates that TIE offers a unified and interpretable framework for robust anomaly detection and calibrated uncertainty estimation (UE) achieving near-perfect OOD detection with \textbf{\(\!\approx\!\) 0 FPR@95\%TPR} when trained on MNIST or FashionMNIST and tested against diverse unseen datasets.",0,arxiv,Ä°statistik,CC-BY/arXiv,TIE: A Training-Inversion-Exclusion Framework for Visually Interpretable and Uncertainty-Guided Out-of-Distribution Detection
"Adaptively collected data has become ubiquitous within modern practice. However, even seemingly benign adaptive sampling schemes can introduce severe biases, rendering traditional statistical inference tools inapplicable. This can be mitigated by a property called stability, which states that if the rate at which an algorithm takes actions converges to a deterministic limit, one can expect that certain parameters are asymptotically normal. Building on a recent line of work for the multi-armed bandit setting, we show that the linear upper confidence bound (LinUCB) algorithm for linear bandits satisfies this property. In doing so, we painstakingly characterize the behavior of the eigenvalues and eigenvectors of the random design feature covariance matrix in the setting where the action set is the unit ball, showing that it decomposes into a rank-one direction that locks onto the true parameter and an almost-isotropic bulk that grows at a predictable $\sqrt{T}$ rate. This allows us to establish a central limit theorem for the LinUCB algorithm, establishing asymptotic normality for the limiting distribution of the estimation error where the convergence occurs at a $T^{-1/4}$ rate. The resulting Wald-type confidence sets and hypothesis tests do not depend on the feature covariance matrix and are asymptotically tighter than existing nonasymptotic confidence sets. Numerical simulations corroborate our findings.",0,arxiv,Ä°statistik,CC-BY/arXiv,Statistical Inference under Adaptive Sampling with LinUCB
"Classical neural networks are known for their ability to approximate mappings between finite-dimensional spaces, but they fall short in capturing complex operator dynamics across infinite-dimensional function spaces. Neural operators, in contrast, have emerged as powerful tools in scientific machine learning for learning such mappings. However, standard neural operators typically lack mechanisms for mixing or attending to input information across space and time. In this work, we introduce the Banach neural operator (BNO) -- a novel framework that integrates Koopman operator theory with deep neural networks to predict nonlinear, spatiotemporal dynamics from partial observations. The BNO approximates a nonlinear operator between Banach spaces by combining spectral linearization (via Koopman theory) with deep feature learning (via convolutional neural networks and nonlinear activations). This sequence-to-sequence model captures dominant dynamic modes and allows for mesh-independent prediction. Numerical experiments on the Navier-Stokes equations demonstrate the method's accuracy and generalization capabilities. In particular, BNO achieves robust zero-shot super-resolution in unsteady flow prediction and consistently outperforms conventional Koopman-based methods and deep learning models.",0,arxiv,Ä°statistik,CC-BY/arXiv,Banach neural operator for Navier-Stokes equations
"Many tasks require mapping continuous input data (e.g. images) to discrete task outputs (e.g. class labels). Yet, how neural networks learn to perform such discrete computations on continuous data manifolds remains poorly understood. Here, we show that signatures of such computations emerge in the representational geometry of neural networks as they learn. By analysing the Riemannian pullback metric across layers of a neural network, we find that network computation can be decomposed into two functions: discretising continuous input features and performing logical operations on these discretised variables. Furthermore, we demonstrate how different learning regimes (rich vs. lazy) have contrasting metric and curvature structures, affecting the ability of the networks to generalise to unseen inputs. Overall, our work provides a geometric framework for understanding how neural networks learn to perform discrete computations on continuous manifolds.",0,arxiv,Ä°statistik,CC-BY/arXiv,Emergent Riemannian geometry over learning discrete computations on continuous manifolds
"Tabular data drive most real-world machine learning applications, yet building general-purpose models for them remains difficult. Mixed numeric and categorical fields, weak feature structure, and limited labeled data make scaling and generalization challenging. To this end, we introduce Orion-Bix, a tabular foundation model that combines biaxial attention with meta-learned in-context reasoning for few-shot tabular learning. Its encoder alternates standard, grouped, hierarchical, and relational attention, fusing their outputs through multi-CLS summarization to capture both local and global dependencies efficiently. A label-aware ICL head adapts on the fly and scales to large label spaces via hierarchical decision routing. Meta-trained on synthetically generated, structurally diverse tables with causal priors, Orion-Bix learns transferable inductive biases across heterogeneous data. Delivered as a scikit-learn compatible foundation model, it outperforms gradient-boosting baselines and remains competitive with state-of-the-art tabular foundation models on public benchmarks, showing that biaxial attention with episodic meta-training enables robust, few-shot-ready tabular learning. The model is publicly available at https://github.com/Lexsi-Labs/Orion-BiX .",0,arxiv,Ä°statistik,CC-BY/arXiv,Orion-Bix: Bi-Axial Attention for Tabular In-Context Learning
"Identifying causal effects in the presence of unmeasured variables is a fundamental challenge in causal inference, for which proxy variable methods have emerged as a powerful solution. We contrast two major approaches in this framework: (1) bridge equation methods, which leverage solutions to integral equations to recover causal targets, and (2) array decomposition methods, which recover latent factors composing counterfactual quantities by exploiting unique determination of eigenspaces. We compare the model restrictions underlying these two approaches and provide insight into implications of the underlying assumptions, clarifying the scope of applicability for each method.",0,arxiv,Ä°statistik,CC-BY/arXiv,Comparing Two Proxy Methods for Causal Identification
"High-dimensional spaces have challenged Bayesian optimization (BO). Existing methods aim to overcome this so-called curse of dimensionality by carefully encoding structural assumptions, from locality to sparsity to smoothness, into the optimization procedure. Surprisingly, we demonstrate that these approaches are outperformed by arguably the simplest method imaginable: Bayesian linear regression. After applying a geometric transformation to avoid boundary-seeking behavior, Gaussian processes with linear kernels match state-of-the-art performance on tasks with 60- to 6,000-dimensional search spaces. Linear models offer numerous advantages over their non-parametric counterparts: they afford closed-form sampling and their computation scales linearly with data, a fact we exploit on molecular optimization tasks with > 20,000 observations. Coupled with empirical analyses, our results suggest the need to depart from past intuitions about BO methods in high-dimensional spaces.",0,arxiv,Ä°statistik,CC-BY/arXiv,We Still Don't Understand High-Dimensional Bayesian Optimization
"This paper studies the role of activation functions in learning modular addition with two-layer neural networks. We first establish a sharp expressivity gap: sine MLPs admit width-$2$ exact realizations for any fixed length $m$ and, with bias, width-$2$ exact realizations uniformly over all lengths. In contrast, the width of ReLU networks must scale linearly with $m$ to interpolate, and they cannot simultaneously fit two lengths with different residues modulo $p$. We then provide a novel Natarajan-dimension generalization bound for sine networks, yielding nearly optimal sample complexity $\widetilde{\mathcal{O}}(p)$ for ERM over constant-width sine networks. We also derive width-independent, margin-based generalization for sine networks in the overparametrized regime and validate it. Empirically, sine networks generalize consistently better than ReLU networks across regimes and exhibit strong length extrapolation.",0,arxiv,Ä°statistik,CC-BY/arXiv,Provable Benefits of Sinusoidal Activation for Modular Addition
"Machine learning models perform well across domains such as diagnostics, weather forecasting, NLP, and autonomous driving, but their limited uncertainty handling restricts use in safety-critical settings. Traditional neural networks often fail to detect out-of-domain (OOD) data and may output confident yet incorrect predictions. Bayesian neural networks (BNNs) address this by providing probabilistic estimates, but incur high computational cost because predictions require sampling weight distributions and multiple forward passes. The Probabilistic Forward Pass (PFP) offers a highly efficient approximation to Stochastic Variational Inference (SVI) by assuming Gaussian-distributed weights and activations, enabling fully analytic uncertainty propagation and replacing sampling with a single deterministic forward pass. We present an end-to-end pipeline for training, compiling, optimizing, and deploying PFP-based BNNs on embedded ARM CPUs. Using the TVM deep learning compiler, we implement a dedicated library of Gaussian-propagating operators for multilayer perceptrons and convolutional neural networks, combined with manual and automated tuning strategies. Ablation studies show that PFP consistently outperforms SVI in computational efficiency, achieving speedups of up to 4200x for small mini-batches. PFP-BNNs match SVI-BNNs on Dirty-MNIST in accuracy, uncertainty estimation, and OOD detection while greatly reducing compute cost. These results highlight the potential of combining Bayesian approximations with code generation to enable efficient BNN deployment on resource-constrained systems.",0,arxiv,Ä°statistik,CC-BY/arXiv,Accelerated Execution of Bayesian Neural Networks using a Single Probabilistic Forward Pass and Code Generation
"Split learning is well known as a method for resolving data privacy concerns by training a model on distributed devices, thereby avoiding data sharing that raises privacy issues. However, high network communication costs are always an impediment to split learning, especially for large foundation models that require transmitting large amounts of high-dimensional data. To resolve this issue, we present a new multimodal model structure that incorporates a learning-based data compression method, which compresses model embeddings into low-bit integers while preserving the model's performance, greatly reducing the transmission costs between partitions. We then determine the optimal number of discrete representation levels based on a solid theoretical foundation from entropy coding.",0,arxiv,Ä°statistik,CC-BY/arXiv,Quantized-Tinyllava: a new multimodal foundation model enables efficient split learning
"Existing reinforcement learning (RL)-based post-training methods for large language models have advanced rapidly, yet their design has largely been guided by heuristics rather than systematic theoretical principles. This gap limits our understanding of the properties of the gradient estimators and the associated optimization algorithms, thereby constraining opportunities to improve training stability and overall performance. In this work, we provide a unified theoretical framework that characterizes the statistical properties of commonly used policy-gradient estimators under mild assumptions. Our analysis establishes unbiasedness, derives exact variance expressions, and yields an optimization-loss upper bound that enables principled reasoning about learning dynamics. Building on these results, we prove convergence guarantees and derive an adaptive learning-rate schedule governed by the signal-to-noise ratio (SNR) of gradients. We further show that the variance-optimal baseline is a gradient-weighted estimator, offering a new principle for variance reduction and naturally enhancing stability beyond existing methods. These insights motivate Optimal Baseline and Learning-Rate Policy Optimization (OBLR-PO), an algorithm that jointly adapts learning rates and baselines in a theoretically grounded manner. Experiments on Qwen3-4B-Base and Qwen3-8B-Base demonstrate consistent gains over existing policy optimization methods, validating that our theoretical contributions translate into practical improvements in large-scale post-training.",0,arxiv,Ä°statistik,CC-BY/arXiv,OBLR-PO: A Theoretical Framework for Stable Reinforcement Learning
"Transformers have proven highly effective across various applications, especially in handling sequential data such as natural languages and time series. However, transformer models often lack clear interpretability, and the success of transformers has not been well understood in theory. In this paper, we study the capability and interpretability of transformers in learning a family of classic statistical models, namely random walks on circles. We theoretically demonstrate that, after training with gradient descent, a one-layer transformer model can achieve optimal accuracy in predicting random walks. Importantly, our analysis reveals that the trained model is interpretable: the trained softmax attention serves as a token selector, focusing on the direct parent state; subsequently, the value matrix executes a one-step probability transition to predict the location of the next state based on this parent state. We also show that certain edge cases not covered by our theory are indeed failure cases, demonstrating that our theoretical conditions are tight. By investigating these success and failure cases, it is revealed that gradient descent with small initialization may fail or struggle to converge to a good solution in certain simple tasks even beyond random walks. Experiments are conducted to support our theoretical findings.",0,arxiv,Ä°statistik,CC-BY/arXiv,Towards Understanding Transformers in Learning Random Walks
"Quantile Regression Forests (QRF) are widely used for non-parametric conditional quantile estimation, yet statistical inference for variable importance measures remains challenging due to the non-smoothness of the loss function and the complex bias-variance trade-off. In this paper, we develop a asymptotic theory for variable importance defined as the difference in pinball loss risks. We first establish the asymptotic normality of the QRF estimator by handling the non-differentiable pinball loss via Knight's identity. Second, we uncover a ""phase transition"" phenomenon governed by the subsampling rate $Î²$ (where $s \asymp n^Î²$). We prove that in the bias-dominated regime ($Î²\ge 1/2$), which corresponds to large subsample sizes typically favored in practice to maximize predictive accuracy, standard inference breaks down as the estimator converges to a deterministic bias constant rather than a zero-mean normal distribution. Finally, we derive the explicit analytic form of this asymptotic bias and discuss the theoretical feasibility of restoring valid inference via analytic bias correction. Our results highlight a fundamental trade-off between predictive performance and inferential validity, providing a theoretical foundation for understanding the intrinsic limitations of random forest inference in high-dimensional settings.",0,arxiv,Ä°statistik,CC-BY/arXiv,Asymptotic Theory and Phase Transitions for Variable Importance in Quantile Regression Forests
"In traditional multivariate data analysis, dimension reduction and regression have been treated as distinct endeavors. Established techniques such as principal component regression (PCR) and partial least squares (PLS) regression traditionally compute latent components as intermediary steps -- although with different underlying criteria -- before proceeding with the regression analysis. In this paper, we introduce an innovative regression methodology named PLS-integrated Lasso (PLS-Lasso) that integrates the concept of dimension reduction directly into the regression process. We present two distinct formulations for PLS-Lasso, denoted as PLS-Lasso-v1 and PLS-Lasso-v2, along with clear and effective algorithms that ensure convergence to global optima. PLS-Lasso-v1 and PLS-Lasso-v2 are compared with Lasso on the task of financial index tracking and show promising results.",0,arxiv,Ä°statistik,CC-BY/arXiv,A PLS-Integrated LASSO Method with Application in Index Tracking
"This study systematically compared data-driven and model-based strategies for metabolite quantification in magnetic resonance spectroscopy (MRS), focusing on resilience to out-of-distribution (OoD) effects and the balance between accuracy, robustness, and generalizability. A neural network designed for MRS quantification was trained using three distinct strategies: supervised regression, self-supervised learning, and test-time adaptation. These were compared against model-based fitting tools. Experiments combined large-scale simulated data, designed to probe metabolite concentration extrapolation and signal variability, with 1H single-voxel 7T in-vivo human brain spectra. In simulations, supervised learning achieved high accuracy for spectra similar to those in the training distribution, but showed marked degradation when extrapolated beyond the training distribution. Test-time adaptation proved more resilient to OoD effects, while self-supervised learning achieved intermediate performance. In-vivo experiments showed larger variance across the methods (data-driven and model-based) due to domain shift. Across all strategies, overlapping metabolites and baseline variability remained persistent challenges. While strong performance can be achieved by data-driven methods for MRS metabolite quantification, their reliability is contingent on careful consideration of the training distribution and potential OoD effects. When such conditions in the target distribution cannot be anticipated, test-time adaptation strategies ensure consistency between the quantification, the data, and the model, enabling reliable data-driven MRS pipelines.",0,arxiv,Ä°statistik,CC-BY/arXiv,Strategies to Minimize Out-of-Distribution Effects in Data-Driven MRS Quantification
"High-capacity kernel Hopfield networks exhibit a \textit{Ridge of Optimization} characterized by extreme stability. While previously linked to \textit{Spectral Concentration}, its origin remains elusive. Here, we analyze the network dynamics on a statistical manifold, revealing that the Ridge corresponds to the Edge of Stability, a critical boundary where the Fisher Information Matrix becomes singular. We demonstrate that the apparent Euclidean force antagonism is a manifestation of \textit{Dual Equilibrium} in the Riemannian space. This unifies learning dynamics and capacity via the Minimum Description Length principle, offering a geometric theory of self-organized criticality.",0,arxiv,Ä°statistik,CC-BY/arXiv,Spectral Concentration at the Edge of Stability: Information Geometry of Kernel Associative Memory
"K-Nearest Neighbors (KNN) is one of the most used ML classifiers. However, if we observe closely, standard distance-weighted KNN and relative variants assume all 'k' neighbors are equally reliable. In heterogeneous feature space, this becomes a limitation that hinders reliability in predicting true levels of the observation.   We propose DW-KNN (Double Weighted KNN), a transparent and robust variant that integrates exponential distance with neighbor validity. This enables instance-level interpretability, suppresses noisy or mislabeled samples, and reduces hyperparameter sensitivity.   Comprehensive evaluation on 9 data-sets helps to demonstrate that DW-KNN achieves 0.8988 accuracy on average. It ranks 2nd among six methods and within 0.2% of the best-performing Ensemble KNN. It also exhibits the lowest cross-validation variance (0.0156), indicating reliable prediction stability. Statistical significance test confirmed ($p < 0.001$) improvement over compactness weighted KNN (+4.09\%) and Kernel weighted KNN (+1.13\%). The method provides a simple yet effective alternative to complex adaptive schemes, particularly valuable for high-stakes applications requiring explainable predictions.",0,arxiv,Ä°statistik,CC-BY/arXiv,DW-KNN: A Transparent Local Classifier Integrating Distance Consistency and Neighbor Reliability
"Measuring how central or typical a data point is underpins robust estimation, ranking, and outlier detection, but classical depth notions become expensive and unstable in high dimensions and are hard to extend beyond Euclidean data. We introduce Fused Unified centrality Score Estimation (FUSE), a neural centrality framework that operates on top of arbitrary representations. FUSE combines a global head, trained from pairwise distance-based comparisons to learn an anchor-free centrality score, with a local head, trained by denoising score matching to approximate a smoothed log-density potential. A single parameter between 0 and 1 interpolates between these calibrated signals, yielding depth-like centrality from different views via one forward pass. Across synthetic distributions, real images, time series, and text data, and standard outlier detection benchmarks, FUSE recovers meaningful classical ordering, reveals multi-scale geometric structures, and attains competitive performance with strong classical baselines while remaining simple and efficient.",0,arxiv,Ä°statistik,CC-BY/arXiv,A Trainable Centrality Framework for Modern Data
"Machine learning (ML) has shown significant promise in studying complex geophysical dynamical systems, including turbulence and climate processes. Such systems often display sensitive dependence on initial conditions, reflected in positive Lyapunov exponents, where even small perturbations in short-term forecasts can lead to large deviations in long-term outcomes. Thus, meaningful inference requires not only accurate short-term predictions, but also consistency with the system's long-term attractor that is captured by the marginal distribution of state variables. Existing approaches attempt to address this challenge by incorporating spatial and temporal dependence, but these strategies become impractical when data are extremely sparse. In this work, we show that prior knowledge of marginal distributions offers valuable complementary information to short-term observations, motivating a distribution-informed learning framework. We introduce a calibration algorithm based on normalization and the Kernelized Stein Discrepancy (KSD) to enhance ML predictions. The method here employs KSD within a reproducing kernel Hilbert space to calibrate model outputs, improving their fidelity to known physical distributions. This not only sharpens pointwise predictions but also enforces consistency with non-local statistical structures rooted in physical principles. Through synthetic experiments-spanning offline climatological CO2 fluxes and online quasi-geostrophic flow simulations-we demonstrate the robustness and broad utility of the proposed framework.",0,arxiv,Ä°statistik,CC-BY/arXiv,Calibrating Geophysical Predictions under Constrained Probabilistic Distributions
"Reinforcement learning in mobile health (mHealth) interventions requires balancing intervention efficacy with user burden, particularly when state measurements (for example, user surveys or feedback) are costly yet essential. The Act-Then-Measure (ATM) heuristic addresses this challenge by decoupling control and measurement actions within the Action-Contingent Noiselessly Observable Markov Decision Process (ACNO-MDP) framework. However, the standard ATM algorithm relies on a temporal-difference-inspired Q-learning method, which is prone to instability in sparse and noisy environments. In this work, we propose a Bayesian extension to ATM that replaces standard Q-learning with a Kalman filter-style Bayesian update, maintaining uncertainty-aware estimates of Q-values and enabling more stable and sample-efficient learning. We evaluate our method in both toy environments and clinically motivated testbeds. In small, tabular environments, Bayesian ATM achieves comparable or improved scalarized returns with substantially lower variance and more stable policy behavior. In contrast, in larger and more complex mHealth settings, both the standard and Bayesian ATM variants perform poorly, suggesting a mismatch between ATM's modeling assumptions and the structural challenges of real-world mHealth domains. These findings highlight the value of uncertainty-aware methods in low-data settings while underscoring the need for new RL algorithms that explicitly model causal structure, continuous states, and delayed feedback under observation cost constraints.",0,arxiv,Ä°statistik,CC-BY/arXiv,Optimizing Algorithms for Mobile Health Interventions with Active Querying Optimization
"Ranking methods or models based on their performance is of prime importance but is tricky because performance is fundamentally multidimensional. In the case of classification, precision and recall are scores with probabilistic interpretations that are both important to consider and complementary. The rankings induced by these two scores are often in partial contradiction. In practice, therefore, it is extremely useful to establish a compromise between the two views to obtain a single, global ranking. Over the last fifty years or so,it has been proposed to take a weighted harmonic mean, known as the F-score, F-measure, or $F_Î²$. Generally speaking, by averaging basic scores, we obtain a score that is intermediate in terms of values. However, there is no guarantee that these scores lead to meaningful rankings and no guarantee that the rankings are good tradeoffs between these base scores. Given the ubiquity of $F_Î²$ scores in the literature, some clarification is in order. Concretely: (1) We establish that $F_Î²$-induced rankings are meaningful and define a shortest path between precision- and recall-induced rankings. (2) We frame the problem of finding a tradeoff between two scores as an optimization problem expressed with Kendall rank correlations. We show that $F_1$ and its skew-insensitive version are far from being optimal in that regard. (3) We provide theoretical tools and a closed-form expression to find the optimal value for $Î²$ for any distribution or set of performances, and we illustrate their use on six case studies.",0,arxiv,Ä°statistik,CC-BY/arXiv,What Is the Optimal Ranking Score Between Precision and Recall? We Can Always Find It and It Is Rarely $F_1$
"Bayesian computational strategies for inference can be inefficient in approximating the posterior distribution in models that exhibit some form of periodicity. This is because the probability mass of the marginal posterior distribution of the parameter representing the period is usually highly concentrated in a very small region of the parameter space. Therefore, it is necessary to provide as much information as possible to the inference method through the parameter prior distribution. We intend to show that it is possible to construct a prior distribution from the data by fitting a Gaussian process (GP) with a periodic kernel. More specifically, we want to show that it is possible to approximate the marginal posterior distribution of the hyperparameter corresponding to the period in the kernel. Subsequently, this distribution can be used as a prior distribution for the inference method. We use an adaptive importance sampling method to approximate the posterior distribution of the hyperparameters of the GP. Then, we use the marginal posterior distribution of the hyperparameter related to the periodicity in order to construct a prior distribution for the period of the parametric model. This workflow is empirical Bayes, implemented as a modular (cut) transfer of a GP posterior for the period to the parametric model. We applied the proposed methodology to both synthetic and real data. We approximated the posterior distribution of the period of the GP kernel and then passed it forward as a posterior-as-prior with no feedback. Finally, we analyzed its impact on the marginal posterior distribution.",0,arxiv,Ä°statistik,CC-BY/arXiv,Data-driven informative priors for Bayesian inference with quasi-periodic data
"Selecting the best alternative from a finite set represents a broad class of pure exploration problems. Traditional approaches to pure exploration have predominantly relied on Gaussian or sub-Gaussian assumptions on the performance distributions of all alternatives, which limit their applicability to non-sub-Gaussian especially heavy-tailed problems. The need to move beyond sub-Gaussianity may become even more critical in large-scale problems, which tend to be especially sensitive to distributional specifications. In this paper, motivated by the widespread use of upper confidence bound (UCB) algorithms in pure exploration and beyond, we investigate their performance in the large-scale, non-sub-Gaussian settings. We consider the simplest category of UCB algorithms, where the UCB value for each alternative is defined as the sample mean plus an exploration bonus that depends only on its own sample size. We abstract this into a meta-UCB algorithm and propose letting it select the alternative with the largest sample size as the best upon stopping. For this meta-UCB algorithm, we first derive a distribution-free lower bound on the probability of correct selection. Building on this bound, we analyze two general non-sub-Gaussian scenarios: (1) all alternatives follow a common location-scale structure and have bounded variance; and (2) when such a structure does not hold, each alternative has a bounded absolute moment of order $q > 3$. In both settings, we show that the meta-UCB algorithm and therefore a broad class of UCB algorithms can achieve the sample optimality. These results demonstrate the applicability of UCB algorithms for solving large-scale pure exploration problems with non-sub-Gaussian distributions. Numerical experiments support our results and provide additional insights into the comparative behaviors of UCB algorithms within and beyond our meta-UCB framework.",0,arxiv,Ä°statistik,CC-BY/arXiv,UCB for Large-Scale Pure Exploration: Beyond Sub-Gaussianity
"Modern deep learning techniques focus on extracting intricate information from data to achieve accurate predictions. However, the training datasets may be crowdsourced and include sensitive information, such as personal contact details, financial data, and medical records. As a result, there is a growing emphasis on developing privacy-preserving training algorithms for neural networks that maintain good performance while preserving privacy. In this paper, we investigate the generalization and privacy performances of the differentially private gradient descent (DP-GD) algorithm, which is a private variant of the gradient descent (GD) by incorporating additional noise into the gradients during each iteration. Moreover, we identify a concrete learning task where DP-GD can achieve superior generalization performance compared to GD in training two-layer Huberized ReLU convolutional neural networks (CNNs). Specifically, we demonstrate that, under mild conditions, a small signal-to-noise ratio can result in GD producing training models with poor test accuracy, whereas DP-GD can yield training models with good test accuracy and privacy guarantees if the signal-to-noise ratio is not too small. This indicates that DP-GD has the potential to enhance model performance while ensuring privacy protection in certain learning tasks. Numerical simulations are further conducted to support our theoretical results.",0,arxiv,Ä°statistik,CC-BY/arXiv,Towards Understanding Generalization in DP-GD: A Case Study in Training Two-Layer CNNs
"We present a Fourier-enhanced recurrent neural network (RNN) for downscaling electrical loads. The model combines (i) a recurrent backbone driven by low-resolution inputs, (ii) explicit Fourier seasonal embeddings fused in latent space, and (iii) a self-attention layer that captures dependencies among high-resolution components within each period. Across four PJM territories, the approach yields RMSE lower and flatter horizon-wise than classical Prophet baselines (with and without seasonality/LAA) and than RNN ablations without attention or Fourier features.",0,arxiv,Ä°statistik,CC-BY/arXiv,Fourier-Enhanced Recurrent Neural Networks for Electrical Load Time Series Downscaling
"This chapter provides three tutorial exercises on physics-constrained regression. These are implemented as toy problems that seek to mimic grand challenges in (1) the super-resolution and data assimilation of the velocity field in image velocimetry, (2) data-driven turbulence modeling, and (3) system identification and digital twinning for forecasting and control. The Python codes for all exercises are provided in the course repository.",0,arxiv,Ä°statistik,CC-BY/arXiv,Learning with Physical Constraints
"This chapter opens with a review of classic tools for regression, a subset of machine learning that seeks to find relationships between variables. With the advent of scientific machine learning this field has moved from a purely data-driven (statistical) formalism to a constrained or ``physics-informed'' formalism, which integrates physical knowledge and methods from traditional computational engineering. In the first part, we introduce the general concepts and the statistical flavor of regression versus other forms of curve fitting. We then move to an overview of traditional methods from machine learning and their classification and ways to link these to traditional computational science. Finally, we close with a note on methods to combine machine learning and numerical methods for physics",0,arxiv,Ä°statistik,CC-BY/arXiv,Fundamentals of Regression
"Kolmogorov-Arnold Networks (KANs) offer a promising path toward interpretable machine learning: their learnable activations can be studied individually, while collectively fitting complex data accurately. In practice, however, trained activations often lack symbolic fidelity, learning pathological decompositions with no meaningful correspondence to interpretable forms. We propose Softly Symbolified Kolmogorov-Arnold Networks (S2KAN), which integrate symbolic primitives directly into training. Each activation draws from a dictionary of symbolic and dense terms, with learnable gates that sparsify the representation. Crucially, this sparsification is differentiable, enabling end-to-end optimization, and is guided by a principled Minimum Description Length objective. When symbolic terms suffice, S2KAN discovers interpretable forms; when they do not, it gracefully degrades to dense splines. We demonstrate competitive or superior accuracy with substantially smaller models across symbolic benchmarks, dynamical systems forecasting, and real-world prediction tasks, and observe evidence of emergent self-sparsification even without regularization pressure.",0,arxiv,Ä°statistik,CC-BY/arXiv,Softly Symbolifying Kolmogorov-Arnold Networks
"We study online statistical inference for the solutions of stochastic optimization problems with equality and inequality constraints. Such problems are prevalent in statistics and machine learning, encompassing constrained $M$-estimation, physics-informed models, safe reinforcement learning, and algorithmic fairness. We develop a stochastic sequential quadratic programming (SSQP) method to solve these problems, where the step direction is computed by sequentially performing a quadratic approximation of the objective and a linear approximation of the constraints. Despite having access to unbiased estimates of population gradients, a key challenge in constrained stochastic problems lies in dealing with the bias in the step direction. As such, we apply a momentum-style gradient moving-average technique within SSQP to debias the step. We show that our method achieves global almost-sure convergence and exhibits local asymptotic normality with an optimal primal-dual limiting covariance matrix in the sense of HÃ¡jek and Le Cam. In addition, we provide a plug-in covariance matrix estimator for practical inference. To our knowledge, the proposed SSQP method is the first fully online method that attains primal-dual asymptotic minimax optimality without relying on projection operators onto the constraint set, which are generally intractable for nonlinear problems. Through extensive experiments on benchmark nonlinear problems, as well as on constrained generalized linear models and portfolio allocation problems using both synthetic and real data, we demonstrate superior performance of our method, showing that the method and its asymptotic behavior not only solve constrained stochastic problems efficiently but also provide valid and practical online inference in real-world applications.",0,arxiv,Ä°statistik,CC-BY/arXiv,Online Inference of Constrained Optimization: Primal-Dual Optimality and Sequential Quadratic Programming
"This work presents a probabilistic digital twin framework for response prediction in dynamical systems governed by misspecified physics. The approach integrates Gaussian Process Latent Force Models (GPLFM) and Bayesian Neural Networks (BNNs) to enable end-to-end uncertainty-aware inference and prediction. In the diagnosis phase, model-form errors (MFEs) are treated as latent input forces to a nominal linear dynamical system and jointly estimated with system states using GPLFM from sensor measurements. A BNN is then trained on posterior samples to learn a probabilistic nonlinear mapping from system states to MFEs, while capturing diagnostic uncertainty. For prognosis, this mapping is used to generate pseudo-measurements, enabling state prediction via Kalman filtering. The framework allows for systematic propagation of uncertainty from diagnosis to prediction, a key capability for trustworthy digital twins. The framework is demonstrated using four nonlinear examples: a single degree of freedom (DOF) oscillator, a multi-DOF system, and two established benchmarks -- the Bouc-Wen hysteretic system and the Silverbox experimental dataset -- highlighting its predictive accuracy and robustness to model misspecification.",0,arxiv,Ä°statistik,CC-BY/arXiv,Probabilistic Digital Twin for Misspecified Structural Dynamical Systems via Latent Force Modeling and Bayesian Neural Networks
"We study the problem of selecting a small, representative action subset from an extremely large action space shared across a family of reinforcement learning (RL) environments -- a fundamental challenge in applications like inventory management and recommendation systems, where direct learning over the entire space is intractable. Our goal is to identify a fixed subset of actions that, for every environment in the family, contains a near-optimal action, thereby enabling efficient learning without exhaustively evaluating all actions.   This work extends our prior results for meta-bandits to the more general setting of Markov Decision Processes (MDPs). We prove that our existing algorithm achieves performance comparable to using the full action space. This theoretical guarantee is established under a relaxed, non-centered sub-Gaussian process model, which accommodates greater environmental heterogeneity. Consequently, our approach provides a computationally and sample-efficient solution for large-scale combinatorial decision-making under uncertainty.",0,arxiv,Ä°statistik,CC-BY/arXiv,Representative Action Selection for Large Action Space: From Bandits to MDPs
"Support vector machines are widely used in machine learning classification tasks, but traditional SVM models suffer from sensitivity to outliers and instability in resampling, which limits their performance in practical applications. To address these issues, this paper proposes a novel rescaled Huberized pinball loss function with asymmetric, non-convex, and smooth properties. Based on this loss function, we develop a corresponding SVM model called RHPSVM (Rescaled Huberized Pinball Loss Support Vector Machine). Theoretical analyses demonstrate that RHPSVM conforms to Bayesian rules, has a strict generalization error bound, a bounded influence function, and controllable optimality conditions, ensuring excellent classification accuracy, outlier insensitivity, and resampling stability. Additionally, RHPSVM can be extended to various advanced SVM variants by adjusting parameters, enhancing its flexibility. We transform the non-convex optimization problem of RHPSVM into a series of convex subproblems using the concave-convex procedure (CCCP) and solve it with the ClipDCD algorithm, which is proven to be convergent. Experimental results on simulated data, UCI datasets, and small-sample crop leaf image classification tasks show that RHPSVM outperforms existing SVM models in both noisy and noise-free scenarios, especially in handling high-dimensional small-sample data.",0,arxiv,Ä°statistik,CC-BY/arXiv,Support Vector Machine Classifier with Rescaled Huberized Pinball Loss
"Uncertainty quantification is vital for decision-making and risk assessment in machine learning. Mean-variance regression models, which predict both a mean and residual noise for each data point, provide a simple approach to uncertainty quantification. However, overparameterized mean-variance models struggle with signal-to-noise ambiguity, deciding whether prediction targets should be attributed to signal (mean) or noise (variance). At one extreme, models fit all training targets perfectly with zero residual noise, while at the other, they provide constant, uninformative predictions and explain the targets as noise. We observe a sharp phase transition between these extremes, driven by model regularization. Empirical studies with varying regularization levels illustrate this transition, revealing substantial variability across repeated runs. To explain this behavior, we develop a statistical field theory framework, which captures the observed phase transition in alignment with experimental results. This analysis reduces the regularization hyperparameter search space from two dimensions to one, significantly lowering computational costs. Experiments on UCI datasets and the large-scale ClimSim dataset demonstrate robust calibration performance, effectively quantifying predictive uncertainty.",0,arxiv,Ä°statistik,CC-BY/arXiv,On the Effect of Regularization on Nonparametric Mean-Variance Regression
"Limited overlap between treated and control groups is a key challenge in observational analysis. Standard approaches like trimming importance weights can reduce variance but introduce a fundamental bias. We propose a sensitivity framework for contextualizing findings under limited overlap, where we assess how irregular the outcome function has to be in order for the main finding to be invalidated. Our approach is based on worst-case confidence bounds on the bias introduced by standard trimming practices, under explicit assumptions necessary to extrapolate counterfactual estimates from regions of overlap to those without. Empirically, we demonstrate how our sensitivity framework protects against spurious findings by quantifying uncertainty in regions with limited overlap.",0,arxiv,Ä°statistik,CC-BY/arXiv,A Sensitivity Approach to Causal Inference Under Limited Overlap
"Most scientific domains elicit the development of efficient algorithms and accessible scientific software. This thesis unifies our developments in three broad domains: Quasi-Monte Carlo (QMC) methods for efficient high-dimensional integration, Gaussian process (GP) regression for high-dimensional interpolation with built-in uncertainty quantification, and scientific machine learning (sciML) for modeling partial differential equations (PDEs) with mesh-free solvers. For QMC, we built new algorithms for vectorized error estimation and developed QMCPy (https://qmcsoftware.github.io/QMCSoftware/): an open-source Python interface to randomized low-discrepancy sequence generators, automatic variable transforms, adaptive error estimation procedures, and diverse use cases. For GPs, we derived new digitally-shift-invariant kernels of higher-order smoothness, developed novel fast multitask GP algorithms, and produced the scalable Python software FastGPs (https://alegresor.github.io/fastgps/). For sciML, we developed a new algorithm capable of machine precision recovery of PDEs with random coefficients. We have also studied a number of applications including GPs for probability of failure estimation, multilevel GPs for the Darcy flow equation, neural surrogates for modeling radiative transfer, and fast GPs for Bayesian multilevel QMC.",0,arxiv,Ä°statistik,CC-BY/arXiv,"Algorithms and Scientific Software for Quasi-Monte Carlo, Fast Gaussian Process Regression, and Scientific Machine Learning"
"Bayesian optimization is widely used for optimizing expensive black box functions, but most existing approaches focus on scalar responses. In many scientific and engineering settings the response is functional, varying smoothly over an index such as time or wavelength, which makes classical formulations inadequate. Existing methods often minimize integrated error, which captures average performance but neglects worst case deviations. To address this limitation we propose min-max Functional Bayesian Optimization (MM-FBO), a framework that directly minimizes the maximum error across the functional domain. Functional responses are represented using functional principal component analysis, and Gaussian process surrogates are constructed for the principal component scores. Building on this representation, MM-FBO introduces an integrated uncertainty acquisition function that balances exploitation of worst case expected error with exploration across the functional domain. We provide two theoretical guarantees: a discretization bound for the worst case objective, and a consistency result showing that as the surrogate becomes accurate and uncertainty vanishes, the acquisition converges to the true min-max objective. We validate the method through experiments on synthetic benchmarks and physics inspired case studies involving electromagnetic scattering by metaphotonic devices and vapor phase infiltration. Results show that MM-FBO consistently outperforms existing baselines and highlights the importance of explicitly modeling functional uncertainty in Bayesian optimization.",0,arxiv,Ä°statistik,CC-BY/arXiv,Bayesian Optimization for Function-Valued Responses under Min-Max Criteria
"We study Sparse Multiple Kernel Learning (SMKL), which is the problem of selecting a sparse convex combination of prespecified kernels for support vector binary classification. Unlike prevailing l1 regularized approaches that approximate a sparsifying penalty, we formulate the problem by imposing an explicit cardinality constraint on the kernel weights and add an l2 penalty for robustness. We solve the resulting non-convex minimax problem via an alternating best response algorithm with two subproblems: the alpha subproblem is a standard kernel SVM dual solved via LIBSVM, while the beta subproblem admits an efficient solution via the Greedy Selector and Simplex Projector algorithm. We reformulate SMKL as a mixed integer semidefinite optimization problem and derive a hierarchy of semidefinite convex relaxations which can be used to certify near-optimality of the solutions returned by our best response algorithm and also to warm start it. On ten UCI benchmarks, our method with random initialization outperforms state-of-the-art MKL approaches in out-of-sample prediction accuracy on average by 3.34 percentage points (relative to the best performing benchmark) while selecting a small number of candidate kernels in comparable runtime. With warm starting, our method outperforms the best performing benchmark's out-of-sample prediction accuracy on average by 4.05 percentage points. Our convex relaxations provide a certificate that in several cases, the solution returned by our best response algorithm is the globally optimal solution.",0,arxiv,Ä°statistik,CC-BY/arXiv,Sparse Multiple Kernel Learning: Alternating Best Response and Semidefinite Relaxations
"Score-based generative models require guidance in order to generate plausible, on-manifold samples. The most popular guidance method, Classifier-Free Guidance (CFG), is only applicable in settings with labeled data and requires training an additional unconditional score-based model. More recently, Auto-Guidance adopts a smaller, less capable version of the original model to guide generation. While each method effectively promotes the fidelity of generated data, each requires labeled data or the training of additional models, making it challenging to guide score-based models when (labeled) training data are not available or training new models is not feasible.   We make the surprising discovery that the positive curvature of log density estimates in saddle regions provides strong guidance for score-based models. Motivated by this, we develop saddle-free guidance (SFG) which maintains estimates of maximal positive curvature of the log density to guide individual score-based models. SFG has the same computational cost of classifier-free guidance, does not require additional training, and works with off-the-shelf diffusion and flow matching models. Our experiments indicate that SFG achieves state-of-the-art FID and FD-DINOv2 metrics in single-model unconditional ImageNet-512 generation. When SFG is combined with Auto-Guidance, its unconditional samples achieve general state-of-the-art in FD-DINOv2 score. Our experiments with FLUX.1-dev and Stable Diffusion v3.5 indicate that SFG boosts the diversity of output images compared to CFG while maintaining excellent prompt adherence and image fidelity.",0,arxiv,Ä°statistik,CC-BY/arXiv,Saddle-Free Guidance: Improved On-Manifold Sampling without Labels or Additional Training
"Causal effect estimation in networked systems is central to data-driven decision making. In such settings, interventions on one unit can spill over to others, and in complex physical or social systems, the interaction pathways driving these interference structures remain largely unobserved. We argue that for identifying population-level causal effects, it is not necessary to recover the exact network structure; instead, it suffices to characterize how those interactions contribute to the evolution of outcomes. Building on this principle, we study an evolution-based approach that investigates how outcomes change across observation rounds in response to interventions, hence compensating for missing network information. Using an exposure-mapping perspective, we give an axiomatic characterization of when the empirical distribution of outcomes follows a low-dimensional recursive equation, and identify minimal structural conditions under which such evolution mappings exist. We frame this as a distributional counterpart to difference-in-differences. Rather than assuming parallel paths for individual units, it exploits parallel evolution patterns across treatment scenarios to estimate counterfactual trajectories. A key insight is that treatment randomization plays a role beyond eliminating latent confounding; it induces an implicit sampling from hidden interference channels, enabling consistent learning about heterogeneous spillover effects. We highlight causal message passing as an instantiation of this method in dense networks while extending to more general interference structures, including influencer networks where a small set of units drives most spillovers. Finally, we discuss the limits of this approach, showing that strong temporal trends or endogenous interference can undermine identification.",0,arxiv,Ä°statistik,CC-BY/arXiv,On Evolution-Based Models for Experimentation Under Interference
"AI/ML models have rapidly gained prominence as innovations for solving previously unsolved problems and their unintended consequences from amplifying human biases. Advocates for responsible AI/ML have sought ways to draw on the richer causal models of system dynamics to better inform the development of responsible AI/ML. However, a major barrier to advancing this work is the difficulty of bringing together methods rooted in different underlying assumptions (i.e., Dana Meadow's ""the unavoidable a priori""). This paper brings system dynamics and structural equation modeling together into a common mathematical framework that can be used to generate systems from distributions, develop methods, and compare results to inform the underlying epistemology of system dynamics for data science and AI/ML applications.",0,arxiv,Ä°statistik,CC-BY/arXiv,Bridging the Unavoidable A Priori: A Framework for Comparative Causal Modeling
"Markov Chain Monte Carlo (MCMC) is a flexible approach to approximate sampling from intractable probability distributions, with a rich theoretical foundation and comprising a wealth of exemplar algorithms. While the qualitative correctness of MCMC algorithms is often easy to ensure, their practical efficiency is contingent on the `target' distribution being reasonably well-behaved.   In this work, we concern ourself with the scenario in which this good behaviour is called into question, reviewing an emerging line of work on `robust' MCMC algorithms which can perform acceptably even in the face of certain pathologies.   We focus on two particular pathologies which, while simple, can already have dramatic effects on standard `local' algorithms. The first is roughness, whereby the target distribution varies so rapidly that the numerical stability of the algorithm is tenuous. The second is flatness, whereby the landscape of the target distribution is instead so barren and uninformative that one becomes lost in uninteresting parts of the state space. In each case, we formulate the pathology in concrete terms, review a range of proposed algorithmic remedies to the pathology, and outline promising directions for future research.",0,arxiv,Ä°statistik,CC-BY/arXiv,Some aspects of robustness in modern Markov Chain Monte Carlo
"A fundamental theoretical question in network analysis is to determine under which conditions community recovery is possible in polynomial time in the Stochastic Block Model (SBM). When the number $K$ of communities remains smaller than $\sqrt{n}$ --where $n$ denotes the number of nodes--, non-trivial community recovery is possible in polynomial time above, and only above, the Kesten--Stigum (KS) threshold, originally postulated using arguments from statistical physics.   When $K \geq \sqrt{n}$, Chin, Mossel, Sohn, and Wein recently proved that, in the \emph{sparse regime}, community recovery in polynomial time is achievable below the KS threshold by counting non-backtracking paths. This finding led them to postulate a new threshold for the many-communities regime $K \geq \sqrt{n}$. Subsequently, Carpentier, Giraud, and Verzelen established the failure of low-degree polynomials below this new threshold across all density regimes, and demonstrated successful recovery above the threshold in certain moderately sparse settings. While these results provide strong evidence that, in the many community setting, the computational barrier lies at the threshold proposed in~Chin et al., the question of achieving recovery above this threshold still remains open in most density regimes.   The present work is a follow-up to~Carpentier et al., in which we prove Conjecture~1.4 stated therein by: \\ 1- Constructing a family of motifs satisfying specific structural properties; and\\ 2- Proving that community recovery is possible above the proposed threshold by counting such motifs.\\ Our results complete the picture of the computational barrier for community recovery in the SBM with $K \geq \sqrt{n}$ communities. They also indicate that, in moderately sparse regimes, the optimal algorithms appear to be fundamentally different from spectral methods.",0,arxiv,Ä°statistik,CC-BY/arXiv,Phase Transition for Stochastic Block Model with more than $\sqrt{n}$ Communities (II)
"The recent proof of quasi-Gaussianity for the 2D stochastic Navier--Stokes (SNS) equations by Coe, Hairer, and Tolomeo establishes that the system's unique invariant measure is equivalent (mutually absolutely continuous) to the Gaussian measure of its corresponding linear Ornstein--Uhlenbeck (OU) process. While Gaussian process (GP) frameworks are increasingly used for fluid dynamics, their priors are often chosen for convenience rather than being rigorously justified by the system's long-term dynamics.   In this work, we bridge this gap by introducing a probabilistic framework for 2D SNS built directly upon this theoretical foundation. We construct our GP prior precisely from the stationary covariance of the linear OU model, which is explicitly defined by the forcing spectrum and dissipation. This provides a principled, GP prior with rigorous long-time dynamical justification for turbulent flows, bridging SPDE theory and practical data assimilation.",0,arxiv,Ä°statistik,CC-BY/arXiv,A Dynamics-Informed Gaussian Process Framework for 2D Stochastic Navier-Stokes via Quasi-Gaussianity
"Large Language Models (LLMs) are very demanding in terms of their computational resources. Low-rank decompositions of LLM weights, e.g. via Singular Value Decomposition (SVD), is a promising approach for LLM compression, but presents several practical hurdles, e.g. selecting appropriate layer-wise ranks and getting rid of its parameter redundancy. In this work, we present two physics-inspired improvements to SVD LLM compression: (1) \textbf{FermiGrad}, a gradient-descent algorithm that determines globally optimal layer-wise ranks by relaxing the discrete singular-value truncation into a continuous optimization using the Fermi function; (2) \textbf{PivGa}, an additional \textit{lossless} compression of the low-rank factors that exploits the intrinsic gauge freedom in their parametrization.",0,arxiv,Ä°statistik,CC-BY/arXiv,Globally optimized SVD compression of LLMs via Fermi-function-based rank selection and gauge fixing
"Post-Double-Lasso is becoming the most popular method for estimating linear regression models with many covariates when the purpose is to obtain an accurate estimate of a parameter of interest, such as an average treatment effect. However, this method can suffer from substantial omitted variable bias in finite sample. We propose a new method called Post-Double-Autometrics, which is based on Autometrics, and show that this method outperforms Post-Double-Lasso. Its use in a standard application of economic growth sheds new light on the hypothesis of convergence from poor to rich economies.",0,arxiv,Ä°statistik,CC-BY/arXiv,Estimation in high-dimensional linear regression: Post-Double-Autometrics as an alternative to Post-Double-Lasso
"Variational inference (VI) is a cornerstone of modern Bayesian learning, enabling approximate inference in complex models that would otherwise be intractable. However, its formulation depends on expectations and divergences defined through high-dimensional integrals, often rendering analytical treatment impossible and necessitating heavy reliance on approximate learning and inference techniques. Possibility theory, an imprecise probability framework, allows to directly model epistemic uncertainty instead of leveraging subjective probabilities. While this framework provides robustness and interpretability under sparse or imprecise information, adapting VI to the possibilistic setting requires rethinking core concepts such as entropy and divergence, which presuppose additivity. In this work, we develop a principled formulation of possibilistic variational inference and apply it to a special class of exponential-family functions, highlighting parallels with their probabilistic counterparts and revealing the distinctive mathematical structures of possibility theory.",0,arxiv,Ä°statistik,CC-BY/arXiv,Maxitive Donsker-Varadhan Formulation for Possibilistic Variational Inference
"Large language models (LLMs) are increasingly used as evaluators in lieu of humans. While scalable, their judgments are noisy due to imperfect specificity and sensitivity of LLMs, leading to biased accuracy estimates. Although bias-correction methods exist, they are underutilized in LLM research and typically assume exact knowledge of the model's specificity and sensitivity. Furthermore, in general we only have estimates of these values and it is not well known how to properly construct confidence intervals using only estimates. This work presents a simple plug-in framework that corrects such bias and constructs confidence intervals reflecting uncertainty from both test and calibration dataset, enabling practical and statistically sound LLM-based evaluation. Additionally, to reduce uncertainty in the accuracy estimate, we introduce an adaptive algorithm that efficiently allocates calibration sample sizes.",0,arxiv,Ä°statistik,CC-BY/arXiv,How to Correctly Report LLM-as-a-Judge Evaluations
"This paper investigates the partial linear model by Least Absolute Deviation (LAD) regression. We parameterize the nonparametric term using Deep Neural Networks (DNNs) and formulate a penalized LAD problem for estimation. Specifically, our model exhibits the following challenges. First, the regularization term can be nonconvex and nonsmooth, necessitating the introduction of infinite dimensional variational analysis and nonsmooth analysis into the asymptotic normality discussion. Second, our network must expand (in width, sparsity level and depth) as more samples are observed, thereby introducing additional difficulties for theoretical analysis. Third, the oracle of the proposed estimator is itself defined through a ultra high-dimensional, nonconvex, and discontinuous optimization problem, which already entails substantial computational and theoretical challenges. Under such the challenges, we establish the consistency, convergence rate, and asymptotic normality of the estimator. Furthermore, we analyze the oracle problem itself and its continuous relaxation. We study the convergence of a proximal subgradient method for both formulations, highlighting their structural differences lead to distinct computational subproblems along the iterations. In particular, the relaxed formulation admits significantly cheaper proximal updates, reflecting an inherent trade-off between statistical accuracy and computational tractability.",0,arxiv,Ä°statistik,CC-BY/arXiv,Nonconvex Penalized LAD Estimation in Partial Linear Models with DNNs: Asymptotic Analysis and Proximal Algorithms
"The rapid growth of high-dimensional datasets across various scientific domains has created a pressing need for new statistical methods to compare distributions supported on their underlying structures. Assessing similarity between datasets whose samples lie on low-dimensional manifolds requires robust techniques capable of separating meaningful signal from noise. We propose a principled framework for statistical inference of similarity and alignment between distributions supported on manifolds underlying high-dimensional datasets in the presence of heterogeneous noise. The key idea is to link the low-rank structure of observed data matrices to their underlying manifold geometry. By analyzing the spectrum of the sample covariance under a manifold signal-plus-noise model, we develop a scale-invariant distance measure between datasets based on their principal variance structures. We further introduce a consistent estimator for this distance and a statistical test for manifold alignability, and establish their asymptotic properties using random matrix theory. The proposed framework accommodates heterogeneous noise across datasets and offers an efficient, theoretically grounded approach for comparing high-dimensional datasets with low-dimensional manifold structures. Through extensive simulations and analyses of multi-sample single-cell datasets, we demonstrate that our method achieves superior robustness and statistical power compared with existing approaches.",0,arxiv,Ä°statistik,CC-BY/arXiv,Statistical Inference for Manifold Similarity and Alignability across Noisy High-Dimensional Datasets
"We propose a novel randomized algorithm for constructing binary neural networks with tunable accuracy. This approach is motivated by hyperdimensional computing (HDC), which is a brain-inspired paradigm that leverages high-dimensional vector representations, offering efficient hardware implementation and robustness to model corruptions. Unlike traditional low-precision methods that use quantization, we consider binary embeddings of data as points in the hypercube equipped with the Hamming distance. We propose a novel family of floating-point neural networks, G-Nets, which are general enough to mimic standard network layers. Each floating-point G-Net has a randomized binary embedding, an embedded hyperdimensional (EHD) G-Net, that retains the accuracy of its floating-point counterparts, with theoretical guarantees, due to the concentration of measure. Empirically, our binary models match convolutional neural network accuracies and outperform prior HDC models by large margins, for example, we achieve almost 30\% higher accuracy on CIFAR-10 compared to prior HDC models. G-Nets are a theoretically justified bridge between neural networks and randomized binary neural networks, opening a new direction for constructing robust binary/quantized deep learning models. Our implementation is available at https://github.com/GNet2025/GNet.",0,arxiv,Ä°statistik,CC-BY/arXiv,G-Net: A Provably Easy Construction of High-Accuracy Random Binary Neural Networks
"Zipf's law in language lacks a definitive origin, debated across fields. This study explains Zipf-like behavior using geometric mechanisms without linguistic elements. The Full Combinatorial Word Model (FCWM) forms words from a finite alphabet, generating a geometric distribution of word lengths. Interacting exponential forces yield a power-law rank-frequency curve, determined by alphabet size and blank symbol probability. Simulations support predictions, matching English, Russian, and mixed-genre data. The symbolic model suggests Zipf-type laws arise from geometric constraints, not communicative efficiency.",0,arxiv,Ä°statistik,CC-BY/arXiv,Zipf Distributions from Two-Stage Symbolic Processes: Stability Under Stochastic Lexical Filtering
"Fine-tuning large language models (LLMs) for downstream tasks typically exhibit a fundamental safety-capability tradeoff, where improving task performance degrades safety alignment even on benign datasets. This degradation persists across standard approaches including supervised finetuning (SFT) and reinforcement learning from human feedback (RLHF). While reinforcement learning with verifiable rewards (RLVR) has emerged as a promising alternative that optimizes models on objectively measurable tasks, its safety implications remain unexplored. We present the first comprehensive theoretical and empirical analysis of safety properties in RLVR. Theoretically, we derive upper bounds on safety drift under KL-constrained optimization and prove conditions under which safety degradation is eliminated. Empirically, we conduct extensive experiments across five adversarial safety benchmarks, demonstrating that RLVR can simultaneously enhance reasoning capabilities while maintaining or improving safety guardrails. Our comprehensive ablation studies examine the effects of optimization algorithms, model scale, and task domains. Our findings challenge the prevailing assumption of an inevitable safety capability trade-off, and establish that a specific training methodology can achieve both objectives simultaneously, providing insights for the safe deployment of reasoning-capable LLMs.",0,arxiv,Ä°statistik,CC-BY/arXiv,Breaking the Safety-Capability Tradeoff: Reinforcement Learning with Verifiable Rewards Maintains Safety Guardrails in LLMs
"We consider the problem of estimating Ising models over $n$ variables in Total Variation (TV) distance, given $l$ independent samples from the model. While the statistical complexity of the problem is well-understood [DMR20], identifying computationally and statistically efficient algorithms has been challenging. In particular, remarkable progress has occurred in several settings, such as when the underlying graph is a tree [DP21, BGPV21], when the entries of the interaction matrix follow a Gaussian distribution [GM24, CK24], or when the bulk of its eigenvalues lie in a small interval [AJK+24, KLV24], but no unified framework for polynomial-time estimation in TV exists so far. Our main contribution is a unified analysis of the Maximum Pseudo-Likelihood Estimator (MPLE) for two general classes of Ising models. The first class includes models that have bounded operator norm and satisfy the Modified Log-Sobolev Inequality (MLSI), a functional inequality that was introduced to study the convergence of the associated Glauber dynamics to stationarity. In the second class of models, the interaction matrix has bounded infinity norm (or bounded width), which is the most common assumption in the literature for structure learning of Ising models. We show how our general results for these classes yield polynomial-time algorithms and optimal or near-optimal sample complexity guarantees in a variety of settings. Our proofs employ a variety of tools from tensorization inequalities to measure decompositions and concentration bounds.",0,arxiv,Ä°statistik,CC-BY/arXiv,Estimating Ising Models in Total Variation Distance
"Modern artificial intelligence systems make critical decisions yet often fail silently when uncertain -- even well-calibrated models provide no mechanism to identify \textit{which specific predictions} are unreliable. We develop a geometric framework addressing both calibration and instance-level uncertainty quantification for neural network probability outputs. Treating probability vectors as points on the $(c-1)$-dimensional probability simplex equipped with the Fisher--Rao metric, we construct: (i) Additive Log-Ratio (ALR) calibration maps that reduce exactly to Platt scaling for binary problems while extending naturally to multi-class settings, and (ii) geometric reliability scores that translate calibrated probabilities into actionable uncertainty measures, enabling principled deferral of ambiguous predictions to human review.   Theoretical contributions include: consistency of the calibration estimator at rate $O_p(n^{-1/2})$ via M-estimation theory (Theorem~1), and tight concentration bounds for reliability scores with explicit sub-Gaussian parameters enabling sample size calculations for validation set design (Theorem~2). We conjecture Neyman--Pearson optimality of our neutral zone construction based on connections to Bhattacharyya coefficients. Empirical validation on Adeno-Associated Virus classification demonstrates that the two-stage framework captures 72.5\% of errors while deferring 34.5\% of samples, reducing automated decision error rates from 16.8\% to 6.9\%. Notably, calibration alone yields marginal accuracy gains; the operational benefit arises primarily from the reliability scoring mechanism, which applies to any well-calibrated probability output. This work bridges information geometry and statistical learning, offering formal guarantees for uncertainty-aware classification in applications requiring rigorous validation.",0,arxiv,Ä°statistik,CC-BY/arXiv,Geometric Calibration and Neutral Zones for Uncertainty-Aware Multi-Class Classification
"Real-time water-level monitoring across many locations is vital for flood response, infrastructure management, and environmental forecasting. Yet many sensing methods rely on fixed instruments - acoustic, radar, camera, or pressure probes - that are costly to install and maintain and are vulnerable during extreme events. We propose a passive, low-cost water-level tracking scheme that uses only LTE downlink power metrics reported by commodity receivers. The method extracts per-antenna RSRP, RSSI, and RSRQ, applies a continuous wavelet transform (CWT) to the RSRP to isolate the semidiurnal tide component, and forms a summed-coefficient signature that simultaneously marks high/low tide (tide-turn times) and tracks the tide-rate (flow speed) over time. These wavelet features guide a lightweight neural network that learns water-level changes over time from a short training segment. Beyond a single serving base station, we also show a multi-base-station cooperative mode: independent CWTs are computed per carrier and fused by a robust median to produce one tide-band feature that improves stability and resilience to local disturbances. Experiments over a 420 m river path under line-of-sight conditions achieve root-mean-square and mean-absolute errors of 0.8 cm and 0.5 cm, respectively. Under a non-line-of-sight setting with vegetation and vessel traffic, the same model transfers successfully after brief fine-tuning, reaching 1.7 cm RMSE and 0.8 cm MAE. Unlike CSI-based methods, the approach needs no array calibration and runs on standard hardware, making wide deployment practical. When signals from multiple base stations are available, fusion further improves robustness.",0,arxiv,Ä°statistik,CC-BY/arXiv,Wavelet-Guided Water-Level Estimation for ISAC
"Recent theoretical work established the unsupervised identifiability of quantized factors under any diffeomorphism. The theory assumes that quantization thresholds correspond to axis-aligned discontinuities in the probability density of the latent factors. By constraining a learned map to have a density with axis-aligned discontinuities, we can recover the quantization of the factors. However, translating this high-level principle into an effective practical criterion remains challenging, especially under nonlinear maps. Here, we develop a criterion for unsupervised disentanglement by encouraging axis-aligned discontinuities. Discontinuities manifest as sharp changes in the estimated density of factors and form what we call cliffs. Following the definition of independent discontinuities from the theory, we encourage the location of the cliffs along a factor to be independent of the values of the other factors. We show that our method, Cliff, outperforms the baselines on all disentanglement benchmarks, demonstrating its effectiveness in unsupervised disentanglement.",0,arxiv,Ä°statistik,CC-BY/arXiv,Operationalizing Quantized Disentanglement
"We study streaming data with categorical features where the vocabulary of categorical feature values is changing and can even grow unboundedly over time. Feature hashing is commonly used as a pre-processing step to map these categorical values into a feature space of fixed size before learning their embeddings. While these methods have been developed and evaluated for offline or batch settings, in this paper we consider online settings. We show that deterministic embeddings are sensitive to the arrival order of categories and suffer from forgetting in online learning, leading to performance deterioration. To mitigate this issue, we propose a probabilistic hash embedding (PHE) model that treats hash embeddings as stochastic and applies Bayesian online learning to learn incrementally from data. Based on the structure of PHE, we derive a scalable inference algorithm to learn model parameters and infer/update the posteriors of hash embeddings and other latent variables. Our algorithm (i) can handle an evolving vocabulary of categorical items, (ii) is adaptive to new items without forgetting old items, (iii) is implementable with a bounded set of parameters that does not grow with the number of distinct observed values on the stream, and (iv) is invariant to the item arrival order. Experiments in classification, sequence modeling, and recommendation systems in online learning setups demonstrate the superior performance of PHE while maintaining high memory efficiency (consumes as low as 2~4 memory of a one-hot embedding table). Supplementary materials are at https://github.com/aodongli/probabilistic-hash-embeddings",0,arxiv,Ä°statistik,CC-BY/arXiv,Probabilistic Hash Embeddings for Online Learning of Categorical Features
"This paper argues that DNNs implement a computational Occam's razor -- finding the `simplest' algorithm that fits the data -- and that this could explain their incredible and wide-ranging success over more traditional statistical methods. We start with the discovery that the set of real-valued function $f$ that can be $Îµ$-approximated with a binary circuit of size at most $cÎµ^{-Î³}$ becomes convex in the `Harder than Monte Carlo' (HTMC) regime, when $Î³>2$, allowing for the definition of a HTMC norm on functions. In parallel one can define a complexity measure on the parameters of a ResNets (a weighted $\ell_1$ norm of the parameters), which induce a `ResNet norm' on functions. The HTMC and ResNet norms can then be related by an almost matching sandwich bound. Thus minimizing this ResNet norm is equivalent to finding a circuit that fits the data with an almost minimal number of nodes (within a power of 2 of being optimal). ResNets thus appear as an alternative model for computation of real functions, better adapted to the HTMC regime and its convexity.",0,arxiv,Ä°statistik,CC-BY/arXiv,Deep Learning as a Convex Paradigm of Computation: Minimizing Circuit Size with ResNets
"State resetting is a fundamental but often overlooked capability of simulators. It supports sample-based planning by allowing resets to previously encountered simulation states, and enables calibration of simulators using real data by resetting to states observed in real-system traces. While often taken for granted, state resetting in complex simulators can be nontrivial: when the simulator comes with latent variables (states), state resetting requires sampling from the posterior over the latent state given the observable history, a.k.a. the belief state (Silver and Veness, 2010). While exact sampling is often infeasible, many approximate belief-state samplers can be constructed, raising the question of how to select among them using only sampling access to the simulator.   In this paper, we show that this problem reduces to a general conditional distribution-selection task and develop a new algorithm and analysis under sampling-only access. Building on this reduction, the belief-state selection problem admits two different formulations: latent state-based selection, which directly targets the conditional distribution of the latent state, and observation-based selection, which targets the induced distribution over the observation. Interestingly, these formulations differ in how their guarantees interact with the downstream roll-out methods: perhaps surprisingly, observation-based selection may fail under the most natural roll-out method (which we call Single-Reset) but enjoys guarantees under the less conventional alternative (which we call Repeated-Reset). Together with discussion on issues such as distribution shift and the choice of sampling policies, our paper reveals a rich landscape of algorithmic choices, theoretical nuances, and open questions, in this seemingly simple problem.",0,arxiv,Ä°statistik,CC-BY/arXiv,Selecting Belief-State Approximations in Simulators with Latent States
"Feature selection has remained a daunting challenge in machine learning and artificial intelligence, where increasingly complex, high-dimensional datasets demand principled strategies for isolating the most informative predictors. Despite widespread adoption, many established techniques suffer from notable limitations; some incur substantial computational cost, while others offer no definite statistical driven stopping criteria or assesses the significance of their importance scores. A common heuristic approach introduces multiple random noise features and retains all predictors ranked above the strongest noise feature. Although intuitive, this strategy lacks theoretical justification and depends heavily on heuristics. This paper proposes a novel feature selection method that addresses these limitations. Our approach introduces multiple random noise features and evaluates each feature's importance against the maximum importance value among these noise features incorporating a non-parametric bootstrap-based hypothesis testing framework to establish a solid theoretical foundation. We establish the conceptual soundness of our approach through statistical derivations that articulate the principles guiding the design of our algorithm. To evaluate its reliability, we generated simulated datasets under controlled statistical settings and benchmarked performance against Boruta and Knockoff-based methods, observing consistently stronger recovery of meaningful signal. As a demonstration of practical utility, we applied the technique across diverse real-world datasets, where it surpassed feature selection techniques including Boruta, RFE, and Extra Trees. Hence, the method emerges as a robust algorithm for principled feature selection, enabling the distillation of informative predictors that support reliable inference, enhanced predictive performance, and efficient computation.",0,arxiv,Ä°statistik,CC-BY/arXiv,When Features Beat Noise: A Feature Selection Technique Through Noise-Based Hypothesis Testing
"In incomplete financial markets, pricing and hedging European options lack a unique no-arbitrage solution due to unhedgeable risks. This paper introduces a constrained deep learning approach to determine option prices and hedging strategies that minimize the Profit and Loss (P&L) distribution around zero. We employ a single neural network to represent the option price function, with its gradient serving as the hedging strategy, optimized via a loss function enforcing the self-financing portfolio condition. A key challenge arises from the non-smooth nature of option payoffs (e.g., vanilla calls are non-differentiable at-the-money, while digital options are discontinuous), which conflicts with the inherent smoothness of standard neural networks. To address this, we compare unconstrained networks against constrained architectures that explicitly embed the terminal payoff condition, drawing inspiration from PDE-solving techniques. Our framework assumes two tradable assets: the underlying and a liquid call option capturing volatility dynamics. Numerical experiments evaluate the method on simple options with varying non-smoothness, the exotic Equinox option, and scenarios with market jumps for robustness. Results demonstrate superior P&L distributions, highlighting the efficacy of constrained networks in handling realistic payoffs. This work advances machine learning applications in quantitative finance by integrating boundary constraints, offering a practical tool for pricing and hedging in incomplete markets.",0,arxiv,Ä°statistik,CC-BY/arXiv,Constrained deep learning for pricing and hedging european options in incomplete markets
"Research increasingly relies on computational methods to analyze experimental data and predict molecular properties. Current approaches often require researchers to use a variety of tools for statistical analysis and machine learning, creating workflow inefficiencies. We present an integrated platform that combines classical statistical methods with Random Forest classification for comprehensive data analysis that can be used in the biological sciences. The platform implements automated hyperparameter optimization, feature importance analysis, and a suite of statistical tests including t tests, ANOVA, and Pearson correlation analysis. Our methodology addresses the gap between traditional statistical software, modern machine learning frameworks and biology, by providing a unified interface accessible to researchers without extensive programming experience. The system achieves this through automatic data preprocessing, categorical encoding, and adaptive model configuration based on dataset characteristics. Initial testing protocols are designed to evaluate classification accuracy across diverse chemical datasets with varying feature distributions. This work demonstrates that integrating statistical rigor with machine learning interpretability can accelerate biological discovery workflows while maintaining methodological soundness. The platform's modular architecture enables future extensions to additional machine learning algorithms and statistical procedures relevant to bioinformatics.",0,arxiv,Ä°statistik,CC-BY/arXiv,Automated Statistical and Machine Learning Platform for Biological Research
"We study the optimization of functions with $n>2$ arguments that have a representation as a sum of several functions that have only $2$ of the $n$ arguments each, termed sums of bivariates, on finite domains. The complexity of optimizing sums of bivariates is shown to be NP-equivalent and it is shown that there exists free lunch in the optimization of sums of bivariates. Based on measure-valued extensions of the objective function, so-called relaxations, $\ell^2$-approximation, and entropy-regularization, we derive several tractable problem formulations solvable with linear programming, coordinate ascent as well as with closed-form solutions. The limits of applying tractable versions of such relaxations to sums of bivariates are investigated using general results for reconstructing measures from their bivariate marginals. Experiments in which the derived algorithms are applied to random functions, vertex coloring, and signal reconstruction problems provide insights into qualitatively different function classes that can be modeled as sums of bivariates.",0,arxiv,Ä°statistik,CC-BY/arXiv,Optimization of Sums of Bivariate Functions: An Introduction to Relaxation-Based Methods for the Case of Finite Domains
"We introduce and analyse active learning markets as a way to purchase labels, in situations where analysts aim to acquire additional data to improve model fitting, or to better train models for predictive analytics applications. This comes in contrast to the many proposals that already exist to purchase features and examples. By originally formalising the market clearing as an optimisation problem, we integrate budget constraints and improvement thresholds into the label acquisition process. We focus on a single-buyer-multiple-seller setup and propose the use of two active learning strategies (variance based and query-by-committee based), paired with distinct pricing mechanisms. They are compared to a benchmark random sampling approach. The proposed strategies are validated on real-world datasets from two critical application domains: real estate pricing and energy forecasting. Results demonstrate the robustness of our approach, consistently achieving superior performance with fewer labels acquired compared to conventional methods. Our proposal comprises an easy-to-implement practical solution for optimising data acquisition in resource-constrained environments.",0,arxiv,Ä°statistik,CC-BY/arXiv,How to Purchase Labels? A Cost-Effective Approach Using Active Learning Markets
"Variational Bayes methods are popular due to their computational efficiency and adaptability to diverse applications. In specifying the variational family, mean-field classes are commonly used, which enables efficient algorithms such as coordinate ascent variational inference (CAVI) but fails to capture parameter dependence and typically underestimates uncertainty. In this work, we introduce a variational bagging approach that integrates a bagging procedure with variational Bayes, resulting in a bagged variational posterior for improved inference. We establish strong theoretical guarantees, including posterior contraction rates for general models and a Bernstein-von Mises (BVM) type theorem that ensures valid uncertainty quantification. Notably, our results show that even when using a mean-field variational family, our approach can recover off-diagonal elements of the limiting covariance structure and provide proper uncertainty quantification. In addition, variational bagging is robust to model misspecification, with covariance structures matching those of the target covariance. We illustrate our variational bagging method in numerical studies through applications to parametric models, finite mixture models, deep neural networks, and variational autoencoders (VAEs).",0,arxiv,Ä°statistik,CC-BY/arXiv,Variational bagging: a robust approach for Bayesian uncertainty quantification
"The abundance of fine-grained spatio-temporal data, such as traffic sensor networks, offers vast opportunities for scientific discovery. However, inferring causal relationships from such observational data remains challenging, particularly due to unobserved confounders that are specific to units (e.g., geographical locations) yet influence outcomes over time. Most existing methods for spatio-temporal causal inference assume that all confounders are observed, an assumption that is often violated in practice. In this paper, we introduce Spatio-Temporal Hierarchical Causal Models (ST-HCMs), a novel graphical framework that extends hierarchical causal modeling to the spatio-temporal domain. At the core of our approach is the Spatio-Temporal Collapse Theorem, which shows that a complex ST-HCM converges to a simpler flat causal model as the amount of subunit data increases. This theoretical result enables a general procedure for causal identification, allowing ST-HCMs to recover causal effects even in the presence of unobserved, time-invariant unit-level confounders, a scenario where standard non-hierarchical models fail. We validate the effectiveness of our framework on both synthetic and real-world datasets, demonstrating its potential for robust causal inference in complex dynamic systems.",0,arxiv,Ä°statistik,CC-BY/arXiv,Spatio-Temporal Hierarchical Causal Models
"Generative modeling is typically framed as learning mapping rules, but from an observer's perspective without access to these rules, the task becomes disentangling the geometric support from the probability distribution. We propose that continuum percolation is uniquely suited to this support analysis, as the sampling process effectively projects high-dimensional density estimation onto a geometric counting problem on the support. In this work, we establish a rigorous correspondence between the topological phase transitions of random geometric graphs and the underlying data manifold in high-dimensional space. By analyzing the relationship between our proposed Percolation Shift metric and FID, we show that this metric captures structural pathologies, such as implicit mode collapse, where standard statistical metrics fail. Finally, we translate this topological phenomenon into a differentiable loss function that guides training. Experimental results confirm that this approach not only prevents manifold shrinkage but also fosters a form of synergistic improvement, where topological stability becomes a prerequisite for sustained high fidelity in both static generation and sequential decision making.",0,arxiv,Ä°statistik,CC-BY/arXiv,Manifold Percolation: from generative model to Reinforce learning
"We prove an impossibility result for conditional Probably Approximately Correct (PAC)-efficient reasoning in large language models. While recent work has established marginal PAC efficiency guarantees for composite models that switch between expensive expert models and cheaper fast models, we show that conditional (pointwise) guarantees are impossible in the distribution-free setting. Specifically, for non-atomic input spaces, any algorithm achieving conditional PAC efficiency must be trivial in the sense that it defers to the expert model with probability at least $1-Î±$ for almost every input.",0,arxiv,Ä°statistik,CC-BY/arXiv,A note on the impossibility of conditional PAC-efficient reasoning in large language models
"Modeling nonlinear systems with Volterra series is challenging because the number of kernel coefficients grows exponentially with the model order. This work introduces Bayesian Tensor Network Volterra kernel machines (BTN-V), extending the Bayesian Tensor Network framework to Volterra system identification. BTN-V represents Volterra kernels using canonical polyadic decomposition, reducing model complexity from O(I^D) to O(DIR). By treating all tensor components and hyperparameters as random variables, BTN-V provides predictive uncertainty estimation at no additional computational cost. Sparsity-inducing hierarchical priors enable automatic rank determination and the learning of fading-memory behavior directly from data, improving interpretability and preventing overfitting. Empirical results demonstrate competitive accuracy, enhanced uncertainty quantification, and reduced computational cost.",0,arxiv,Ä°statistik,CC-BY/arXiv,A Fully Probabilistic Tensor Network for Regularized Volterra System Identification
"The predict-then-optimize paradigm bridges online learning and contextual optimization in dynamic environments. Previous works have investigated the sequential updating of predictors using feedback from downstream decisions to minimize regret in the full-information settings. However, existing approaches are predominantly frequentist, rely heavily on gradient-based strategies, and employ deterministic predictors that could yield high variance in practice despite their asymptotic guarantees. This work introduces, to the best of our knowledge, the first Bayesian online contextual optimization framework. Grounded in PAC-Bayes theory and general Bayesian updating principles, our framework achieves $\mathcal{O}(\sqrt{T})$ regret for bounded and mixable losses via a Gibbs posterior, eliminates the dependence on gradients through sequential Monte Carlo samplers, and thereby accommodates nondifferentiable problems. Theoretical developments and numerical experiments substantiate our claims.",0,arxiv,Ä°statistik,CC-BY/arXiv,PAC-Bayes Meets Online Contextual Optimization
"In this thesis, we extend the recently introduced theory of stochastic modified equations (SMEs) for stochastic gradient optimization algorithms.   In Ch. 3 we study time-inhomogeneous SDEs driven by Brownian motion. For certain SDEs we prove a 1st and 2nd-order weak approximation properties, and we compute their linear error terms explicitly, under certain regularity conditions. In Ch. 4 we instantiate our results for SGD, working out the example of linear regression explicitly. We use this example to compare the linear error terms of gradient flow and two commonly used 1st-order SMEs for SGD in Ch. 5.   In the second part of the thesis we introduce and study a novel diffusion approximation for SGD without replacement (SGDo) in the finite-data setting. In Ch. 6 we motivate and define the notion of an epoched Brownian motion (EBM). We argue that Young differential equations (YDEs) driven by EBMs serve as continuous-time models for SGDo for any shuffling scheme whose induced permutations converge to a det. permuton. Further, we prove a.s. convergence for these YDEs in the strongly convex setting. Moreover, we compute an upper asymptotic bound on the convergence rate which is as sharp as, or better than previous results for SGDo. In Ch. 7 we study scaling limits of families of random walks (RW) that share the same increments up to a random permutation. We show weak convergence under the assumption that the sequence of permutations converges to a det. (higher-dimensional) permuton. This permuton determines the covariance function of the limiting Gaussian process. Conversely, we show that every Gaussian process with a covariance function determined by a permuton in this way arises as a weak scaling limit of families of RW with shared increments. Finally, we apply our weak convergence theory to show that EBMs arise as scaling limits of RW with finitely many distinct increments.",0,arxiv,Ä°statistik,CC-BY/arXiv,Modified Equations for Stochastic Optimization
"We extend the convergence analysis of AdaSLS and AdaSPS in [Jiang and Stich, 2024] to the nonconvex setting, presenting a unified convergence analysis of stochastic gradient descent with adaptive Armijo line-search (AdaSLS) and Polyak stepsize (AdaSPS) for nonconvex optimization. Our contributions include: (1) an $\mathcal{O}(1/\sqrt{T})$ convergence rate for general nonconvex smooth functions, (2) an $\mathcal{O}(1/T)$ rate under quasar-convexity and interpolation, and (3) an $\mathcal{O}(1/T)$ rate under the strong growth condition for general nonconvex functions.",0,arxiv,Ä°statistik,CC-BY/arXiv,Adaptive SGD with Line-Search and Polyak Stepsizes: Nonconvex Convergence and Accelerated Rates
"Bayesian additive regression trees (BART) are popular Bayesian ensemble models used in regression and classification analysis. Under this modeling framework, the regression function is approximated by an ensemble of decision trees, interpreted as weak learners that capture different features of the data. In this work, we propose a generalization of the BART model that has two main features: first, it automatically selects the number of decision trees using the given data; second, the model allows clusters of observations to have different regression functions since each data point can only use a selection of weak learners, instead of all of them. This model generalization is accomplished by including a binary weight matrix in the conditional distribution of the response variable, which activates only a specific subset of decision trees for each observation. Such a matrix is endowed with an Indian Buffet process prior, and sampled within the MCMC sampler, together with the other BART parameters. We then compare the Infinite BART model with the classic one on simulated and real datasets. Specifically, we provide examples illustrating variable importance, partial dependence and causal estimation.",0,arxiv,Ä°statistik,CC-BY/arXiv,An Infinite BART model
We are extending Popularity Bias Memorization theorem from arXiv:archive/2404.12008 in several directions. We extend it to arbitrary degree distributions and also prove both upper and lower estimates for the alignment with top-k singular hyperspace.,0,arxiv,Ä°statistik,CC-BY/arXiv,Popularity Bias Alignment Estimates
"Universal online learning aims to achieve optimal regret guarantees without requiring prior knowledge of the curvature of online functions. Existing methods have established minimax-optimal regret bounds for universal online learning, where a single algorithm can simultaneously attain $\mathcal{O}(\sqrt{T})$ regret for convex functions, $\mathcal{O}(d \log T)$ for exp-concave functions, and $\mathcal{O}(\log T)$ for strongly convex functions, where $T$ is the number of rounds and $d$ is the dimension of the feasible domain. However, these methods still lack problem-dependent adaptivity. In particular, no universal method provides regret bounds that scale with the gradient variation $V_T$, a key quantity that plays a crucial role in applications such as stochastic optimization and fast-rate convergence in games. In this work, we introduce UniGrad, a novel approach that achieves both universality and adaptivity, with two distinct realizations: UniGrad.Correct and UniGrad.Bregman. Both methods achieve universal regret guarantees that adapt to gradient variation, simultaneously attaining $\mathcal{O}(\log V_T)$ regret for strongly convex functions and $\mathcal{O}(d \log V_T)$ regret for exp-concave functions. For convex functions, the regret bounds differ: UniGrad.Correct achieves an $\mathcal{O}(\sqrt{V_T \log V_T})$ bound while preserving the RVU property that is crucial for fast convergence in online games, whereas UniGrad.Bregman achieves the optimal $\mathcal{O}(\sqrt{V_T})$ regret bound through a novel design. Both methods employ a meta algorithm with $\mathcal{O}(\log T)$ base learners, which naturally requires $\mathcal{O}(\log T)$ gradient queries per round. To enhance computational efficiency, we introduce UniGrad++, which retains the regret while reducing the gradient query to just $1$ per round via surrogate optimization. We further provide various implications.",0,arxiv,Ä°statistik,CC-BY/arXiv,Adaptivity and Universality: Problem-dependent Universal Regret for Online Convex Optimization
"Decision trees remain central for tabular prediction but struggle with (i) capturing spatial dependence and (ii) producing locally stable (robust) explanations. We present SX-GeoTree, a self-explaining geospatial regression tree that integrates three coupled objectives during recursive splitting: impurity reduction (MSE), spatial residual control (global Moran's I), and explanation robustness via modularity maximization on a consensus similarity network formed from (a) geographically weighted regression (GWR) coefficient distances (stimulus-response similarity) and (b) SHAP attribution distances (explanatory similarity). We recast local Lipschitz continuity of feature attributions as a network community preservation problem, enabling scalable enforcement of spatially coherent explanations without per-sample neighborhood searches. Experiments on two exemplar tasks (county-level GDP in Fujian, n=83; point-wise housing prices in Seattle, n=21,613) show SX-GeoTree maintains competitive predictive accuracy (within 0.01 $R^{2}$ of decision trees) while improving residual spatial evenness and doubling attribution consensus (modularity: Fujian 0.19 vs 0.09; Seattle 0.10 vs 0.05). Ablation confirms Moran's I and modularity terms are complementary; removing either degrades both spatial residual structure and explanation stability. The framework demonstrates how spatial similarity - extended beyond geometric proximity through GWR-derived local relationships - can be embedded in interpretable models, advancing trustworthy geospatial machine learning and offering a transferable template for domain-aware explainability.",0,arxiv,Ä°statistik,CC-BY/arXiv,SX-GeoTree: Self-eXplaining Geospatial Regression Tree Incorporating the Spatial Similarity of Feature Attributions
"We introduce the Cisco Time Series Model, a univariate zero-shot forecaster. This time series foundation model is the result of a general architectural innovation to a time series model enabling it to accept multiresolution input, applied to a popular decoder-only time series model (TimesFM). The resulting multiresolution decoder-only model is trained on over 300B unique data points, with more than half coming from the observability domain. Quantitative and qualitative evaluations demonstrate that the resulting model achieves superior performance on observability datasets while retaining very similar performance on a standard general-purpose forecasting benchmark (GIFT-Eval), and suggest that the multiresolution structure enables the model to make more accurate predictions on long context input.",0,arxiv,Ä°statistik,CC-BY/arXiv,Cisco Time Series Model Technical Report
"Identifying key driver genes governing biological processes such as development and disease progression remains a challenge. While existing methods can reconstruct cellular trajectories or infer static gene regulatory networks (GRNs), they often fail to quantify time-resolved regulatory effects within specific temporal windows. Here, we present Time-varying Network Driver Estimation (TNDE), a computational framework quantifying dynamic gene driver effects from single-cell snapshot data under a linear Markov assumption. TNDE leverages a shared graph attention encoder to preserve the local topological structure of the data. Furthermore, by incorporating partial optimal transport, TNDE accounts for unmatched cells arising from proliferation or apoptosis, thereby enabling trajectory alignment in non-equilibrium processes. Benchmarking on simulated datasets demonstrates that TNDE outperforms existing baseline methods across diverse complex regulatory scenarios. Applied to mouse erythropoiesis data, TNDE identifies stage-specific driver genes, the functional relevance of which is corroborated by biological validation. TNDE offers an effective quantitative tool for dissecting dynamic regulatory mechanisms underlying complex biological processes.",0,arxiv,Ä°statistik,CC-BY/arXiv,Time-Varying Network Driver Estimation (TNDE) Quantifies Stage-Specific Regulatory Effects From Single-Cell Snapshots
"We present a case study for the calibration of Low-cost air-quality (LCAQ) CO sensors from one of the largest multi-site-multi-season-multi-sensor-multi-pollutant mobile air-quality monitoring network deployments in India. LCAQ sensors have been shown to play a critical role in the establishment of dense, expansive air-quality monitoring networks and combating elevated pollution levels. The calibration of LCAQ sensors against regulatory-grade monitors is an expensive, laborious and time-consuming process, especially when a large number of sensors are to be deployed in a geographically diverse layout. In this work, we present the RESPIRE technique to calibrate LCAQ sensors to detect ambient CO (Carbon Monoxide) levels. RESPIRE offers specific advantages over baseline calibration methods popular in literature, such as improved prediction in cross-site, cross-season, and cross-sensor settings. RESPIRE offers a training algorithm that is provably resistant to outliers and an explainable model with the ability to flag instances of model overfitting. Empirical results are presented based on data collected during an extensive deployment spanning four sites, two seasons and six sensor packages. RESPIRE code is available at https://github.com/purushottamkar/respire.",0,arxiv,Ä°statistik,CC-BY/arXiv,Provably Outlier-resistant Semi-parametric Regression for Transferable Calibration of Low-cost Air-quality Sensors
"We investigate complex-valued Variational AutoEncoders (CVAE) for radar Out-Of-Distribution (OOD) detection in complex radar environments. We proposed several detection metrics: the reconstruction error of CVAE (CVAE-MSE), the latent-based scores (Mahalanobis, Kullback-Leibler divergence (KLD)), and compared their performance against the classical ANMF-Tyler detector (ANMF-FP). The performance of all these detectors is analyzed on synthetic and experimental radar data, showing the advantages and the weaknesses of each detector.",0,arxiv,Ä°statistik,CC-BY/arXiv,Latent-space metrics for Complex-Valued VAE out-of-distribution detection under radar clutter
"We propose Terminal Velocity Matching (TVM), a generalization of flow matching that enables high-fidelity one- and few-step generative modeling. TVM models the transition between any two diffusion timesteps and regularizes its behavior at its terminal time rather than at the initial time. We prove that TVM provides an upper bound on the $2$-Wasserstein distance between data and model distributions when the model is Lipschitz continuous. However, since Diffusion Transformers lack this property, we introduce minimal architectural changes that achieve stable, single-stage training. To make TVM efficient in practice, we develop a fused attention kernel that supports backward passes on Jacobian-Vector Products, which scale well with transformer architectures. On ImageNet-256x256, TVM achieves 3.29 FID with a single function evaluation (NFE) and 1.99 FID with 4 NFEs. It similarly achieves 4.32 1-NFE FID and 2.94 4-NFE FID on ImageNet-512x512, representing state-of-the-art performance for one/few-step models from scratch.",0,arxiv,Ä°statistik,CC-BY/arXiv,Terminal Velocity Matching
"Recent machine learning papers often report 1-2 percentage point improvements from a single run on a benchmark. These gains are highly sensitive to random seeds, data ordering, and implementation details, yet are rarely accompanied by uncertainty estimates or significance tests. It is therefore unclear when a reported +1-2% reflects a real algorithmic advance versus noise.   We revisit this problem under realistic compute budgets, where only a few runs are affordable. We propose a simple, PC-friendly evaluation protocol based on paired multi-seed runs, bias-corrected and accelerated (BCa) bootstrap confidence intervals, and a sign-flip permutation test on per-seed deltas. The protocol is intentionally conservative and is meant as a guardrail against over-claiming.   We instantiate it on CIFAR-10, CIFAR-10N, and AG News using synthetic no-improvement, small-gain, and medium-gain scenarios. Single runs and unpaired t-tests often suggest significant gains for 0.6-2.0 point improvements, especially on text. With only three seeds, our paired protocol never declares significance in these settings. We argue that such conservative evaluation is a safer default for small gains under tight budgets.",0,arxiv,Ä°statistik,CC-BY/arXiv,When +1% Is Not Enough: A Paired Bootstrap Protocol for Evaluating Small Improvements
"The validation of a data-driven model is the process of assessing the model's ability to generalize to new, unseen data in the population of interest. This paper proposes a set of general rules for model validation. These rules are designed to help practitioners create reliable validation plans and report their results transparently. While no validation scheme is flawless, these rules can help practitioners ensure their strategy is sufficient for practical use, openly discuss any limitations of their validation strategy, and report clear, comparable performance metrics.",0,arxiv,Ä°statistik,CC-BY/arXiv,A Set of Rules for Model Validation
"Vector autoregressive (VAR) processes are ubiquitously used in economics, finance, and biology. Order selection is an essential step in fitting VAR models. While many order selection methods exist, all come with weaknesses. Order selection by minimizing AIC is a popular approach but is known to consistently overestimate the true order for processes of small dimension. On the other hand, methods based on BIC or the Hannan-Quinn (HQ) criteria are shown to require large sample sizes in order to accurately estimate the order for larger-dimensional processes. We propose the mean square information criterion (MIC) based on the observation that the expected squared error loss is flat once the fitted order reaches or exceeds the true order. MIC is shown to consistently estimate the order of the process under relatively mild conditions. Our simulation results show that MIC offers better performance relative to AIC, BIC, and HQ under misspecification. This advantage is corroborated when forecasting COVID-19 outcomes in New York City. Order selection by MIC is implemented in the micvar R package available on CRAN.",0,arxiv,Ä°statistik,CC-BY/arXiv,Order Selection in Vector Autoregression by Mean Square Information Criterion
"Clustering is widely used in unsupervised learning to find homogeneous groups of observations within a dataset. However, clustering mixed-type data remains a challenge, as few existing approaches are suited for this task. This study presents the state-of-the-art of these approaches and compares them using various simulation models. The compared methods include the distance-based approaches k-prototypes, PDQ, and convex k-means, and the probabilistic methods KAy-means for MIxed LArge data (KAMILA), the mixture of Bayesian networks (MBNs), and latent class model (LCM). The aim is to provide insights into the behavior of different methods across a wide range of scenarios by varying some experimental factors such as the number of clusters, cluster overlap, sample size, dimension, proportion of continuous variables in the dataset, and clusters' distribution. The degree of cluster overlap and the proportion of continuous variables in the dataset and the sample size have a significant impact on the observed performances. When strong interactions exist between variables alongside an explicit dependence on cluster membership, none of the evaluated methods demonstrated satisfactory performance. In our experiments KAMILA, LCM, and k-prototypes exhibited the best performance, with respect to the adjusted rand index (ARI). All the methods are available in R.",0,arxiv,Ä°statistik,CC-BY/arXiv,Clustering Approaches for Mixed-Type Data: A Comparative Study
"Inverse problems are fundamental to science and engineering, where the goal is to infer an underlying signal or state from incomplete or noisy measurements. Recent approaches employ diffusion models as powerful implicit priors for such problems, owing to their ability to capture complex data distributions. However, existing diffusion-based methods for inverse problems often rely on strong approximations of the posterior distribution, require computationally expensive gradient backpropagation through the score network, or are restricted to linear measurement models.   In this work, we propose Restart for Posterior Sampling (RePS), a general and efficient framework for solving both linear and non-linear inverse problems using pre-trained diffusion models. RePS builds on the idea of restart-based sampling, previously shown to improve sample quality in unconditional diffusion, and extends it to posterior inference. Our method employs a conditioned ODE applicable to any differentiable measurement model and introduces a simplified restart strategy that contracts accumulated approximation errors during sampling. Unlike some of the prior approaches, RePS avoids backpropagation through the score network, substantially reducing computational cost.   We demonstrate that RePS achieves faster convergence and superior reconstruction quality compared to existing diffusion-based baselines across a range of inverse problems, including both linear and non-linear settings.",0,arxiv,Ä°statistik,CC-BY/arXiv,Solving Diffusion Inverse Problems with Restart Posterior Sampling
"We derive a new theoretical interpretation of the reweighted losses that are widely used for training diffusion models. Our method is based on constructing a cascade of time-dependent variational lower bounds on the data log-likelihood, that provably improves upon the standard evidence lower bound and results in reduced data-model KL-divergences. Combining such bounds gives rise to reweighted objectives that can be applied to any generative diffusion model including both continuous Gaussian diffusion and masked (discrete) diffusion models. Then, we showcase this framework in masked diffusion and report significant improvements over previous training losses in pixel-space image modeling, approaching sample quality comparable to continuous diffusion models. Our results also provide a theoretical justification for the simple weighting scheme widely used in masked image models.",0,arxiv,Ä°statistik,CC-BY/arXiv,Demystifying Diffusion Objectives: Reweighted Losses are Better Variational Bounds
"Although upper bound guarantees for bilevel optimization have been widely studied, progress on lower bounds has been limited due to the complexity of the bilevel structure. In this work, we focus on the smooth nonconvex-strongly-convex setting and develop new hard instances that yield nontrivial lower bounds under deterministic and stochastic first-order oracle models. In the deterministic case, we prove that any first-order zero-respecting algorithm requires at least $Î©(Îº^{3/2}Îµ^{-2})$ oracle calls to find an $Îµ$-accurate stationary point, improving the optimal lower bounds known for single-level nonconvex optimization and for nonconvex-strongly-convex min-max problems. In the stochastic case, we show that at least $Î©(Îº^{5/2}Îµ^{-4})$ stochastic oracle calls are necessary, again strengthening the best known bounds in related settings. Our results expose substantial gaps between current upper and lower bounds for bilevel optimization and suggest that even simplified regimes, such as those with quadratic lower-level objectives, warrant further investigation toward understanding the optimal complexity of bilevel optimization under standard first-order oracles.",0,arxiv,Ä°statistik,CC-BY/arXiv,Lower Complexity Bounds for Nonconvex-Strongly-Convex Bilevel Optimization with First-Order Oracles
"Early and accurate detection of Alzheimer's disease (AD) is crucial for enabling timely intervention and improving outcomes. However, developing reliable machine learning (ML) models for AD diagnosis is challenging due to limited labeled data, multi-site heterogeneity, and class imbalance. We propose a Transformer-based diagnostic framework that combines diffusion-based synthetic data generation with graph representation learning and transfer learning. A class-conditional denoising diffusion probabilistic model (DDPM) is trained on the real-world NACC dataset to generate a large synthetic cohort that mirrors multimodal clinical and neuroimaging feature distributions while balancing diagnostic classes. Modality-specific Graph Transformer encoders are first pretrained on this synthetic data to learn robust, class-discriminative representations and are then frozen while a neural classifier is trained on embeddings from the original NACC data. We quantify distributional alignment between real and synthetic cohorts using metrics such as Maximum Mean Discrepancy (MMD), Frechet distance, and energy distance, and complement discrimination metrics with calibration and fixed-specificity sensitivity analyses. Empirically, our framework outperforms standard baselines, including early and late fusion deep neural networks and the multimodal graph-based model MaGNet, yielding higher AUC, accuracy, sensitivity, and specificity under subject-wise cross-validation on NACC. These results show that diffusion-based synthetic pretraining with Graph Transformers can improve generalization in low-sample, imbalanced clinical prediction settings.",0,arxiv,Ä°statistik,CC-BY/arXiv,Pretraining Transformer-Based Models on Diffusion-Generated Synthetic Graphs for Alzheimer's Disease Prediction
"This study investigates the limitations of applying Markov Chain Monte Carlo (MCMC) methods to arbitrary objective functions, focusing on a two-block MCMC framework which alternates between Metropolis-Hastings and Gibbs sampling. While such approaches are often considered advantageous for enabling data-driven regularization, we show that their performance critically depends on the sharpness of the employed likelihood form. By introducing a sharpness parameter and exploring alternative likelihood formulations proportional to the target objective function, we demonstrate how likelihood curvature governs both in-sample performance and the degree of regularization inferred by the training data. Empirical applications are conducted on reinforcement learning tasks: including a navigation problem and the game of tic-tac-toe. The study concludes with a separate analysis examining the implications of extreme likelihood sharpness on arbitrary objective functions stemming from the classic game of blackjack, where the first block of the two-block MCMC framework is replaced with an iterative optimization step. The resulting hybrid approach achieves performance nearly identical to the original MCMC framework, indicating that excessive likelihood sharpness effectively collapses posterior mass onto a single dominant mode.",0,arxiv,Ä°statistik,CC-BY/arXiv,Optimization and Regularization Under Arbitrary Objectives
"Diffusion models for image generation often exhibit a trade-off between perceptual sample quality and data likelihood: training objectives emphasizing high-noise denoising steps yield realistic images but poor likelihoods, whereas likelihood-oriented training overweights low-noise steps and harms visual fidelity. We introduce a simple plug-and-play sampling method that combines two pretrained diffusion experts by switching between them along the denoising trajectory. Specifically, we apply an image-quality expert at high noise levels to shape global structure, then switch to a likelihood expert at low noise levels to refine pixel statistics. The approach requires no retraining or fine-tuning -- only the choice of an intermediate switching step. On CIFAR-10 and ImageNet32, the merged model consistently matches or outperforms its base components, improving or preserving both likelihood and sample quality relative to each expert alone. These results demonstrate that expert switching across noise levels is an effective way to break the likelihood-quality trade-off in image diffusion models.",0,arxiv,Ä°statistik,CC-BY/arXiv,Breaking the Likelihood-Quality Trade-off in Diffusion Models by Merging Pretrained Experts
"We study the problem of nonparametric instrumental variable regression with observed covariates, which we refer to as NPIV-O. Compared with standard nonparametric instrumental variable regression (NPIV), the additional observed covariates facilitate causal identification and enables heterogeneous causal effect estimation. However, the presence of observed covariates introduces two challenges for its theoretical analysis. First, it induces a partial identity structure, which renders previous NPIV analyses - based on measures of ill-posedness, stability conditions, or link conditions - inapplicable. Second, it imposes anisotropic smoothness on the structural function. To address the first challenge, we introduce a novel Fourier measure of partial smoothing; for the second challenge, we extend the existing kernel 2SLS instrumental variable algorithm with observed covariates, termed KIV-O, to incorporate Gaussian kernel lengthscales adaptive to the anisotropic smoothness. We prove upper $L^2$-learning rates for KIV-O and the first $L^2$-minimax lower learning rates for NPIV-O. Both rates interpolate between known optimal rates of NPIV and nonparametric regression (NPR). Interestingly, we identify a gap between our upper and lower bounds, which arises from the choice of kernel lengthscales tuned to minimize a projected risk. Our theoretical analysis also applies to proximal causal inference, an emerging framework for causal effect estimation that shares the same conditional moment restriction as NPIV-O.",0,arxiv,Ä°statistik,CC-BY/arXiv,Nonparametric Instrumental Variable Regression with Observed Covariates
"This work studies information-computation gaps for statistical problems. A common approach for providing evidence of such gaps is to show sample complexity lower bounds (that are stronger than the information-theoretic optimum) against natural models of computation. A popular such model in the literature is the family of low-degree polynomial tests. While these tests are defined in such a way that make them easy to analyze, the class of algorithms that they rule out is somewhat restricted. An important goal in this context has been to obtain lower bounds against the stronger and more natural class of low-degree Polynomial Threshold Function (PTF) tests, i.e., any test that can be expressed as comparing some low-degree polynomial of the data to a threshold. Proving lower bounds against PTF tests has turned out to be challenging. Indeed, we are not aware of any non-trivial PTF testing lower bounds in the literature.   In this paper, we establish the first non-trivial PTF testing lower bounds for a range of statistical tasks. Specifically, we prove a near-optimal PTF testing lower bound for Non-Gaussian Component Analysis (NGCA). Our NGCA lower bound implies similar lower bounds for a number of other statistical problems. Our proof leverages a connection to recent work on pseudorandom generators for PTFs and recent techniques developed in that context. At the technical level, we develop several tools of independent interest, including novel structural results for analyzing the behavior of low-degree polynomials restricted to random directions.",0,arxiv,Ä°statistik,CC-BY/arXiv,PTF Testing Lower Bounds for Non-Gaussian Component Analysis
"Conditional diffusion models provide a natural framework for probabilistic prediction of dynamical systems and have been successfully applied to fluid dynamics and weather prediction. However, in many settings, the available information at a given time represents only a small fraction of what is needed to predict future states, either due to measurement uncertainty or because only a small fraction of the state can be observed. This is true for example in solar physics, where we can observe the Sun's surface and atmosphere, but its evolution is driven by internal processes for which we lack direct measurements. In this paper, we tackle the probabilistic prediction of partially observable, long-memory dynamical systems, with applications to solar dynamics and the evolution of active regions. We show that standard inference schemes, such as autoregressive rollouts, fail to capture long-range dependencies in the data, largely because they do not integrate past information effectively. To overcome this, we propose a multiscale inference scheme for diffusion models, tailored to physical processes. Our method generates trajectories that are temporally fine-grained near the present and coarser as we move farther away, which enables capturing long-range temporal dependencies without increasing computational cost. When integrated into a diffusion model, we show that our inference scheme significantly reduces the bias of the predicted distributions and improves rollout stability.",0,arxiv,Ä°statistik,CC-BY/arXiv,Predicting partially observable dynamical systems via diffusion models with a multiscale inference scheme
"Neural models have shown promise in solving NP-hard graph combinatorial optimization (CO) problems. Once trained, they offer fast inference and reasonably high-quality solutions for in-distribution testing instances, but they generally fall short in terms of absolute solution quality compared to classical search-based algorithms that are admittedly slower but offer optimality guarantee once search finishes.   We propose a novel framework that combines the inference efficiency and exploratory power of neural models with the solution quality guarantee of search-based algorithms. In particular, we use parameterized algorithms (PAs) as the search component. PAs are dedicated to identifying easy instances of generally NP-hard problems, and allow for practically efficient search by exploiting structural simplicity (of the identified easy instances). Under our framework, we use parameterized analysis to identify the structurally hard parts of a CO instance. The neural model handles the hard parts by generating advisory signals based on its data-driven understanding. The PA-based search component then integrates the advisory signals to systematically and efficiently searches through the remaining structurally easy parts. Notably, our framework is agnostic to the choice of neural model and produces strictly better solutions than neural solvers alone.   We examine our framework on multiple CO tasks. Empirical results show that it achieves superior solution quality, competitive with that of commercial solvers. Furthermore, by using the neural model only for exploratory advisory signals, our framework exhibits improved out-of-distribution generalization, addressing a key limitation of existing neural CO solvers.",0,arxiv,Ä°statistik,CC-BY/arXiv,Neural Tractability via Structure: Learning-Augmented Algorithms for Graph Combinatorial Optimization
"This document proposes a Unified Robust Framework that re-engineers the estimation of the Average Treatment Effect on the Overlap (ATO). It synthesizes gamma-Divergence for outlier robustness, Graduated Non-Convexity (GNC) for global optimization, and a ""Gatekeeper"" mechanism to address the impossibility of higher-order orthogonality in Gaussian regimes.",0,arxiv,Ä°statistik,CC-BY/arXiv,The Unified Non-Convex Framework for Robust Causal Inference: Overcoming the Gaussian Barrier and Optimization Fragility
"Open weight models, which are ubiquitous, rarely provide access to their training data or loss function. This makes modifying such models for tasks such as pruning or unlearning constrained by this unavailability an active area of research. Existing techniques typically require gradients or ground-truth labels, rendering them infeasible in settings with limited computational resources. In this work, we investigate the fundamental question of identifying components that are critical to the model's predictive performance, without access to either gradients or the loss function, and with only distributional access such as synthetic data. We theoretically demonstrate that the global reconstruction error is linearly bounded by local reconstruction errors for Lipschitz-continuous networks such as CNNs and well-trained Transformers (which, contrary to existing literature, we find exhibit Lipschitz continuity). This motivates using the locally reconstructive behavior of component subsets to quantify their global importance, via a metric that we term Subset Fidelity. In the uncorrelated features setting, selecting individual components via their Subset Fidelity scores is optimal, which we use to propose ModHiFi, an algorithm for model modification that requires no training data or loss function access. ModHiFi-P, for structured pruning, achieves an 11% speedup over the current state of the art on ImageNet models and competitive performance on language models. ModHiFi-U, for classwise unlearning, achieves complete unlearning on CIFAR-10 without fine-tuning and demonstrates competitive performance on Swin Transformers.",0,arxiv,Ä°statistik,CC-BY/arXiv,ModHiFi: Identifying High Fidelity predictive components for Model Modification
"Searching large and complex design spaces for a global optimum can be infeasible and unnecessary. A practical alternative is to iteratively refine the neighborhood of an initial design using local optimization methods such as gradient descent. We propose local entropy search (LES), a Bayesian optimization paradigm that explicitly targets the solutions reachable by the descent sequences of iterative optimizers. The algorithm propagates the posterior belief over the objective through the optimizer, resulting in a probability distribution over descent sequences. It then selects the next evaluation by maximizing mutual information with that distribution, using a combination of analytic entropy calculations and Monte-Carlo sampling of descent sequences. Empirical results on high-complexity synthetic objectives and benchmark problems show that LES achieves strong sample efficiency compared to existing local and global Bayesian optimization methods.",0,arxiv,Ä°statistik,CC-BY/arXiv,Local Entropy Search over Descent Sequences for Bayesian Optimization
"This paper introduces a novel Kalman filter framework designed to achieve robust state estimation under both process and measurement noise. Inspired by the Weighted Observation Likelihood Filter (WoLF), which provides robustness against measurement outliers, we applied generalized Bayesian approach to build a framework considering both process and measurement noise outliers.",0,arxiv,Ä°statistik,CC-BY/arXiv,A Robust State Filter Against Unmodeled Process And Measurement Noise
"Masked Diffusion Models (MDMs) have emerged as one of the most promising paradigms for generative modeling over discrete domains. It is known that MDMs effectively train to decode tokens in a random order, and that this ordering has significant performance implications in practice. This observation raises a fundamental question: can we design a training framework that optimizes for a favorable decoding order? We answer this in the affirmative, showing that the continuous-time variational objective of MDMs, when equipped with multivariate noise schedules, can identify and optimize for a decoding order during training. We establish a direct correspondence between decoding order and the multivariate noise schedule and show that this setting breaks invariance of the MDM objective to the noise schedule. Furthermore, we prove that the MDM objective decomposes precisely into a weighted auto-regressive losses over these orders, which establishes them as auto-regressive models with learnable orders.",0,arxiv,Ä°statistik,CC-BY/arXiv,Masked Diffusion Models are Secretly Learned-Order Autoregressive Models
"We propose a new formulation of the maximum score estimator that uses compositions of rectified linear unit (ReLU) functions, instead of indicator functions as in Manski (1975,1985), to encode the sign alignment restrictions. Since the ReLU function is Lipschitz, our new ReLU-based maximum score criterion function is substantially easier to optimize using standard gradient-based optimization pacakges. We also show that our ReLU-based maximum score (RMS) estimator can be generalized to an umbrella framework defined by multi-index single-crossing (MISC) conditions, while the original maximum score estimator cannot be applied. We establish the $n^{-s/(2s+1)}$ convergence rate and asymptotic normality for the RMS estimator under order-$s$ Holder smoothness. In addition, we propose an alternative estimator using a further reformulation of RMS as a special layer in a deep neural network (DNN) architecture, which allows the estimation procedure to be implemented via state-of-the-art software and hardware for DNN.",0,arxiv,Ä°statistik,CC-BY/arXiv,ReLU-Based and DNN-Based Generalized Maximum Score Estimators
"We study core stability in non-centroid clustering under the max-loss objective, where each agent's loss is the maximum distance to other members of their cluster. We prove that for all $k\geq 3$ there exist metric instances with $n\ge 9$ agents, with $n$ divisible by $k$, for which no clustering lies in the $Î±$-core for any $Î±<2^{\frac{1}{5}}\sim 1.148$. The bound is tight for our construction. Using a computer-aided proof, we also identify a two-dimensional Euclidean point set whose associated lower bound is slightly smaller than that of our general construction. This is, to our knowledge, the first impossibility result showing that the core can be empty in non-centroid clustering under the max-loss objective.",0,arxiv,Ä°statistik,CC-BY/arXiv,The Core in Max-Loss Non-Centroid Clustering Can Be Empty
"Unbalanced optimal transport (UOT) provides a flexible way to match or compare nonnegative finite Radon measures. However, UOT requires a predefined ground transport cost, which may misrepresent the data's underlying geometry. Choosing such a cost is particularly challenging when datasets live in heterogeneous spaces, often motivating practitioners to adopt Gromov-Wasserstein formulations. To address this challenge, we introduce cost-regularized unbalanced optimal transport (CR-UOT), a framework that allows the ground cost to vary while allowing mass creation and removal. We show that CR-UOT incorporates unbalanced Gromov-Wasserstein type problems through families of inner-product costs parameterized by linear transformations, enabling the matching of measures or point clouds across Euclidean spaces. We develop algorithms for such CR-UOT problems using entropic regularization and demonstrate that this approach improves the alignment of heterogeneous single-cell omics profiles, especially when many cells lack direct matches.",0,arxiv,Ä°statistik,CC-BY/arXiv,Structured Matching via Cost-Regularized Unbalanced Optimal Transport
"The mixture model is undoubtedly one of the greatest contributions to clustering. For continuous data, Gaussian models are often used and the Expectation-Maximization (EM) algorithm is particularly suitable for estimating parameters from which clustering is inferred. If these models are particularly popular in various domains including image clustering, they however suffer from the dimensionality and also from the slowness of convergence of the EM algorithm. However, the Classification EM (CEM) algorithm, a classifying version, offers a fast convergence solution while dimensionality reduction still remains a challenge. Thus we propose in this paper an algorithm combining simultaneously and non-sequentially the two tasks --Data embedding and Clustering-- relying on Principal Component Analysis (PCA) and CEM. We demonstrate the interest of such approach in terms of clustering and data embedding. We also establish different connections with other clustering approaches.",0,arxiv,Ä°statistik,CC-BY/arXiv,Classification EM-PCA for clustering and embedding
"Cross-subject motor-imagery decoding remains a major challenge in EEG-based brain-computer interfaces due to strong subject variability and the curved geometry of covariance matrices on the symmetric positive definite (SPD) manifold. We address the zero-shot cross-subject setting, where no target-subject labels or adaptation are allowed, by introducing novel geometry-aware preprocessing modules and deep congruence networks that operate directly on SPD covariance matrices. Our preprocessing modules, DCR and RiFU, extend Riemannian Alignment by improving action separation while reducing subject-specific distortions. We further propose two manifold classifiers, SPD-DCNet and RiFUNet, which use hierarchical congruence transforms to learn discriminative, subject-invariant covariance representations. On the BCI-IV 2a benchmark, our framework improves cross-subject accuracy by 3-4% over the strongest classical baselines, demonstrating the value of geometry-aware transformations for robust EEG decoding.",0,arxiv,Ä°statistik,CC-BY/arXiv,Geometry-Aware Deep Congruence Networks for Manifold Learning in Cross-Subject Motor Imagery
"The increasing use of machine learning in sensitive applications demands algorithms that simultaneously preserve data privacy and ensure fairness across potentially sensitive sub-populations. While privacy and fairness have each been extensively studied, their joint treatment remains poorly understood. Existing research often frames them as conflicting objectives, with multiple studies suggesting that strong privacy notions such as differential privacy inevitably compromise fairness. In this work, we challenge that perspective by showing that differential privacy can be integrated into a fairness-enhancing pipeline with minimal impact on fairness guarantees. We design a postprocessing algorithm, called DP2DP, that enforces both demographic parity and differential privacy. Our analysis reveals that our algorithm converges towards its demographic parity objective at essentially the same rate (up logarithmic factor) as the best non-private methods from the literature. Experiments on both synthetic and real datasets confirm our theoretical results, showing that the proposed algorithm achieves state-of-the-art accuracy/fairness/privacy trade-offs.",0,arxiv,Ä°statistik,CC-BY/arXiv,Fairness Meets Privacy: Integrating Differential Privacy and Demographic Parity in Multi-class Classification
"Deep neural networks are prone to learning shortcuts, spurious and easily learned correlations in training data that cause severe failures in out-of-distribution (OOD) generalization. A dominant line of work seeks robustness by learning a robust representation, often explicitly partitioning the latent space into core and spurious components; this approach can be complex, brittle, and difficult to scale. We take a different approach, instead of a robust representation, we learn a robust function. We present a simple and effective training method that renders the classifier functionally invariant to shortcut signals. Our method operates within a disentangled latent space, which is essential as it isolates spurious and core features into distinct dimensions. This separation enables the identification of candidate shortcut features by their strong correlation with the label, used as a proxy for semantic simplicity. The classifier is then desensitized to these features by injecting targeted, anisotropic latent noise during training. We analyze this as targeted Jacobian regularization, which forces the classifier to ignore spurious features and rely on more complex, core semantic signals. The result is state-of-the-art OOD performance on established shortcut learning benchmarks.",0,arxiv,Ä°statistik,CC-BY/arXiv,Shortcut Invariance: Targeted Jacobian Regularization in Disentangled Latent Space
"Persistent homology (PH) is a crucial concept in computational topology, providing a multiscale topological description of a space. It is particularly significant in topological data analysis, which aims to make statistical inference from a topological perspective. In this work, we introduce a new topological summary for Bayesian neural networks, termed the predictive topological uncertainty (pTU). The proposed pTU measures the uncertainty in the interaction between the model and the inputs. It provides insights from the model perspective: if two samples interact with a model in a similar way, then they are considered identically distributed. We also show that the pTU is insensitive to the model architecture. As an application, pTU is used to solve the out-of-distribution (OOD) detection problem, which is critical to ensure model reliability. Failure to detect OOD input can lead to incorrect and unreliable predictions. To address this issue, we propose a significance test for OOD based on the pTU, providing a statistical framework for this issue. The effectiveness of the framework is validated through various experiments, in terms of its statistical power, sensitivity, and robustness.",0,arxiv,Ä°statistik,CC-BY/arXiv,Uncertainty of Network Topology with Applications to Out-of-Distribution Detection
"We study the problem of excess risk evaluation for empirical risk minimization (ERM) under general convex loss functions. Our contribution is an efficient refitting procedure that computes the excess risk and provides high-probability upper bounds under the fixed-design setting. Assuming only black-box access to the training algorithm and a single dataset, we begin by generating two sets of artificially modified pseudo-outcomes termed wild response, created by stochastically perturbing the gradient vectors with carefully chosen scaling. Using these two pseudo-labeled datasets, we then refit the black-box procedure twice to obtain two corresponding wild predictors. Finally, leveraging the original predictor, the two wild predictors, and the constructed wild responses, we derive an efficient excess risk upper bound. A key feature of our analysis is that it requires no prior knowledge of the complexity of the underlying function class. As a result, the method is essentially model-free and holds significant promise for theoretically evaluating modern opaque machine learning system--such as deep nerral networks and generative model--where traditional capacity-based learning theory becomes infeasible due to the extreme complexity of the hypothesis class.",0,arxiv,Ä°statistik,CC-BY/arXiv,Doubly Wild Refitting: Model-Free Evaluation of High Dimensional Black-Box Predictions under Convex Losses
"Class imbalance remains a critical challenge in semi-supervised learning (SSL), especially when distributional mismatches between labeled and unlabeled data lead to biased classification. Although existing methods address this issue by adjusting logits based on the estimated class distribution of unlabeled data, they often handle model imbalance in a coarse-grained manner, conflating data imbalance with bias arising from varying class-specific learning difficulties. To address this issue, we propose a unified framework, SC-SSL, which suppresses model bias through decoupled sampling control. During training, we identify the key variables for sampling control under ideal conditions. By introducing a classifier with explicit expansion capability and adaptively adjusting sampling probabilities across different data distributions, SC-SSL mitigates feature-level imbalance for minority classes. In the inference phase, we further analyze the weight imbalance of the linear classifier and apply post-hoc sampling control with an optimization bias vector to directly calibrate the logits. Extensive experiments across various benchmark datasets and distribution settings validate the consistency and state-of-the-art performance of SC-SSL.",0,arxiv,Ä°statistik,CC-BY/arXiv,Sampling Control for Imbalanced Calibration in Semi-Supervised Learning
"Statistical inference from data generated by multi-armed bandit (MAB) algorithms is challenging due to their adaptive, non-i.i.d. nature. A classical manifestation is that sample averages of arm rewards under bandit sampling may fail to satisfy a central limit theorem. Lai and Wei's stability condition provides a sufficient, and essentially necessary criterion, for asymptotic normality in bandit problems. While the celebrated Upper Confidence Bound (UCB) algorithm satisfies this stability condition, it is not minimax optimal, raising the question of whether minimax optimality and statistical stability can be achieved simultaneously. In this paper, we analyze the stability properties of a broad class of bandit algorithms that are based on the optimism principle. We establish general structural conditions under which such algorithms violate the Lai-Wei stability criterion. As a consequence, we show that widely used minimax-optimal UCB-style algorithms, including MOSS, Anytime-MOSS, Vanilla-MOSS, ADA-UCB, OC-UCB, KL-MOSS, KL-UCB++, KL-UCB-SWITCH, and Anytime KL-UCB-SWITCH, are unstable. We further complement our theoretical results with numerical simulations demonstrating that, in all these cases, the sample means fail to exhibit asymptotic normality.   Overall, our findings suggest a fundamental tension between stability and minimax optimal regret, raising the question of whether it is possible to design bandit algorithms that achieve both. Understanding whether such simultaneously stable and minimax optimal strategies exist remains an important open direction.",0,arxiv,Ä°statistik,CC-BY/arXiv,On Instability of Minimax Optimal Optimism-Based Bandit Algorithms
"Time series anomaly detection is widely used in IoT and cyber-physical systems, yet its evaluation remains challenging due to diverse application objectives and heterogeneous metric assumptions. This study introduces a problem-oriented framework that reinterprets existing metrics based on the specific evaluation challenges they are designed to address, rather than their mathematical forms or output structures. We categorize over twenty commonly used metrics into six dimensions: 1) basic accuracy-driven evaluation; 2) timeliness-aware reward mechanisms; 3) tolerance to labeling imprecision; 4) penalties reflecting human-audit cost; 5) robustness against random or inflated scores; and 6) parameter-free comparability for cross-dataset benchmarking. Comprehensive experiments are conducted to examine metric behavior under genuine, random, and oracle detection scenarios. By comparing their resulting score distributions, we quantify each metric's discriminative ability -- its capability to distinguish meaningful detections from random noise. The results show that while most event-level metrics exhibit strong separability, several widely used metrics (e.g., NAB, Point-Adjust) demonstrate limited resistance to random-score inflation. These findings reveal that metric suitability must be inherently task-dependent and aligned with the operational objectives of IoT applications. The proposed framework offers a unified analytical perspective for understanding existing metrics and provides practical guidance for selecting or developing more context-aware, robust, and fair evaluation methodologies for time series anomaly detection.",0,arxiv,Ä°statistik,CC-BY/arXiv,A Problem-Oriented Taxonomy of Evaluation Metrics for Time Series Anomaly Detection
"We consider the problem of joint estimation of the parameters of $m$ linear dynamical systems, given access to single realizations of their respective trajectories, each of length $T$. The linear systems are assumed to reside on the nodes of an undirected and connected graph $G = ([m], \mathcal{E})$, and the system matrices are assumed to either vary smoothly or exhibit small number of ``jumps'' across the edges. We consider a total variation penalized least-squares estimator and derive non-asymptotic bounds on the mean squared error (MSE) which hold with high probability. In particular, the bounds imply for certain choices of well connected $G$ that the MSE goes to zero as $m$ increases, even when $T$ is constant. The theoretical results are supported by extensive experiments on synthetic and real data.",0,arxiv,Ä°statistik,CC-BY/arXiv,Joint learning of a network of linear dynamical systems via total variation penalization
"Global ocean forecasting aims to predict key ocean variables such as temperature, salinity, and currents, which is essential for understanding and describing oceanic phenomena. In recent years, data-driven deep learning-based ocean forecast models, such as XiHe, WenHai, LangYa and AI-GOMS, have demonstrated significant potential in capturing complex ocean dynamics and improving forecasting efficiency. Despite these advancements, the absence of open-source, standardized benchmarks has led to inconsistent data usage and evaluation methods. This gap hinders efficient model development, impedes fair performance comparison, and constrains interdisciplinary collaboration. To address this challenge, we propose OceanForecastBench, a benchmark offering three core contributions: (1) A high-quality global ocean reanalysis data over 28 years for model training, including 4 ocean variables across 23 depth levels and 4 sea surface variables. (2) A high-reliability satellite and in-situ observations for model evaluation, covering approximately 100 million locations in the global ocean. (3) An evaluation pipeline and a comprehensive benchmark with 6 typical baseline models, leveraging observations to evaluate model performance from multiple perspectives. OceanForecastBench represents the most comprehensive benchmarking framework currently available for data-driven ocean forecasting, offering an open-source platform for model development, evaluation, and comparison. The dataset and code are publicly available at: https://github.com/Ocean-Intelligent-Forecasting/OceanForecastBench.",0,arxiv,Ä°statistik,CC-BY/arXiv,OceanForecastBench: A Benchmark Dataset for Data-Driven Global Ocean Forecasting
"Scaling laws describe how learning performance improves with data, compute, or training time, and have become a central theme in modern deep learning. We study this phenomenon in a canonical nonlinear model: phase retrieval with anisotropic Gaussian inputs whose covariance spectrum follows a power law. Unlike the isotropic case, where dynamics collapse to a two-dimensional system, anisotropy yields a qualitatively new regime in which an infinite hierarchy of coupled equations governs the evolution of the summary statistics. We develop a tractable reduction that reveals a three-phase trajectory: (i) fast escape from low alignment, (ii) slow convergence of the summary statistics, and (iii) spectral-tail learning in low-variance directions. From this decomposition, we derive explicit scaling laws for the mean-squared error, showing how spectral decay dictates convergence times and error curves. Experiments confirm the predicted phases and exponents. These results provide the first rigorous characterization of scaling laws in nonlinear regression with anisotropic data, highlighting how anisotropy reshapes learning dynamics.",0,arxiv,Ä°statistik,CC-BY/arXiv,"Fast Escape, Slow Convergence: Learning Dynamics of Phase Retrieval under Power-Law Data"
"Corrupted training data are ubiquitous. Corrective Machine Unlearning (CMU) seeks to remove the influence of such corruption post-training. Prior CMU typically assumes access to identified corrupted training samples (a ""forget set""). However, in many real-world scenarios the training data are no longer accessible. We formalize source-free CMU, where the original training data are unavailable and, consequently, no forget set of identified corrupted training samples can be specified. Instead, we assume a small proxy (surrogate) set of corrupted samples that reflect the suspected corruption type without needing to be the original training samples. In this stricter setting, methods relying on forget set are ineffective or narrow in scope. We introduce Corrective Unlearning in Task Space (CUTS), a lightweight weight space correction method guided by the proxy set using task arithmetic principles. CUTS treats the clean and the corruption signal as distinct tasks. Specifically, we briefly fine-tune the corrupted model on the proxy to amplify the corruption mechanism in the weight space, compute the difference between the corrupted and fine-tuned weights as a proxy task vector, and subtract a calibrated multiple of this vector to cancel the corruption. Without access to clean data or a forget set, CUTS recovers a large fraction of the lost utility under label noise and, for backdoor triggers, nearly eliminates the attack with minimal damage to utility, outperforming state-of-the-art specialized CMU methods in source-free setting.",0,arxiv,Ä°statistik,CC-BY/arXiv,Subtract the Corruption: Training-Data-Free Corrective Machine Unlearning using Task Arithmetic
"Sampling multiple outputs from a Large Language Model (LLM) and selecting the most frequent (Self-consistency) or highest-scoring (Best-of-N) candidate is a popular approach to achieve higher accuracy in tasks with discrete final answers. Best-of-N (BoN) selects the output with the highest reward, and with perfect rewards, it often achieves near-perfect accuracy. With imperfect rewards from reward models, however, BoN fails to reliably find the correct answer and its performance degrades drastically. We consider the distribution of BoN's outputs and highlight that, although the correct answer does not usually have a probability close to one under imperfect rewards, it is often the most likely outcome. This suggests that the mode of this distribution can be more reliably correct than a sample from it. Based on this idea, we propose Majority-of-the-Bests (MoB), a novel selection mechanism that estimates the output distribution of BoN via bootstrapping and selects its mode. Experimental results across five benchmarks, three different base LLMs, and two reward models demonstrate consistent improvements over BoN in 25 out of 30 setups. We also provide theoretical results for the consistency of the bootstrapping. MoB serves as a simple, yet strong alternative to BoN and self-consistency, and more broadly, motivates further research in more nuanced selection mechanisms.",0,arxiv,Ä°statistik,CC-BY/arXiv,Majority of the Bests: Improving Best-of-N via Bootstrapping
"Label shift, a prevalent challenge in supervised learning, arises when the class prior distribution of test data differs from that of training data, leading to significant degradation in classifier performance. To accurately estimate the test priors and enhance classification accuracy, we propose a Bayesian framework for label shift estimation, termed Full Maximum A Posterior Label Shift (FMAPLS), along with its online version, online-FMAPLS. Leveraging batch and online Expectation-Maximization (EM) algorithms, these methods jointly and dynamically optimize Dirichlet hyperparameters $\boldsymbolÎ±$ and class priors $\boldsymbolÏ€$, thereby overcoming the rigid constraints of the existing Maximum A Posterior Label Shift (MAPLS) approach. Moreover, we introduce a linear surrogate function (LSF) to replace gradient-based hyperparameter updates, yielding closed-form solutions that reduce computational complexity while retaining asymptotic equivalence. The online variant substitutes the batch E-step with a stochastic approximation, enabling real-time adaptation to streaming data. Furthermore, our theoretical analysis reveals a fundamental trade-off between online convergence rate and estimation accuracy. Extensive experiments on CIFAR100 and ImageNet datasets under shuffled long-tail and Dirichlet test priors demonstrate that FMAPLS and online-FMAPLS respectively achieve up to 40% and 12% lower KL divergence and substantial improvements in post-shift accuracy over state-of-the-art baselines, particularly under severe class imbalance and distributional uncertainty. These results confirm the robustness, scalability, and suitability of the proposed methods for large-scale and dynamic learning scenarios.",0,arxiv,Ä°statistik,CC-BY/arXiv,Bayesian-based Online Label Shift Estimation with Dynamic Dirichlet Priors
"Reinforcement Learning from Human Feedback (RLHF) is widely used for aligning large language models, yet practitioners face a persistent puzzle: improving safety often reduces fairness, scaling to diverse populations becomes computationally intractable, and making systems robust often amplifies majority biases. We formalize this tension as the Alignment Trilemma: no RLHF system can simultaneously achieve (i) epsilon-representativeness across diverse human values, (ii) polynomial tractability in sample and compute complexity, and (iii) delta-robustness against adversarial perturbations and distribution shift. Through a complexity-theoretic analysis integrating statistical learning theory and robust optimization, we prove that achieving both representativeness (epsilon <= 0.01) and robustness (delta <= 0.001) for global-scale populations requires Omega(2^{d_context}) operations, which is super-polynomial in the context dimensionality. We show that current RLHF implementations resolve this trilemma by sacrificing representativeness: they collect only 10^3--10^4 samples from homogeneous annotator pools while 10^7--10^8 samples are needed for true global representation. Our framework provides a unified explanation for documented RLHF pathologies including preference collapse, sycophancy, and systematic bias amplification. We conclude with concrete directions for navigating these fundamental trade-offs through strategic relaxations of alignment requirements.",0,arxiv,Ä°statistik,CC-BY/arXiv,Position: The Complexity of Perfect AI Alignment -- Formalizing the RLHF Trilemma
"Dependent data underlies many statistical studies in the social and health sciences, which often involve sensitive or private information. Differential privacy (DP) and in particular \textit{user-level} DP provide a natural formalization of privacy requirements for processing dependent data where each individual provides multiple observations to the dataset. However, dependence introduced, e.g., through repeated measurements challenges the existing statistical theory under DP-constraints. In \iid{} settings, noisy Winsorized mean estimators have been shown to be minimax optimal for standard (\textit{item-level}) and \textit{user-level} DP estimation of a mean $Î¼\in \R^d$. Yet, their behavior on potentially dependent observations has not previously been studied. We fill this gap and show that Winsorized mean estimators can also be used under dependence for bounded and unbounded data, and can lead to asymptotic and finite sample guarantees that resemble their \iid{} counterparts under a weak notion of dependence. For this, we formalize dependence via log-Sobolev inequalities on the joint distribution of observations. This enables us to adapt the stable histogram by Karwa and Vadhan (2018) to a non-\iid{} setting, which we then use to estimate the private projection intervals of the Winsorized estimator. The resulting guarantees for our item-level mean estimator extend to \textit{user-level} mean estimation and transfer to the local model via a randomized response histogram. Using the mean estimators as building blocks, we provide extensions to random effects models, longitudinal linear regression and nonparametric regression. Therefore, our work constitutes a first step towards a systematic study of DP for dependent data.",0,arxiv,Ä°statistik,CC-BY/arXiv,Differential privacy with dependent data
"Conformal prediction (CP) provides distribution-free, finite-sample coverage guarantees but critically relies on exchangeability, a condition often violated under distribution shift. We study the robustness of split conformal prediction under adversarial perturbations at test time, focusing on both coverage validity and the resulting prediction set size. Our theoretical analysis characterizes how the strength of adversarial perturbations during calibration affects coverage guarantees under adversarial test conditions. We further examine the impact of adversarial training at the model-training stage. Extensive experiments support our theory: (i) Prediction coverage varies monotonically with the calibration-time attack strength, enabling the use of nonzero calibration-time attack to predictably control coverage under adversarial tests; (ii) target coverage can hold over a range of test-time attacks: with a suitable calibration attack, coverage stays within any chosen tolerance band across a contiguous set of perturbation levels; and (iii) adversarial training at the training stage produces tighter prediction sets that retain high informativeness.",0,arxiv,Ä°statistik,CC-BY/arXiv,Ensuring Calibration Robustness in Split Conformal Prediction Under Adversarial Attacks
"We develop an all-at-once modeling framework for learning systems of ordinary differential equations (ODE) from scarce, partial, and noisy observations of the states. The proposed methodology amounts to a combination of sparse recovery strategies for the ODE over a function library combined with techniques from reproducing kernel Hilbert space (RKHS) theory for estimating the state and discretizing the ODE. Our numerical experiments reveal that the proposed strategy leads to significant gains in terms of accuracy, sample efficiency, and robustness to noise, both in terms of learning the equation and estimating the unknown states. This work demonstrates capabilities well beyond existing and widely used algorithms while extending the modeling flexibility of other recent developments in equation discovery.",0,arxiv,Ä°statistik,CC-BY/arXiv,A joint optimization approach to identifying sparse dynamics using least squares kernel collocation
"We study the problem of matching correlated VAR time series databases, where a multivariate time series is observed along with a perturbed and permuted version, and the goal is to recover the unknown matching between them. To model this, we introduce a probabilistic framework in which two time series $(x_t)_{t\in[T]},(x^\#_t)_{t\in[T]}$ are jointly generated, such that $x^\#_t=x_{Ï€^*(t)}+Ïƒ\tilde{x}_{Ï€^*(t)}$, where $(x_t)_{t\in[T]},(\tilde{x}_t)_{t\in[T]}$ are independent and identically distributed vector autoregressive (VAR) time series of order $1$ with Gaussian increments, for a hidden $Ï€^*$. The objective is to recover $Ï€^*$, from the observation of $(x_t)_{t\in[T]},(x^\#_t)_{t\in[T]}$. This generalizes the classical problem of matching independent point clouds to the time series setting.   We derive the maximum likelihood estimator (MLE), leading to a quadratic optimization over permutations, and theoretically analyze an estimator based on linear assignment. For the latter approach, we establish recovery guarantees, identifying thresholds for $Ïƒ$ that allow for perfect or partial recovery. Additionally, we propose solving the MLE by considering convex relaxations of the set of permutation matrices (e.g., over the Birkhoff polytope). This allows for efficient estimation of $Ï€^*$ and the VAR parameters via alternating minimization. Empirically, we find that linear assignment often matches or outperforms MLE relaxation based approaches.",0,arxiv,Ä°statistik,CC-BY/arXiv,Matching correlated VAR time series
"This paper presents robust inference methods for general linear hypotheses in linear panel data models with latent group structure in the coefficients. We employ a selective conditional inference approach, deriving the conditional distribution of coefficient estimates given the group structure estimated from the data. Our procedure provides valid inference under possible violations of group separation, where distributional properties of group-specific coefficients remain unestablished. Furthermore, even when group separation does hold, our method demonstrates superior finite-sample properties compared to traditional asymptotic approaches. This improvement stems from our procedure's ability to account for statistical uncertainty in the estimation of group structure. We demonstrate the effectiveness of our approach through Monte Carlo simulations and apply the methods to two datasets on: (i) the relationship between income and democracy, and (ii) the cyclicality of firm-level R&D investment.",0,arxiv,Ä°statistik,CC-BY/arXiv,Robust Inference Methods for Latent Group Panel Models under Possible Group Non-Separation
"We propose a way of transforming the problem of conditional density estimation into a single nonparametric regression task via the introduction of auxiliary samples. This allows leveraging regression methods that work well in high dimensions, such as neural networks and decision trees. Our main theoretical result characterizes and establishes the convergence of our estimator to the true conditional density in the data limit. We develop condensitÃ©, a method that implements this approach. We demonstrate the benefit of the auxiliary samples on synthetic data and showcase that condensitÃ© can achieve good out-of-the-box results. We evaluate our method on a large population survey dataset and on a satellite imaging dataset. In both cases, we find that condensitÃ© matches or outperforms the state of the art and yields conditional densities in line with established findings in the literature on each dataset. Our contribution opens up new possibilities for regression-based conditional density estimation and the empirical results indicate strong promise for applied research.",0,arxiv,Ä°statistik,CC-BY/arXiv,Transforming Conditional Density Estimation Into a Single Nonparametric Regression Task
"We study the problem of selecting the best heterogeneous treatment effect (HTE) estimator from a collection of candidates in settings where the treatment effect is fundamentally unobserved. We cast estimator selection as a multiple testing problem and introduce a ground-truth-free procedure based on a cross-fitted, exponentially weighted test statistic. A key component of our method is a two-way sample splitting scheme that decouples nuisance estimation from weight learning and ensures the stability required for valid inference. Leveraging a stability-based central limit theorem, we establish asymptotic familywise error rate control under mild regularity conditions. Empirically, our procedure provides reliable error control while substantially reducing false selections compared with commonly used methods across ACIC 2016, IHDP, and Twins benchmarks, demonstrating that our method is feasible and powerful even without ground-truth treatment effects.",0,arxiv,Ä°statistik,CC-BY/arXiv,Reliable Selection of Heterogeneous Treatment Effect Estimators
"RFX (Random Forests X), where X stands for compression or quantization, presents a production-ready implementation of Breiman and Cutler's Random Forest classification methodology in Python. RFX v1.0 provides complete classification: out-of-bag error estimation, overall and local importance measures, proximity matrices with QLORA compression, case-wise analysis, and interactive visualization (rfviz)--all with CPU and GPU acceleration. Regression, unsupervised learning, CLIQUE importance, and RF-GAP proximity are planned for v2.0.   This work introduces four solutions addressing the proximity matrix memory bottleneck limiting Random Forest analysis to ~60,000 samples: (1) QLORA (Quantized Low-Rank Adaptation) compression for GPU proximity matrices, reducing memory from 80GB to 6.4MB for 100k samples (12,500x compression with INT8 quantization) while maintaining 99% geometric structure preservation, (2) CPU TriBlock proximity--combining upper-triangle storage with block-sparse thresholding--achieving 2.7x memory reduction with lossless quality, (3) SM-aware GPU batch sizing achieving 95% GPU utilization, and (4) GPU-accelerated 3D MDS visualization computing embeddings directly from low-rank factors using power iteration.   Validation across four implementation modes (GPU/CPU x case-wise/non-case-wise) demonstrates correct implementation. GPU achieves 1.4x speedup over CPU for overall importance with 500+ trees. Proximity computation scales from 1,000 to 200,000+ samples (requiring GPU QLORA), with CPU TriBlock filling the gap for medium-scale datasets (10K-50K samples). RFX v1.0 eliminates the proximity memory bottleneck, enabling proximity-based Random Forest analysis on datasets orders of magnitude larger than previously feasible. Open-source production-ready classification following Breiman and Cutler's original methodology.",0,arxiv,Ä°statistik,CC-BY/arXiv,RFX: High-Performance Random Forests with GPU Acceleration and QLORA Compression
"Recent work has demonstrated the utility of Random Forest (RF) proximities for various supervised machine learning tasks, including outlier detection, missing data imputation, and visualization. However, the utility of the RF proximities depends upon the success of the RF model, which itself is not the ideal model in all contexts. RF proximities have recently been extended to time series by means of the distance-based Proximity Forest (PF) model, among others, affording time series analysis with the benefits of RF proximities. In this work, we introduce the generalized PF model, thereby extending RF proximities to all contexts in which supervised distance-based machine learning can occur. Additionally, we introduce a variant of the PF model for regression tasks. We also introduce the notion of using the generalized PF model as a meta-learning framework, extending supervised imputation capability to any pre-trained classifier. We experimentally demonstrate the unique advantages of the generalized PF model compared with both the RF model and the $k$-nearest neighbors model.",0,arxiv,Ä°statistik,CC-BY/arXiv,The Generalized Proximity Forest
"Motivated by recent work involving the analysis of leveraging spatial correlations in sparsified mean estimation, we present a novel procedure for constructing covariance estimator. The proposed Random-knots (Random-knots-Spatial) and B-spline (Bspline-Spatial) estimators of the covariance function are computationally efficient. Asymptotic pointwise of the covariance are obtained for sparsified individual trajectories under some regularity conditions. Our proposed nonparametric method well perform the functional principal components analysis for the case of sparsified data, where the number of repeated measurements available per subject is small. In contrast, classical functional data analysis requires a large number of regularly spaced measurements per subject. Model selection techniques, such as the Akaike information criterion, are used to choose the model dimension corresponding to the number of eigenfunctions in the model. Theoretical results are illustrated with Monte Carlo simulation experiments. Finally, we cluster multi-domain data by replacing the covariance function with our proposed covariance estimator during PCA.",0,arxiv,Ä°statistik,CC-BY/arXiv,Efficient Covariance Estimation for Sparsified Functional Data
"Quantum machine learning seeks to leverage quantum computers to improve upon classical machine learning algorithms. Currently, robust uncertainty quantification methods remain underdeveloped in the quantum domain, despite the critical need for reliable and trustworthy predictions. Recent work has introduced quantum conformal prediction, a framework that produces prediction sets that are guaranteed to contain the true outcome with user-specified probability. In this work, we formalise how the time-varying noise inherent in quantum processors can undermine conformal guarantees, even when calibration and test data are exchangeable. To address this challenge, we draw on Adaptive Conformal Inference, a method which maintains validity over time via repeated recalibration. We introduce Adaptive Quantum Conformal Prediction (AQCP), an algorithm which preserves asymptotic average coverage guarantees under arbitrary hardware noise conditions. Empirical studies on an IBM quantum processor demonstrate that AQCP achieves target coverage levels and exhibits greater stability than quantum conformal prediction.",0,arxiv,Ä°statistik,CC-BY/arXiv,Adaptive Conformal Prediction for Quantum Machine Learning
"Ecological Momentary Assessment provides real-time data on suicidal thoughts and behaviors, but predicting suicide attempts remains challenging due to their rarity and patient heterogeneity. We show that single models fit to all patients perform poorly, while individualized models improve performance but still overfit to patients with limited data. To address this, we introduce Latent Similarity Gaussian Processes (LSGPs) to capture patient heterogeneity, enabling those with little data to leverage similar patients' trends. Preliminary results show promise: even without kernel-design, we outperform all but one baseline while offering a new understanding of patient similarity.",0,arxiv,Ä°statistik,CC-BY/arXiv,Improving Forecasts of Suicide Attempts for Patients with Little Data
"We propose and analyze a variant of Sparse Polyak for high dimensional M-estimation problems. Sparse Polyak proposes a novel adaptive step-size rule tailored to suitably estimate the problem's curvature in the high-dimensional setting, guaranteeing that the algorithm's performance does not deteriorate when the ambient dimension increases. However, convergence guarantees can only be obtained by sacrificing solution sparsity and statistical accuracy. In this work, we introduce a variant of Sparse Polyak that retains its desirable scaling properties with respect to the ambient dimension while obtaining sparser and more accurate solutions.",0,arxiv,Ä°statistik,CC-BY/arXiv,Sparse Polyak with optimal thresholding operators for high-dimensional M-estimation
"In this work, we propose a set of conformal prediction procedures tailored to compositional responses, where outcomes are proportions that must be positive and sum to one. Building on Dirichlet regression, we introduce a split conformal approach based on quantile residuals and a highest-density region strategy that combines a fast coordinate-floor approximation with an internal grid refinement to restore sharpness. Both constructions are model-agnostic at the conformal layer and guarantee finite-sample marginal coverage under exchangeability, while respecting the geometry of the simplex. A comprehensive Monte Carlo study spanning homoscedastic and heteroscedastic designs shows that the quantile residual and grid-refined HDR methods achieve empirical coverage close to the nominal 90\% level and produce substantially narrower regions than the coordinate-floor approximation, which tends to be conservative. We further demonstrate the methods on household budget shares from the BudgetItaly dataset, using standardized socioeconomic and price covariates with a train, calibration, and test split. In this application, the grid-refined HDR attains coverage closest to the target with the smallest average widths, closely followed by the quantile residual approach, while the simple triangular HDR yields wider, less informative sets. Overall, the results indicate that conformal prediction on the simplex can be both calibrated and efficient, providing practical uncertainty quantification for compositional prediction tasks.",0,arxiv,Ä°statistik,CC-BY/arXiv,Conformal Prediction for Compositional Data
"Accurately solving partial differential equations (PDEs) is critical to understanding complex scientific and engineering phenomena, yet traditional numerical solvers are computationally expensive. Surrogate models offer a more efficient alternative, but their development is hindered by the cost of generating sufficient training data from numerical solvers. In this paper, we present a novel framework for active learning (AL) in PDE surrogate modeling that reduces this cost. Unlike the existing AL methods for PDEs that always acquire entire PDE trajectories, our approach strategically generates only the most important time steps with the numerical solver, while employing the surrogate model to approximate the remaining steps. This dramatically reduces the cost incurred by each trajectory and thus allows the active learning algorithm to try out a more diverse set of trajectories given the same budget. To accommodate this novel framework, we develop an acquisition function that estimates the utility of a set of time steps by approximating its resulting variance reduction. We demonstrate the effectiveness of our method on several benchmark PDEs, including the Burgers' equation, Korteweg-De Vries equation, Kuramoto-Sivashinsky equation, the incompressible Navier-Stokes equation, and the compressible Navier-Stokes equation. Experiments show that our approach improves performance by large margins over the best existing method. Our method not only reduces average error but also the 99\%, 95\%, and 50\% quantiles of error, which is rare for an AL algorithm. All in all, our approach offers a data-efficient solution to surrogate modeling for PDEs.",0,arxiv,Ä°statistik,CC-BY/arXiv,Active Learning with Selective Time-Step Acquisition for PDEs
"Wasserstein-Fisher-Rao (WFR) gradient flows have been recently proposed as a powerful sampling tool that combines the advantages of pure Wasserstein (W) and pure Fisher-Rao (FR) gradient flows. Existing algorithmic developments implicitly make use of operator splitting techniques to numerically approximate the WFR partial differential equation, whereby the W flow is evaluated over a given step size and then the FR flow (or vice versa). This works investigates the impact of the order in which the W and FR operator are evaluated and aims to provide a quantitative analysis. Somewhat surprisingly, we show that with a judicious choice of step size and operator ordering, the split scheme can converge to the target distribution faster than the exact WFR flow (in terms of model time). We obtain variational formulae describing the evolution over one time step of both sequential splitting schemes and investigate in which settings the W-FR split should be preferred to the FR-W split. As a step towards this goal we show that the WFR gradient flow preserves log-concavity and obtain the first sharp decay bound for WFR.",0,arxiv,Ä°statistik,CC-BY/arXiv,An operator splitting analysis of Wasserstein--Fisher--Rao gradient flows
"Hierarchical clustering seeks to uncover nested structures in data by constructing a tree of clusters, where deeper levels reveal finer-grained relationships. Traditional methods, including linkage approaches, face three major limitations: (i) they always return a hierarchy, even if none exists, (ii) they are restricted to binary trees, even if the true hierarchy is non-binary, and (iii) they are highly sensitive to the choice of linkage function. In this paper, we address these issues by introducing the notion of a valid hierarchy and defining a partial order over the set of valid hierarchies. We prove the existence of a finest valid hierarchy, that is, the hierarchy that encodes the maximum information consistent with the similarity structure of the data set. In particular, the finest valid hierarchy is not constrained to binary structures and, when no hierarchical relationships exist, collapses to a star tree. We propose a simple two-step algorithm that first constructs a binary tree via a linkage method and then prunes it to enforce validity. We establish necessary and sufficient conditions on the linkage function under which this procedure exactly recovers the finest valid hierarchy, and we show that all linkage functions satisfying these conditions yield the same hierarchy after pruning. Notably, classical linkage rules such as single, complete, and average satisfy these conditions, whereas Ward's linkage fails to do so.",0,arxiv,Ä°statistik,CC-BY/arXiv,Hierarchical Linkage Clustering Beyond Binary Trees and Ultrametrics
"This paper presents a real time, data driven decision support framework for epidemic control. We combine a compartmental epidemic model with sequential Bayesian inference and reinforcement learning (RL) controllers that adaptively choose intervention levels to balance disease burden, such as intensive care unit (ICU) load, against socio economic costs. We construct a context specific cost function using empirical experiments and expert feedback. We study two RL policies: an ICU threshold rule computed via Monte Carlo grid search, and a policy based on a posterior averaged Q learning agent. We validate the framework by fitting the epidemic model to publicly available ICU occupancy data from the COVID 19 pandemic in England and then generating counterfactual roll out scenarios under each RL controller, which allows us to compare the RL policies to the historical government strategy. Over a 300 day period and for a range of cost parameters, both controllers substantially reduce ICU burden relative to the observed interventions, illustrating how Bayesian sequential learning combined with RL can support the design of epidemic control policies.",0,arxiv,Ä°statistik,CC-BY/arXiv,"On a Reinforcement Learning Methodology for Epidemic Control, with application to COVID-19"
"Coreset selection compresses large datasets into compact, representative subsets, reducing the energy and computational burden of training deep neural networks. Existing methods are either: (i) DNN-based, which are tied to model-specific parameters and introduce architectural bias; or (ii) DNN-free, which rely on heuristics lacking theoretical guarantees. Neither approach explicitly constrains distributional equivalence, largely because continuous distribution matching is considered inapplicable to discrete sampling. Moreover, prevalent metrics (e.g., MSE, KL, MMD, CE) cannot accurately capture higher-order moment discrepancies, leading to suboptimal coresets. In this work, we propose FAST, the first DNN-free distribution-matching coreset selection framework that formulates the coreset selection task as a graph-constrained optimization problem grounded in spectral graph theory and employs the Characteristic Function Distance (CFD) to capture full distributional information in the frequency domain. We further discover that naive CFD suffers from a ""vanishing phase gradient"" issue in medium and high-frequency regions; to address this, we introduce an Attenuated Phase-Decoupled CFD. Furthermore, for better convergence, we design a Progressive Discrepancy-Aware Sampling strategy that progressively schedules frequency selection from low to high, preserving global structure before refining local details and enabling accurate matching with fewer frequencies while avoiding overfitting. Extensive experiments demonstrate that FAST significantly outperforms state-of-the-art coreset selection methods across all evaluated benchmarks, achieving an average accuracy gain of 9.12%. Compared to other baseline coreset methods, it reduces power consumption by 96.57% and achieves a 2.2x average speedup, underscoring its high performance and energy efficiency.",0,arxiv,Ä°statistik,CC-BY/arXiv,FAST: Topology-Aware Frequency-Domain Distribution Matching for Coreset Selection
"We study differentially private model training with stochastic gradient descent under learning rate scheduling and correlated noise. Although correlated noise, in particular via matrix factorizations, has been shown to improve accuracy, prior theoretical work focused primarily on the prefix-sum workload. That workload assumes a constant learning rate, whereas in practice learning rate schedules are widely used to accelerate training and improve convergence. We close this gap by deriving general upper and lower bounds for a broad class of learning rate schedules in both single- and multi-epoch settings. Building on these results, we propose a learning-rate-aware factorization that achieves improvements over prefix-sum factorizations under both MaxSE and MeanSE error metrics. Our theoretical analysis yields memory-efficient constructions suitable for practical deployment, and experiments on CIFAR-10 and IMDB datasets confirm that schedule-aware factorizations improve accuracy in private training.",0,arxiv,Ä°statistik,CC-BY/arXiv,Learning Rate Scheduling with Matrix Factorization for Private Training
"Clustering in stationary and nonstationary settings, where data distributions remain static or evolve over time, requires models that can adapt to distributional shifts while preserving previously learned cluster structures. This paper proposes an Adaptive Resonance Theory (ART)-based topological clustering algorithm that autonomously adjusts its recalculation interval and vigilance threshold through a diversity-driven adaptation mechanism. This mechanism enables hyperparameter-free learning that maintains cluster stability and continuity in dynamic environments. Experiments on 24 real-world datasets demonstrate that the proposed algorithm outperforms state-of-the-art methods in both clustering performance and continual learning capability. These results highlight the effectiveness of the proposed parameter adaptation in mitigating catastrophic forgetting and maintaining consistent clustering in evolving data streams. Source code is available at https://github.com/Masuyama-lab/IDAT",0,arxiv,Ä°statistik,CC-BY/arXiv,An Adaptive Resonance Theory-based Topological Clustering Algorithm with a Self-Adjusting Vigilance Parameter
"We develop a divergence-minimization (DM) framework for robust and efficient inference in latent-mixture models. By optimizing a residual-adjusted divergence, the DM approach recovers EM as a special case and yields robust alternatives through different divergence choices. We establish that the sample objective decreases monotonically along the iterates, leading the DM sequence to stationary points under standard conditions, and that at the population level the operator exhibits local contractivity near the minimizer. Additionally, we verify consistency and $\sqrt{n}$-asymptotic normality of minimum-divergence estimators and of finitely many DM iterations, showing that under correct specification their limiting covariance matches the Fisher information. Robustness is analyzed via the residual-adjustment function, yielding bounded influence functions and a strictly positive breakdown bound for bounded-RAF divergences, and we contrast this with the non-robust behaviour of KL/EM. Next, we address the challenge of determining the number of mixture components by proposing a penalized divergence criterion combined with repeated sample splitting, which delivers consistent order selection and valid post-selection inference. Empirically, DM instantiations based on Hellinger and negative exponential divergences deliver accurate inference and remain stable under contamination in mixture and image-segmentation tasks. The results clarify connections to MM and proximal-point methods and offer practical defaults, making DM a drop-in alternative to EM for robust latent-structure inference.",0,arxiv,Ä°statistik,CC-BY/arXiv,"Divergence-Minimization for Latent-Structure Models: Monotone Operators, Contraction Guarantees, and Robust Inference"
"Intelligent agents equipped with causal knowledge can optimize their action spaces to avoid unnecessary exploration. The structural causal bandit framework provides a graphical characterization for identifying actions that are unable to maximize rewards by leveraging prior knowledge of the underlying causal structure. While such knowledge enables an agent to estimate the expected rewards of certain actions based on others in online interactions, there has been little guidance on how to transfer information inferred from arbitrary combinations of datasets collected under different conditions -- observational or experimental -- and from heterogeneous environments. In this paper, we investigate the structural causal bandit with transportability, where priors from the source environments are fused to enhance learning in the deployment setting. We demonstrate that it is possible to exploit invariances across environments to consistently improve learning. The resulting bandit algorithm achieves a sub-linear regret bound with an explicit dependence on informativeness of prior data, and it may outperform standard bandit approaches that rely solely on online learning.",0,arxiv,Ä°statistik,CC-BY/arXiv,On Transportability for Structural Causal Bandits
"Many deployed learning systems must update models on streaming data under memory constraints. The default strategy, sequential fine-tuning on each new phase, is architecture-agnostic but often suffers catastrophic forgetting when later phases correspond to different sub-populations or tasks. Replay with a finite buffer is a simple alternative, yet its behaviour across generative and predictive objectives is not well understood. We present a unified study of stateful replay for streaming autoencoding, time series forecasting, and classification. We view both sequential fine-tuning and replay as stochastic gradient methods for an ideal joint objective, and use a gradient alignment analysis to show when mixing current and historical samples should reduce forgetting. We then evaluate a single replay mechanism on six streaming scenarios built from Rotated MNIST, ElectricityLoadDiagrams 2011-2014, and Airlines delay data, using matched training budgets and three seeds. On heterogeneous multi task streams, replay reduces average forgetting by a factor of two to three, while on benign time based streams both methods perform similarly. These results position stateful replay as a strong and simple baseline for continual learning in streaming environments.",0,arxiv,Ä°statistik,CC-BY/arXiv,Mitigating Catastrophic Forgetting in Streaming Generative and Predictive Learning via Stateful Replay
"Distributed Fiber Optic Sensing (DFOS) has shown strong potential in perimeter security due to its capability of monitoring vibration events across long distances with fine spatial resolution. However, practical DFOS systems face three critical challenges: (1) signal patterns of the same activity vary drastically under different fiber deployment types (e.g., underground, wall-mounted), causing domain shift; (2) labeled data in new deployment scenarios is often scarce or entirely unavailable, limiting model adaptability; and (3) even within source domains, data scarcity makes it difficult to capture intra-class diversity for robust learning.   To address these challenges, we propose a novel meta-learning framework, DUPLE, for cross-deployment DFOS activity identification. First, a dual-domain multi-prototype learner fuses temporal and frequency domain features, enhancing the model's generalization ability under signal distribution shifts. Second, a Statistical Guided Network (SGN) infers domain importance and prototype sensitivity from raw statistical features, providing data-driven prior information for learning in unlabeled or unseen domains. Third, a query-aware prototype aggregation module adaptively selects and combines relevant prototypes, thereby improving classification performance even with limited data.   Extensive experiments on cross-deployment DFOS datasets demonstrate that our method significantly outperforms baseline approaches in domain generalization settings, enabling robust event recognition across diverse fiber configurations with minimal labeled data.",0,arxiv,Ä°statistik,CC-BY/arXiv,Statistically-Guided Dual-Domain Meta-Learning with Adaptive Multi-Prototype Aggregation for Distributed Fiber Optic Sensing
"We develop an arbitrage-free deep learning framework for yield curve and bond price forecasting based on the Heath-Jarrow-Morton (HJM) term-structure model and a dynamic Nelson-Siegel parameterization of forward rates. Our approach embeds a no-arbitrage drift restriction into a neural state-space architecture by combining Kalman, extended Kalman, and particle filters with recurrent neural networks (LSTM/CLSTM), and introduces an explicit arbitrage error regularization (AER) term during training. The model is applied to U.S. Treasury and corporate bond data, and its performance is evaluated for both yield-space and price-space predictions at 1-day and 5-day horizons. Empirically, arbitrage regularization leads to its strongest improvements at short maturities, particularly in 5-day-ahead forecasts, increasing market-consistency as measured by bid-ask hit rates and reducing dollar-denominated prediction errors.",0,arxiv,Ä°statistik,CC-BY/arXiv,Arbitrage-Free Bond and Yield Curve Forecasting with Neural Filters under HJM Constraints
"Conformal prediction (CP) is a general framework to quantify the predictive uncertainty of machine learning models that uses a set prediction to include the true label with a valid probability. To align the uncertainty measured by CP, conformal training methods minimize the size of the prediction sets. A typical way is to use a surrogate indicator function, usually Sigmoid or Gaussian error function. However, these surrogate functions do not have a uniform error bound to the indicator function, leading to uncontrollable learning bounds. In this paper, we propose a simple cost-sensitive conformal training algorithm that does not rely on the indicator approximation mechanism. Specifically, we theoretically show that minimizing the expected size of prediction sets is upper bounded by the expected rank of true labels. To this end, we develop a rank weighting strategy that assigns the weight using the rank of true label on each data sample. Our analysis provably demonstrates the tightness between the proposed weighted objective and the expected size of conformal prediction sets. Extensive experiments verify the validity of our theoretical insights, and superior empirical performance over other conformal training in terms of predictive efficiency with 21.38% reduction for average prediction set size.",0,arxiv,Ä°statistik,CC-BY/arXiv,Cost-Sensitive Conformal Training with Provably Controllable Learning Bounds
"Transformers can acquire Chain-of-Thought (CoT) capabilities to solve complex reasoning tasks through fine-tuning. Reinforcement learning (RL) and supervised fine-tuning (SFT) are two primary approaches to this end, yet their underlying mechanisms and differences remain theoretically unclear. In this work, we examine these aspects specifically for learning $k$-sparse Boolean functions with a one-layer transformer and intermediate supervision that is akin to CoT. In particular, we consider $k$-sparse Boolean functions that can be recursively decomposed into fixed 2-sparse Boolean functions. We analyze the learning dynamics of fine-tuning the transformer via either RL or SFT with CoT to identify sufficient conditions for it to provably learn these functions. We verify that these conditions hold for three basic examples, including $k$-PARITY, $k$-AND, and $k$-OR, thus demonstrating the learnability of both approaches. Notably, we reveal that RL and SFT exhibit distinct learning behaviors: RL learns the whole CoT chain simultaneously, whereas SFT learns the CoT chain step-by-step. Overall, our findings provide theoretical insights into the underlying mechanisms of RL and SFT as well as how they differ in triggering the CoT capabilities of transformers.",0,arxiv,Ä°statistik,CC-BY/arXiv,"Transformers with RL or SFT Provably Learn Sparse Boolean Functions, But Differently"
"Interpretable topic modeling is essential for tracking how research interests evolve within co-author communities. In scientific corpora, where novelty is prized, identifying underrepresented niche topics is particularly important. However, contemporary models built from dense transformer embeddings tend to miss rare topics and therefore also fail to capture smooth temporal alignment. We propose a geometric method that integrates multimodal text and co-author network data, using Hellinger distances and Ward's linkage to construct a hierarchical topic dendrogram. This approach captures both local and global structure, supporting multiscale learning across semantic and temporal dimensions. Our method effectively identifies rare-topic structure and visualizes smooth topic drift over time. Experiments highlight the strength of interpretable bag-of-words models when paired with principled geometric alignment.",0,arxiv,Ä°statistik,CC-BY/arXiv,A Multiscale Geometric Method for Capturing Relational Topic Alignment
"Deterministic inference is increasingly critical for large language model (LLM) applications such as LLM-as-a-judge evaluation, multi-agent systems, and Reinforcement Learning (RL). However, existing LLM serving frameworks exhibit non-deterministic behavior: identical inputs can yield different outputs when system configurations (e.g., tensor parallel (TP) size, batch size) vary, even under greedy decoding. This arises from the non-associativity of floating-point arithmetic and inconsistent reduction orders across GPUs. While prior work has addressed batch-size-related nondeterminism through batch-invariant kernels, determinism across different TP sizes remains an open problem, particularly in RL settings, where the training engine typically uses Fully Sharded Data Parallel (i.e., TP = 1) while the rollout engine relies on multi-GPU TP to maximize the inference throughput, creating a natural mismatch between the two. This precision mismatch problem may lead to suboptimal performance or even collapse for RL training. We identify and analyze the root causes of TP-induced inconsistency and propose Tree-Based Invariant Kernels (TBIK), a set of TP-invariant matrix multiplication and reduction primitives that guarantee bit-wise identical results regardless of TP size. Our key insight is to align intra- and inter-GPU reduction orders through a unified hierarchical binary tree structure. We implement these kernels in Triton and integrate them into vLLM and FSDP. Experiments confirm zero probability divergence and bit-wise reproducibility for deterministic inference across different TP sizes. Also, we achieve bit-wise identical results between vLLM and FSDP in RL training pipelines with different parallel strategy. Code is available at https://github.com/nanomaoli/llm_reproducibility.",0,arxiv,Ä°statistik,CC-BY/arXiv,Deterministic Inference across Tensor Parallel Sizes That Eliminates Training-Inference Mismatch
"Clustering algorithms have long been the topic of research, representing the more popular side of unsupervised learning. Since clustering analysis is one of the best ways to find some clarity and structure within raw data, this paper explores a novel approach to \textit{k}-means clustering. Here we present a \textit{k}-means clustering algorithm that takes both the within cluster distance (WCD) and the inter cluster distance (ICD) as the distance metric to cluster the data into \emph{k} clusters pre-determined by the Calinski-Harabasz criterion in order to provide a more robust output for the clustering analysis. The idea with this approach is that by including both the measurement metrics, the convergence of the data into their clusters becomes solidified and more robust. We run the algorithm with some synthetically produced data and also some benchmark data sets obtained from the UCI repository. The results show that the convergence of the data into their respective clusters is more accurate by using both WCD and ICD measurement metrics. The algorithm is also better at clustering the outliers into their true clusters as opposed to the traditional \textit{k} means method. We also address some interesting possible research topics that reveal themselves as we answer the questions we initially set out to address.",0,arxiv,Ä°statistik,CC-BY/arXiv,A novel k-means clustering approach using two distance measures for Gaussian data
"In list-decodable learning, we are given a set of data points such that an $Î±$-fraction of these points come from a nice distribution $D$, for some small $Î±\ll 1$, and the goal is to output a short list of candidate solutions, such that at least one element of this list recovers some non-trivial information about $D$. By now, there is a large body of work on this topic; however, while many algorithms can achieve optimal list size in terms of $Î±$, all known algorithms must incur error which decays, in some cases quite poorly, with $1 / Î±$. In this paper, we ask if this is inherent: is it possible to trade off list size with accuracy in list-decodable learning? More formally, given $Îµ> 0$, can we can output a slightly larger list in terms of $Î±$ and $Îµ$, but so that one element of this list has error at most $Îµ$ with the ground truth? We call this problem high-accuracy list-decodable learning. Our main result is that non-trivial high-accuracy guarantees, both information-theoretically and algorithmically, are possible for the canonical setting of list-decodable mean estimation of identity-covariance Gaussians. Specifically, we demonstrate that there exists a list of candidate means of size at most $L = \exp \left( O\left( \tfrac{\log^2 1 / Î±}{Îµ^2} \right)\right)$ so that one of the elements of this list has $\ell_2$ distance at most $Îµ$ to the true mean. We also design an algorithm that outputs such a list with runtime and sample complexity $n = d^{O(\log L)} + \exp \exp (\widetilde{O}(\log L))$. We do so by demonstrating a completely novel proof of identifiability, as well as a new algorithmic way of leveraging this proof without the sum-of-squares hierarchy, which may be of independent technical interest.",0,arxiv,Ä°statistik,CC-BY/arXiv,High-Accuracy List-Decodable Mean Estimation
"Verifying uniform conditions over continuous spaces through random sampling is fundamental in machine learning and control theory, yet classical coverage analyses often yield conservative bounds, particularly at small failure probabilities. We study uniform random sampling on the $d$-dimensional unit hypercube and analyze the number of uncovered subcubes after discretization. By applying a concentration inequality to the uncovered-count statistic, we derive a sample complexity bound with a logarithmic dependence on the failure probability ($Î´$), i.e., $M =O( \tilde{C}\ln(\frac{2\tilde{C}}Î´))$, which contrasts sharply with the classical linear $1/Î´$ dependence. Under standard Lipschitz and uniformity assumptions, we present a self-contained derivation and compare our result with classical coupon-collector rates. Numerical studies across dimensions, precision levels, and confidence targets indicate that our bound tracks practical coverage requirements more tightly and scales favorably as $Î´\to 0$. Our findings offer a sharper theoretical tool for algorithms that rely on grid-based coverage guarantees, enabling more efficient sampling, especially in high-confidence regimes.",0,arxiv,Ä°statistik,CC-BY/arXiv,Improved Sample Complexity for Full Coverage in Compact and Continuous Spaces
"Node popularity is recognized as a key factor in modeling real-world networks, capturing heterogeneity in connectivity across communities. This concept is equally important in bipartite networks, where nodes in different partitions may exhibit varying popularity patterns, motivating models such as the Two-Way Node Popularity Model (TNPM). Existing methods, such as the Two-Stage Divided Cosine (TSDC) algorithm, provide a scalable estimation approach but may have limitations in terms of accuracy or applicability across different types of networks. In this paper, we develop a computationally efficient and theoretically justified variational expectation-maximization (VEM) framework for the TNPM. We establish label consistency for the estimated community assignments produced by the proposed variational estimator in bipartite networks. Through extensive simulation studies, we show that our method achieves superior estimation accuracy across a range of bipartite as well as undirected networks compared to existing algorithms. Finally, we evaluate our method on real-world bipartite and undirected networks, further demonstrating its practical effectiveness and robustness.",0,arxiv,Ä°statistik,CC-BY/arXiv,Variational Estimators for Node Popularity Models
"Agnostic learning of Boolean halfspaces is a fundamental problem in computational learning theory, but it is known to be computationally hard even for weak learning. Recent work [CKKMK24] proposed smoothed analysis as a way to bypass such hardness, but existing frameworks rely on additive Gaussian perturbations, making them unsuitable for discrete domains. We introduce a new smoothed agnostic learning framework for Boolean inputs, where perturbations are modeled via random bit flips. This defines a natural discrete analogue of smoothed optimality generalizing the Gaussian case. Under strictly subexponential assumptions on the input distribution, we give an efficient algorithm for learning halfspaces in this model, with runtime and sample complexity approximately n raised to a poly(1/(sigma * epsilon)) factor. Previously, such algorithms were known only with strong structural assumptions for the discrete hypercube, for example, independent coordinates or symmetric distributions. Our result provides the first computationally efficient guarantee for smoothed agnostic learning of halfspaces over the Boolean hypercube, bridging the gap between worst-case intractability and practical learnability in discrete settings.",0,arxiv,Ä°statistik,CC-BY/arXiv,Smoothed Agnostic Learning of Halfspaces over the Hypercube
"We prove that a denoising diffusion sampler equipped with a sequential bias across the batch dimension is exactly an Euler-Maruyama integrator for overdamped Langevin dynamics. Each reverse denoising step, with its associated spring stiffness, can be interpreted as one step of a stochastic differential equation with an effective time step set jointly by the noise schedule and that stiffness. The learned score then plays the role of the drift, equivalently the gradient of a learned energy, yielding a precise correspondence between diffusion sampling and Langevin time evolution.   This equivalence recasts molecular dynamics (MD) in terms of diffusion models. Accuracy is no longer tied to a fixed, extremely small MD time step; instead, it is controlled by two scalable knobs: model capacity, which governs how well the drift is approximated, and the number of denoising steps, which sets the integrator resolution. In practice, this leads to a fully data-driven MD framework that learns forces from uncorrelated equilibrium snapshots, requires no hand-engineered force fields, uses no trajectory data for training, and still preserves the Boltzmann distribution associated with the learned energy.   We derive trajectory-level, information-theoretic error bounds that cleanly separate discretization error from score-model error, clarify how temperature enters through the effective spring, and show that the resulting sampler generates molecular trajectories with MD-like temporal correlations, even though the model is trained only on static configurations.",0,arxiv,Ä°statistik,CC-BY/arXiv,Diffusion Models are Molecular Dynamics Simulators
"Data assimilation is a fundamental task in updating forecasting models upon observing new data, with applications ranging from weather prediction to online reinforcement learning. Deep generative forecasting models (DGFMs) have shown excellent performance in these areas, but assimilating data into such models is challenging due to their intractable likelihood functions. This limitation restricts the use of standard Bayesian data assimilation methodologies for DGFMs. To overcome this, we introduce prequential posteriors, based upon a predictive-sequential (prequential) loss function; an approach naturally suited for temporally dependent data which is the focus of forecasting tasks. Since the true data-generating process often lies outside the assumed model class, we adopt an alternative notion of consistency and prove that, under mild conditions, both the prequential loss minimizer and the prequential posterior concentrate around parameters with optimal predictive performance. For scalable inference, we employ easily parallelizable wastefree sequential Monte Carlo (SMC) samplers with preconditioned gradient-based kernels, enabling efficient exploration of high-dimensional parameter spaces such as those in DGFMs. We validate our method on both a synthetic multi-dimensional time series and a real-world meteorological dataset; highlighting its practical utility for data assimilation for complex dynamical systems.",0,arxiv,Ä°statistik,CC-BY/arXiv,Prequential posteriors
"This study proposes a Quantum Fourier Transform (QFT)-enhanced quantum kernel for short-term time-series forecasting. Each signal is windowed, amplitude-encoded, transformed by a QFT, then passed through a protective rotation layer to avoid the QFT/QFT adjoint cancellation; the resulting kernel is used in kernel ridge regression (KRR). Exogenous predictors are incorporated by convexly fusing feature-specific kernels. On multi-station solar irradiance data across Koppen climate classes, the proposed kernel consistently improves median R2 and nRMSE over reference classical RBF and polynomials kernels, while also reducing bias (nMBE); complementary MAE/ERMAX analyses indicate tighter average errors with remaining headroom under sharp transients. For both quantum and classical models, the only tuned quantities are the feature-mixing weights and the KRR ridge alpha; classical hyperparameters (gamma, r, d) are fixed, with the same validation set size for all models. Experiments are conducted on a noiseless simulator (5 qubits; window length L=32). Limitations and ablations are discussed, and paths toward NISQ execution are outlined.",0,arxiv,Ä°statistik,CC-BY/arXiv,Quantum Fourier Transform Based Kernel for Solar Irrandiance Forecasting
"Self-supervised learning (SSL) has recently advanced through non-contrastive methods that couple an invariance term with variance, covariance, or redundancy-reduction penalties. While such objectives shape first- and second-order statistics of the representation, they largely ignore the local geometry of the underlying data manifold. In this paper, we introduce CurvSSL, a curvature-regularized self-supervised learning framework, and its RKHS extension, kernel CurvSSL. Our approach retains a standard two-view encoder-projector architecture with a Barlow Twins-style redundancy-reduction loss on projected features, but augments it with a curvature-based regularizer. Each embedding is treated as a vertex whose $k$ nearest neighbors define a discrete curvature score via cosine interactions on the unit hypersphere; in the kernel variant, curvature is computed from a normalized local Gram matrix in an RKHS. These scores are aligned and decorrelated across augmentations by a Barlow-style loss on a curvature-derived matrix, encouraging both view invariance and consistency of local manifold bending. Experiments on MNIST and CIFAR-10 datasets with a ResNet-18 backbone show that curvature-regularized SSL yields competitive or improved linear evaluation performance compared to Barlow Twins and VICReg. Our results indicate that explicitly shaping local geometry is a simple and effective complement to purely statistical SSL regularizers.",0,arxiv,Ä°statistik,CC-BY/arXiv,Self-Supervised Learning by Curvature Alignment
"Our work introduces SAVeD (Semantically Aware Version Detection), a contrastive learning-based framework for identifying versions of structured datasets without relying on metadata, labels, or integration-based assumptions. SAVeD addresses a common challenge in data science of repeated labor due to a difficulty of similar work or transformations on datasets. SAVeD employs a modified SimCLR pipeline, generating augmented table views through random transformations (e.g., row deletion, encoding perturbations). These views are embedded via a custom transformer encoder and contrasted in latent space to optimize semantic similarity. Our model learns to minimize distances between augmented views of the same dataset and maximize those between unrelated tables. We evaluate performance using validation accuracy and separation, defined respectively as the proportion of correctly classified version/non-version pairs on a hold-out set, and the difference between average similarities of versioned and non-versioned tables (defined by a benchmark, and not provided to the model). Our experiments span five canonical datasets from the Semantic Versioning in Databases Benchmark, and demonstrate substantial gains post-training. SAVeD achieves significantly higher accuracy on completely unseen tables in, and a significant boost in separation scores, confirming its capability to distinguish semantically altered versions. Compared to untrained baselines and prior state-of-the-art dataset-discovery methods like Starmie, our custom encoder achieves competitive or superior results.",0,arxiv,Ä°statistik,CC-BY/arXiv,SAVeD: Semantic Aware Version Discovery
"From a Bayesian perspective, score-based diffusion solves inverse problems through joint inference, embedding the likelihood with the prior to guide the sampling process. However, this formulation fails to explain its practical behavior: the prior offers limited guidance, while reconstruction is largely driven by the measurement-consistency term, leading to an inference process that is effectively decoupled from the diffusion dynamics. To clarify this structure, we reinterpret the role of diffusion in inverse problem solving as an initialization stage within an expectation--maximization (EM)--style framework, where the diffusion stage and the data-driven refinement are fully decoupled. We introduce \textbf{DAPS++}, which allows the likelihood term to guide inference more directly while maintaining numerical stability and providing insight into why unified diffusion trajectories remain effective in practice. By requiring fewer function evaluations (NFEs) and measurement-optimization steps, \textbf{DAPS++} achieves high computational efficiency and robust reconstruction performance across diverse image restoration tasks.",0,arxiv,Ä°statistik,CC-BY/arXiv,DAPS++: Rethinking Diffusion Inverse Problems with Decoupled Posterior Annealing
"Deep equilibrium models (DEQs) have recently emerged as a powerful paradigm for training infinitely deep weight-tied neural networks that achieve state of the art performance across many modern machine learning tasks. Despite their practical success, theoretically understanding the gradient descent dynamics for training DEQs remains an area of active research. In this work, we rigorously study the gradient descent dynamics for DEQs in the simple setting of linear models and single-index models, filling several gaps in the literature. We prove a conservation law for linear DEQs which implies that the parameters remain trapped on spheres during training and use this property to show that gradient flow remains well-conditioned for all time. We then prove linear convergence of gradient descent to a global minimizer for linear DEQs and deep equilibrium single-index models under appropriate initialization and with a sufficiently small step size. Finally, we validate our theoretical findings through experiments.",0,arxiv,Ä°statistik,CC-BY/arXiv,Gradient flow for deep equilibrium single-index models
"We propose the Diffusion-Inversion-Net (DIN) framework for inverse modeling of groundwater flow and solute transport processes. DIN utilizes an offline-trained Denoising Diffusion Probabilistic Model (DDPM) as a powerful prior leaner, which flexibly incorporates sparse, multi-source observational data, including hydraulic head, solute concentration, and hard conductivity data, through conditional injection mechanisms. These conditioning inputs subsequently guide the generative inversion process during sampling. Bypassing iterative forward simulations, DIN leverages stochastic sampling and probabilistic modeling mechanisms to directly generate ensembles of posterior parameter fields by repeatedly executing the reverse denoising process. Two representative posterior scenarios, Gaussian and non-Gaussian, are investigated. The results demonstrate that DIN can produce multiple constraint-satisfying realizations under identical observational conditions, accurately estimate hydraulic-conductivity fields, and achieve reliable uncertainty quantification. The framework exhibits strong generalization capability across diverse data distributions, offering a robust and unified alternative to conventional multi-stage inversion methodologies.",0,arxiv,Ä°statistik,CC-BY/arXiv,Diffusion-Inversion-Net (DIN): An End-to-End Direct Probabilistic Framework for Characterizing Hydraulic Conductivities and Quantifying Uncertainty
"We introduce the Bayesian Information-Theoretic Sampling for hierarchical GAussian Process Surrogates (BITS for GAPS) framework to emulate latent components in hybrid physical systems. BITS for GAPS supports serial hybrid modeling, where known physics governs part of the system and residual dynamics are represented as a latent function inferred from data. A Gaussian process prior is placed over the latent function, with hierarchical priors on its hyperparameters to encode physically meaningful structure in the predictive posterior.   To guide data acquisition, we derive entropy-based acquisition functions that quantify expected information gain from candidate input locations, identifying samples most informative for training the surrogate. Specifically, we obtain a closed-form expression for the differential entropy of the predictive posterior and establish a tractable lower bound for efficient evaluation. These derivations approximate the predictive posterior as a finite, uniformly weighted mixture of Gaussian processes.   We demonstrate the framework's utility by modeling activity coefficients in vapor-liquid equilibrium systems, embedding the surrogate into extended Raoult's law for distillation design. Numerical results show that entropy-guided sampling improves sample efficiency by targeting regions of high uncertainty and potential information gain. This accelerates surrogate convergence, enhances predictive accuracy in non-ideal regimes, and preserves physical consistency. Overall, BITS for GAPS provides an efficient, interpretable, and uncertainty-aware framework for hybrid modeling of complex physical systems.",0,arxiv,Ä°statistik,CC-BY/arXiv,BITS for GAPS: Bayesian Information-Theoretic Sampling for hierarchical GAussian Process Surrogates
"Penalty-based methods have become popular for solving bilevel optimization (BLO) problems, thanks to their effective first-order nature. However, they often require inner-loop iterations to solve the lower-level (LL) problem and small outer-loop step sizes to handle the increased smoothness induced by large penalty terms, leading to suboptimal complexity. This work considers the general BLO problems with coupled constraints (CCs) and leverages a novel penalty reformulation that decouples the upper- and lower-level variables. This yields an improved analysis of the smoothness constant, enabling larger step sizes and reduced iteration complexity for Penalty-Based Gradient Descent algorithms in ALTernating fashion (ALT-PBGD). Building on the insight of reduced smoothness, we propose PBGD-Free, a novel fully single-loop algorithm that avoids inner loops for the uncoupled constraint BLO. For BLO with CCs, PBGD-Free employs an efficient inner-loop with substantially reduced iteration complexity. Furthermore, we propose a novel curvature condition describing the ""flatness"" of the upper-level objective with respect to the LL variable. This condition relaxes the traditional upper-level Lipschitz requirement, enables smaller penalty constant choices, and results in a negligible penalty gradient term during upper-level variable updates. We provide rigorous convergence analysis and validate the method's efficacy through hyperparameter optimization for support vector machines and fine-tuning of large language models.",0,arxiv,Ä°statistik,CC-BY/arXiv,"Efficient Penalty-Based Bilevel Methods: Improved Analysis, Novel Updates, and Flatness Condition"
"As digital communication grows in importance when connecting with healthcare providers, traditional behavioral and content message features are imbued with renewed significance. If one is to meaningfully connect with them, it is crucial to understand what drives them to engage and respond. In this study, the authors analyzed several million text messages sent through the Impiricus platform to learn which factors influenced whether or not a doctor clicked on a link in a message. Several key insights came to light through the use of logistic regression, random forest, and neural network models, the details of which the authors discuss in this paper.",0,arxiv,Ä°statistik,CC-BY/arXiv,Predicting Healthcare Provider Engagement in SMS Campaigns
"We study community detection in the \emph{symmetric $k$-stochastic block model}, where $n$ nodes are evenly partitioned into $k$ clusters with intra- and inter-cluster connection probabilities $p$ and $q$, respectively.   Our main result is a polynomial-time algorithm that achieves the minimax-optimal misclassification rate   \begin{equation*}   \exp \Bigl(-\bigl(1 \pm o(1)\bigr) \tfrac{C}{k}\Bigr),   \quad \text{where } C = (\sqrt{pn} - \sqrt{qn})^2,   \end{equation*}   whenever $C \ge K\,k^2\,\log k$ for some universal constant $K$, matching the Kesten--Stigum (KS) threshold up to a $\log k$ factor.   Notably, this rate holds even when an adversary corrupts an $Î·\le \exp\bigl(- (1 \pm o(1)) \tfrac{C}{k}\bigr)$ fraction of the nodes.   To the best of our knowledge, the minimax rate was previously only attainable either via computationally inefficient procedures [ZZ15] or via polynomial-time algorithms that require strictly stronger assumptions such as $C \ge K k^3$ [GMZZ17].   In the node-robust setting, the best known algorithm requires the substantially stronger condition $C \ge K k^{102}$ [LM22].   Our results close this gap by providing the first polynomial-time algorithm that achieves the minimax rate near the KS threshold in both settings.   Our work has two key technical contributions:   (1) we robustify majority voting via the Sum-of-Squares framework,   (2) we develop a novel graph bisection algorithm via robust majority voting, which allows us to significantly improve the misclassification rate to $1/\mathrm{poly}(k)$ for the initial estimation near the KS threshold.",0,arxiv,Ä°statistik,CC-BY/arXiv,Rate-optimal community detection near the KS threshold via node-robust algorithms
"This brief note clarifies that, in Generator Matching (which subsumes a large family of flow matching and diffusion models over continuous, manifold, and discrete spaces), both the Bregman divergence loss and the linear parameterization of the generator can depend on both the current state $X_t$ and the time $t$, and we show that the expectation over time in the loss can be taken with respect to a broad class of time distributions. We also show this for Edit Flows, which falls outside of Generator Matching. That the loss can depend on $t$ clarifies that time-dependent loss weighting schemes, often used in practice to stabilize training, are theoretically justified when the specific flow or diffusion scheme is a special case of Generator Matching (or Edit Flows). It also often simplifies the construction of $X_1$-predictor schemes, which are sometimes preferred for model-related reasons. We show examples that rely upon the dependence of linear parameterizations, and of the Bregman divergence loss, on $t$ and $X_t$.",0,arxiv,Ä°statistik,CC-BY/arXiv,Time dependent loss reweighting for flow matching and diffusion models is theoretically justified
"We propose ECPv2, a scalable and theoretically grounded algorithm for global optimization of Lipschitz-continuous functions with unknown Lipschitz constants. Building on the Every Call is Precious (ECP) framework, which ensures that each accepted function evaluation is potentially informative, ECPv2 addresses key limitations of ECP, including high computational cost and overly conservative early behavior. ECPv2 introduces three innovations: (i) an adaptive lower bound to avoid vacuous acceptance regions, (ii) a Worst-m memory mechanism that restricts comparisons to a fixed-size subset of past evaluations, and (iii) a fixed random projection to accelerate distance computations in high dimensions. We theoretically show that ECPv2 retains ECP's no-regret guarantees with optimal finite-time bounds and expands the acceptance region with high probability. We further empirically validate these findings through extensive experiments and ablation studies. Using principled hyperparameter settings, we evaluate ECPv2 across a wide range of high-dimensional, non-convex optimization problems. Across benchmarks, ECPv2 consistently matches or outperforms state-of-the-art optimizers, while significantly reducing wall-clock time.",0,arxiv,Ä°statistik,CC-BY/arXiv,"ECPv2: Fast, Efficient, and Scalable Global Optimization of Lipschitz Functions"
"We provide counterexamples showing that uniform laws of large numbers do not hold for subdifferentials under natural assumptions. Our results apply to random Lipschitz functions and random convex functions with a finite number of smooth pieces. Consequently, they resolve the questions posed by Shapiro and Xu [J. Math. Anal. Appl., 325(2), 2007] in the negative and highlight the obstacles nonsmoothness poses to uniform results.",0,arxiv,Ä°statistik,CC-BY/arXiv,Failure of uniform laws of large numbers for subdifferentials and beyond
"Clinical trials face mounting challenges: fragmented patient populations, slow enrollment, and unsustainable costs, particularly for late phase trials in oncology and rare diseases. While external control arms built from real-world data have been explored, a promising alternative is the generation of synthetic control arms using generative AI. A central challenge is the generation of time-to-event outcomes, which constitute primary endpoints in oncology and rare disease trials, but are difficult to model under censoring and small sample sizes. Existing generative approaches, largely GAN-based, are data-hungry, unstable, and rely on strong assumptions such as independent censoring. We introduce a variational autoencoder (VAE) that jointly generates mixed-type covariates and survival outcomes within a unified latent variable framework, without assuming independent censoring. Across synthetic and real trial datasets, we evaluate our model in two realistic scenarios: (i) data sharing under privacy constraints, where synthetic controls substitute for original data, and (ii) control-arm augmentation, where synthetic patients mitigate imbalances between treated and control groups. Our method outperforms GAN baselines on fidelity, utility, and privacy metrics, while revealing systematic miscalibration of type I error and power. We propose a post-generation selection procedure that improves calibration, highlighting both progress and open challenges for generative survival modeling.",0,arxiv,Ä°statistik,CC-BY/arXiv,Toward Valid Generative Clinical Trial Data with Survival Endpoints
"Explainable AI (XAI) is increasingly essential as modern models become more complex and high-stakes applications demand transparency, trust, and regulatory compliance. Existing global attribution methods often incur high computational costs, lack stability under correlated inputs, and fail to scale efficiently to large or heterogeneous datasets. We address these gaps with \emph{ExCIR} (Explainability through Correlation Impact Ratio), a correlation-aware attribution score equipped with a lightweight transfer protocol that reproduces full-model rankings using only a fraction of the data. ExCIR quantifies sign-aligned co-movement between features and model outputs after \emph{robust centering} (subtracting a robust location estimate, e.g., median or mid-mean, from features and outputs). We further introduce \textsc{BlockCIR}, a \emph{groupwise} extension of ExCIR that scores \emph{sets} of correlated features as a single unit. By aggregating the same signed-co-movement numerators and magnitudes over predefined or data-driven groups, \textsc{BlockCIR} mitigates double-counting in collinear clusters (e.g., synonyms or duplicated sensors) and yields smoother, more stable rankings when strong dependencies are present. Across diverse text, tabular, signal, and image datasets, ExCIR shows trustworthy agreement with established global baselines and the full model, delivers consistent top-$k$ rankings across settings, and reduces runtime via lightweight evaluation on a subset of rows. Overall, ExCIR provides \emph{computationally efficient}, \emph{consistent}, and \emph{scalable} explainability for real-world deployment.",0,arxiv,Ä°statistik,CC-BY/arXiv,Correlation-Aware Feature Attribution Based Explainable AI
"We investigate how to optimally design local differential privacy (LDP) mechanisms that reduce data unfairness and thereby improve fairness in downstream classification. We first derive a closed-form optimal mechanism for binary sensitive attributes and then develop a tractable optimization framework that yields the corresponding optimal mechanism for multi-valued attributes. As a theoretical contribution, we establish that for discrimination-accuracy optimal classifiers, reducing data unfairness necessarily leads to lower classification unfairness, thus providing a direct link between privacy-aware pre-processing and classification fairness. Empirically, we demonstrate that our approach consistently outperforms existing LDP mechanisms in reducing data unfairness across diverse datasets and fairness metrics, while maintaining accuracy close to that of non-private models. Moreover, compared with leading pre-processing and post-processing fairness methods, our mechanism achieves a more favorable accuracy-fairness trade-off while simultaneously preserving the privacy of sensitive attributes. Taken together, these results highlight LDP as a principled and effective pre-processing fairness intervention technique.",0,arxiv,Ä°statistik,CC-BY/arXiv,Optimal Fairness under Local Differential Privacy
"Scalable Gaussian process (GP) inference is essential for sequential decision-making tasks, yet improving GP scalability remains a challenging problem with many open avenues of research. This paper focuses on iterative GPs, where iterative linear solvers, such as conjugate gradients, stochastic gradient descent or alternative projections, are used to approximate the GP posterior. We propose a new method which improves solver convergence of a large linear system by leveraging the known solution to a smaller system contained within. This is significant for tasks with incremental data additions, and we show that our technique achieves speed-ups when solving to tolerance, as well as improved Bayesian optimisation performance under a fixed compute budget.",0,arxiv,Ä°statistik,CC-BY/arXiv,Improving Iterative Gaussian Processes via Warm Starting Sequential Posteriors
"Linear probes are widely used to interpret and evaluate neural representations, yet their reliability remains unclear, as probes may appear accurate in some regimes but collapse unpredictably in others. We uncover a spectral mechanism behind this phenomenon and formalize it as the Spectral Identifiability Principle (SIP), a verifiable Fisher-inspired condition for probe stability. When the eigengap separating task-relevant directions is larger than the Fisher estimation error, the estimated subspace concentrates and accuracy remains consistent, whereas closing this gap induces instability in a phase-transition manner. Our analysis connects eigengap geometry, sample size, and misclassification risk through finite-sample reasoning, providing an interpretable diagnostic rather than a loose generalization bound. Controlled synthetic studies, where Fisher quantities are computed exactly, confirm these predictions and show how spectral inspection can anticipate unreliable probes before they distort downstream evaluation.",0,arxiv,Ä°statistik,CC-BY/arXiv,Spectral Identifiability for Interpretable Probe Geometry
"In unsupervised or weakly-supervised approaches for speech dereverberation, the target clean (dry) signals are considered to be unknown during training. In that context, evaluating to what extent information can be retrieved from the sole knowledge of reverberant (wet) speech becomes critical. This work investigates the role of the reverberant (wet) phase in the time-frequency domain. Based on Statistical Wave Field Theory, we show that late reverberation perturbs phase components with white, uniformly distributed noise, except at low frequencies. Consequently, the wet phase carries limited useful information and is not essential for weakly supervised dereverberation. To validate this finding, we train dereverberation models under a recent weak supervision framework and demonstrate that performance can be significantly improved by excluding the reverberant phase from the loss function.",0,arxiv,Ä°statistik,CC-BY/arXiv,Is Phase Really Needed for Weakly-Supervised Dereverberation ?
"Quantum neural networks (QNNs) are an analog of classical neural networks in the world of quantum computing, which are represented by a unitary matrix with trainable parameters. Inspired by the universal approximation property of classical neural networks, ensuring that every continuous function can be arbitrarily well approximated uniformly on a compact set of a Euclidean space, some recent works have established analogous results for QNNs, ranging from single-qubit to multi-qubit QNNs, and even hybrid classical-quantum models. In this paper, we study the approximation capabilities of QNNs for periodic functions with respect to the supremum norm. We use the Jackson inequality to approximate a given function by implementing its approximating trigonometric polynomial via a suitable QNN. In particular, we see that by restricting to the class of periodic functions, one can achieve a quadratic reduction of the number of parameters, producing better approximation results than in the literature. Moreover, the smoother the function, the fewer parameters are needed to construct a QNN to approximate the function.",0,arxiv,Ä°statistik,CC-BY/arXiv,Approximation rates of quantum neural networks for periodic functions via Jackson's inequality
"Graph spectral representations are fundamental in graph signal processing, offering a rigorous framework for analyzing and processing graph-structured data. The graph fractional Fourier transform (GFRFT) extends the classical graph Fourier transform (GFT) with a fractional-order parameter, enabling flexible spectral analysis while preserving mathematical consistency. The angular graph Fourier transform (AGFT) introduces angular control via GFT eigenvector rotation; however, existing constructions fail to degenerate to the GFT at zero angle, which is a critical flaw that undermines theoretical consistency and interpretability. To resolve these complementary limitations - GFRFT's lack of angular regulation and AGFT's defective degeneracy - this study proposes an angular GFRFT (AGFRFT), a unified framework that integrates fractional-order and angular spectral analyses with theoretical rigor. A degeneracy-friendly rotation matrix family ensures exact GFT degeneration at zero angle, with two AGFRFT variants (I-AGFRFT and II-AGFRFT) defined accordingly. Rigorous theoretical analyses confirm their unitarity, invertibility, and smooth parameter dependence. Both support learnable joint parameterization of the angle and fractional order, enabling adaptive spectral processing for diverse graph signals. Extensive experiments on real-world data denoising, image denoising, and point cloud denoising demonstrate that AGFRFT outperforms GFRFT and AGFT in terms of spectral concentration, reconstruction quality, and controllable spectral manipulation, establishing a robust and flexible tool for integrated angular fractional spectral analysis in graph signal processing.",0,arxiv,Ä°statistik,CC-BY/arXiv,Angular Graph Fractional Fourier Transform: Theory and Application
"Quantum machine learning (QML) is a computational paradigm that seeks to apply quantum-mechanical resources to solve learning problems. As such, the goal of this framework is to leverage quantum processors to tackle optimization, supervised, unsupervised and reinforcement learning, and generative modeling-among other tasks-more efficiently than classical models. Here we offer a high level overview of QML, focusing on settings where the quantum device is the primary learning or data generating unit. We outline the field's tensions between practicality and guarantees, access models and speedups, and classical baselines and claimed quantum advantages-flagging where evidence is strong, where it is conditional or still lacking, and where open questions remain. By shedding light on these nuances and debates, we aim to provide a friendly map of the QML landscape so that the reader can judge when-and under what assumptions-quantum approaches may offer real benefits.",0,arxiv,Ä°statistik,CC-BY/arXiv,A Primer on Quantum Machine Learning
"We propose a semiparametric Bayesian methodology for estimating the average treatment effect (ATE) within the potential outcomes framework using observational data with high-dimensional nuisance parameters. Our method introduces a Bayesian debiasing procedure that corrects for bias arising from nuisance estimation and employs a targeted modeling strategy based on summary statistics rather than the full data. These summary statistics are identified in a debiased manner, enabling the estimation of nuisance bias via weighted observables and facilitating hierarchical learning of the ATE. By combining debiasing with sample splitting, our approach separates nuisance estimation from inference on the target parameter, reducing sensitivity to nuisance model specification. We establish that, under mild conditions, the marginal posterior for the ATE satisfies a Bernstein-von Mises theorem when both nuisance models are correctly specified and remains consistent and robust when only one is correct, achieving Bayesian double robustness. This ensures asymptotic efficiency and frequentist validity. Extensive simulations confirm the theoretical results, demonstrating accurate point estimation and credible intervals with nominal coverage, even in high-dimensional settings. The proposed framework can also be extended to other causal estimands, and its key principles offer a general foundation for advancing Bayesian semiparametric inference more broadly.",0,arxiv,Ä°statistik,CC-BY/arXiv,Bayesian Semiparametric Causal Inference: Targeted Doubly Robust Estimation of Treatment Effects
"Causal inference starts with a simple idea: compare groups that differ by treatment, not much else. Traditionally, similar groups are constructed using only observed covariates; however, it remains a long-standing challenge to incorporate available outcome data into the study design while preserving valid inference. In this paper, we study the general problem of covariate adjustment, effect estimation, and statistical inference when balancing features are constructed or selected with the aid of outcome information from the data. We propose cross-balancing, a method that uses sample splitting to separate the error in feature construction from the error in weight estimation. Our framework addresses two cases: one where the features are learned functions and one where they are selected from a potentially high-dimensional dictionary. In both cases, we establish mild and general conditions under which cross-balancing produces consistent, asymptotically normal, and efficient estimators. In the learned-function case, cross-balancing achieves finite-sample bias reduction relative to plug-in-type estimators, and is multiply robust when the learned features converge at slow rates. In the variable-selection case, cross-balancing only requires a product condition on how well the selected variables approximate true functions. We illustrate cross-balancing in extensive simulations and an observational study, showing that careful use of outcome information can substantially improve both estimation and inference while maintaining interpretability.",0,arxiv,Ä°statistik,CC-BY/arXiv,Cross-Balancing for Data-Informed Design and Efficient Analysis of Observational Studies
"Ensemble Kalman methods were initially developed to solve nonlinear data assimilation problems in oceanography, but are now popular in applications far beyond their original use cases. Of particular interest is climate model calibration. As hybrid physics and machine-learning models evolve, the number of parameters and complexity of parameterizations in climate models will continue to grow. Thus, robust calibration of these parameters plays an increasingly important role. We focus on learning climate model parameters from minimizing the misfit between modeled and observed climate statistics in an idealized setting. Ensemble Kalman methods are a natural choice for this problem because they are derivative-free, scalable to high dimensions, and robust to noise caused by statistical observations. Given the many variants of ensemble methods proposed, an important question is: Which ensemble Kalman method should be used for climate model calibration? To answer this question, we perform systematic numerical experiments to explore the relative computational efficiencies of several ensemble Kalman methods. The numerical experiments involve statistical observations of Lorenz-type models of increasing complexity, frequently used to represent simplified atmospheric systems, and some feature neural network parameterizations. For each test problem, several ensemble Kalman methods and a derivative-based method ""race"" to reach a specified accuracy, and we measure the computational cost required to achieve the desired accuracy. We investigate how prior information and the parameter or data dimensions play a role in choosing the ensemble method variant. The derivative-based method consistently fails to complete the race because it does not adaptively handle the noisy loss landscape.",0,arxiv,Ä°statistik,CC-BY/arXiv,The Ensemble Kalman Inversion Race
"In real-world applications, data often reside in restricted domains with unknown boundaries, or as high-dimensional point clouds lying on a lower-dimensional, nontrivial, unknown manifold. Traditional Gaussian Processes (GPs) struggle to capture the underlying geometry in such settings. Some existing methods assume a flat space embedded in a point cloud, which can be represented by a single latent chart (latent space), while others exhibit weak performance when the point cloud is sparse or irregularly sampled. The goal of this work is to address these challenges. The main contributions are twofold: (1) We establish the Atlas Brownian Motion (BM) framework for estimating the heat kernel on point clouds with unknown geometries and nontrivial topological structures; (2) Instead of directly using the heat kernel estimates, we construct a Riemannian corrected kernel by combining the global heat kernel with local RBF kernel and leading to the formulation of Riemannian-corrected Atlas Gaussian Processes (RC-AGPs). The resulting RC-AGPs are applied to regression tasks across synthetic and real-world datasets. These examples demonstrate that our method outperforms existing approaches in both heat kernel estimation and regression accuracy. It improves statistical inference by effectively bridging the gap between complex, high-dimensional observations and manifold-based inferences.",0,arxiv,Ä°statistik,CC-BY/arXiv,Atlas Gaussian processes on restricted domains and point clouds
"We introduce a new low-noise condition for classification, the Model Margin Noise (MM noise) assumption, and derive enhanced $\mathcal{H}$-consistency bounds under this condition. MM noise is weaker than Tsybakov noise condition: it is implied by Tsybakov noise condition but can hold even when Tsybakov fails, because it depends on the discrepancy between a given hypothesis and the Bayes-classifier rather than on the intrinsic distributional minimal margin (see Figure 1 for an illustration of an explicit example). This hypothesis-dependent assumption yields enhanced $\mathcal{H}$-consistency bounds for both binary and multi-class classification. Our results extend the enhanced $\mathcal{H}$-consistency bounds of Mao, Mohri, and Zhong (2025a) with the same favorable exponents but under a weaker assumption than the Tsybakov noise condition; they interpolate smoothly between linear and square-root regimes for intermediate noise levels. We also instantiate these bounds for common surrogate loss families and provide illustrative tables.",0,arxiv,Ä°statistik,CC-BY/arXiv,Beyond Tsybakov: Model Margin Noise and $\mathcal{H}$-Consistency Bounds
"Multidimensional scaling visualizes dissimilarities among objects and reduces data dimensionality. While many methods address symmetric proximity data, asymmetric and especially three-way proximity data (capturing relationships across multiple occasions) remain underexplored. Recent developments, such as the h-plot, enable the analysis of asymmetric and non-reflexive relationships by embedding dissimilarities in a Euclidean space, allowing further techniques like archetypoid analysis to identify representative extreme profiles. However, no existing methods extract archetypal profiles from three-way asymmetric proximity data. This work extends the h-plot methodology to three-way proximity data under both symmetric and asymmetric, conditional and unconditional frameworks. The proposed approach offers several advantages: intuitive interpretability through a unified Euclidean representation; an explicit, eigenvector-based analytical solution free from local minima; scale invariance under linear transformations; computational efficiency for large matrices; and a straightforward goodness-of-fit evaluation. Furthermore, it enables the identification of archetypal profiles and clustering structures for three-way asymmetric proximities. Its performance is compared with existing models for multidimensional scaling and clustering, and illustrated through a financial application. All data and code are provided to facilitate reproducibility.",0,arxiv,Ä°statistik,CC-BY/arXiv,Multidimensional scaling of two-mode three-way asymmetric dissimilarities: finding archetypal profiles and clustering
"Front-door adjustment gives a simple closed-form identification formula under the classical front-door criterion, but its applicability is often viewed as narrow. By contrast, the general ID algorithm can identify many more causal effects in arbitrary graphs, yet typically outputs algebraically complex expressions that are hard to estimate and interpret. We show that many such graphs can in fact be reduced to a standard front-door setting via front-door reducibility (FDR), a graphical condition on acyclic directed mixed graphs that aggregates variables into super-nodes $(\boldsymbol{X}^{*},\boldsymbol{Y}^{*},\boldsymbol{M}^{*})$. We characterize the FDR criterion, prove it is equivalent (at the graph level) to the existence of an FDR adjustment, and present FDR-TID, an exact algorithm that finds an admissible FDR triple with correctness, completeness, and finite-termination guarantees. Empirical examples show that many graphs far outside the textbook front-door setting are FDR, yielding simple, estimable adjustments where general ID expressions would be cumbersome. FDR therefore complements existing identification methods by prioritizing interpretability and computational simplicity without sacrificing generality across mixed graphs.",0,arxiv,Ä°statistik,CC-BY/arXiv,Front-door Reducibility: Reducing ADMGs to the Standard Front-door Setting via a Graphical Criterion
"Characterizing the differential privacy (DP) of learning algorithms has become a major challenge in recent years. In parallel, many studies suggested investigating the behavior of stochastic gradient descent (SGD) with heavy-tailed noise, both as a model for modern deep learning models and to improve their performance. However, most DP bounds focus on light-tailed noise, where satisfactory guarantees have been obtained but the proposed techniques do not directly extend to the heavy-tailed setting. Recently, the first DP guarantees for heavy-tailed SGD were obtained. These results provide $(0,Î´)$-DP guarantees without requiring gradient clipping. Despite casting new light on the link between DP and heavy-tailed algorithms, these results have a strong dependence on the number of parameters and cannot be extended to other DP notions like the well-established RÃ©nyi differential privacy (RDP). In this work, we propose to address these limitations by deriving the first RDP guarantees for heavy-tailed SDEs, as well as their discretized counterparts. Our framework is based on new RÃ©nyi flow computations and the use of well-established fractional PoincarÃ© inequalities. Under the assumption that such inequalities are satisfied, we obtain DP guarantees that have a much weaker dependence on the dimension compared to prior art.",0,arxiv,Ä°statistik,CC-BY/arXiv,RÃ©nyi Differential Privacy for Heavy-Tailed SDEs via Fractional PoincarÃ© Inequalities
"Ordinary differential equations (ODEs) are a conventional way to describe the observed dynamics of physical systems. Scientists typically hypothesize about dynamical behavior, propose a mathematical model, and compare its predictions to data. However, modern computing and algorithmic advances now enable purely data-driven learning of governing dynamics directly from observations. In data-driven settings, one learns the ODE's right-hand side (RHS). Dense measurements are often assumed, yet high temporal resolution is typically both cumbersome and expensive. Consequently, one usually has only sparsely sampled data. In this work we introduce ChaosODE (CODE), a Polynomial Chaos ODE Expansion in which we use an arbitrary Polynomial Chaos Expansion (aPCE) for the ODE's right-hand side, resulting in a global orthonormal polynomial representation of dynamics. We evaluate the performance of CODE in several experiments on the Lotka-Volterra system, across varying noise levels, initial conditions, and predictions far into the future, even on previously unseen initial conditions. CODE exhibits remarkable extrapolation capabilities even when evaluated under novel initial conditions and shows advantages compared to well-examined methods using neural networks (NeuralODE) or kernel approximators (KernelODE) as the RHS representer. We observe that the high flexibility of NeuralODE and KernelODE degrades extrapolation capabilities under scarce data and measurement noise. Finally, we provide practical guidelines for robust optimization of dynamics-learning problems and illustrate them in the accompanying code.",0,arxiv,Ä°statistik,CC-BY/arXiv,CODE: A global approach to ODE dynamics learning
"This paper presents a tractable algorithm for estimating an unknown Lipschitz function from noisy observations and establishes an upper bound on its convergence rate. The approach extends max-affine methods from convex shape-restricted regression to the more general Lipschitz setting. A key component is a nonlinear feature expansion that maps max-affine functions into a subclass of delta-convex functions, which act as universal approximators of Lipschitz functions while preserving their Lipschitz constants. Leveraging this property, the estimator attains the minimax convergence rate (up to logarithmic factors) with respect to the intrinsic dimension of the data under squared loss and subgaussian distributions in the random design setting. The algorithm integrates adaptive partitioning to capture intrinsic dimension, a penalty-based regularization mechanism that removes the need to know the true Lipschitz constant, and a two-stage optimization procedure combining a convex initialization with local refinement. The framework is also straightforward to adapt to convex shape-restricted regression. Experiments demonstrate competitive performance relative to other theoretically justified methods, including nearest-neighbor and kernel-based regressors.",0,arxiv,Ä°statistik,CC-BY/arXiv,Near-optimal delta-convex estimation of Lipschitz functions
"Parameter estimation remains a challenging task across many areas of engineering. Because data acquisition can often be costly, limited, or prone to inaccuracies (noise, uncertainty) it is crucial to identify sensor configurations that provide the maximum amount of information about the unknown parameters, in particular for the case of distributed-parameter systems, where spatial variations are important. Physics-Informed Neural Networks (PINNs) have recently emerged as a powerful machine-learning (ML) tool for parameter estimation, particularly in cases with sparse or noisy measurements, overcoming some of the limitations of traditional optimization-based and Bayesian approaches. Despite the widespread use of PINNs for solving inverse problems, relatively little attention has been given to how their performance depends on sensor placement. This study addresses this gap by introducing a comprehensive PINN-based framework that simultaneously tackles optimal sensor placement and parameter estimation. Our approach involves training a PINN model in which the parameters of interest are included as additional inputs. This enables the efficient computation of sensitivity functions through automatic differentiation, which are then used to determine optimal sensor locations exploiting the D-optimality criterion. The framework is validated on two illustrative distributed-parameter reaction-diffusion-advection problems of increasing complexity. The results demonstrate that our PINNs-based methodology consistently achieves higher accuracy compared to parameter values estimated from intuitively or randomly selected sensor positions.",0,arxiv,Ä°statistik,CC-BY/arXiv,A Physics Informed Machine Learning Framework for Optimal Sensor Placement and Parameter Estimation
"We study the tradeoff between sample complexity and round complexity in on-demand sampling, where the learning algorithm adaptively samples from $k$ distributions over a limited number of rounds. In the realizable setting of Multi-Distribution Learning (MDL), we show that the optimal sample complexity of an $r$-round algorithm scales approximately as $dk^{Î˜(1/r)} / Îµ$. For the general agnostic case, we present an algorithm that achieves near-optimal sample complexity of $\widetilde O((d + k) / Îµ^2)$ within $\widetilde O(\sqrt{k})$ rounds. Of independent interest, we introduce a new framework, Optimization via On-Demand Sampling (OODS), which abstracts the sample-adaptivity tradeoff and captures most existing MDL algorithms. We establish nearly tight bounds on the round complexity in the OODS setting. The upper bounds directly yield the $\widetilde O(\sqrt{k})$-round algorithm for agnostic MDL, while the lower bounds imply that achieving sub-polynomial round complexity would require fundamentally new techniques that bypass the inherent hardness of OODS.",0,arxiv,Ä°statistik,CC-BY/arXiv,Sample-Adaptivity Tradeoff in On-Demand Sampling
"The Gini score is a popular tool in statistical modeling and machine learning for model validation and model selection. It is a purely rank based score that allows one to assess risk rankings. The Gini score for statistical modeling has mainly been used in a binary context, in which it has many equivalent reformulations such as the receiver operating characteristic (ROC) or the area under the curve (AUC). In the actuarial literature, this rank based score for binary responses has been extended to general real-valued random variables using Lorenz curves and concentration curves. While these initial concepts assume that the risk ranking is generated by a continuous distribution function, we discuss in this paper how the Gini score can be used in the case of ties in the risk ranking. Moreover, we adapt the Gini score to the common actuarial situation of having case weights.",0,arxiv,Ä°statistik,CC-BY/arXiv,Gini Score under Ties and Case Weights
"In high-dimensional statistics, the Lasso is a cornerstone method for simultaneous variable selection and parameter estimation. However, its reliance on the squared loss function renders it highly sensitive to outliers and heavy-tailed noise, potentially leading to unreliable model selection and biased estimates. To address this limitation, we introduce the Exponential Lasso, a novel robust method that integrates an exponential-type loss function within the Lasso framework. This loss function is designed to achieve a smooth trade-off between statistical efficiency under Gaussian noise and robustness against data contamination. Unlike other methods that cap the influence of large residuals, the exponential loss smoothly redescends, effectively downweighting the impact of extreme outliers while preserving near-quadratic behavior for small errors. We establish theoretical guarantees showing that the Exponential Lasso achieves strong statistical convergence rates, matching the classical Lasso under ideal conditions while maintaining its robustness in the presence of heavy-tailed contamination. Computationally, the estimator is optimized efficiently via a Majorization-Minimization (MM) algorithm that iteratively solves a series of weighted Lasso subproblems. Numerical experiments demonstrate that the proposed method is highly competitive, outperforming the classical Lasso in contaminated settings and maintaining strong performance even under Gaussian noise.   Our method is implemented in the \texttt{R} package \texttt{heavylasso} available on Github: https://github.com/tienmt/heavylasso",0,arxiv,Ä°statistik,CC-BY/arXiv,Exponential Lasso: robust sparse penalization under heavy-tailed noise and outliers with exponential-type loss
"Biological data sets are often high-dimensional, noisy, and governed by complex interactions among sparse signals. This poses major challenges for interpretability and reliable feature selection. Tasks such as identifying motif interactions in genomics exemplify these difficulties, as only a small subset of biologically relevant features (e.g., motifs) are typically active, and their effects are often non-linear and context-dependent. While statistical approaches often result in more interpretable models, deep learning models have proven effective in modeling complex interactions and prediction accuracy, yet their black-box nature limits interpretability. We introduce BaGGLS, a flexible and interpretable probabilistic binary regression model designed for high-dimensional biological inference involving feature interactions. BaGGLS incorporates a Bayesian group global-local shrinkage prior, aligned with the group structure introduced by interaction terms. This prior encourages sparsity while retaining interpretability, helping to isolate meaningful signals and suppress noise. To enable scalable inference, we employ a partially factorized variational approximation that captures posterior skewness and supports efficient learning even in large feature spaces. In extensive simulations, we can show that BaGGLS outperforms the other methods with regard to interaction detection and is many times faster than MCMC sampling under the horseshoe prior. We also demonstrate the usefulness of BaGGLS in the context of interaction discovery from motif scanner outputs and noisy attribution scores from deep learning models. This shows that BaGGLS is a promising approach for uncovering biologically relevant interaction patterns, with potential applicability across a range of high-dimensional tasks in computational biology.",0,arxiv,Ä°statistik,CC-BY/arXiv,BaGGLS: A Bayesian Shrinkage Framework for Interpretable Modeling of Interactions in High-Dimensional Biological Data
"Bayesian Optimization is critically vulnerable to extreme outliers. Existing provably robust methods typically assume a bounded cumulative corruption budget, which makes them defenseless against even a single corruption of sufficient magnitude. To address this, we introduce a new adversary whose budget is only bounded in the frequency of corruptions, not in their magnitude. We then derive RCGP-UCB, an algorithm coupling the famous upper confidence bound (UCB) approach with a Robust Conjugate Gaussian Process (RCGP). We present stable and adaptive versions of RCGP-UCB, and prove that they achieve sublinear regret in the presence of up to $O(T^{1/2})$ and $O(T^{1/3})$ corruptions with possibly infinite magnitude. This robustness comes at near zero cost: without outliers, RCGP-UCB's regret bounds match those of the standard GP-UCB algorithm.",0,arxiv,Ä°statistik,CC-BY/arXiv,Robust Bayesian Optimisation with Unbounded Corruptions
"High-dimensional multimodal sampling problems from lattice field theory (LFT) have become important benchmarks for machine learning assisted sampling methods. We show that GPU-accelerated particle methods, Sequential Monte Carlo (SMC) and nested sampling, provide a strong classical baseline that matches or outperforms state-of-the-art neural samplers in sample quality and wall-clock time on standard scalar field theory benchmarks, while also estimating the partition function. Using only a single data-driven covariance for tuning, these methods achieve competitive performance without problem-specific structure, raising the bar for when learned proposals justify their training cost.",0,arxiv,Ä°statistik,CC-BY/arXiv,Particle Monte Carlo methods for Lattice Field Theory
"Conformal prediction (CP) constructs uncertainty sets for model outputs with finite-sample coverage guarantees. A candidate output is included in the prediction set if its non-conformity score is not considered extreme relative to the scores observed on a set of calibration examples. However, this procedure is only straightforward when scores are scalar-valued, which has limited CP to real-valued scores or ad-hoc reductions to one dimension. The problem of ordering vectors has been studied via optimal transport (OT), which provides a principled method for defining vector-ranks and multivariate quantile regions, though typically with only asymptotic coverage guarantees. We restore finite-sample, distribution-free coverage by conformalizing the vector-valued OT quantile region. Here, a candidate's rank is defined via a transport map computed for the calibration scores augmented with that candidate's score. This defines a continuum of OT problems for which we prove that the resulting optimal assignment is piecewise-constant across a fixed polyhedral partition of the score space. This allows us to characterize the entire prediction set tractably, and provides the machinery to address a deeper limitation of prediction sets: that they only indicate which outcomes are plausible, but not their relative likelihood. In one dimension, conformal predictive distributions (CPDs) fill this gap by producing a predictive distribution with finite-sample calibration. Extending CPDs beyond one dimension remained an open problem. We construct, to our knowledge, the first multivariate CPDs with finite-sample calibration, i.e., they define a valid multivariate distribution where any derived uncertainty region automatically has guaranteed coverage. We present both conservative and exact randomized versions, the latter resulting in a multivariate generalization of the classical Dempster-Hill procedure.",0,arxiv,Ä°statistik,CC-BY/arXiv,Beyond Uncertainty Sets: Leveraging Optimal Transport to Extend Conformal Predictive Distribution to Multivariate Settings
"In deep learning, a central issue is to understand how neural networks efficiently learn high-dimensional features. To this end, we explore the gradient descent learning of a general Gaussian Multi-index model $f(\boldsymbol{x})=g(\boldsymbol{U}\boldsymbol{x})$ with hidden subspace $\boldsymbol{U}\in \mathbb{R}^{r\times d}$, which is the canonical setup to study representation learning. We prove that under generic non-degenerate assumptions on the link function, a standard two-layer neural network trained via layer-wise gradient descent can agnostically learn the target with $o_d(1)$ test error using $\widetilde{\mathcal{O}}(d)$ samples and $\widetilde{\mathcal{O}}(d^2)$ time. The sample and time complexity both align with the information-theoretic limit up to leading order and are therefore optimal. During the first stage of gradient descent learning, the proof proceeds via showing that the inner weights can perform a power-iteration process. This process implicitly mimics a spectral start for the whole span of the hidden subspace and eventually eliminates finite-sample noise and recovers this span. It surprisingly indicates that optimal results can only be achieved if the first layer is trained for more than $\mathcal{O}(1)$ steps. This work demonstrates the ability of neural networks to effectively learn hierarchical functions with respect to both sample and time efficiency.",0,arxiv,Ä°statistik,CC-BY/arXiv,Neural Networks Learn Generic Multi-Index Models Near Information-Theoretic Limit
"Understanding the relationships between data points in the latent decision space derived by the deep learning system is critical to evaluating and interpreting the performance of the system on real world data. Detecting \textit{out-of-distribution} (OOD) data for deep learning systems continues to be an active research topic. We investigate the connection between latent space OOD detection and classification accuracy of the model. Using open source simulated and measured Synthetic Aperture RADAR (SAR) datasets, we empirically demonstrate that the OOD detection cannot be used as a proxy measure for model performance. We hope to inspire additional research into the geometric properties of the latent space that may yield future insights into deep learning robustness and generalizability.",0,arxiv,Ä°statistik,CC-BY/arXiv,Latent space analysis and generalization to out-of-distribution data
"Supervised learning with large-scale data usually leads to complex optimization problems, especially for classification tasks with multiple classes. Stochastic subgradient methods can enable efficient learning with a large number of samples for classification techniques that minimize the average loss over the training samples. However, recent techniques, such as minimax risk classifiers (MRCs), minimize the maximum expected loss and are not amenable to stochastic subgradient methods. In this paper, we present a learning algorithm based on the combination of constraint and column generation that enables efficient learning of MRCs with large-scale data for classification tasks with multiple classes. Experiments on multiple benchmark datasets show that the proposed algorithm provides upto a 10x speedup for general large-scale data and around a 100x speedup with a sizeable number of classes.",0,arxiv,Ä°statistik,CC-BY/arXiv,Efficient Large-Scale Learning of Minimax Risk Classifiers
"Wasserstein gradient flow provides a general framework for minimizing an energy functional $J$ over the space of probability measures on a Riemannian manifold $(M,g)$. Its canonical time-discretization, the Jordan-Kinderlehrer-Otto (JKO) scheme, produces for any step size $Î·>0$ a sequence of probability distributions $Ï_k^Î·$ that approximate to first order in $Î·$ Wasserstein gradient flow on $J$. But the JKO scheme also has many other remarkable properties not shared by other first order integrators, e.g. it preserves energy dissipation and exhibits unconditional stability for $Î»$-geodesically convex functionals $J$. To better understand the JKO scheme we characterize its implicit bias at second order in $Î·$. We show that $Ï_k^Î·$ are approximated to order $Î·^2$ by Wasserstein gradient flow on a modified energy \[ J^Î·(Ï) = J(Ï) - \fracÎ·{4}\int_M \Big\lVert \nabla_g \frac{Î´J}{Î´Ï} (Ï) \Big\rVert_{2}^{2} \,Ï(dx), \] obtained by subtracting from $J$ the squared metric curvature of $J$ times $Î·/4$. The JKO scheme therefore adds at second order in $Î·$ a deceleration in directions where the metric curvature of $J$ is rapidly changing. This corresponds to canonical implicit biases for common functionals: for entropy the implicit bias is the Fisher information, for KL-divergence it is the Fisher-Hyv{Ã¤}rinen divergence, and for Riemannian gradient descent it is the kinetic energy in the metric $g$. To understand the differences between minimizing $J$ and $J^Î·$ we study JKO-Flow, Wasserstein gradient flow on $J^Î·$, in several simple numerical examples. These include exactly solvable Langevin dynamics on the Bures-Wasserstein space and Langevin sampling from a quartic potential in 1D.",0,arxiv,Ä°statistik,CC-BY/arXiv,Implicit Bias of the JKO Scheme
"On many learning platforms, the optimization criteria guiding model training reflect the priorities of the designer rather than those of the individuals they affect. Consequently, users may act strategically to obtain more favorable outcomes, effectively contesting the platform's predictions. While past work has studied strategic user behavior on learning platforms, the focus has largely been on strategic responses to a deployed model, without considering the behavior of other users. In contrast, look-ahead reasoning takes into account that user actions are coupled, and -- at scale -- impact future predictions. Within this framework, we first formalize level-$k$ thinking, a concept from behavioral economics, where users aim to outsmart their peers by looking one step ahead. We show that, while convergence to an equilibrium is accelerated, the equilibrium remains the same, providing no benefit of higher-level reasoning for individuals in the long run. Then, we focus on collective reasoning, where users take coordinated actions by optimizing through their joint impact on the model. By contrasting collective with selfish behavior, we characterize the benefits and limits of coordination; a new notion of alignment between the learner's and the users' utilities emerges as a key concept. We discuss connections to several related mathematical frameworks, including strategic classification, performative prediction, and algorithmic collective action.",0,arxiv,Ä°statistik,CC-BY/arXiv,Look-Ahead Reasoning on Learning Platforms
"We establish the first global convergence result of neural networks for two stage least squares (2SLS) approach in nonparametric instrumental variable regression (NPIV). This is achieved by adopting a lifted perspective through mean-field Langevin dynamics (MFLD), unlike standard MFLD, however, our setting of 2SLS entails a \emph{bilevel} optimization problem in the space of probability measures. To address this challenge, we leverage the penalty gradient approach recently developed for bilevel optimization which formulates bilevel optimization as a Lagrangian problem. This leads to a novel fully first-order algorithm, termed \texttt{F$^2$BMLD}. Apart from the convergence bound, we further provide a generalization bound, revealing an inherent trade-off in the choice of the Lagrange multiplier between optimization and statistical guarantees. Finally, we empirically validate the effectiveness of the proposed method on an offline reinforcement learning benchmark.",0,arxiv,Ä°statistik,CC-BY/arXiv,Towards a Unified Analysis of Neural Networks in Nonparametric Instrumental Variable Regression: Optimization and Generalization
"Attention is the brain's ability to selectively focus on a few specific aspects while ignoring irrelevant ones. This biological principle inspired the attention mechanism in modern Transformers. Transformers now underpin large language models (LLMs) such as GPT, but at the cost of massive training and inference energy, leading to a large carbon footprint. While brain attention emerges from neural circuits, Transformer attention relies on dot-product similarity to weight elements in the input sequence. Neuromorphic computing, especially spiking neural networks (SNNs), offers a brain-inspired path to energy-efficient intelligence. Despite recent work on attention-based spiking Transformers, the core attention layer remains non-neuromorphic. Current spiking attention (i) relies on dot-product or element-wise similarity suited to floating-point operations, not event-driven spikes; (ii) keeps attention matrices that suffer from the von Neumann bottleneck, limiting in-memory computing; and (iii) still diverges from brain-like computation. To address these issues, we propose the Spiking STDP Transformer (S$^{2}$TDPT), a neuromorphic Transformer that implements self-attention through spike-timing-dependent plasticity (STDP), embedding query--key correlations in synaptic weights. STDP, a core mechanism of memory and learning in the brain and widely studied in neuromorphic devices, naturally enables in-memory computing and supports non-von Neumann hardware. On CIFAR-10 and CIFAR-100, our model achieves 94.35\% and 78.08\% accuracy with only four timesteps and 0.49 mJ on CIFAR-100, an 88.47\% energy reduction compared to a standard ANN Transformer. Grad-CAM shows that the model attends to semantically relevant regions, enhancing interpretability. Overall, S$^{2}$TDPT illustrates how biologically inspired attention can yield energy-efficient, hardware-friendly, and explainable neuromorphic models.",0,arxiv,Ä°statistik,CC-BY/arXiv,Attention via Synaptic Plasticity is All You Need: A Biologically Inspired Spiking Neuromorphic Transformer
"Structural nested mean models (SNMMs) are a principled approach to estimate the treatment effects over time. A particular strength of SNMMs is to break the joint effect of treatment sequences over time into localized, time-specific ``blip effects''. This decomposition promotes interpretability through the incremental effects and enables the efficient offline evaluation of optimal treatment policies without re-computation. However, neural frameworks for SNMMs are lacking, as their inherently sequential g-estimation scheme prevents end-to-end, gradient-based training. Here, we propose DeepBlip, the first neural framework for SNMMs, which overcomes this limitation with a novel double optimization trick to enable simultaneous learning of all blip functions. Our DeepBlip seamlessly integrates sequential neural networks like LSTMs or transformers to capture complex temporal dependencies. By design, our method correctly adjusts for time-varying confounding to produce unbiased estimates, and its Neyman-orthogonal loss function ensures robustness to nuisance model misspecification. Finally, we evaluate our DeepBlip across various clinical datasets, where it achieves state-of-the-art performance.",0,arxiv,Ä°statistik,CC-BY/arXiv,DeepBlip: Estimating Conditional Average Treatment Effects Over Time
"To distinguish Markov equivalent graphs in causal discovery, it is necessary to restrict the structural causal model. Crucially, we need to be able to distinguish cause $X$ from effect $Y$ in bivariate models, that is, distinguish the two graphs $X \to Y$ and $Y \to X$. Location-scale noise models (LSNMs), in which the effect $Y$ is modeled based on the cause $X$ as $Y = f(X) + g(X)N$, form a flexible class of models that is general and identifiable in most cases. Estimating these models for arbitrary noise terms $N$, however, is challenging. Therefore, practical estimators are typically restricted to symmetric distributions, such as the normal distribution. As we showcase in this paper, when $N$ is a skewed random variable, which is likely in real-world domains, the reliability of these approaches decreases. To approach this limitation, we propose SkewD, a likelihood-based algorithm for bivariate causal discovery under LSNMs with skewed noise distributions. SkewD extends the usual normal-distribution framework to the skew-normal setting, enabling reliable inference under symmetric and skewed noise. For parameter estimation, we employ a combination of a heuristic search and an expectation conditional maximization algorithm. We evaluate SkewD on novel synthetically generated datasets with skewed noise as well as established benchmark datasets. Throughout our experiments, SkewD exhibits a strong performance and, in comparison to prior work, remains robust under high skewness.",0,arxiv,Ä°statistik,CC-BY/arXiv,Skewness-Robust Causal Discovery in Location-Scale Noise Models
"Causal discovery combines data with knowledge provided by experts to learn the DAG representing the causal relationships between a given set of variables. When data are scarce, bagging is used to measure our confidence in an average DAG obtained by aggregating bootstrapped DAGs. However, the aggregation step has received little attention from the specialized literature: the average DAG is constructed using only the confidence in the individual edges of the bootstrapped DAGs, thus disregarding complex higher-order edge structures. In this paper, we introduce a novel theoretical framework based on higher-order structures and describe a new DAG aggregation algorithm. We perform a simulation study, discussing the advantages and limitations of the proposed approach. Our proposal is both computationally efficient and effective, outperforming state-of-the-art solutions, especially in low sample size regimes and under high dimensionality settings.",0,arxiv,Ä°statistik,CC-BY/arXiv,Causal Discovery on Higher-Order Interactions
"Clinical and genomic models are both used to predict breast cancer outcomes, but they are often combined using simple linear rules that do not account for how their risk scores relate, especially at the extremes. Using the METABRIC breast cancer cohort, we studied whether directly modeling the joint relationship between clinical and genomic machine learning risk scores could improve risk stratification for 5-year cancer-specific mortality. We created a binary 5-year cancer-death outcome and defined two sets of predictors: a clinical set (demographic, tumor, and treatment variables) and a genomic set (gene-expression $z$-scores). We trained several supervised classifiers, such as Random Forest and XGBoost, and used 5-fold cross-validated predicted probabilities as unbiased risk scores. These scores were converted to pseudo-observations on $(0,1)^2$ to fit Gaussian, Clayton, and Gumbel copulas. Clinical models showed good discrimination (AUC 0.783), while genomic models had moderate performance (AUC 0.681). The joint distribution was best captured by a Gaussian copula (bootstrap $p=0.997$), which suggests a symmetric, moderately strong positive relationship. When we grouped patients based on this relationship, Kaplan-Meier curves showed clear differences: patients who were high-risk in both clinical and genomic scores had much poorer survival than those high-risk in only one set. These results show that copula-based fusion works in real-world cohorts and that considering dependencies between scores can better identify patient subgroups with the worst prognosis.",0,arxiv,Ä°statistik,CC-BY/arXiv,Copula Based Fusion of Clinical and Genomic Machine Learning Risk Scores for Breast Cancer Risk Stratification
"We propose a distributionally robust formulation for simultaneously estimating the covariance matrix and the precision matrix of a random vector.The proposed model minimizes the worst-case weighted sum of the Frobenius loss of the covariance estimator and Stein's loss of the precision matrix estimator against all distributions from an ambiguity set centered at the nominal distribution. The radius of the ambiguity set is measured via convex spectral divergence. We demonstrate that the proposed distributionally robust estimation model can be reduced to a convex optimization problem, thereby yielding quasi-analytical estimators. The joint estimators are shown to be nonlinear shrinkage estimators. The eigenvalues of the estimators are shrunk nonlinearly towards a positive scalar, where the scalar is determined by the weight coefficient of the loss terms. By tuning the coefficient carefully, the shrinkage corrects the spectral bias of the empirical covariance/precision matrix estimator. By this property, we call the proposed joint estimator the Spectral concentrated COvariance and Precision matrix Estimator (SCOPE). We demonstrate that the shrinkage effect improves the condition number of the estimator. We provide a parameter-tuning scheme that adjusts the shrinkage target and intensity that is asymptotically optimal. Numerical experiments on synthetic and real data show that our shrinkage estimators perform competitively against state-of-the-art estimators in practical applications.",0,arxiv,Ä°statistik,CC-BY/arXiv,SCOPE: Spectral Concentration by Distributionally Robust Joint Covariance-Precision Estimation
"Automated scoring of written constructed responses typically relies on separate models per task, straining computational resources, storage, and maintenance in real-world education settings. We propose UniMoE-Guided, a knowledge-distilled multi-task Mixture-of-Experts (MoE) approach that transfers expertise from multiple task-specific large models (teachers) into a single compact, deployable model (student). The student combines (i) a shared encoder for cross-task representations, (ii) a gated MoE block that balances shared and task-specific processing, and (iii) lightweight task heads. Trained with both ground-truth labels and teacher guidance, the student matches strong task-specific models while being far more efficient to train, store, and deploy. Beyond efficiency, the MoE layer improves transfer and generalization: experts develop reusable skills that boost cross-task performance and enable rapid adaptation to new tasks with minimal additions and tuning. On nine NGSS-aligned science-reasoning tasks (seven for training/evaluation and two held out for adaptation), UniMoE-Guided attains performance comparable to per-task models while using $\sim$6$\times$ less storage than maintaining separate students, and $87\times$ less than the 20B-parameter teacher. The method offers a practical path toward scalable, reliable, and resource-efficient automated scoring for classroom and large-scale assessment systems.",0,arxiv,Ä°statistik,CC-BY/arXiv,Generalizable and Efficient Automated Scoring with a Knowledge-Distilled Multi-Task Mixture-of-Experts
"Estimating causal effects on time-to-event outcomes from observational data is particularly challenging due to censoring, limited sample sizes, and non-random treatment assignment. The need for answering such ""when-if"" questions--how the timing of an event would change under a specified intervention--commonly arises in real-world settings with heterogeneous treatment adoption and confounding. To address these challenges, we propose Synthetic Survival Control (SSC) to estimate counterfactual hazard trajectories in a panel data setting where multiple units experience potentially different treatments over multiple periods. In such a setting, SSC estimates the counterfactual hazard trajectory for a unit of interest as a weighted combination of the observed trajectories from other units. To provide formal justification, we introduce a panel framework with a low-rank structure for causal survival analysis. Indeed, such a structure naturally arises under classical parametric survival models. Within this framework, for the causal estimand of interest, we establish identification and finite sample guarantees for SSC. We validate our approach using a multi-country clinical dataset of cancer treatment outcomes, where the staggered introduction of new therapies creates a quasi-experimental setting. Empirically, we find that access to novel treatments is associated with improved survival, as reflected by lower post-intervention hazard trajectories relative to their synthetic counterparts. Given the broad relevance of survival analysis across medicine, economics, and public policy, our framework offers a general and interpretable tool for counterfactual survival inference using observational data.",0,arxiv,Ä°statistik,CC-BY/arXiv,"Synthetic Survival Control: Extending Synthetic Controls for ""When-If"" Decision"
"Generative models on curved spaces rely on charts to map Euclidean spaces to manifolds. Exponential maps preserve geodesics but have stiff, radius-dependent Jacobians, while volume-preserving charts maintain densities but distort geodesic distances. Both approaches entangle curvature with model parameters, inflating gradient variance. In high-dimensional latent normalizing flows, the wrapped exponential prior can stretch radii far beyond the curvature scale, leading to poor test likelihoods and stiff solvers. We introduce Radial Compensation (RC), an information-geometric method that selects the base density in the tangent space so that the likelihood depends only on geodesic distance from a pole, decoupling parameter semantics from curvature. RC lets radial parameters retain their usual meaning in geodesic units, while the chart can be tuned as a numerical preconditioner. We extend RC to manifolds with known geodesic polar volume and show that RC is the only construction for geodesic-radial likelihoods with curvature-invariant Fisher information. We derive the Balanced-Exponential (bExp) chart family, balancing volume distortion and geodesic error. Under RC, all bExp settings preserve the same manifold density and Fisher information, with smaller dial values reducing gradient variance and flow cost. Empirically, RC yields stable generative models across densities, VAEs, flows on images and graphs, and protein models. RC improves likelihoods, restores clean geodesic radii, and prevents radius blow-ups in high-dimensional flows, making RC-bExp a robust default for likelihood-trained generative models on manifolds.",0,arxiv,Ä°statistik,CC-BY/arXiv,Radial Compensation: Stable and Semantically Decoupled Generative Models on Riemannian Manifolds
"Small and medium-sized enterprises (SMEs) represent 99.9% of U.S. businesses yet remain systematically excluded from AI due to a mismatch between their operational scale and modern machine learning's data requirements. This paper introduces SmallML, a Bayesian transfer learning framework achieving enterprise-level prediction accuracy with datasets as small as 50-200 observations.   We develop a three-layer architecture integrating transfer learning, hierarchical Bayesian modeling, and conformal prediction. Layer 1 extracts informative priors from 22,673 public records using a SHAP-based procedure transferring knowledge from gradient boosting to logistic regression. Layer 2 implements hierarchical pooling across J=5-50 SMEs with adaptive shrinkage, balancing population patterns with entity-specific characteristics. Layer 3 provides conformal sets with finite-sample coverage guarantees P(y in C(x)) >= 1-alpha for distribution-free uncertainty quantification.   Validation on customer churn data demonstrates 96.7% +/- 4.2% AUC with 100 observations per business -- a +24.2 point improvement over independent logistic regression (72.5% +/- 8.1%), with p < 0.000001. Conformal prediction achieves 92% empirical coverage at 90% target. Training completes in 33 minutes on standard CPU hardware. By enabling enterprise-grade predictions for 33 million U.S. SMEs previously excluded from machine learning, SmallML addresses a critical gap in AI democratization.   Keywords: Bayesian transfer learning, hierarchical models, conformal prediction, small-data analytics, SME machine learning",0,arxiv,Ä°statistik,CC-BY/arXiv,SmallML: Bayesian Transfer Learning for Small-Data Predictive Analytics
"We introduce a highly expressive class of function approximators called Splat Regression Models. Model outputs are mixtures of heterogeneous and anisotropic bump functions, termed splats, each weighted by an output vector. The power of splat modeling lies in its ability to locally adjust the scale and direction of each splat, achieving both high interpretability and accuracy. Fitting splat models reduces to optimization over the space of mixing measures, which can be implemented using Wasserstein-Fisher-Rao gradient flows. As a byproduct, we recover the popular Gaussian Splatting methodology as a special case, providing a unified theoretical framework for this state-of-the-art technique that clearly disambiguates the inverse problem, the model, and the optimization algorithm. Through numerical experiments, we demonstrate that the resulting models and algorithms constitute a flexible and promising approach for solving diverse approximation, estimation, and inverse problems involving low-dimensional data.",0,arxiv,Ä°statistik,CC-BY/arXiv,Splat Regression Models
"We study the running time, in terms of first order oracle queries, of differentially private empirical/population risk minimization of Lipschitz convex losses. We first consider the setting where the loss is non-smooth and the optimizer interacts with a private proxy oracle, which sends only private messages about a minibatch of gradients. In this setting, we show that expected running time $Î©(\min\{\frac{\sqrt{d}}{Î±^2}, \frac{d}{\log(1/Î±)}\})$ is necessary to achieve $Î±$ excess risk on problems of dimension $d$ when $d \geq 1/Î±^2$. Upper bounds via DP-SGD show these results are tight when $d>\tildeÎ©(1/Î±^4)$. We further show our lower bound can be strengthened to $Î©(\min\{\frac{d}{\bar{m}Î±^2}, \frac{d}{\log(1/Î±)} \})$ for algorithms which use minibatches of size at most $\bar{m} < \sqrt{d}$. We next consider smooth losses, where we relax the private oracle assumption and give lower bounds under only the condition that the optimizer is private. Here, we lower bound the expected number of first order oracle calls by $\tildeÎ©\big(\frac{\sqrt{d}}Î± + \min\{\frac{1}{Î±^2}, n\}\big)$, where $n$ is the size of the dataset. Modifications to existing algorithms show this bound is nearly tight. Compared to non-private lower bounds, our results show that differentially private optimizers pay a dimension dependent runtime penalty. Finally, as a natural extension of our proof technique, we show lower bounds in the non-smooth setting for optimizers interacting with information limited oracles. Specifically, if the proxy oracle transmits at most $Î“$-bits of information about the gradients in the minibatch, then $Î©\big(\min\{\frac{d}{Î±^2Î“}, \frac{d}{\log(1/Î±)}\}\big)$ oracle calls are needed. This result shows fundamental limitations of gradient quantization techniques in optimization.",0,arxiv,Ä°statistik,CC-BY/arXiv,On the Gradient Complexity of Private Optimization with Private Oracles
"Algorithms developed under stationary Markov Decision Processes (MDPs) often face challenges in non-stationary environments, and infinite-horizon formulations may not directly apply to finite-horizon tasks. To address these limitations, we introduce the Non-stationary and Varying-discounting MDP (NVMDP) framework, which naturally accommodates non-stationarity and allows discount rates to vary with time and transitions. Infinite-horizon, stationary MDPs emerge as special cases of NVMDPs for identifying an optimal policy, and finite-horizon MDPs are also subsumed within the NVMDP formulations. Moreover, NVMDPs provide a flexible mechanism to shape optimal policies, without altering the state space, action space, or the reward structure. We establish the theoretical foundations of NVMDPs, including assumptions, state- and action-value formulation and recursion, matrix representation, optimality conditions, and policy improvement under finite state and action spaces. Building on these results, we adapt dynamic programming and generalized Q-learning algorithms to NVMDPs, along with formal convergence proofs. For problems requiring function approximation, we extend the Policy Gradient Theorem and the policy improvement bound in Trust Region Policy Optimization (TRPO), offering proofs in both scalar and matrix forms. Empirical evaluations in a non-stationary gridworld environment demonstrate that NVMDP-based algorithms successfully recover optimal trajectories under multiple reward and discounting schemes, whereas original Q-learning fails. These results collectively show that NVMDPs provide a theoretically sound and practically effective framework for reinforcement learning, requiring only minor algorithmic modifications while enabling robust handling of non-stationarity and explicit optimal policy shaping.",0,arxiv,Ä°statistik,CC-BY/arXiv,Non-stationary and Varying-discounting Markov Decision Processes for Reinforcement Learning
"We develop an empirical likelihood (EL) framework for random forests and related ensemble methods, providing a likelihood-based approach to quantify their statistical uncertainty. Exploiting the incomplete $U$-statistic structure inherent in ensemble predictions, we construct an EL statistic that is asymptotically chi-squared when subsampling induced by incompleteness is not overly sparse. Under sparser subsampling regimes, the EL statistic tends to over-cover due to loss of pivotality; we therefore propose a modified EL that restores pivotality through a simple adjustment. Our method retains key properties of EL while remaining computationally efficient. Theory for honest random forests and simulations demonstrate that modified EL achieves accurate coverage and practical reliability relative to existing inference methods.",0,arxiv,Ä°statistik,CC-BY/arXiv,Empirical Likelihood for Random Forests and Ensembles
"Despite recent progress in predicting biomarker trajectories from real clinical data, uncertainty in the predictions poses high-stakes risks (e.g., misdiagnosis) that limit their clinical deployment. To enable safe and reliable use of such predictions in healthcare, we introduce a conformal method for uncertainty-calibrated prediction of biomarker trajectories resulting from randomly-timed clinical visits of patients. Our approach extends conformal prediction to the setting of randomly-timed trajectories via a novel nonconformity score that produces prediction bands guaranteed to cover the unknown biomarker trajectories with a user-prescribed probability. We apply our method across a wide range of standard and state-of-the-art predictors for two well-established brain biomarkers of Alzheimer's disease, using neuroimaging data from real clinical studies. We observe that our conformal prediction bands consistently achieve the desired coverage, while also being tighter than baseline prediction bands. To further account for population heterogeneity, we develop group-conditional conformal bands and test their coverage guarantees across various demographic and clinically relevant subpopulations. Moreover, we demonstrate the clinical utility of our conformal bands in identifying subjects at high risk of progression to Alzheimer's disease. Specifically, we introduce an uncertainty-calibrated risk score that enables the identification of 17.5% more high-risk subjects compared to standard risk scores, highlighting the value of uncertainty calibration in real-world clinical decision making. Our code is available at github.com/vatass/ConformalBiomarkerTrajectories.",0,arxiv,Ä°statistik,CC-BY/arXiv,Uncertainty-Calibrated Prediction of Randomly-Timed Biomarker Trajectories with Conformal Bands
"A decision-theoretic characterization of perfect calibration is that an agent seeking to minimize a proper loss in expectation cannot improve their outcome by post-processing a perfectly calibrated predictor. Hu and Wu (FOCS'24) use this to define an approximate calibration measure called calibration decision loss ($\mathsf{CDL}$), which measures the maximal improvement achievable by any post-processing over any proper loss. Unfortunately, $\mathsf{CDL}$ turns out to be intractable to even weakly approximate in the offline setting, given black-box access to the predictions and labels.   We suggest circumventing this by restricting attention to structured families of post-processing functions $K$. We define the calibration decision loss relative to $K$, denoted $\mathsf{CDL}_K$ where we consider all proper losses but restrict post-processings to a structured family $K$. We develop a comprehensive theory of when $\mathsf{CDL}_K$ is information-theoretically and computationally tractable, and use it to prove both upper and lower bounds for natural classes $K$. In addition to introducing new definitions and algorithmic techniques to the theory of calibration for decision making, our results give rigorous guarantees for some widely used recalibration procedures in machine learning.",0,arxiv,Ä°statistik,CC-BY/arXiv,Efficient Calibration for Decision Making
"Modern scientific simulations, observations, and large-scale experiments generate data at volumes that often exceed the limits of storage, processing, and analysis. This challenge drives the development of data reduction methods that efficiently manage massive datasets while preserving essential physical features and quantities of interest. In many scientific workflows, it is also crucial to enable data recovery from compressed representations - a task known as super-resolution - with guarantees on the preservation of key physical characteristics. A notable example is checkpointing and restarting, which is essential for long-running simulations to recover from failures, resume after interruptions, or examine intermediate results. In this work, we introduce a novel framework for scientific data compression and super-resolution, grounded in recent advances in learning exponential families. Our method preserves and quantifies uncertainty in physical quantities of interest and supports flexible trade-offs between compression ratio and reconstruction fidelity.",0,arxiv,Ä°statistik,CC-BY/arXiv,Scientific Data Compression and Super-Resolution Sampling
"We propose a nonparametric estimator of multivariate joint entropy based on partitioned sample spacing (PSS). The method extends univariate spacing ideas to $\mathbb{R}^{d}$ by partitioning into localized cells and aggregating within-cell statistics, with strong consistency guarantees under mild conditions. In benchmarks across diverse distributions, PSS consistently outperforms $k$-nearest neighbor estimators and achieves accuracy competitive with recent normalizing flow-based methods, while requiring no training or auxiliary density modeling. The estimator scales favorably in moderately high dimensions ($d = 10$--$40$) and shows particular robustness to correlated or skewed distributions. These properties position PSS as a practical and reliable alternative to both $k$NN and NF-based entropy estimators, with broad utility in information-theoretic machine learning tasks such as total-correlation estimation, representation learning, and feature selection.",0,arxiv,Ä°statistik,CC-BY/arXiv,Nonparametric Estimation of Joint Entropy via Partitioned Sample-Spacing
"Modern business and economic datasets often exhibit nonlinear, multi-scale structures that traditional linear tools under-represent. Topological Data Analysis (TDA) offers a geometric lens for uncovering robust patterns, such as connected components, loops and voids, across scales. This paper provides an intuitive, figure-driven introduction to persistent homology and a practical, reproducible TDA pipeline for applied analysts. Through comparative case studies in consumer behavior, equity markets (SAX/eSAX vs.\ TDA) and foreign exchange dynamics, we demonstrate how topological features can reveal segmentation patterns and structural relationships beyond classical statistical methods. We discuss methodological choices regarding distance metrics, complex construction and interpretation, and we introduce the \textit{Topological Stability Index} (TSI), a simple yet interpretable indicator of structural variability derived from persistence lifetimes. We conclude with practical guidelines for TDA implementation, visualization and communication in business and economic analytics.",0,arxiv,Ä°statistik,CC-BY/arXiv,The Shape of Data: Topology Meets Analytics. A Practical Introduction to Topological Analytics and the Stability Index (TSI) in Business
"Since the 21st century, artificial intelligence has been leading a new round of industrial revolution. Under the training framework, the optimization algorithm aims to stably converge high-dimensional optimization to local and even global minima. Entering the era of large language models, although the scale of model parameters and data has increased, Adam remains the mainstream optimization algorithm. However, compared with stochastic gradient descent (SGD) based optimization algorithms, Adam is more likely to converge to non-flat minima. To address this issue, the AdamNX algorithm is proposed. Its core innovation lies in the proposition of a novel type of second-order moment estimation exponential decay rate, which gradually weakens the learning step correction strength as training progresses, and degrades to momentum SGD in the stable training period, thereby improving the stability of training in the stable period and possibly enhancing generalization ability. Experimental results show that our second-order moment estimation exponential decay rate is better than the current second-order moment estimation exponential decay rate, and AdamNX can stably outperform Adam and its variants in terms of performance. Our code is open-sourced at https://github.com/mengzhu0308/AdamNX.",0,arxiv,Ä°statistik,CC-BY/arXiv,AdamNX: An Adam improvement algorithm based on a novel exponential decay mechanism for the second-order moment estimate
"While data scaling laws of large language models (LLMs) have been widely examined in the one-pass regime with massive corpora, their form under limited data and repeated epochs remains largely unexplored. This paper presents a theoretical analysis of how a common workaround, training for multiple epochs on the same dataset, reshapes the data scaling laws in linear regression. Concretely, we ask: to match the performance of training on a dataset of size $N$ for $K$ epochs, how much larger must a dataset be if the model is trained for only one pass? We quantify this using the \textit{effective reuse rate} of the data, $E(K, N)$, which we define as the multiplicative factor by which the dataset must grow under one-pass training to achieve the same test loss as $K$-epoch training. Our analysis precisely characterizes the scaling behavior of $E(K, N)$ for SGD in linear regression under either strong convexity or Zipf-distributed data: (1) When $K$ is small, we prove that $E(K, N) \approx K$, indicating that every new epoch yields a linear gain; (2) As $K$ increases, $E(K, N)$ plateaus at a problem-dependent value that grows with $N$ ($Î˜(\log N)$ for the strongly-convex case), implying that larger datasets can be repeated more times before the marginal benefit vanishes. These theoretical findings point out a neglected factor in a recent empirical study (Muennighoff et al. (2023)), which claimed that training LLMs for up to $4$ epochs results in negligible loss differences compared to using fresh data at each step, \textit{i.e.}, $E(K, N) \approx K$ for $K \le 4$ in our notation. Supported by further empirical validation with LLMs, our results reveal that the maximum $K$ value for which $E(K, N) \approx K$ in fact depends on the data size and distribution, and underscore the need to explicitly model both factors in future studies of scaling laws with data reuse.",0,arxiv,Ä°statistik,CC-BY/arXiv,Larger Datasets Can Be Repeated More: A Theoretical Analysis of Multi-Epoch Scaling in Linear Regression
"Bayesian parameter inference for complex stochastic simulators is challenging due to intractable likelihood functions. Existing simulation-based inference methods often require large number of simulations and become costly to use in high-dimensional parameter spaces or in problems with partially uninformative outputs. We propose a new method for differentiable simulators that delivers accurate posterior inference with substantially reduced runtimes. Building on the Optimization Monte Carlo framework, our approach reformulates stochastic simulation as deterministic optimization problems. Gradient-based methods are then applied to efficiently navigate toward high-density posterior regions and avoid wasteful simulations in low-probability areas. A JAX-based implementation further enhances the performance through vectorization of key method components. Extensive experiments, including high-dimensional parameter spaces, uninformative outputs, multiple observations and multimodal posteriors show that our method consistently matches, and often exceeds, the accuracy of state-of-the-art approaches, while reducing the runtime by a substantial margin.",0,arxiv,Ä°statistik,CC-BY/arXiv,Fast and Robust Simulation-Based Inference With Optimization Monte Carlo
"Recent advances in deep learning have improved multivariate time series (MTS) classification and regression by capturing complex patterns, but their lack of transparency hinders decision-making. Explainable AI (XAI) methods offer partial insights, yet often fall short of conveying the full decision space. Counterfactual Explanations (CE) provide a promising alternative, but current approaches typically prioritize either accuracy, proximity or sparsity -- rarely all -- limiting their practical value. To address this, we propose CONFETTI, a novel multi-objective CE method for MTS. CONFETTI identifies key MTS subsequences, locates a counterfactual target, and optimally modifies the time series to balance prediction confidence, proximity and sparsity. This method provides actionable insights with minimal changes, improving interpretability, and decision support. CONFETTI is evaluated on seven MTS datasets from the UEA archive, demonstrating its effectiveness in various domains. CONFETTI consistently outperforms state-of-the-art CE methods in its optimization objectives, and in six other metrics from the literature, achieving $\geq10\%$ higher confidence while improving sparsity in $\geq40\%$.",0,arxiv,Ä°statistik,CC-BY/arXiv,Counterfactual Explainable AI (XAI) Method for Deep Learning-Based Multivariate Time Series Classification
"The manifold hypothesis posits that high-dimensional data typically resides on low-dimensional sub spaces. In this paper, we assume manifold hypothesis to investigate graph-based semi-supervised learning   methods. In particular, we examine Laplace Learning in the Wasserstein space, extending the classical   notion of graph-based semi-supervised learning algorithms from finite-dimensional Euclidean spaces to   an infinite-dimensional setting. To achieve this, we prove variational convergence of a discrete graph p- Dirichlet energy to its continuum counterpart. In addition, we characterize the Laplace-Beltrami operator   on asubmanifold of the Wasserstein space. Finally, we validate the proposed theoretical framework through   numerical experiments conducted on benchmark datasets, demonstrating the consistency of our classification performance in high-dimensional settings.",0,arxiv,Ä°statistik,CC-BY/arXiv,Laplace Learning in Wasserstein Space
"The transformer architecture has demonstrated strong performance in classification tasks involving structured and high-dimensional data. However, its success often hinges on large- scale training data and careful regularization to prevent overfitting. In this paper, we intro- duce a novel likelihood-guided variational Ising-based regularization framework for Vision Transformers (ViTs), which simultaneously enhances model generalization and dynamically prunes redundant parameters. The proposed variational Ising-based regularization approach leverages Bayesian sparsification techniques to impose structured sparsity on model weights, allowing for adaptive architecture search during training. Unlike traditional dropout-based methods, which enforce fixed sparsity patterns, the variational Ising-based regularization method learns task-adaptive regularization, improving both efficiency and interpretability. We evaluate our approach on benchmark vision datasets, including MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100, demonstrating improved generalization under sparse, complex data and allowing for principled uncertainty quantification on both weights and selection parameters. Additionally, we show that the Ising regularizer leads to better-calibrated probability estimates and structured feature selection through uncertainty-aware attention mechanisms. Our results highlight the effectiveness of structured Bayesian sparsification in enhancing transformer-based architectures, offering a principled alternative to standard regularization techniques.",0,arxiv,Ä°statistik,CC-BY/arXiv,Likelihood-guided Regularization in Attention Based Models
"In a multivariate nonparametric regression setting we construct explicit asymptotic uniform confidence bands for centered purely random forests. Since the most popular example in this class of random forests, namely the uniformly centered purely random forests, is well known to suffer from suboptimal rates, we propose a new type of purely random forests, called the Ehrenfest centered purely random forests, which achieve minimax optimal rates. Our main confidence band theorem applies to both random forests. The proof is based on an interpretation of random forests as generalized U-Statistics together with a Gaussian approximation of the supremum of empirical processes. Our theoretical findings are illustrated in simulation examples.",0,arxiv,Ä°statistik,CC-BY/arXiv,Asymptotic confidence bands for centered purely random forests
"We study a matrix completion problem where both the ground truth $R$ matrix and the unknown sampling distribution $P$ over observed entries are low-rank matrices, and \textit{share a common subspace}. We assume that a large amount $M$ of \textit{unlabeled} data drawn from the sampling distribution $P$ is available, together with a small amount $N$ of labeled data drawn from the same distribution and noisy estimates of the corresponding ground truth entries. This setting is inspired by recommender systems scenarios where the unlabeled data corresponds to `implicit feedback' (consisting in interactions such as purchase, click, etc. ) and the labeled data corresponds to the `explicit feedback', consisting of interactions where the user has given an explicit rating to the item. Leveraging powerful results from the theory of low-rank subspace recovery, together with classic generalization bounds for matrix completion models, we show error bounds consisting of a sum of two error terms scaling as $\widetilde{O}\left(\sqrt{\frac{nd}{M}}\right)$ and $\widetilde{O}\left(\sqrt{\frac{dr}{N}}\right)$ respectively, where $d$ is the rank of $P$ and $r$ is the rank of $M$. In synthetic experiments, we confirm that the true generalization error naturally splits into independent error terms corresponding to the estimations of $P$ and and the ground truth matrix $\ground$ respectively. In real-life experiments on Douban and MovieLens with most explicit ratings removed, we demonstrate that the method can outperform baselines relying only on the explicit ratings, demonstrating that our assumptions provide a valid toy theoretical setting to study the interaction between explicit and implicit feedbacks in recommender systems.",0,arxiv,Ä°statistik,CC-BY/arXiv,Generalization Bounds for Semi-supervised Matrix Completion with Distributional Side Information
"We consider the problem of reconstructing the intrinsic geometry of a manifold from noisy pairwise distance observations. Specifically, let $M$ denote a diameter 1 d-dimensional manifold and $Î¼$ a probability measure on $M$ that is mutually absolutely continuous with the volume measure. Suppose $X_1,\dots,X_N$ are i.i.d. samples of $Î¼$ and we observe noisy-distance random variables $d'(X_j, X_k)$ that are related to the true geodesic distances $d(X_j,X_k)$. With mild assumptions on the distributions and independence of the noisy distances, we develop a new framework for recovering all distances between points in a sufficiently dense subsample of $M$. Our framework improves on previous work which assumed i.i.d. additive noise with known moments. Our method is based on a new way to estimate $L_2$-norms of certain expectation-functions $f_x(y)=\mathbb{E}d'(x,y)$ and use them to build robust clusters centered at points of our sample. Using a new geometric argument, we establish that, under mild geometric assumptions--bounded curvature and positive injectivity radius--these clusters allow one to recover the true distances between points in the sample up to an additive error of $O(\varepsilon \log \varepsilon^{-1})$. We develop two distinct algorithms for producing these clusters. The first achieves a sample complexity $N \asymp \varepsilon^{-2d-2}\log(1/\varepsilon)$ and runtime $o(N^3)$. The second introduces novel geometric ideas that warrant further investigation. In the presence of missing observations, we show that a quantitative lower bound on sampling probabilities suffices to modify the cluster construction in the first algorithm and extend all recovery guarantees. Our main technical result also elucidates which properties of a manifold are necessary for the distance recovery, which suggests further extension of our techniques to a broader class of metric probability spaces.",0,arxiv,Ä°statistik,CC-BY/arXiv,Reconstruction of Manifold Distances from Noisy Observations
"Synthetic tabular data, which are widely used in domains such as healthcare, enterprise operations, and customer analytics, are increasingly evaluated to ensure that they preserve both privacy and utility. While existing evaluation practices typically focus on distributional similarity (e.g., the Kullback-Leibler divergence) or predictive performance (e.g., Train-on-Synthetic-Test-on-Real (TSTR) accuracy), these approaches fail to assess semantic fidelity, that is, whether models trained on synthetic data follow reasoning patterns consistent with those trained on real data. To address this gap, we introduce the SHapley Additive exPlanations (SHAP) Distance, a novel explainability-aware metric that is defined as the cosine distance between the global SHAP attribution vectors derived from classifiers trained on real versus synthetic datasets. By analyzing datasets that span clinical health records with physiological features, enterprise invoice transactions with heterogeneous scales, and telecom churn logs with mixed categorical-numerical attributes, we demonstrate that the SHAP Distance reliably identifies semantic discrepancies that are overlooked by standard statistical and predictive measures. In particular, our results show that the SHAP Distance captures feature importance shifts and underrepresented tail effects that the Kullback-Leibler divergence and Train-on-Synthetic-Test-on-Real accuracy fail to detect. This study positions the SHAP Distance as a practical and discriminative tool for auditing the semantic fidelity of synthetic tabular data, and offers practical guidelines for integrating attribution-based evaluation into future benchmarking pipelines.",0,arxiv,Ä°statistik,CC-BY/arXiv,SHAP Distance: An Explainability-Aware Metric for Evaluating the Semantic Fidelity of Synthetic Tabular Data
"Leveraging the Wasserstein distance -- a summation of sample-wise transport distances in data space -- is advantageous in many applications for measuring support differences between two underlying density functions. However, when supports significantly overlap while densities exhibit substantial pointwise differences, it remains unclear whether and how this transport information can accurately identify these differences, particularly their analytic characterization in finite-sample settings. We address this issue by conducting an analysis of the information processing capabilities of the one-dimensional Wasserstein distance with finite samples. By utilizing the Poisson process and isolating the rate factor, we demonstrate the capability of capturing the pointwise density difference with Wasserstein distances and how this information harmonizes with support differences. The analyzed properties are confirmed using neural spike train decoding and amino acid contact frequency data. The results reveal that the one-dimensional Wasserstein distance highlights meaningful density differences related to both rate and support.",0,arxiv,Ä°statistik,CC-BY/arXiv,On the Information Processing of One-Dimensional Wasserstein Distances with Finite Samples
"Modern machine learning models with a large number of parameters often generalize well despite perfectly interpolating noisy training data - a phenomenon known as benign overfitting. A foundational explanation for this in linear classification was recently provided by Hashimoto et al. (2025). However, this analysis was limited to the setting of ""homogeneous"" models, which lack a bias (intercept) term - a standard component in practice. This work directly extends Hashimoto et al.'s results to the more realistic inhomogeneous case, which incorporates a bias term. Our analysis proves that benign overfitting persists in these more complex models. We find that the presence of the bias term introduces new constraints on the data's covariance structure required for generalization, an effect that is particularly pronounced when label noise is present. However, we show that in the isotropic case, these new constraints are dominated by the requirements inherited from the homogeneous model. This work provides a more complete picture of benign overfitting, revealing the non-trivial impact of the bias term on the conditions required for good generalization.",0,arxiv,Ä°statistik,CC-BY/arXiv,Benign Overfitting in Linear Classifiers with a Bias Term
"Sampling from a target distribution induced by training data is central to Bayesian learning, with Stochastic Gradient Langevin Dynamics (SGLD) serving as a key tool for scalable posterior sampling and decentralized variants enabling learning when data are distributed across a network of agents. This paper introduces DIGing-SGLD, a decentralized SGLD algorithm designed for scalable Bayesian learning in multi-agent systems operating over time-varying networks. Existing decentralized SGLD methods are restricted to static network topologies, and many exhibit steady-state sampling bias caused by network effects, even when full batches are used. DIGing-SGLD overcomes these limitations by integrating Langevin-based sampling with the gradient-tracking mechanism of the DIGing algorithm, originally developed for decentralized optimization over time-varying networks, thereby enabling efficient and bias-free sampling without a central coordinator. To our knowledge, we provide the first finite-time non-asymptotic Wasserstein convergence guarantees for decentralized SGLD-based sampling over time-varying networks, with explicit constants. Under standard strong convexity and smoothness assumptions, DIGing-SGLD achieves geometric convergence to an $O(\sqrtÎ·)$ neighborhood of the target distribution, where $Î·$ is the stepsize, with dependence on the target accuracy matching the best-known rates for centralized and static-network SGLD algorithms using constant stepsize. Numerical experiments on Bayesian linear and logistic regression validate the theoretical results and demonstrate the strong empirical performance of DIGing-SGLD under dynamically evolving network conditions.",0,arxiv,Ä°statistik,CC-BY/arXiv,DIGing--SGLD: Decentralized and Scalable Langevin Sampling over Time--Varying Networks
"A finite-horizon variant of the quickest change detection (QCD) problem that is of relevance to learning in non-stationary environments is studied. The metric characterizing false alarms is the probability of a false alarm occurring before the horizon ends. The metric that characterizes the delay is \emph{latency}, which is the smallest value such that the probability that detection delay exceeds this value is upper bounded to a predetermined latency level. The objective is to minimize the latency (at a given latency level), while maintaining a low false alarm probability. Under the pre-specified latency and false alarm levels, a universal lower bound on the latency, which any change detection procedure needs to satisfy, is derived. Change detectors are then developed, which are order-optimal in terms of the horizon. The case where the pre- and post-change distributions are known is considered first, and then the results are generalized to the non-parametric case when they are unknown except that they are sub-Gaussian with different means. Simulations are provided to validate the theoretical results.",0,arxiv,Ä°statistik,CC-BY/arXiv,Finite-Horizon Quickest Change Detection Balancing Latency with False Alarm Probability
"Bayesian optimization (BO) has been widely used to optimize expensive and gradient-free objective functions across various domains. However, existing BO methods have not addressed the objective where both inputs and outputs are functions, which increasingly arise in complex systems as advanced sensing technologies. To fill this gap, we propose a novel function-on-function Bayesian optimization (FFBO) framework. Specifically, we first introduce a function-on-function Gaussian process (FFGP) model with a separable operator-valued kernel to capture the correlations between function-valued inputs and outputs. Compared to existing Gaussian process models, FFGP is modeled directly in the function space. Based on FFGP, we define a scalar upper confidence bound (UCB) acquisition function using a weighted operator-based scalarization strategy. Then, a scalable functional gradient ascent algorithm (FGA) is developed to efficiently identify the optimal function-valued input. We further analyze the theoretical properties of the proposed method. Extensive experiments on synthetic and real-world data demonstrate the superior performance of FFBO over existing approaches.",0,arxiv,Ä°statistik,CC-BY/arXiv,Function-on-Function Bayesian Optimization
"We introduce Conformal Online Learning of Koopman embeddings (COLoKe), a novel framework for adaptively updating Koopman-invariant representations of nonlinear dynamical systems from streaming data. Our modeling approach combines deep feature learning with multistep prediction consistency in the lifted space, where the dynamics evolve linearly. To prevent overfitting, COLoKe employs a conformal-style mechanism that shifts the focus from evaluating the conformity of new states to assessing the consistency of the current Koopman model. Updates are triggered only when the current model's prediction error exceeds a dynamically calibrated threshold, allowing selective refinement of the Koopman operator and embedding. Empirical results on benchmark dynamical systems demonstrate the effectiveness of COLoKe in maintaining long-term predictive accuracy while significantly reducing unnecessary updates and avoiding overfitting.",0,arxiv,Ä°statistik,CC-BY/arXiv,Conformal Online Learning of Deep Koopman Linear Embeddings
"Intermittent demand forecasting poses unique challenges due to sparse observations, cold-start items, and obsolescence. Classical models such as Croston, SBA, and the Teunter-Syntetos-Babai (TSB) method provide simple heuristics but lack a principled generative foundation. Deep learning models address these limitations but often require large datasets and sacrifice interpretability.   We introduce TSB-HB, a hierarchical Bayesian extension of TSB. Demand occurrence is modeled with a Beta-Binomial distribution, while nonzero demand sizes follow a Log-Normal distribution. Crucially, hierarchical priors enable partial pooling across items, stabilizing estimates for sparse or cold-start series while preserving heterogeneity. This framework yields a fully generative and interpretable model that generalizes classical exponential smoothing.   On the UCI Online Retail dataset, TSB-HB achieves lower RMSE and RMSSE than Croston, SBA, TSB, ADIDA, IMAPA, ARIMA and Theta, and on a subset of the M5 dataset it outperforms all classical baselines we evaluate. The model provides calibrated probabilistic forecasts and improved accuracy on intermittent and lumpy items by combining a generative formulation with hierarchical shrinkage, while remaining interpretable and scalable.",0,arxiv,Ä°statistik,CC-BY/arXiv,TSB-HB: A Hierarchical Bayesian Extension of the TSB Model for Intermittent Demand Forecasting
"Federated Learning (FL) enables decentralized, privacy-preserving model training but struggles to balance global generalization and local personalization due to non-identical data distributions across clients. Personalized Fine-Tuning (PFT), a popular post-hoc solution, fine-tunes the final global model locally but often overfits to skewed client distributions or fails under domain shifts. We propose adapting Linear Probing followed by full Fine-Tuning (LP-FT), a principled centralized strategy for alleviating feature distortion (Kumar et al., 2022), to the FL setting. Through systematic evaluation across seven datasets and six PFT variants, we demonstrate LP-FT's superiority in balancing personalization and generalization. Our analysis uncovers federated feature distortion, a phenomenon where local fine-tuning destabilizes globally learned features, and theoretically characterizes how LP-FT mitigates this via phased parameter updates. We further establish conditions (e.g., partial feature overlap, covariate-concept shift) under which LP-FT outperforms standard fine-tuning, offering actionable guidelines for deploying robust personalization in FL.",0,arxiv,Ä°statistik,CC-BY/arXiv,A Closer Look at Personalized Fine-Tuning in Heterogeneous Federated Learning
"In this paper, we study the finite-sample statistical rates of distributional temporal difference (TD) learning with linear function approximation. The purpose of distributional TD learning is to estimate the return distribution of a discounted Markov decision process for a given policy. Previous works on statistical analysis of distributional TD learning focus mainly on the tabular case. We first consider the linear function approximation setting and conduct a fine-grained analysis of the linear-categorical Bellman equation. Building on this analysis, we further incorporate variance reduction techniques in our new algorithms to establish tight sample complexity bounds independent of the support size $K$ when $K$ is large. Our theoretical results imply that, when employing distributional TD learning with linear function approximation, learning the full distribution of the return function from streaming data is no more difficult than learning its expectation. This work provide new insights into the statistical efficiency of distributional reinforcement learning algorithms.",0,arxiv,Ä°statistik,CC-BY/arXiv,Accelerated Distributional Temporal Difference Learning with Linear Function Approximation
"The fundamental theorem of statistical learning states that binary PAC learning is governed by a single parameter -- the Vapnik-Chervonenkis (VC) dimension -- which determines both learnability and sample complexity. Extending this to multiclass classification has long been challenging, since Natarajan's work in the late 80s proposing the Natarajan dimension (Nat) as a natural analogue of VC. Daniely and Shalev-Shwartz (2014) introduced the DS dimension, later shown by Brukhim et al. (2022) to characterize multiclass learnability. Brukhim et al. also showed that Nat and DS can diverge arbitrarily, suggesting that multiclass learning is governed by DS rather than Nat. We show that agnostic multiclass PAC sample complexity is in fact governed by two distinct dimensions. Specifically, we prove nearly tight agnostic sample complexity bounds that, up to log factors, take the form $\frac{DS^{1.5}}Îµ + \frac{Nat}{Îµ^2}$ where $Îµ$ is the excess risk. This bound is tight up to a $\sqrt{DS}$ factor in the first term, nearly matching known $Nat/Îµ^2$ and $DS/Îµ$ lower bounds. The first term reflects the DS-controlled regime, while the second shows that the Natarajan dimension still dictates asymptotic behavior for small $Îµ$. Thus, unlike binary or online classification -- where a single dimension (VC or Littlestone) controls both phenomena -- multiclass learning inherently involves two structural parameters. Our technical approach departs from traditional agnostic learning methods based on uniform convergence or reductions to realizable cases. A key ingredient is a novel online procedure based on a self-adaptive multiplicative-weights algorithm performing a label-space reduction, which may be of independent interest.",0,arxiv,Ä°statistik,CC-BY/arXiv,Sample Complexity of Agnostic Multiclass Classification: Natarajan Dimension Strikes Back
"Low latency event-selection (trigger) algorithms are essential components of Large Hadron Collider (LHC) operation. Modern machine learning (ML) models have shown great offline performance as classifiers and could improve trigger performance, thereby improving downstream physics analyses. However, inference on such large models does not satisfy the $40\text{MHz}$ online latency constraint at the LHC. In this work, we propose \texttt{PHAZE}, a novel framework built on cryptographic techniques like hashing and zero-knowledge machine learning (zkML) to achieve low latency inference, via a certifiable, early-exit mechanism from an arbitrarily large baseline model. We lay the foundations for such a framework to achieve nanosecond-order latency and discuss its inherent advantages, such as built-in anomaly detection, within the scope of LHC triggers, as well as its potential to enable a dynamic low-level trigger in the future.",0,arxiv,Ä°statistik,CC-BY/arXiv,"Knowledge is Overrated: A zero-knowledge machine learning and cryptographic hashing-based framework for verifiable, low latency inference at the LHC"
"Stochastic multi-objective optimization (SMOOP) requires ranking multivariate distributions; yet, most empirical studies perform scalarization, which loses information and is unreliable. Based on the optimal transport theory, we introduce the center-outward q-dominance relation and prove it implies strong first-order stochastic dominance (FSD). Also, we develop an empirical test procedure based on q-dominance, and derive an explicit sample size threshold, $n^*(Î´)$, to control the Type I error. We verify the usefulness of our approach in two scenarios: (1) as a ranking method in hyperparameter tuning; (2) as a selection method in multi-objective optimization algorithms. For the former, we analyze the final stochastic Pareto sets of seven multi-objective hyperparameter tuners on the YAHPO-MO benchmark tasks with q-dominance, which allows us to compare these tuners when the expected hypervolume indicator (HVI, the most common performance metric) of the Pareto sets becomes indistinguishable. For the latter, we replace the mean value-based selection in the NSGA-II algorithm with $q$-dominance, which shows a superior convergence rate on noise-augmented ZDT benchmark problems. These results establish center-outward q-dominance as a principled, tractable foundation for seeking truly stochastically dominant solutions for SMOOPs.",0,arxiv,Ä°statistik,CC-BY/arXiv,Center-Outward q-Dominance: A Sample-Computable Proxy for Strong Stochastic Dominance in Multi-Objective Optimisation
"Most estimators collapse all uncertainty modes into a single confidence score, preventing reliable reasoning about when to allocate more compute or adjust inference. We introduce Uncertainty-Guided Inference-Time Selection, a lightweight inference time framework that disentangles aleatoric (data-driven) and epistemic (model-driven) uncertainty directly in deep feature space. Aleatoric uncertainty is estimated using a regularized global density model, while epistemic uncertainty is formed from three complementary components that capture local support deficiency, manifold spectral collapse, and cross-layer feature inconsistency. These components are empirically orthogonal and require no sampling, no ensembling, and no additional forward passes. We integrate the decomposed uncertainty into a distribution free conformal calibration procedure that yields significantly tighter prediction intervals at matched coverage. Using these components for uncertainty guided adaptive model selection reduces compute by approximately 60 percent on MOT17 with negligible accuracy loss, enabling practical self regulating visual inference. Additionally, our ablation results show that the proposed orthogonal uncertainty decomposition consistently yields higher computational savings across all MOT17 sequences, improving margins by 13.6 percentage points over the total-uncertainty baseline.",0,arxiv,Ä°statistik,CC-BY/arXiv,Calibrated Decomposition of Aleatoric and Epistemic Uncertainty in Deep Features for Inference-Time Adaptation
"Self-consistency (SC) is a widely used test-time inference technique for improving performance in chain-of-thought reasoning. It involves generating multiple responses, or samples from a large language model (LLM) and selecting the most frequent answer. This procedure can naturally be viewed as a majority vote or empirical mode estimation. Despite its effectiveness, SC is prohibitively expensive at scale when naively applied to datasets, and it lacks a unified theoretical treatment of sample efficiency and scaling behavior. In this paper, we provide the first comprehensive analysis of SC's scaling behavior and its variants, drawing on mode estimation and voting theory. We derive and empirically validate power law scaling for self-consistency across datasets, and analyze the sample efficiency for fixed-allocation and dynamic-allocation sampling schemes. From these insights, we introduce Blend-ASC, a novel variant of self-consistency that dynamically allocates samples to questions during inference, achieving state-of-the-art sample efficiency. Our approach uses 6.8x fewer samples than vanilla SC on average, outperforming both fixed- and dynamic-allocation SC baselines, thereby demonstrating the superiority of our approach in terms of efficiency. In contrast to existing variants, Blend-ASC is hyperparameter-free and can fit an arbitrary sample budget, ensuring it can be easily applied to any self-consistency application.",0,arxiv,Ä°statistik,CC-BY/arXiv,Optimal Self-Consistency for Efficient Reasoning with Large Language Models
"High-dimensional data often contain low-dimensional signals obscured by structured background noise, which limits the effectiveness of standard PCA. Motivated by contrastive learning, we address the problem of recovering shared signal subspaces from positive pairs, paired observations sharing the same signal but differing in background. Our baseline, PCA+, uses alignment-only contrastive learning and succeeds when background variation is mild, but fails under strong noise or high-dimensional regimes. To address this, we introduce PCA++, a hard uniformity-constrained contrastive PCA that enforces identity covariance on projected features. PCA++ has a closed-form solution via a generalized eigenproblem, remains stable in high dimensions, and provably regularizes against background interference. We provide exact high-dimensional asymptotics in both fixed-aspect-ratio and growing-spike regimes, showing uniformity's role in robust signal recovery. Empirically, PCA++ outperforms standard PCA and alignment-only PCA+ on simulations, corrupted-MNIST, and single-cell transcriptomics, reliably recovering condition-invariant structure. More broadly, we clarify uniformity's role in contrastive learning, showing that explicit feature dispersion defends against structured noise and enhances robustness.",0,arxiv,Ä°statistik,CC-BY/arXiv,PCA++: How Uniformity Induces Robustness to Background Noise in Contrastive Learning
"Incomplete multi-view unsupervised feature selection (IMUFS), which aims to identify representative features from unlabeled multi-view data containing missing values, has received growing attention in recent years. Despite their promising performance, existing methods face three key challenges: 1) by focusing solely on the view-missing problem, they are not well-suited to the more prevalent mixed-missing scenario in practice, where some samples lack entire views or only partial features within views; 2) insufficient utilization of consistency and diversity across views limits the effectiveness of feature selection; and 3) the lack of theoretical analysis makes it unclear how feature selection and data imputation interact during the joint learning process. Being aware of these, we propose CLIM-FS, a novel IMUFS method designed to address the mixed-missing problem. Specifically, we integrate the imputation of both missing views and variables into a feature selection model based on nonnegative orthogonal matrix factorization, enabling the joint learning of feature selection and adaptive data imputation. Furthermore, we fully leverage consensus cluster structure and cross-view local geometrical structure to enhance the synergistic learning process. We also provide a theoretical analysis to clarify the underlying collaborative mechanism of CLIM-FS. Experimental results on eight real-world multi-view datasets demonstrate that CLIM-FS outperforms state-of-the-art methods.",0,arxiv,Ä°statistik,CC-BY/arXiv,Cross-view Joint Learning for Mixed-Missing Multi-view Unsupervised Feature Selection
"This paper proposes a novel Bayesian framework for solving Poisson inverse problems by devising a Monte Carlo sampling algorithm which accounts for the underlying non-Euclidean geometry. To address the challenges posed by the Poisson likelihood -- such as non-Lipschitz gradients and positivity constraints -- we derive a Bayesian model which leverages exact and asymptotically exact data augmentations. In particular, the augmented model incorporates two sets of splitting variables both derived through a Bregman divergence based on the Burg entropy. Interestingly the resulting augmented posterior distribution is characterized by conditional distributions which benefit from natural conjugacy properties and preserve the intrinsic geometry of the latent and splitting variables. This allows for efficient sampling via Gibbs steps, which can be performed explicitly for all conditionals, except the one incorporating the regularization potential. For this latter, we resort to a Hessian Riemannian Langevin Monte Carlo (HRLMC) algorithm which is well suited to handle priors with explicit or easily computable score functions. By operating on a mirror manifold, this Langevin step ensures that the sampling satisfies the positivity constraints and more accurately reflects the underlying problem structure. Performance results obtained on denoising, deblurring, and positron emission tomography (PET) experiments demonstrate that the method achieves competitive performance in terms of reconstruction quality compared to optimization- and sampling-based approaches.",0,arxiv,Ä°statistik,CC-BY/arXiv,Bregman geometry-aware split Gibbs sampling for Bayesian Poisson inverse problems
"Coral bleaching is a major concern for marine ecosystems; more than half of the world's coral reefs have either bleached or died over the past three decades. Increasing sea surface temperatures, along with various spatiotemporal environmental factors, are considered the primary reasons behind coral bleaching. The statistical and machine learning communities have focused on multiple aspects of the environment in detail. However, the literature on various stochastic modeling approaches for assessing coral bleaching is extremely scarce. Data-driven strategies are crucial for effective reef management, and this review article provides an overview of existing statistical and machine learning methods for assessing coral bleaching. Statistical frameworks, including simple regression models, generalized linear models, generalized additive models, Bayesian regression models, spatiotemporal models, and resilience indicators, such as Fisher's Information and Variance Index, are commonly used to explore how different environmental stressors influence coral bleaching. On the other hand, machine learning methods, including random forests, decision trees, support vector machines, and spatial operators, are more popular for detecting nonlinear relationships, analyzing high-dimensional data, and allowing integration of heterogeneous data from diverse sources. In addition to summarizing these models, we also discuss potential data-driven future research directions, with a focus on constructing statistical and machine learning models in specific contexts related to coral bleaching.",0,arxiv,Ä°statistik,CC-BY/arXiv,A Review of Statistical and Machine Learning Approaches for Coral Bleaching Assessment
"Contrastive learning has emerged as a cornerstone of unsupervised representation learning across vision, language, and graph domains, with InfoNCE as its dominant objective. Despite its empirical success, the theoretical underpinnings of InfoNCE remain limited. In this work, we introduce an explicit feature space to model augmented views of samples and a transition probability matrix to capture data augmentation dynamics. We demonstrate that InfoNCE optimizes the probability of two views sharing the same source toward a constant target defined by this matrix, naturally inducing feature clustering in the representation space. Leveraging this insight, we propose Scaled Convergence InfoNCE (SC-InfoNCE), a novel loss function that introduces a tunable convergence target to flexibly control feature similarity alignment. By scaling the target matrix, SC-InfoNCE enables flexible control over feature similarity alignment, allowing the training objective to better match the statistical properties of downstream data. Experiments on benchmark datasets, including image, graph, and text tasks, show that SC-InfoNCE consistently achieves strong and reliable performance across diverse domains.",0,arxiv,Ä°statistik,CC-BY/arXiv,Understanding InfoNCE: Transition Probability Matrix Induced Feature Clustering
"Modeling normal behavior in dynamic, nonlinear time series data is challenging for effective anomaly detection. Traditional methods, such as nearest neighbor and clustering approaches, often depend on rigid assumptions, such as a predefined number of reliable neighbors or clusters, which frequently break down in complex temporal scenarios. To address these limitations, we introduce the Granular-ball One-Class Network (GBOC), a novel approach based on a data-adaptive representation called Granular-ball Vector Data Description (GVDD). GVDD partitions the latent space into compact, high-density regions represented by granular-balls, which are generated through a density-guided hierarchical splitting process and refined by removing noisy structures. Each granular-ball serves as a prototype for local normal behavior, naturally positioning itself between individual instances and clusters while preserving the local topological structure of the sample set. During training, GBOC improves the compactness of representations by aligning samples with their nearest granular-ball centers. During inference, anomaly scores are computed based on the distance to the nearest granular-ball. By focusing on dense, high-quality regions and significantly reducing the number of prototypes, GBOC delivers both robustness and efficiency in anomaly detection. Extensive experiments validate the effectiveness and superiority of the proposed method, highlighting its ability to handle the challenges of time series anomaly detection.",0,arxiv,Ä°statistik,CC-BY/arXiv,Finding Time Series Anomalies using Granular-ball Vector Data Description
"Stock trading strategies play a critical role in investment. However, it is challenging to design a profitable strategy in a complex and dynamic stock market. In this paper, we propose an ensemble strategy that employs deep reinforcement schemes to learn a stock trading strategy by maximizing investment return. We train a deep reinforcement learning agent and obtain an ensemble trading strategy using three actor-critic based algorithms: Proximal Policy Optimization (PPO), Advantage Actor Critic (A2C), and Deep Deterministic Policy Gradient (DDPG). The ensemble strategy inherits and integrates the best features of the three algorithms, thereby robustly adjusting to different market situations. In order to avoid the large memory consumption in training networks with continuous action space, we employ a load-on-demand technique for processing very large data. We test our algorithms on the 30 Dow Jones stocks that have adequate liquidity. The performance of the trading agent with different reinforcement learning algorithms is evaluated and compared with both the Dow Jones Industrial Average index and the traditional min-variance portfolio allocation strategy. The proposed deep ensemble strategy is shown to outperform the three individual algorithms and two baselines in terms of the risk-adjusted return measured by the Sharpe ratio. This work is fully open-sourced at \href{https://github.com/AI4Finance-Foundation/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020}{GitHub}.",0,arxiv,Ä°statistik,CC-BY/arXiv,Deep Reinforcement Learning for Automated Stock Trading: An Ensemble Strategy
"Conformal prediction offers a distribution-free framework for constructing prediction sets with finite-sample coverage. Yet, efficiently leveraging multiple conformity scores to reduce prediction set size remains a major open challenge. Instead of selecting a single best score, this work introduces a principled aggregation strategy, COnfidence-Level Allocation (COLA), that optimally allocates confidence levels across multiple conformal prediction sets to minimize empirical set size while maintaining provable coverage. Two variants are further developed, COLA-s and COLA-f, which guarantee finite-sample marginal coverage via sample splitting and full conformalization, respectively. In addition, we develop COLA-l, an individualized allocation strategy that promotes local size efficiency while achieving asymptotic conditional coverage. Extensive experiments on synthetic and real-world datasets demonstrate that COLA achieves considerably smaller prediction sets than state-of-the-art baselines while maintaining valid coverage.",0,arxiv,Ä°statistik,CC-BY/arXiv,Aggregating Conformal Prediction Sets via Î±-Allocation
"We propose the Modified Mahalanobis Distance Conformal Prediction (MMDCP), a unified framework for multi-class classification and outlier detection under label shift, where the training and test distributions may differ. In such settings, many existing methods construct nonconformity scores based on empirical cumulative or density functions combined with data-splitting strategies. However, these approaches are often computationally expensive due to their heavy reliance on resampling procedures and tend to produce overly conservative prediction sets with unstable coverage, especially in small samples. To address these challenges, MMDCP combines class-specific distance measures with full conformal prediction to construct a score function, thereby producing adaptive prediction sets that effectively capture both inlier and outlier structures. Under mild regularity conditions, we establish convergence rates for the resulting sets and provide the first theoretical characterization of the gap between oracle and empirical conformal $p$-values, which ensures valid coverage and effective control of the class-wise false discovery rate (CW-FDR). We further introduce the Summarized Class-Wise FDR (SCW-FDR), a novel global error metric aggregating false discoveries across classes, and show that it can be effectively controlled within the MMDCP framework. Extensive simulations and two real-data applications support our theoretical findings and demonstrate the advantages of the proposed method.",0,arxiv,Ä°statistik,CC-BY/arXiv,MMDCP: A Distribution-free Approach to Outlier Detection and Classification with Coverage Guarantees and SCW-FDR Control
"The Kondo effect, a hallmark of many-body physics, emerges from the antiferromagnetic coupling between localized spins and conduction fermions, leading to a correlated many-body singlet state. Here we propose to use the mixed-dimensional (mixD) bilayer Hubbard geometry as a platform to study Kondo lattice physics with current ultracold atom experiments. At experimentally feasible temperatures, we predict that key features of the Kondo effect can be observed, including formation of the Kondo cloud around a single impurity and the competition of singlet formation with Ruderman-Kittel-Kasuya-Yosida (RKKY) interactions for multiple impurities, summarized in the Doniach phase diagram. Moreover, we show that the mixD platform provides a natural bridge between the Doniach phase diagram of the Kondo lattice model, relevant to heavy-fermion materials, and the phase diagram of cuprate superconductors as described by a single-layer Zhang-Rice type $t$-$J$ model: It is possible to continuously tune between the two regimes by changing the interlayer Kondo coupling. Our findings demonstrate that the direct connection between high-temperature superconductivity and heavy-fermion physics can be experimentally studied using currently available quantum simulation platforms.",0,arxiv,Kuantum,CC-BY/arXiv,Connecting single-layer $t$-$J$ to Kondo lattice models: Exploration with cold atoms
"We study the photon emission by charged spinless particles with phase vortices and an orbital angular momentum (OAM) projection in longitudinal electric and magnetic fields within the scalar QED. A realistic wave packet of an electron or ion accelerated by a radio-frequency wave locally feels a constant and spatially homogeneous field, which allows us to develop an effective model for losing the angular momentum of the vortex particle due to photon emission. For the fields typical for accelerator facilities, we find that an effective lifetime of the vortex state greatly exceeds the acceleration time. This proves that the acceleration of vortex electrons, ions, muons, and so forth to relativistic energies is possible in conventional linacs, as well as in the wake-field accelerators with higher field gradients, the OAM losses due to the photon emission are mostly negligible, and that the vortex quantum state is highly robust against these losses.",0,arxiv,Kuantum,CC-BY/arXiv,Photon emission by vortex particles accelerated in a linac
"We analyze two simple models derived from a quantum-mechanical particle on an elliptical path. The first Hamiltonian operator is non-Hermitian but isomorphic to an Hermitian operator. It appears to exhibit the same two-fold degeneracy as the particle on a circular path. More precisely, $E_n=n^2E_1,\ n=1,2,\ldots$ (in addition to an exact eigenvalue $E_0=0$). The second Hamiltonian operator is Hermitian and does not exhibit such degeneracy. In this case the nth excited energy level splits at the nth order of perturbation theory. Both models can be described in terms of the same point-group symmetry.",0,arxiv,Kuantum,CC-BY/arXiv,Two simple models derived from a quantum-mechanical particle on an elliptical path
"We give an efficient 0.8395-approximation algorithm for the EPR Hamiltonian. Our improvement comes from a new nonlinear monogamy-of-entanglement bound on star graphs and a refined parameterization of a shallow quantum circuit from previous works. We also prove limitations showing that current methods cannot achieve substantially better approximation ratios, indicating that further progress will require fundamentally new techniques.",0,arxiv,Kuantum,CC-BY/arXiv,A 0.8395-approximation algorithm for the EPR problem
"Non-Hermitian Hamiltonians enrich quantum physics by extending conventional phase diagrams, enabling novel topological phenomena, and realizing exceptional points with potential applications in quantum sensing. Here, we present an experimental photonic platform capable of simulating a non-unitary quantum walk generated by a peculiar type of non-Hermitian Hamiltonian, largely unexplored in the literature. The novelty of this platform lies in its direct access to the reciprocal space, which enables us to scan the quasi-momentum across the entire Brillouin zone and thus achieve a precise tomographic reconstruction of the underlying non-Hermitian Hamiltonian, indicated by the comparison between theoretical predictions and experimental measurements. From the inferred Hamiltonian, it is possible to retrieve complex-valued band structures, resolve exceptional points in momentum space, and detect the associated parity-time symmetry breaking through eigenvector coalescence. Our results, presented entirely in quasi-momentum space, represent a substantial shift in perspective in the study of non-Hermitian phenomena.",0,arxiv,Kuantum,CC-BY/arXiv,Tomographic characterization of non-Hermitian Hamiltonians in reciprocal space
"Quantum error mitigation (QEM) is typically viewed as a suite of practical techniques for today's noisy intermediate-scale quantum devices, with limited relevance once fault-tolerant quantum computers become available. In this work, we challenge this conventional wisdom by showing that QEM can continue to provide substantial benefits in the era of quantum error correction (QEC), and in an even more efficient manner than it does on current devices. We introduce a framework for logical-level QEM that leverages soft information naturally produced by QEC decoders, requiring no additional data, hardware modifications, or runtime overhead beyond what QEC protocols already provide. Within this framework, we develop and analyze three logical-level QEM techniques: post-selection and runtime abort policies, probabilistic error cancellation, and zero-noise extrapolation. Our techniques reduce logical error rates by more than 100x while discarding fewer than 0.1% of shots; they also provide in situ characterization of logical channels for QEM protocols. As a proof of principle, we benchmark our approach using a surface-code architecture and two state-of-the-art decoders based on tensor-network contraction and minimum-weight perfect matching. We evaluate logical-level QEM on random Clifford circuits and molecular simulation algorithms and find that, compared to previous approaches relying on QEC only or QEC combined with QEM, we can achieve up to 87.4% spacetime overhead savings. Our results demonstrate that logical-level QEM with QEC decoder soft information can reliably improve logical performance, underscoring the efficiency and usefulness of QEM techniques for fault-tolerant quantum computers.",0,arxiv,Kuantum,CC-BY/arXiv,Error Mitigation of Fault-Tolerant Quantum Circuits with Soft Information
"Random number generation is fundamental for many modern applications including cryptography, simulations and machine learning. Traditional pseudo-random numbers may offer statistical unpredictability, but are ultimately deterministic. On the other hand, True Random Number Generation (TRNG) offers true randomness. One way of obtaining such randomness are quantum systems, including quantum computers. As such the use of quantum computers for TRNG has received considerable attention in recent years. However, existing studies almost exclusively consider IBM quantum computers, often stop at using simulations and usually test only a handful of different TRNG quantum circuits. In this paper, we address those issues by presenting a study of TRNG circuits on Odra 5 a real-life quantum computer installed at WrocÅ‚aw University of Science and Technology. It is also the first study to utilize the IQM superconducting architecture. Since Odra 5 is available on-premises it allows for much more comprehensive study of various TRNG circuits. In particular, we consider 5 types of TRNG circuits with 105 circuit subvariants in total. Each circuit is used to generate 1 million bits. We then perform an analysis of the quality of the obtained random sequences using the NIST SP 800-22 and NIST SP 800-90B test suites. We also provide a comprehensive review of existing literature on quantum computer-based TRNGs.",0,arxiv,Kuantum,CC-BY/arXiv,True Random Number Generators on IQM Spark
"In this work, we present a practical and efficient framework for verifying entangled states when only a tomographically incomplete measurement setting is available-specifically, when access to observables is severely limited. We show how the experimental estimation of a small number of observables can be directly exploited to construct a large family of entanglement witnesses, enabling the efficient identification of entangled states. Moreover, we introduce an optimization approach, formulated as a semidefinite program, that systematically searches for those witnesses best suited to reveal entanglement under the given measurement constraints. We demonstrate the practicality of the approach in a proof-of-principle experiment with photon-polarization qubits, where entanglement is certified using only a fraction of the full measurement data. These results reveal the maximal usefulness of incomplete measurement settings for entanglement verification in realistic scenarios.",0,arxiv,Kuantum,CC-BY/arXiv,Practical and Efficient Verification of Entanglement with Incomplete Measurement Settings
"We demonstrate deterministic preparation of arbitrary two-component product states of fermionic $^6$Li atoms in an 8$\times$8 optical tweezer array, achieving motional ground-state fidelities above 98.5\%. Leveraging the large differential magnetic moments for spin-resolution, with parallelized site- and number-resolved control, our approach addresses key challenges for low-entropy quantum state engineering. Combined with high-fidelity spin-, site-, and density-resolved readout within a single \qty{20}{\us} exposure, and \qty{3}{\s} experimental cycles, these advances establish a fast, scalable, and programmable architecture for fermionic quantum simulation.",0,arxiv,Kuantum,CC-BY/arXiv,Programmable Assembly of Ground State Fermionic Tweezer Arrays
"Transformers have gained popularity in machine learning due to their application in the field of natural language processing. They manipulate and process text efficiently, capturing long-range dependencies among data and performing the next word prediction. On the other hand, gate-based quantum computing is based on controlling the register of qubits in the quantum hardware by applying a sequence of gates, a process which can be interpreted as a low level text programming language. We develop a transformer model capable of transpiling quantum circuits from the qasm standard to other sets of gates native suited for a specific target quantum hardware, in our case the set for the trapped-ion quantum computers of IonQ. The feasibility of a translation up to five qubits is demonstrated with a percentage of correctly transpiled target circuits equal or superior to 99.98%. Regardless the depth of the register and the number of gates applied, we prove that the complexity of the transformer model scales, in the worst case scenario, with a polynomial trend by increasing the depth of the register and the length of the circuit, allowing models with a higher number of parameters to be efficiently trained on HPC infrastructures.",0,arxiv,Kuantum,CC-BY/arXiv,Transpiling quantum circuits by a transformers-based algorithm
"We introduce a quantum algorithm for computing the Ollivier Ricci curvature, a discrete analogue of the Ricci curvature defined via optimal transport on graphs and general metric spaces. This curvature has seen applications ranging from signaling fragility in financial networks to serving as basic quantities in combinatorial quantum gravity. For inputs given as a point cloud with pairwise distances, we show that our algorithm can achieve an exponential speedup over the best-known classical methods for two particular classes of problem. Our work is another step toward quantum algorithms for geometrical problems that are capable of delivering practical value while also informing fundamental theory.",0,arxiv,Kuantum,CC-BY/arXiv,Quantum Algorithm for Estimating Ollivier-Ricci Curvature
"Scaling fault tolerant quantum computers, especially cryogenic systems, to millions of qubits is challenging due to poorly-scaling data processing and power consumption overheads. One key challenge is the design of decoders for real-time quantum error correction (QEC), which demands high data rates for error processing; this is particularly apparent in systems with cryogenic qubits and room temperature (RT) decoders. In response, cryogenic predecoding using lightweight logic has been proposed to handle common, sparse errors in the cryogenic domain. However, prior work only accounts for a subset of error sources present in real-world quantum systems with limited accuracy, often degrading performance below a useful level in practical scenarios. Furthermore, prior reliance on SFQ logic precludes detailed architecture-technology co-optimization.   To address these shortcomings, this paper introduces Pinball, a comprehensive design in cryogenic CMOS of a QEC predecoder tailored to realistic, circuit-level noise. By accounting for error generation and propagation through QEC circuits, our design achieves higher predecoding accuracy, outperforming logical error rates (LER) of the current state-of-the-art cryogenic predecoder by nearly six orders of magnitude. Remarkably, despite operating under much stricter power and area constraints, Pinball also reduces LER by 32.58x and 5x, respectively, compared to the state-of-the-art RT predecoder and RT ensemble configurations. By increasing cryogenic coverage, we also reduce syndrome bandwidth up to 3780.72x. Through co-design with 4 K-characterized 22 nm FDSOI technology, we achieve a peak power consumption under 0.56 mW. Voltage/frequency scaling and body biasing enable 22.2x lower typical power consumption, yielding up to 67.4x total energy savings. Assuming a 4 K power budget of 1.5 W, our predecoder supports up to 2,668 logical qubits at d=21.",0,arxiv,Kuantum,CC-BY/arXiv,Pinball: A Cryogenic Predecoder for Quantum Error Correction Decoding Under Circuit-Level Noise
"We study the problem of certifying local Hamiltonians from real-time access to their dynamics. Given oracle access to $e^{-itH}$ for an unknown $k$-local Hamiltonian $H$ and a fully specified target Hamiltonian $H_0$, the goal is to decide whether $H$ is exactly equal to $H_0$ or differs from $H_0$ by at least $\varepsilon$ in normalized Frobenius norm, while minimizing the total evolution time. We introduce the first intolerant Hamiltonian certification protocol that achieves optimal performance for all constant-locality Hamiltonians. For general $n$-qubit, $k$-local, traceless Hamiltonians, our procedure uses $O(c^k/\varepsilon)$ total evolution time for a universal constant $c$, and succeeds with high probability. In particular, for $O(1)$-local Hamiltonians, the total evolution time becomes $Î˜(1/\varepsilon)$, matching the known $Î©(1/\varepsilon)$ lower bounds and achieving the gold-standard Heisenberg-limit scaling. Prior certification methods either relied on implementing inverse evolution of $H$, required controlled access to $e^{-itH}$, or achieved near-optimal guarantees only in restricted settings such as the Ising case ($k=2$). In contrast, our algorithm requires neither inverse evolution nor controlled operations: it uses only forward real-time dynamics and achieves optimal intolerant certification for all constant-locality Hamiltonians.",0,arxiv,Kuantum,CC-BY/arXiv,Optimal certification of constant-local Hamiltonians
"We investigate theoretically the ability of an optical centrifuge - a laser pulse whose linear polarization is rotating at an accelerated rate, to control molecular rotation in the regime when the rigid-rotor approximation breaks down due to coupling between the vibrational and rotational degrees of freedom. Our analysis demonstrates that the centrifuge field enables controlled excitation of high rotational states while maintaining relatively low spread along the vibrational coordinate. We contrast this to the rotational excitation by a linearly polarized Gaussian pulse of equal spectral width and pulse energy which, although comparable to the centrifuge-induced rotation, is unavoidably accompanied by a substantial broadening of the vibrational wavepacket.",0,arxiv,Kuantum,CC-BY/arXiv,Rotational excitation of molecules in the regime of strong ro-vibrational coupling: Comparison between an optical centrifuge and a transform-limited pulse
"We propose a single auxiliary-assisted purification-based framework for quantum error correction, capable of correcting errors that drive a system from its ground-state subspace into excited-state sectors. The protocol consists of a joint time evolution of the system-auxiliary duo under a specially engineered interaction Hamiltonian, followed by a single measurement of the auxiliary in its energy eigenbasis and a subsequent post-selection of one of the measurement outcomes. We show that the resulting purified state always achieves unit fidelity, while the probability of obtaining any energy of the auxiliary other than its ground state energy yields the success rate of the protocol. We demonstrate the power of this proposed method for several low-distance quantum codes, including the three-, four-, and five-qubit codes, and for the one-dimensional isotropic Heisenberg model, subjected to bit-flip, phase-flip, and amplitude-damping noises acting on all qubits. Notably, the protocol expands the class of correctable errors for a given code, particularly in the presence of amplitude-damping noise. We further analyze the impact of replacing the auxiliary qudit with a single auxiliary qubit, and the changes in the performance of the protocol under the realistic scenario where noise remains active during the correction cycle.",0,arxiv,Kuantum,CC-BY/arXiv,Quantum error correction via purification using a single auxiliary
"The fundamental question of when a static or dynamic system should be deemed intrinsically quantum remains a challenge to address in absolute terms. A rigorous criterion, however, can be established by focusing on the measurable or reconstructible features of the system. This determination transcends mere issues of a system's classical simulability or computational complexity. Instead, the critical requirement lies in the certification (ideally, in real-time) of the emergence and persistence of genuine quantum features, principally entanglement and quantum superposition. Quantum Non-Demolition Measurements (QNDM) serve as the appropriate instrument for this certification, both from a theoretical and experimental standpoint. In this review paper, we demonstrate, with accessible clarity, how the implementation of QNDM can be directly linked to a necessary and sufficient condition for the violation of macrorealism in finite-dimensional systems, establishing a conceptual parallel with Leggett-Garg inequalities. Using concrete examples that detail the detection of negative terms in the quasi-probability density function resulting from QNDM, we introduce the core concepts for certifying genuinely quantum features. As specific examples, we discuss an application where the quantum-to-classical transition due to the interaction with an environment can be tracked by QNDM. Moreover, we argue about the robustness of QNDM protocols in the presence of noise sources and their advantages with respect to the Leggett-Garg inequalities. Because of its straightforward implementation, the QNDM approach can be of direct relevance to both the foundations of quantum mechanics and quantum information theory, where a controlled generation and certification of genuinely quantum resources is a central concern.",0,arxiv,Kuantum,CC-BY/arXiv,Quantumness certification via non-demolition measurements
"The necessity of random numbers for various tasks, from simulation to cryptography, is crucial and immense. Here we demonstrate CV-QRNG using the CV payload of the SPOQC mission. The homodyne setup for QRNG uses the laser from the payload, in addition to potentially being used as detector in the case of an uplink scenario. Here we quantify the extractable secure randomness from the QRNG setup, that involves homodyne measurement of the vacuum states. The extracted randomness is tested against NIST test suite in addition to formally upper bounding the min-entropy. With the raw key length being $\approx1$ Mb in a given satellite pass, we get a total length of $\approx19.5$ Kb of certified random numbers from the 12-bit ADC.",0,arxiv,Kuantum,CC-BY/arXiv,Quantum random number generation from the continuous variable payload for the SPOQC mission
"Photon subtraction and addition are essential non-Gaussian processes in quantum optics, where conventional methods using linear optics and number-resolving detection often suffer from low success probability. Here, we introduce the concept of \textit{dynamic stimulated emission}, whereby a quantum emitter undergoes stimulated emission with a time-dependent coupling. We show that, for both two- and three-level emitters, this process can be used to deterministically add or subtract a photon to a single propagating optical mode. We provide semi-analytic solutions to this problem for Fock states, enabling deterministic and unconditional single-photon subtraction and addition with fidelity ${\cal F}>0.996$. Our semi-analytic solutions are provided for both dynamically coupled two-level systems and for three-level systems whose dynamical coupling is controlled by a coherent laser drive. Moving beyond individual Fock states, we further showcase the ability to subtract and add single photons to photon-number superposition states. We show that SchrÃ¶dinger cat states can be prepared from squeezed vacuum input via cascaded subtraction or cascaded addition. Finally, we show that our photon-addition process can be used to add a photon to any squeezed and displaced state with high success probability and fidelity ${\cal F}>0.99$, thereby potentially converting quantum emitters from single-photon sources to sources of single-photon-added Gaussian states without the need for inline squeezing. Our protocols provide a path towards integrating quantum emitters to construct efficient sources of single-mode non-Gaussian light beyond single photons.",0,arxiv,Kuantum,CC-BY/arXiv,Dynamic stimulated emission for deterministic addition and subtraction of propagating photons
"Device-independent quantum secret sharing (DI-QSS) is a cryptographic protocol that overcomes the security limitations posed by untrusted quantum devices. We propose a DI-QSS protocol based on the multipartite pseudo-telepathy parity game, which achieves device-independence with simultaneous key generation without requiring dedicated test rounds, unlike CHSH-based schemes [Zhang et al., Phys. Rev. A, 2024]. Notably, the proposed scheme allows simultaneous device-independence verification and key-generation phases, achieving optimal performance for a seven-qubit GHZ state configuration. Further, we analyse the security of our protocol against collective attack and establish reduced resource requirement for the same length of the raw key compared to the previous protocol. Finally, we show that our protocol remains robust even in a noisy environment.",0,arxiv,Kuantum,CC-BY/arXiv,Device Independent Quantum Secret Sharing Using Multiparty Pseudo-telepathy Game
"Three-body interactions are fundamental for realizing novel quantum phenomena beyond pairwise physics, yet their implementation -- particularly among distinct quantum systems -- remains challenging. Here, we propose a hybrid quantum architecture comprising a magnonic mode (in a YIG sphere), an Andreev spin qubit (ASQ), and a superconducting qubit (SCQ), to realize a strong three-body interaction at the single-quantum level. Leveraging the spin-dependent supercurrent and circuit-integration flexibility of the ASQ, it is possible to engineer a strong tripartite coupling that jointly excites both qubits upon magnon annihilation (or excites magnons and SCQs upon ASQ deexcitation). Through analytical and numerical studies, we demonstrate that this interaction induces synchronized collapse and revival in qubit populations when the magnon is initially prepared in a coherent state. Notably, during the collapse region -- where populations remain static -- the entanglement structure undergoes a dramatic and continuous reorganization. We show that the genuine tripartite entanglement is redistributed into bipartite entanglement between the two qubits, and vice versa, with the total entanglement conserved. These phenomena, unattainable via two-body couplings, underscore the potential of three-body interactions for exploring intrinsically new quantum effects and advancing hybrid quantum information platforms.",0,arxiv,Kuantum,CC-BY/arXiv,Three-body interaction in a magnon-Andreev-superconducting qubit system: collapse-revival phenomena and entanglement redistribution
I propose a new quantum key distribution protocol that uses the five qubit error correction code to detect the presence of eavesdropper reliably. The protocol turns any information theoretical attacks into a classical guess about the pattern. The logical qubit is encoded with a specific pattern into a block of five physical qubits. The security of the protocol relies on the correct pattern choice of Alice and Bob. Decoding with any wrong pattern choice increases multi qubit error rate and the 5 qubit code transforms an eavesdropper's logical disturbance into a signature that is detectable and distinguishable from natural channel noise up to a certain distance.,0,arxiv,Kuantum,CC-BY/arXiv,Pattern Based Quantum Key Distribution using the five qubit perfect code for eavesdropper detection
"We study the possibility of detection of ``spin-boson'' entanglement by qubit only measurements. Such entanglement is impossible to detect by previously proposed schemes that involve a fixed system-environment interaction, because of inherent symmetries within the coupling and the initial state of the environment. We take advantage of the possibility of tuning of qubit-environment coupling, that is available in some qubit realizations. As an example we study a superconducting transmon qubit interacting with a microwave cavity, which is one of such systems and is, furthermore, essential in the context of quantum information processing. We propose suitable Hamiltonian parameters for the preparation and measurement phases of the detection scheme that allow for an experimental test, and verify that the reported signal is nonnegligibly large still at finite temperatures.",0,arxiv,Kuantum,CC-BY/arXiv,Entanglement with a mode observable via a tunable interaction with a qubit
"Distributed, large-scale quantum computing will need architectures that combine matter-based qubits with photonic links, but today's software stacks target either gate-based chips or linear-optical devices in isolation. We introduce Optyx, an open-source Python framework offering a unified language to program, simulate, and prototype hybrid, networked systems: users create experiments that mix qubit registers, discrete-variable photonic modes, lossy channels, heralded measurements, and real-time feedback; Optyx compiles them via ZX/ZW calculus into optimised tensor-network forms, and executes with state-of-the-art contraction schedulers based on Quimb and Cotengra. Benchmarking on exact multi-photon circuit simulations shows that, versus permanent-based methods, tensor network contraction can deliver speedups of orders of magnitude for low-depth circuits and entangled photon sources, and natively supports loss and distinguishability -- establishing it as both a high-performance simulator and a rapid-prototyping environment for next-generation photonic-network experiments.",0,arxiv,Kuantum,CC-BY/arXiv,Optyx: A ZX-based Python library for networked quantum architectures
We investigate massless representations related to the extension of PoincarÃ¨ group constructed in [1]. These representations differ from Wigner's ones of standard PoincarÃ¨ group because the stabilizer of lightlike orbits has extra degrees of freedom. The unitary irreducible representations (UIRs) of massless particles in this extension must decompose as a direct sum of a massless forward (positive zeroth component momentum) and massless backward (negative zeroth component momentum) Wigner's representations linked by internal two valued degree of freedom. We prove that these representations are unitarily equivalent to entangled states of two qubits. This provides a geometric origin of quantum entanglement for photons in the framework of quantum field theory: photons appear as superpositions of backward and forward propagating electromagnetic waves depending on a two valued parameter and this dependency gives rise to correlations between the values of local observables identical to those experienced with an entangled state of two qubits. Finally we describe an experiment capable of distinguishing the two different values of the parameter that links backward and forward massless representations providing experimental falsification of the theory.,0,arxiv,Kuantum,CC-BY/arXiv,Geometric Origin of Quantum Entanglement
"In this study, we propose the Quantum Gradient Flow Algorithm (QGFA), a novel quantum algorithm for solving symmetric positive definite (SPD) linear systems based on the variational formulation and time-evolution dynamics. Conventional quantum linear solvers, such as the quantum matrix inverse algorithm (QMIA), focus on approximating the matrix inverse through quantum signal processing (QSP). However, QMIA suffers from a crucial drawback: its computational efficiency deteriorates as the condition number increases. In contrast, classical SPD linear solvers, such as the steepest descent and conjugate gradient methods, are known for their fast convergence, which stems from the variational optimization principle of SPD systems. Inspired by this, we develop QGFA, which obtains the solution vector through the gradient-flow process of the corresponding quadratic energy functional. To validate the proposed method, we apply QGFA to the displacement-based finite element method (FEM) for two-dimensional linear elastic problems under plane stress conditions. The algorithm demonstrates accurate convergence toward classical FEM solutions even with a moderate number of QSP phase factors. Compared with QMIA, QGFA achieves lower relative errors and faster convergence when initialized with suitable initial states, demonstrating its potential as an efficient preconditioned quantum linear solver. The proposed framework provides a physically interpretable connection between classical iterative solvers and quantum computational paradigms. These findings suggest that QGFA can serve as a foundation for future developments in Quantum Computer-Aided Engineering (Quantum CAE), including nonlinear and multiphysics simulations.",0,arxiv,Kuantum,CC-BY/arXiv,Quantum Gradient Flow Algorithm for Symmetric Positive Definite Systems via Quantum Eigenvalue Transformation: Towards Quantum CAE
"Many-body localization (MBL) is understood theoretically through the existence of an extensive number of local integrals of motion (LIOMs). These conserved quantities are related to the microscopic quantum degrees of freedom that are spatially localized. Here, we present a general framework for constructing exact LIOMs with the desired locality and quantum numbers supplied as input rather than arising as emergent properties. We show that one can express the task of finding LIOMs as an optimization problem. In simple cases, solving this problem amounts to matrix diagonalization, while in more complex settings, it connects to the question of finding classical ground states of spin-glass models. We illustrate our theory using paradigmatic examples of single-particle Anderson localization and MBL in interacting spin chains. These developments unify previous results and reveal intriguing connections among many-body localization, spin-glass physics and constrained optimization problems.",0,arxiv,Kuantum,CC-BY/arXiv,Unified theory of local integrals of motion
"Exceptional points are singularities in the spectrum of non-Hermitian systems in which several eigenvectors are linearly dependent and their eigenvalues are equal to each other. Usually it is assumed that the order of the exceptional point is limited by the number of degrees of freedom of a non-Hermitian system. In this letter, we refute this common opinion and show that non-Markovian effects can lead to dynamics characteristic of systems with exceptional points of higher orders than the number of degrees of freedom in the system. This takes place when the energy returns from reservoir to the system such that the dynamics of the system are divided into intervals in which it describes by the product of the exponential and a polynomial function of ever-increasing order. We demonstrate that by choosing the observation time, it is possible to observe exceptional points of arbitrary high orders.",0,arxiv,Kuantum,CC-BY/arXiv,Exceptional points of arbitrary high orders induced by non-Markovian dynamics
"Uncertainty relations are fundamental to quantum mechanics, encoding limits on the simultaneous measurement of conjugate observables. Violations of joint uncertainty bounds can certify entanglement -- a resource critical for quantum information protocols and increasingly relevant in strong-field physics. Here, we investigate the pairwise time-delay and frequency-bandwidth uncertainties for arbitrary multimode quantum states of light, deriving a general lower bound for their joint product. We find that the nonclassical correction scales inversely with the average photon number, a behavior rooted in the so-called ``monogamy of entanglement''. These results clarify the intensity scaling of quantum advantages in nonclassical light states and highlight the interplay between entanglement and photon statistics.",0,arxiv,Kuantum,CC-BY/arXiv,Can Intense Quantum Light Beat Classical Uncertainty Relations?
"Witnessing and tracking topological phase transitions induced by interactions with the environment is a crucial challenge. Among the various experimental approaches to detect topological properties, the Mean Chiral Displacement (MCD) has emerged as a powerful bulk probe in one-dimensional chiral systems, allowing the extraction of the topological invariant from single-particle dynamics. Here we study the dynamics of a single particle in a one-dimensional Su-Schrieffer-Heeger chain coupled to multiple cavity modes via inter-cell hopping terms, focusing on the out-of-equilibrium behavior of the MCD. We show that, whenever the frequency is larger than the static hopping amplitudes, the coupling induces a discontinuous jump in the MCD, already at small times, signaling that such a coupling also leaves a signature in the survival edge probability when the dynamics are initialized at one of the two edges. For frequencies comparable to the static hopping amplitudes, topological order competes with dissipative effects, which makes the MCD behave smoothly, retaining information about the driven-dissipative topology.",0,arxiv,Kuantum,CC-BY/arXiv,Single particle dynamical signature of topology induced by single mode cavities in Su-Schrieffer-Heeger chain
"Accurate knowledge of the uneven free spectral range of an optical microresonator, which provides direct insight into group velocity dispersion, is essential for understanding and controlling Kerr frequency comb dynamics. In this work, we present a simple and highly precise method formeasuring the free spectral range over a 5 THz bandwidth in silicon nitride microresonators, leveraging a wavemeter with 0.4 MHz resolution. Our fully fibered plug-and-play experimental setup enables the accurate extraction of resonance frequencies. By carefully analyzing the spectral position of each resonance, we measure both second- and third-order free spectral range expansion coefficients. This approach offers a robust and accessible tool for dispersion characterization in integrated photonic circuits, paving the way for next-generation of Kerr comb sources and quantum photonic technologies.",0,arxiv,Kuantum,CC-BY/arXiv,High-resolution broadband characterization of resonance dispersion in an optical microresonator
"Quantum neural networks (QNNs) and parameterized quantum circuits (PQCs) are key building blocks for near-term quantum machine learning. However, their scalability is constrained by excessive parameters, barren plateaus, and hardware limitations. We propose LiePrune, the first mathematically grounded one-shot structured pruning framework for QNNs that leverages Lie group structure and quantum geometric information. Each gate is jointly represented in a Lie group--Lie algebra dual space and a quantum geometric feature space, enabling principled redundancy detection and aggressive compression. Experiments on quantum classification (MNIST, FashionMNIST), quantum generative modeling (Bars-and-Stripes), and quantum chemistry (LiH VQE) show that LiePrune achieves over $10\times$ compression with negligible or even improved task performance, while providing provable guarantees on redundancy detection, functional approximation, and computational complexity.",0,arxiv,Kuantum,CC-BY/arXiv,LiePrune: Lie Group and Quantum Geometric Dual Representation for One-Shot Structured Pruning of Quantum Neural Networks
"When light propagates through complex media, its output spatial distribution is highly sensitive to its wavelength. This fundamentally limits the bandwidth of applications ranging from imaging to communication. Here, we demonstrate analytically and numerically that the spatial correlations of hyper-entangled photon pairs, simultaneously entangled spatially and spectrally, remain stable across a broad bandwidth: The chromatic modal dispersion experienced by one photon is canceled to first order by its spectrally anti-correlated twin, defining a ""two-photon bandwidth"" that can far exceed its classical counterpart. We illustrate this modal dispersion cancellation in multimode fibers, thin diffusers and blazed gratings, and demonstrate its utility for broadband wavefront shaping of quantum states. These findings advance our fundamental understanding of quantum light in complex media with applications in quantum imaging, communication, and sensing.",0,arxiv,Kuantum,CC-BY/arXiv,Two-Photon Bandwidth of Hyper-Entangled Photons in Complex Media
"Quantum frequency converters are key enabling technologies in photonic quantum information science to bridge the gap between quantum emitters and telecom photons. Here, we report a co- herent frequency converter scheme combining a fiber-coupled nonlinear optical Lithium Niobate waveguide with a fiber-pigtailed single-photon source based on semiconductor quantum dots. Single and indistinguishable photons are converted from 925.7 nm to the telecommunication C-band, with a 48.4% end-to-end efficiency and full preservation of single-photon purity and indistinguishability. The integration of the two fiber-based modules achieving top-level performance represents an im- portant step toward the practical interconnection of future quantum information processing systems operating at different wavelengths.",0,arxiv,Kuantum,CC-BY/arXiv,Compact and efficient quantum frequency conversion of a fiber-pigtailed single-photon source
"Analyzing routes of transport for open quantum systems with non-equilibrium initial conditions is extremely challenging. The state-to-state approach [A. Bose, and P.L. Walters, J. Chem. Theory Comput. 2023, 19, 15, 4828-4836] has proven to be a useful method for understanding transport mechanisms in quantum systems interacting with dissipative thermal baths, and has been recently extended to non-Hermitian systems to account for empirical loss. These non-Hermitian descriptions are, however, not capable of describing empirical processes of more general nature, including but not limited to a variety of pumping processes. We extend the state-to-state analysis to account for Lindbladian descriptions of generic dissipative, pumping and decohering processes acting on a system which is exchanging energy with a thermal bath. This Lindblad state-to-state method can elucidate routes of transport in systems coupled to a bath and additionally acted upon by Lindblad jump operators. The method is demonstrated using examples of excitonic aggregates subject to incoherent pumping and draining processes. Using this new state-to-state formalism, we demonstrate the establishment of steady-state excitonic currents across molecular aggregates, yielding a different first-principles approach to quantifying the same.",0,arxiv,Kuantum,CC-BY/arXiv,Routes of Transport in the Path Integral Lindblad Dynamics through State-to-State Analysis
"The Mpemba effect (MpE), where a far-from-equilibrium state of a system relaxes faster compared to a state closer to it, is a well-known counterintuitive phenomenon in classical and quantum systems. Various system-specific theories have been proposed to explain this anomalous behavior in driven systems, though the fundamental mechanism of MpE in undriven systems, where MpE was first observed, remains unresolved. This paper provides a generic model of MpE for a quantum system following Markovian relaxation dynamics, regardless of system structure or environment. The key lies in the overlap of initial states with the fast relaxation mode; here, the constituents create a fast decay mode via interaction through the shared environment to show MpE, indicating MpE happens due to the collective behavior of the system. I also show that a system with anisotropic relaxation naturally exhibits MpE, even without a shared environment among the particles.",0,arxiv,Kuantum,CC-BY/arXiv,Mpemba as an Emergent Effect of System Relaxation
"We theoretically propose an experimentally viable scheme to explore the transfer of nonclassical correlations from a dipolar Bose-Einstein condensate (BEC) to a pair of impurities immersed in it. Operating at ultra-low temperature, density fluctuations of the dipolar BEC emulate a vacuum field with Lorentz-violating dispersion, while the two impurities function as Unruh-DeWitt detectors for the BEC quasiparticles. We study the harvesting of entanglement from the quantum vacuum of this analogue Lorentz-violating quantum field by spatially separated Unruh-DeWitt detectors. Our analysis reveals key parameter dependencies that optimize the harvesting of entanglement. In particular, unlike the Lorentz-invariant case, smoother detector switchings does not enhance the entanglement harvesting efficiency from the Lorentz-violating quantum field vacuum. Moreover, the strength of the Lorentz-invariant violation can shift the optimal energy structure of the detectors for harvesting entanglement from the Lorentz-violating quantum field vacuum-a clear deviation from the Lorentz-invariant scenario. As a fundamental quantum mechanical setup, our quantum fluid platform provides an experimentally realizable testbed for examining the entanglement harvesting protocol from an effective Lorentz-violating quantum field vacuum using a pair of impurity probers, which may also has potential implications for exploring the Lorentz-invariant violation in quantum field theory.",0,arxiv,Kuantum,CC-BY/arXiv,Harvesting entanglement from the Lorentz-violating quantum field vacuum in a dipolar Bose-Einstein condensate
"We examine a mechanism of spontaneous decoherence in which the generator of quantum dynamics is replaced by the imaginary-order spectral deformation $H^{1+iÎ²}$ of a positive Hamiltonian $H$. The deformation modifies dynamical phases through the factor $E^{iÎ²} = e^{iÎ²\log E}$, whose rapid oscillation suppresses interference between distinct energies. A non-stationary-phase analysis yields quantitative estimates showing that oscillatory contributions to amplitudes or decoherence functionals decay at least as $O(1/|Î²|)$. The Born rule and the Hilbert-space inner product remain unchanged; the modification is entirely dynamical.   The physical motivation for the deformation arises from clock imperfections, renormalization-group and effective-action corrections that introduce logarithmic spectral terms, and semiclassical quantum-gravity analyses in which complex actions produce spectral factors of the form $E^{iÎ²}$. Examples including FRW minisuperspace, quartic potentials, curved-background Hamiltonians, and a Schwarzschild interior-type model illustrate how the mechanism yields explicit decoherence rates. The parameter $Î²$ may be experimentally constrained through precision coherence measurements in low-noise quantum platforms. The mechanism contrasts with Milburn-type intrinsic decoherence, Diosi-Penrose gravitational collapse, and real-order fractional dynamics in that it acts purely through deterministic spectral phases of a single Hamiltonian. The analysis positions the framework as a compact and testable phenomenological representation of logarithmic spectral corrections appearing in quantum-gravity-motivated effective theories.",0,arxiv,Kuantum,CC-BY/arXiv,Spontaneous Decoherence from Imaginary-Order Spectral Deformations
"The nitrogen-vacancy (NV) center can serve as a magnetic sensor for electron paramagnetic resonance (EPR) measurements. Benefiting from its atomic size, the diamond chip can integrate a tremendous amount of NV centers to improve the magnetic-field sensitivity. However, EPR spectroscopy using NV ensembles is less efficient due to inhomogeneities in both sensors and targets. Spectral line broadening induced by ensemble averaging is even detrimental to spectroscopy. Here we show a kind of cross-relaxation EPR spectroscopy at zero field, where the sensor is tuned by an amplitude-modulated control field to match the target. The modulation makes detection robust to the sensor's inhomogeneity, while zero-field EPR is naturally robust to the target's inhomogeneity. We demonstrate an efficient EPR measurement on an ensemble of roughly 30000 NV centers. Our method shows the ability to not only acquire unambiguous EPR spectra of free radicals, but also monitor their spectroscopic dynamics in real time.",0,arxiv,Kuantum,CC-BY/arXiv,Parallel accelerated electron paramagnetic resonance spectroscopy using diamond sensors
"Stabilizer-based simulation of quantum error-correcting codes typically relies on the Pauli-twirling approximation (PTA) to render non-Clifford noise classically tractable, but PTA can distort the behavior of physically relevant channels such as thermal relaxation. Physically accurate noise simulation is needed to train decoders and understand the noise suppression capabilities of quantum error correction codes. In this work, we develop an exact and stabilizer-compatible model of qubit thermal relaxation noise and show that the combined amplitude damping and dephasing channel admits a fully positive probability decomposition into Clifford operations and reset whenever $T_2 \leqslant T_1$. For $T_2 > T_1$, the resulting decomposition is negative, but allows a smaller sampling overhead versus independent channels. We further introduce an approximated error channel with reset that removes the negativity of the decomposition while achieving higher channel fidelity to the true thermal relaxation than PTA, and extend our construction to finite temperature relaxation. We apply the exact combined model to investigate large surface codes and bivariate bicycle codes on superconducting platforms with realistic thermal relaxation error. The differing logical performances across code states further indicate that noise-model-informed decoders will be essential for accurately capturing thermal-noise structure in future fault-tolerant architectures.",0,arxiv,Kuantum,CC-BY/arXiv,Exact and Efficient Stabilizer Simulation of Thermal-Relaxation Noise for Quantum Error Correction
"In this work, we investigate the interaction between a uniformly accelerated single qubit and a fermionic spinor field. Here we consider both the massless and the massive fermionic spinor fields. The qubit-field interaction occurs over a finite time and was evolved via perturbation theory. This approach yields the transition probability rates, from which we subsequently evaluate the quantum coherence of an Unruh-DeWitt (UDW) detector initially prepared in a qubit state. Our findings reveal that the UDW detector responds more when coupled with the fermionic field, and consequently, quantum coherence (for the fermionic case) degrades much more rapidly when compared to the case of the qubit linearly coupled with the scalar field. Moreover, the analysis suggests that particle mass plays a protective role against Unruh-induced decoherence as the rest mass energy becomes comparable to the detector's energy-level spacing, the detector's excitation probability and response decreases, which leads to the mitigation of quantum coherence degradation in accelerated quantum systems.",0,arxiv,Kuantum,CC-BY/arXiv,Transition rates and their applications in accelerated single-qubit for fermionic spinor field coupling
"Spin squeezing is a powerful resource for quantum metrology, and recent hardware platforms based on interacting qubits provide multiple possible architectures to generate and reverse squeezing during a sensing protocol. In this work, we compare the sensing performance of three such architectures: quantum reservoir computers (QRCs), quantum perceptrons, and multi-layer quantum neural networks (QNNs), when used as squeezing-based field sensors. For all models, we consider a standard metrological sequence consisting of coherent-spin preparation, one-axis-twisting dynamics, field encoding via a weak rotation, time-reversal, and collective readout. We show that a single quantum perceptron generates the same optimal sensitivity as a QRC, but in the perturbative regime it benefits from accelerated squeezing due to steering by the output qubit. Stacking perceptrons into a QNN further amplifies this effect: in a 2-layer QNN with N_in input and N_out output qubits, the optimal squeezing time is reduced by a factor of N_out, while the achievable phase sensitivity remains Heisenberg-limited, Delta phi ~ 1/(N_in + N_out). Moreover, if the layers are used sequentially, first using the outputs to squeeze the inputs and then the inputs to squeeze the outputs, the two contributions to the response add constructively. This yields a sqrt(2) enhancement in sensitivity over a QRC when N_in = N_out and requires shorter total squeezing time. Generalizing to L layers, we show that the metrological gain scales as sqrt(L) while the required squeezing time decreases as 1/N_l, where N_l is the number of qubits per layer. Our results demonstrate that the structure of quantum neural networks can be exploited not only for computation, but also to engineer faster and more sensitive squeezing-based quantum sensors.",0,arxiv,Kuantum,CC-BY/arXiv,Enhanced Squeezing and Faster Metrology from Layered Quantum Neural Networks
"In two previous papers the author described ``Islands of Instability"" that may appear in wavefunction models with nonlinear evolution (of a type proposed originally in the context of the Measurement Problem). Such ``IsoI"" represent a new scenario for Hamiltonian systems implying so-called ``chaos"". Criteria was derived for, and shown to be fulfilled in, some finite-dimensional (multi-qubit) models, and generalized in the second paper to continuum models. But the only example produced of the latter was a model whose linear Schrodinger equation was exactly-solvable. As exact solutions of many-body problems are rare, here I show that the instability criteria can be verified by plugging test-functions into certain computable expressions, bypassing the solvability blockade. The method can accommodate realistic inter-molecular potentials and so may be relevant to instabilities in fluids and gasses.",0,arxiv,Kuantum,CC-BY/arXiv,"Islands of Instability in Nonlinear Wavefunction Models in the Continuum: A Different Route to ""Chaos"""
"Building on the recent proposal that a single ``bona fide'' clock suffices to define spacetime's metric, we introduce an Entangled Clock protocol based on singlet-state correlations. Invoking Zeilinger's Foundational Principle, we argue that while the local flow of time, operationally defined as a sequence of detector ``ticks,'' is irreducibly random (one bit per elementary system), the synchronized flow between spatially separated observers depends on their measurement geometry. Comparing the quantum prediction for the coincidence rate with Peres' classical ``bomb fragment'' model, we find that at obtuse relative angles the entangled clock exhibits a 13 percent higher synchronized tick rate than this linear classical benchmark. This ``temporal acceleration'' is linked to contextuality: following Peres, ``unperformed experiments have no results,'' and quantum systems are not constrained to maintain consistency with all counterfactual measurement settings. We stress, however, that for any single measurement angle a suitably tailored classical model can reproduce the quantum rate. The genuinely nonclassical character of the entangled clock emerges only when correlations at several angles are considered simultaneously and are shown to violate Bell-type inequalities. In this sense, the violation of Bell-type bounds serves as a certification that the shared time standard is genuinely quantum.",0,arxiv,Kuantum,CC-BY/arXiv,"Quantum Clocks Tick Faster: Entanglement, Contextuality, and the Flow of Time"
"When an ensemble of quantum emitters interacts with a common radiation field, their emission becomes collective, giving rise to superradiant and subradiant states, characterized by broadened and narrowed linewidths. In this work, we propose to harness subradiant states for quantum metrology; such states naturally arise in subwavelength-spaced atomic arrays in free space and in small ensembles of emitters coupled to one-dimensional waveguides. We demonstrate that their collective optical response yields sharp, narrow features in the transmittance spectrum, which can be used to enhance sensitivity to external perturbations. This improved sensitivity can be applied to atomic clock operation, spatially resolved imaging of emitter positions, and enables precise detection of both global and spatially varying detunings (such as those induced by electromagnetic fields or gravitational gradients).",0,arxiv,Kuantum,CC-BY/arXiv,Subradiant collective states for precision sensing via transmission spectra
"Quantum point contacts (QPC) are a key instrument in investigating the physics of edge excitations in the quantum Hall effect. However, at not-so-high bias voltage values, the predictions of the conventional point QPC model often deviate from the experimental data both in the integer and (more prominently) in the fractional quantum Hall regime. One of the possible explanations for such behaviors is the dependence of the tunneling between the edges on energy, an effect not present in the conventional model. Here we introduce two models that take QPC spatial extension into account: wide-QPC model that accounts for the distance along which the edges are in contact; long-QPC model accounts for the fact that the tunneling amplitude originates from a finite bulk gap and a finite distance between the two edges. We investigate the predictions of these two models in the integer quantum Hall regime for the energy dependence of the tunneling amplitude. We find that these two models predict opposite dependences: the amplitude decreasing or increasing away from the Fermi level. We thus elucidate the effect of the QPC geometry on the energy dependence of the tunneling amplitude and investigate its implications for transport observables.",0,arxiv,Kuantum,CC-BY/arXiv,On modeling quantum point contacts in quantum Hall systems
"In quantum science applications, ranging from many-body physics to quantum metrology, dipolar interactions in spin ensembles are controlled via Floquet engineering. However, this technique typically reduces the interaction strength between spins, and effectively weakens the coupling to a target sensing field, limiting the metrological sensitivity. In this work, we develop and demonstrate a method for direct tuning of the native interaction in an ensemble of nitrogen-vacancy (NV) centers in diamond. Our approach utilizes dressed-state qubit encoding under a magnetic field perpendicular to the crystal lattice orientation. This method leads to a $3.2\times$ enhancement of the dimensionless coherence parameter $JT_2$ compared to state-of-the-art Floquet engineering, and a $2.6\times$ ($8.3~$dB) enhanced sensitivity in AC magnetometry. Utilizing the extended coherence we experimentally probe spin transport at intermediate to late times. Our results provide a powerful Hamiltonian engineering tool for future studies with NV ensembles and other interacting higher-spin ($S>\frac{1}{2}$) systems.",0,arxiv,Kuantum,CC-BY/arXiv,Dressed-state Hamiltonian engineering in a strongly interacting solid-state spin ensemble
"We study the quantum-mechanical bootstrap as it applies to the bound states of several central potentials in three dimensions. As part of this effort, we show how the bootstrap approach may be applied to ``non-algebraic'' potentials, such as the Yukawa potential (which asymptotically decays as an exponential) and a Gaussian potential. We additionally review the bootstrap of the Coulomb potential, demonstrate a high-precision bootstrap of the Cornell potential, and study conformal quantum mechanics. These results further recommend the bootstrap as a numerical method for high-precision calculations of ground-state physics, where applicable: for example, we are able to determine the critical coupling in the Cornell potential to better than one part in $10^7$, the most precise determination to date. Lower bounds on energies are also of high precision, occasionally one part in greater than $10^8$. Finally, we discuss the circumstances under which we are able to obtain meaningful upper bounds on ground-state energies.",0,arxiv,Kuantum,CC-BY/arXiv,Quantum bootstrap for central potentials
"Quantum spin liquids are elusive long-range entangled states. Motivated by experiments in Rydberg quantum simulators, recent excitement has centered on the possibility of dynamically preparing a state with quantum spin liquid correlation even when the ground state phase diagram does not exhibit such a topological phase. Understanding the microscopic nature of such quantum spin ""lake"" states and their relationship to equilibrium spin liquid order remains an essential question. Here, we extend the use of approximately symmetric neural quantum states for real-time evolution and directly simulate the dynamical preparation in systems of up to $N=384$ atoms. We analyze a variety of spin liquid diagnostics as a function of the preparation protocol and optimize the extent of the quantum spin lake thus obtained. In the optimal case, the prepared state shows spin-liquid properties extending over half the system size, with a topological entanglement entropy plateauing close to $Î³= \ln 2$. We extract two physical length scales $Î»$ and $Î¾$ which constrain the extent of the quantum spin lake $\ell$ from above and below.",0,arxiv,Kuantum,CC-BY/arXiv,Optimizing the dynamical preparation of quantum spin lakes on the ruby lattice
"We show that the spectral properties of driven quantum systems with a classically chaotic counterpart and spatially localized openness, such as optical or microwave billiards with leaks, deviate from predictions of Ginibre ensembles. Our analysis focuses on the leaky quantum standard map (QSM) of the kicked rotor. We compare its complex resonance spectrum with both Ginibre and truncated circular orthogonal ensembles (TCOEs). We find that the long-lived resonances follow TCOE statistics, reproducing the density of states and level spacing correlations, but depart from Ginibre predictions. Short-lived resonances, however, do not show a clear correspondence with either random-matrix ensemble. We also demonstrate that increasing the leak size takes the density of states of the TCOE toward the Ginibre limit, yet their spectral correlations remain distinct.",0,arxiv,Kuantum,CC-BY/arXiv,Beyond Ginibre statistics in open Floquet chaotic systems with localized leaks
"The dynamics of long-range quantum Ising models represents a current frontier in experimental physics, notably in trapped ions or Rydberg atomic systems. However, a theoretical description of these dynamics beyond 1D remains a significant challenge for conventional methods. Here, we address this challenge by means of neural quantum states to simulate global quenches from the fully polarized ferromagnetic state in the 2D quantum Ising model with power-law decaying interactions. From these numerically exact simulations, we find that the dynamics exhibit slow relaxation with long-lived oscillations. We explain this behavior through a theory for the formation of magnon bound states, which are generated, as we show, through effective attractive interactions between magnons that persist over several lattice sites due to the power-law nature of the interactions. Our results are readily observable in current quantum simulation platforms realizing long-range interacting models such as in Rydberg atomic systems.",0,arxiv,Kuantum,CC-BY/arXiv,Slow dynamics and magnon bound states in the 2D long-range quantum Ising model
"Non-classical quantum correlations underpin both the foundations of quantum mechanics and modern quantum technologies. Among them, Bell nonlocality is a central example. For bipartite Bell inequalities, nonlocal correlations obey strict monogamy: a violation of one inequality precludes violations of other inequalities on the overlapping subsystems. In the multipartite setting, however, Bell nonlocality becomes inherently polygamous. This was previously shown for subsystems obtained by removing a single particle from an $N$-partite system. Here, we generalize this result to arbitrary $(N-k)$-partite subsystems with $k>0$. We demonstrate that a single $N$-qubit state can violate all $\binom{N}{k}$ relevant Bell inequalities simultaneously. We further construct an $N$-qubit Bell inequality, obtained by symmetrizing the $(N-k)$-qubit ones, that is maximally violated by states exhibiting this generalized polygamy. We compare these violations with those achievable by GHZ states and show that polygamy offers an advantage in multipartite scenarios, providing new insights into scalable certification of non-classicality in quantum devices. Our analysis relies on symmetry properties of the MABK inequalities. Finally, we show that this behavior can occur across multiple subsystem sizes, a phenomenon we call hyper-polygamy. These structures reveal the remarkable abundance of nonlocality present in multipartite quantum states and offer perspectives for their applications in quantum technologies.",0,arxiv,Kuantum,CC-BY/arXiv,The Richness of Bell Nonlocality: Generalized Bell Polygamy and Hyper-Polygamy
"A superfluid flows without friction below a critical velocity, exhibiting zero drag force on impurities. Above this threshold, superfluidity breaks down, and the internal energy is redistributed into incoherent excitations such as vortices. We demonstrate that a finite-mass, mobile impurity immersed in a flowing two-dimensional paraxial superfluid of light can \textit{swim} against the superfluid current when this critical velocity is exceeded. This self-propulsion is achieved by the periodic emission of quantized vortex-antivortex pairs downstream, which impart an upstream recoil momentum that results in a net propulsive force. Analogous to biological systems that minimize effort by exploiting wake turbulence, the impurity harnesses this vortex backreaction as a passive mechanism of locomotion. Reducing the impurity dynamics to the motion of its center of mass and using a point-vortex model, we quantitatively describe how this mechanism depends on the impurity geometry and the surrounding flow velocity. Our findings establish a fundamental link between internal-energy dissipation in quantum fluids and concepts of self-propulsion in active-matter systems, and opens new possibilities for exploiting vortices for controlled quantum transport at the microscale.",0,arxiv,Kuantum,CC-BY/arXiv,Swimming against a superfluid flow: Self-propulsion via vortex-antivortex shedding in a quantum fluid of light
"Recent studies of delocalization-localization transitions in disordered quantum chains have highlighted the role of rare, chain-breaking events that favor localization, in particular for high-energy eigenstates related to many-body localization. In this context, we revisit the random-field XXZ spin-1/2 chain at zero temperature with ferromagnetic interactions, equivalent to interacting fermions or hard-core bosons in a random potential with attractive interactions. We argue that localization in this model can be characterized by chain-breaking events, which are probed by the extreme values of simple local observables, such as the on-site density or the local magnetization, that are readily accessible in both experiments and numerical simulations. Adopting a bosonic language, we study the disorder-induced Berezinskii-Kosterlitz-Thouless (BKT) quantum phase transition from superfluid (SF) to Bose glass (BG), and focus on the strong disorder regime where localization is driven by weak links. Based on high-precision density matrix renormalization group simulations, we numerically show that extreme local densities accurately capture the BKT transition, even for relatively short chains ranging from a few dozen to a hundred sites. We also discuss the SF-BG transition in the weak disorder regime, where finite-size effects pose greater challenges. Overall, our work seeks to establish a solid foundation for using extreme statistics of local observables, such as density, to probe delocalization-localization transitions in disordered quantum chains, both in the ground state and at high energy.",0,arxiv,Kuantum,CC-BY/arXiv,Extreme statistics as a probe of the superfluid to Bose-glass Berezinskii-Kosterlitz-Thouless transition
"Integrated photonics in trapped-ion systems are critical for the realization of applications such as portable optical atomic clocks and scalable quantum computers. However, system-level integration of all required functionalities remains a key challenge. In this work, we demonstrate an autonomously operating optical clock having a short-term frequency instability of $3.14(5)\times 10^{-14} / \sqrtÏ„$ using an ensemble of four $^{171}\textrm{Yb}^{+}$ ions trapped in a multi-site surface-electrode trap at room temperature. All clock operations are performed with light delivered via on-chip waveguides. We showcase the system's resilience through sustained, autonomous operation featuring automated ion shuttling and reloading to mitigate ion loss during interleaved clock measurements. This work paves the way beyond component-level functionality to establish a viable and robust architecture for the next generation of portable, multi-ion quantum sensors and computers.",0,arxiv,Kuantum,CC-BY/arXiv,Autonomous multi-ion optical clock with on-chip integrated photonic light delivery
"It is a well-known fact in classical information theory that no deterministic procedure can extract close-to-ideal randomness from an arbitrary entropy source. On the other hand, if additional knowledge about the source is available -- e.g., that it is a sequence of independent Bernoulli trials -- then deterministic extractors do exist. For quantum entropy sources, where in addition to classical random variables we consider quantum side information, the use of extra knowledge about their structure was pioneered in a recent publication [C. Foreman and L. Masanes, Quantum 9, 1654 (2025)]. In that work, the authors provide deterministic extractors for device-independent randomness generation with memoryless devices achieving a sufficiently high CHSH score. In this work, we extend their construction to the prepare-and-measure scenario. Specifically, we prove that the considered functions are also extractors for memoryless devices in a semi-device-independent setting under an overlap assumption on the prepared quantum states. We then simulate the resulting randomness generation protocol on a novel and experimentally relevant family of behaviors, observing positive key rates already for $7\times 10^3$ rounds.",0,arxiv,Kuantum,CC-BY/arXiv,Deterministic randomness extraction for semi-device-independent quantum random number generation
"Logical qubits encoded in quantum error correcting codes can exhibit non-Markovian dynamical evolution, even when the underlying physical noise is Markovian. To understand this emergent non-Markovianity, we define a Markovianity condition appropriate to logical gate operations, and study it by relating logical operations to their physical implementation (operations on the data qubits into which the logical qubit is encoded). We apply our analysis to small quantum codes, and show that they exhibit non-Markovian dynamics even for very simple physical noise models. We show that non-Markovianity can emerge from Markovian physical operations if (and only if) the physical qubits are not necessarily returned to the code subspace after every round of QEC. In this situation, the syndrome qubits can act as a memory, mediating time correlations and enabling violation of the Markov condition. We quantify the emergent non-Markovianity in simple examples, and propose sufficient conditions for reliable use of gate-based characterization techniques like gate set tomography in early fault-tolerant quantum devices.",0,arxiv,Kuantum,CC-BY/arXiv,Emergent Non-Markovianity in Logical Qubit Dynamics
"We introduce a driven-dissipative Floquet model in which a single harmonic oscillator with modulated frequency and decay realizes a non-Hermitian synthetic lattice with an effective electric field gradient in frequency space. Using the Floquet-Green's function and its doubled-space representation, we identify a topological regime that supports directional amplification and frequency conversion, accurately captured by a local winding number. The underlying mode structure is well described by a Jackiw-Rebbi-like continuum theory with Dirac cones and solitonic zero modes in synthetic frequency. Our results establish a simple and experimentally feasible route to non-Hermitian topological amplification, naturally implementable in current quantum technologies such as superconducting circuits.",0,arxiv,Kuantum,CC-BY/arXiv,Floquet Topological Frequency-Converting Amplifier
"In this paper, we consider a four-dimensional system composed of a mass-dimension-one fermionic field, also known as Elko, interacting with a real scalar field. Our main objective is to analyze the Casimir effects associated with this system, assuming that both the Elko and scalar fields satisfy Dirichlet boundary conditions on two large parallel plates separated by a distance $L$. In this scenario, we calculate the vacuum energy density and its first-order correction in the coupling constants of the theory. Additionally, we consider the mass correction for each field separately, namely the topological mass that arises from the boundary conditions imposed on the fields and which also depends on the coupling constants. To develop this analysis, we use the mathematical formalism known as the effective potential, expressed as a path integral in quantum field theory.",0,arxiv,Kuantum,CC-BY/arXiv,Vacuum Energy and Topological Mass in Interacting Elko and Scalar Field Theories
"We reconstruct all (2+1)D quantum double models of finite groups from their boundary symmetries through the repeated application of a gauging procedure, extending the existing construction for abelian groups. We employ the recently proposed categorical gauging framework, based on matrix product operators (MPOs), to derive the appropriate gauging procedure for the $\mathsf{Rep}\, G$ symmetries appearing in our construction and give an explicit description of the dual emergent $G$ symmetry, which is our main technical contribution. Furthermore, we relate the possible gapped boundaries of the quantum double models to the quantum phases of the one-dimensional input state to the iterated gauging procedure. Finally, we propose a gauging procedure for 1-form $\mathsf{Rep}\, G$ symmetries on a two-dimensional lattice and use it to extend our results to the construction of (3+1)D quantum doubles models through the iterative gauging of (2+1)-dimensional symmetries.",0,arxiv,Kuantum,CC-BY/arXiv,Non-abelian quantum double models from iterated gauging
"Supersolidity, combining superfluid and crystalline orders, has been realized in dipolar Bose-Einstein condensates by tuning interatomic interactions. Here we show that supersolidity can also emerge from mode coupling at a superfluid-solid interface, without modifying bulk interactions and for a broad class of superfluids. Using an analytical and numerical treatment of the coupled superfluid and phonon fields, we derive the criterion for a density-modulation instability driven by interfacial coupling and dependent on dimensionality. In superfluid helium, the instability first appears at the roton mode, while in a Bose-Einstein condensate with contact interactions it occurs at the lowest accessible wave vector set by the system size. Beyond the threshold, the ground state acquires an interfacial density modulation while the bulk remains superfluid, forming a hybrid superfluid-supersolid phase. Our results identify interfacial mode coupling as a promising route to supersolidity, enabling the simultaneous exploitation of interfacial supersolid and bulk superfluid quantum properties.",0,arxiv,Kuantum,CC-BY/arXiv,Fluctuation-Induced Supersolidity at the Superfluid-Solid Interface
"The hypothesis of composite $XHe$ dark atoms offers a compelling framework to address the challenges in direct dark matter particles detection, as their neutral, atom-like configuration evades conventional experimental signatures. A critical issue may arise in interaction between $XHe$ and atomic nuclei due to the unshielded nuclear attraction, which could destabilize the dark atom's bound state. To resolve this, we propose a novel numerical quantum mechanical approach that accounts for self-consistent electromagnetic-nuclear couplings. This method addresses to eliminate the inherent complexity of the $XHe$-nucleus three-body system, where analytical solutions are intractable. By reconstructing the effective interaction potential - including dipole Coulomb barrier and shallow potential well - we demonstrate how these features lead to the formation of $XHe$-nucleus bound states and modulate low-energy capture processes. Our model enables validation of the dark atom hypothesis, particularly in interpreting experimental anomalies like annual modulation signals observed in DAMA/LIBRA. These findings advance the theoretical foundation for dark matter interactions and provide a robust framework for future experimental design.",0,arxiv,Kuantum,CC-BY/arXiv,The bound state of dark atom with the nucleus of substance
"We explore the influence of geometry in the critical behavior of sparse long-range spin models. We examine a model with interactions that can be continuously tuned to induce distinct changes in the metric, topology, and dimensionality of the coupling graph. This underlying geometry acts as the driver of criticality, with structural changes in the graph coinciding with and dictating the phase boundaries. We further discuss how this framework connects naturally to realizations in tweezer arrays with Rydberg excitations. In certain cases, the effective geometry can be incorporated in the layout of atoms in tweezers to realize phase transitions that preserve universal features, simplifying their implementation in near-term experiments.",0,arxiv,Kuantum,CC-BY/arXiv,Geometry-driven transitions in sparse long-range spin models with cold atoms
"Lee-Yang theory offers a unifying framework for understanding classical phase transitions and dynamical quantum phase transitions through the analysis of partition functions and Loschmidt echoes. Recently, this framework is extended to characterize quantum phase transitions in arXiv:2509.20258 by introducing the concepts of non-Hermitian symmetry breaking and fidelity zeros. Here, we generalize the theory by studying a broad class of quantum models, including the XY model, the XXZ model, the XYZ model, and the $\mathbb{Z}_3$ clock model in one dimension, subject to complex external magnetic field. For the XY, XXZ and XYZ models, we find that the complex field breaks parity symmetry and induces oscillations of the ground state between the two parity sectors, giving rise to fidelity zeros within the ordered phases. For the $\mathbb{Z}_3$ clock model, the complex field splits the real part of the ground-state energy between the neutral sector ($q=0$) and the charged sectors ($q=1,2$), while preserving the degeneracy within the charged sector. Fidelity zeros arise only after projecting out one of the charged sectors, and the finite-size scaling of these zeros produces critical exponents fully consistent with analytical predictions.",0,arxiv,Kuantum,CC-BY/arXiv,Non-Hermitian symmetry breaking and Lee-Yang theory for quantum XYZ and clock models
"Quantum unitaries of the form $Î£_{c}\ket{c}\bra{c}\otimes U_{c}$ are ubiquitous in quantum algorithms. This class encompasses not only standard uniformly controlled gates (UCGs) but also a wide range of circuits with uniformly controlled structures. However, their circuit-depth and gate-count complexities have not been systematically analyzed within a unified framework. In this work, we study the general decomposition problem for UCG and UCG-like structure. We then introduce the restricted Uniformly Controlled Gates (rUCGs) as a unified algebraic model, defined by a 2-divisible Abelian group that models the controlled gate set. This model captures uniformly controlled rotations, multi-qubit uniformly controlled gates, and diagonal unitaries. Furthermore, this model also naturally incorporates k-sparse version (k-rUCGs), where only a subset of control qubits participate in each multi-qubit gate. Building on this algebraic model, we develop a general framework. For an n-control rUCG, the framework reduce the gate complexity from ${O(n2^n)}$ to ${O(2^n})$ and the circuit depth from ${O(2^n\log n)}$ to ${O(2^n\log n/n)}$. The framework further provides systematic size and depth bounds for k-rUCGs by exploiting sparsity in the control space, with same optimization coefficient as rUCG, respectively. Empirical evaluations on representative QAOA circuits confirm reductions in depth and size, which highlight that the rUCG model and its associated decomposition framework unify circuits previously considered structurally distinct under a single, asymptotically optimal synthesis paradigm.",0,arxiv,Kuantum,CC-BY/arXiv,A Unified Framework for Optimizing Uniformly Controlled Structures in Quantum Circuits
"Topological photonic phases are typically identified through band reconstruction, steady-state transmission, or real-space imaging of edge modes. In this work, we present a framework for spectroscopic readout of chiral photonic topology in a single driven optical cavity containing a spin-orbit-coupled Bose-Einstein condensate. We demonstrate that the cavity transmission power spectral density provides a direct and measurable proxy for a momentum- and frequency-resolved photonic Chern marker, enabling topological characteristics to be inferred from spectral data without the need for bulk-band tomography. In the loss-dominated regime, where cavity decay exceeds atomic dissipation, the power spectral density exhibits Dirac-like gapped hybrid modes with a vanishing Chern marker, indicating a trivial phase. When the dissipation imbalance is reversed, a bright, gap-spanning spectral ridge emerges, co-localized with peaks in both the Chern marker and Berry curvature. The complex spectrum reveals parity-time symmetric coalescences and gain-loss bifurcations, marking exceptional points and enabling chiral, gap-traversing transport. By linking noise spectroscopy to geometric and non-Hermitian topology in a minimal cavity-QED architecture, this work provides a framework for spectroscopic detection of topological order in driven quantum systems. This approach offers a pathway to compact, tunable topological photonics across a broad range of light-matter platforms, providing a method for the study and control of topological phases in hybrid quantum systems.",0,arxiv,Kuantum,CC-BY/arXiv,Spectroscopic readout of chiral photonic topology in a single-cavity spin-orbit-coupled Bose-Einstein condensate
"Quantum microcombs generated in high-Q microresonators provide compact, multiplexed sources of entangled modes for continuous-variable (CV) quantum information processing. While deterministic generation of CV states via Kerr-induced two-mode squeezing has been demonstrated, achieving spectrally uniform squeezing remains challenging because of asymmetry and anomalies in the dispersion profile. Here we overcome these limitations by combining a microresonator with an engineered mode spectrum and optimized pump conditions. We realize a CV quantum microcomb comprising 14 independent two-mode squeezed states, each exhibiting more than 4 dB of raw squeezing (up to 4.3 dB) across a 0.7 THz bandwidth. This uniform, high-performance quantum resource represents a key step toward scalable, integrated CV quantum technologies operating beyond classical limits.",0,arxiv,Kuantum,CC-BY/arXiv,Perfect continuous-variable quantum microcombs
"We establish that the exact quantum dynamics of a Brownian particle in the Caldeira-Leggett model can be mapped, at any temperature, onto a classical, non-Markovian stochastic process in phase space. Starting from a correlated thermal equilibrium state between the particle and bath, we prove that this correspondence is exact for quadratic potentials under arbitrary quantum state preparations of the particle itself. For more general, smooth potentials, we identify and exploit a natural small parameter: the density matrix becomes strongly quasidiagonal in the coordinate representation, with its off-diagonal width shrinking as the bath's spectral cutoff increases, providing a controlled parameter for accurate approximation. The framework is fully general: arbitrary initial quantum states-including highly non-classical superpositions-are incorporated via their Wigner functions, which serve as statistical weights for trajectory ensembles. Furthermore, the formalism naturally accommodates external manipulations and measurements modeled by preparation functions acting at arbitrary times, enabling the simulation of complex driven-dissipative quantum protocols.",0,arxiv,Kuantum,CC-BY/arXiv,Quantum Brownian Motion as a Classical Stochastic Process in Phase Space
"The Michelson interferometric phase detection resolution can be enhanced by replacing conventional beam splitters with novel directionally unbiased four-port scatterers, such as Grover coins. We present a quantitative analysis of the noise-to-signal ratio of sideband frequencies generated by gravitational wave-induced phase perturbations in a Grover-Michelson interferometer (GMI). We discuss the principles of GMI signal enhancement and demonstrate how combining this configuration with additional light-recycling arrangements further enhances the performance.",0,arxiv,Kuantum,CC-BY/arXiv,Strain sensitivity enhancement in a Grover-Michelson interferometer
"Processes with indefinite causal order can arise when quantum theory is locally valid. Here, we identify an information-theoretic principle, termed parity erasure, that completely characterizes such processes. Our characterization does not rely on the formalism of quantum theory itself, but instead is derived from a set of axioms for general operational probabilistic theories, and thus holds also for a large class of theories beyond quantum theory. This informational approach reveals a fundamental property of information exchange in scenarios with indefinite causal structure.",0,arxiv,Kuantum,CC-BY/arXiv,Parity erasure: a foundational principle for indefinite causal order
"To understand the intricate exchange between electrons of different bands in strongly correlated materials, it is essential to treat multi-orbital models accurately. For this purpose, dynamical mean-field theory (DMFT) provides an established framework, whose scope crucially hinges on the availability of efficient quantum impurity solvers. Here we present a real-frequency impurity solver based on neural quantum states (NQS) combined with an operator-Lanczos construction. NQS are an asymptotically unbiased variational ground-state ansatz that employs neural networks to capture long-range correlations on complicated graph structures. We leverage this ability to solve multi-orbital impurity problems using a systematically improvable Segmented Commutator Operator-Lanczos (SCOL) construction. Our benchmarks on both the single-orbital Anderson model and the multi-orbital Hubbard-Kanamori impurity Hamiltonian reveal excellent ground-state precision and the capacity to accurately resolve zero temperature spectral functions and self-energies. These results open avenues for extending DMFT to more challenging problems.",0,arxiv,Kuantum,CC-BY/arXiv,Operator Lanczos Approach enabling Neural Quantum States as Real-Frequency Impurity Solvers
"We propose a new secret communication scheme over the bosonic wiretap channel. It uses readily available hardware such as lasers and direct photodetectors. The scheme is based on randomness extractors, pulse-position modulation, and Reed-Solomon codes and is therefore computationally efficient. It is secure against an eavesdropper performing coherent joint measurements on the quantum states it observes. In the low-photon-flow limit, the scheme is asymptotically optimal and achieves the same dominant term as the secrecy capacity of the same channel.",0,arxiv,Kuantum,CC-BY/arXiv,An Efficient Secret Communication Scheme for the Bosonic Wiretap Channel
"We investigate magnomechanically induced transparency in a parity-time-symmetric cavity magnomechanical system with traveling-field-induced non-Hermiticity. The setup consists of a microwave cavity mode coupled to magnons in a single-crystal yttrium iron garnet sphere, which in turn are hybridized with a vibrational mechanical mode through magnetostrictive interaction. In the Hermitian regime, strong photon-magnon coupling generates a single transparency window in the cavity transmission, which splits into a doublet when the magnon is coherently hybridized with the mechanical mode via magnomechanical coupling. This establishes a versatile platform in which the transparency spectrum can be engineered from single- to multi-window response using experimentally accessible, scaled magnomechanical interactions. When a non-Hermitian coupling is introduced, the system enters a parity-time-broken regime in which the transparency ceases to be purely passive and becomes gain assisted, leading to asymmetric transmission with amplification on one side of the resonance and enhanced absorption on the other. By tuning the cavity detuning, we convert magnomechanical transparency into Fano-type line shapes with strongly non-Lorentzian phase dispersion and map their deformation into asymmetric, gain-assisted Fano ridges in the joint space of probe and magnon detunings. Finally, we analyze the associated group delay and show that both slow- and fast-light behavior can be widely tuned by varying the photon-magnon and magnomechanical couplings together with the non-Hermitian strength, highlighting parity-time-symmetric cavity magnomechanics as a promising platform for reconfigurable quantum signal processing and enhanced sensing.",0,arxiv,Kuantum,CC-BY/arXiv,$\mathcal{PT}$-symmetric cavity magnomechanics with gain-assisted transparency and amplification
"Closing the gap between ray tracing simulations and experimentally observed electron jetting in bilayer graphene (BLG), we study all-electronic, gate-defined BLG cavities using tight-binding simulations and semiclassical equations of motion. Such cavities offer a rich playground to investigate anisotropic electron transport due to the trigonally warped Fermi surfaces. In this work, we achieve two things: First, we verify the existence of triangular modes (as predicted by classical ray tracing calculations) in the quantum solutions of closed circular BLG cavities. Then, we explore signatures of said triangular modes in transport through open BLG cavities connected to leads. We show that the triangular symmetry translates into anisotropic transport and present an optimal setup for experimental detection of the triangular modes as well as for controlled modulation of transport in preferred directions.",0,arxiv,Kuantum,CC-BY/arXiv,Anisotropic transport in ballistic bilayer graphene cavities
"We study the radiative process of three entangled quantum probes initially prepared in a tripartite W state. As a basic set-up, we consider the probes to be inertial in flat spacetime and investigate how the radiative process is affected by different probe configurations. We take the quantum probes as either static or moving with uniform velocities and consider different switching scenarios. Our main observation confirms that the radiative process depends distinctively on the initial configuration in which the probes are arranged, as well as on the direction of the probe velocity. We also extend our analysis to a thermal environment, thereby simulating a more realistic background. We thoroughly discuss the effects due to different switchings, the thermal background, and probe motion on the radiative process of these tripartite entangled probes. We also comment on how the observations from this work can help prepare a set-up least affected by quantum decoherence.",0,arxiv,Kuantum,CC-BY/arXiv,Radiative process of tripartite entangled probes in inertial motion
"Using the odd and even ``basis vectors'', which are the superposition of odd and even products of $Î³^a$'s, to describe the internal spaces of the second quantised fermion and boson fields, respectively, offers in even-dimensional spaces, like it is $d=(13+1)$, the unique description of all the properties of the observed fermion fields (quarks and leptons and antiquarks and antileptons appearing in families) and boson fields (gravitons, photons, weak bosons, gluons and scalars) in a unique way, providing that all the fields have non zero momenta only in $d =(3+1)$ of the ordinary space-time, bosons have the space index $Î±$ (which is for tensors and vectors $Î¼=(0,1,2,3)$ and for scalars $Ïƒ\ge 5$). In any even-dimensional space, there is the same number of internal states of fermions appearing in families and their Hermitian conjugate partners as it is of the two orthogonal groups of boson fields having the Hermitian conjugate partners within the same group. A simple action for massless fermion and boson fields describes all the fields uniquely. The paper overviews the theory, presents new achievements and discusses the open problems of this theory.",0,arxiv,Kuantum,CC-BY/arXiv,"Internal spaces of fermion and boson fields, described with the superposition of odd and even products of $Î³^{a}$, enable understanding of all the second-quantised fields in an equivalent way"
"The notion of ``picture'' is fundamental in quantum mechanics. In this work, a new picture, which we call entanglement picture, is proposed based on the novel channel-state duality, whose importance is revealed in quantum information science. We illustrate the application of entanglement picture in quantum algorithms for the simulation of many-body dynamics, quantum field theory, thermal physics, and more generic quantities.",0,arxiv,Kuantum,CC-BY/arXiv,Quantum simulation in the entanglement picture
"In this paper, a method for deriving master equation is developed for a generic small quantum system, which is locally coupled to an environment as a many-body quantum chaotic system that satisfies the eigenstate thermalization hypothesis ansatz, resorting to neither the Born approximation nor the Markov approximation. The total system undergoes SchrÃ¶dinger evolution, under an initial condition in which the environmental branches possess no correlation with the interaction Hamiltonian. Derivation of the master equation is based on piecewise usage of a second-order expansion of a formal expression, which is derived for the evolution of the environmental branches. Approximations used in the derivation are mainly based on dynamic properties of the environment.",0,arxiv,Kuantum,CC-BY/arXiv,An ETH-ansatz-based environmental-branch approach to master equation
"The full characterization of quantum states of light is a central task in quantum optics and information science. Double homodyne detection provides a powerful method for the direct measurement of the Husimi Q quasi-probability distribution, offering a complete state representation in a simple experimental setting and a limited time frame. Here, we demonstrate that double homodyne detection can serve as more than a passive measurement apparatus. By intentionally unbalancing the input beamsplitter that splits the quantum signal, we show that the detection scheme itself performs an effective squeezing or anti-squeezing transformation on the state being measured. The resulting measurement directly samples the Q function of the input state as if it were acted upon by a squeezing operator whose strength is a tunable experimental parameter : the beamsplitter's reflectivity. We experimentally realize this technique using a robust polarization-encoded double homodyne detection to characterize a squeezed vacuum state. Our results demonstrate the controlled deformation of the measured Q function's phase-space distribution, confirming that unbalanced double homodyne detection is a versatile tool for simultaneous quantum state manipulation and characterization.",0,arxiv,Kuantum,CC-BY/arXiv,Tunable passive squeezing of squeezed light through unbalanced double homodyne detection
"Why do local actions and exponential Euclidean weights arise so universally in classical, statistical, and quantum theories? We offer a structural explanation from minimal constraints on finite descriptions of admissible histories. Assume that histories admit finite, self-delimiting (prefix-free) generative codes that can be decoded sequentially in a single forward pass. These purely syntactic requirements define a minimal descriptive cost, interpretable as a smoothed minimal program length, that is additive over local segments. First, any continuous local additive cost whose stationary sector coincides with the empirically identified classical variational sector is forced into a unique Euler--Lagrange equivalence class. Hence the universal form of an action is fixed by descriptional structure alone, while the specific microscopic Lagrangian and couplings remain system-dependent semantic input. Second, independently of microscopic stochasticity, finite prefix-free languages exhibit exponential redundancy: many distinct programs encode the same coarse history, and this redundancy induces a universal exponential multiplicity weight on histories. Requiring this weight to be real and bounded below selects a real Euclidean representative for stable local bosonic systems, yielding the standard Euclidean path-integral form. When Osterwalder--Schrader reflection positivity holds, the Euclidean measure reconstructs a unitary Lorentzian amplitude.",0,arxiv,Kuantum,CC-BY/arXiv,"Syntactic Structure, Quantum Weights"
"Tunable Josephson harmonics open up for new qubit design. We demonstrate a superconducting circuit element with a tunnel junction in series with a SQUID loop, yielding a highly magnetic-flux tunable harmonic content of the Josephson potential. We analyze spectroscopy of the first four qubit transitions with a circuit model which includes the internal mode, revealing a second harmonic up to $\sim10\%$ of the fundamental harmonic. Interestingly, a sweet spot where the dispersive shift vanishes is achieved by balancing the dispersive couplings to the internal and qubit modes. The highly tunable set-up provides a route toward protected qubits, and customizable nonlinear microwave devices.",0,arxiv,Kuantum,CC-BY/arXiv,Higher Josephson harmonics in a tunable double-junction transmon qubit
"Entangled states of photons form the foundation of quantum communication, computation, and metrology. Yet their generation remains fundamentally constrained: in the absence of intrinsic photon-photon interactions, the generation of such states is inherently probabilistic rather than deterministic. The prevalent technique of post-selection verifies the creation of an entangled state by detecting and thus destroying it. Heralding offers a solution in which measuring ancillary photons in auxiliary modes signals the state generation without the need to measure it. Here, we report an experiment to generate a three-mode two-photon NOON state, where the detection of a single photon in one heralding mode signifies the presence of the state in three target modes. We validate the generated state by estimating a fidelity of 0.823 +/- 0.018 with respect to an ideal three-mode NOON state and certifying genuine multipartite entanglement. By virtue of the high success probability and small resource overhead of our scheme, our work provides a theoretical and experimental stepping stone for entangled multi-mode state generation, which is realizable with current technology. These multi-mode entangled states represent a key direction for linear optical quantum information that is complementary to multi-qubit state encoding.",0,arxiv,Kuantum,CC-BY/arXiv,Heralded generation of a three-mode NOON state
"We report on the generation and characterization of ultraviolet (wavelength 266 nm) twisted light with high orbital angular momentum (OAM) using three types of fabricated diffractive optical elements (DOEs): a reflective fork grating, a high-charge spiral phase plate (SPP), and binary axicons. All elements were integrated into a drive-laser beamline of an electron RF-photoinjector, enabling direct evaluation under accelerator-relevant conditions. The SPP produced a high-purity Laguerre-Gaussian mode with OAM l = 64 and a measured conversion efficiency of approximately 80%. Binary axicons generated quasi-Bessel twisted light with topological charges up to m = 10, exhibiting low divergence and stable multi-lobe ring structures. The fork grating reliably produced lower-order modes, l = 2-8, with good agreement between simulations and cylindrical-lens diagnostics. These results constitute, to our knowledge, the first comprehensive experimental demonstration of deep-UV high-OAM beams generated with fabricated DOEs and validated through mode-conversion measurements. The demonstrated techniques are compatible with high-power UV laser systems used in RF-photoinjectors and offer a practical route toward structured photocathode illumination and the generation of relativistic vortex electrons at a particle accelerator facility.",0,arxiv,Kuantum,CC-BY/arXiv,High-OAM Deep Ultraviolet Twisted Light Generation for RF-Photoinjector Applications
"The original boson sampling paradigm-consisting of multiple single-photon input states, a large interferometer, and multi-channel click detection-was originally proposed as a photonic route to quantum computational advantage. Its non-Gaussian resources, essential for outperforming any classical system, are provided by single-photon inputs and click detection. Yet the drive toward larger experiments has led to the replacement of experimentally demanding single-photon sources with Gaussian states, thereby diminishing the available non-Gaussianity-a critical quantum resource. As the community broadens its focus from the initial sampling task to possible real-world applications, it becomes crucial to quantify the performance cost associated with reducing non-Gaussian resources and to benchmark sampling platforms that employ different input states.   To address this need, we introduce the Paderborn Quantum Sampler (PaQS), a hybrid platform capable of performing sampling experiments with eight Gaussian or non-Gaussian input states in a 12-mode interferometer within a single experimental run. This architecture enables direct, side-by-side benchmarking of distinct sampling regimes under otherwise identical conditions. By employing a semi-device-independent framework, offering certification that does not rely on prior knowledge of the interferometer or the input states, we verify that the observed data cannot be reproduced by any classical model-a prerequisite for demonstrating quantum advantage. Applying this framework, we observe clear performance gains arising from non-Gaussian input states.",0,arxiv,Kuantum,CC-BY/arXiv,Benchmarking Gaussian and non-Gaussian input states with a hybrid sampling platform
"Grover's algorithm is a fundamental quantum algorithm that offers a quadratic speedup for the unstructured search problem by alternately applying physically implementable oracle and diffusion operators. In this paper, we reformulate the unstructured search as a maximization problem on the unitary manifold and solve it via the Riemannian gradient ascent (RGA) method. To overcome the difficulty that generic RGA updates do not, in general, correspond to physically implementable quantum operators, we introduce Grover-compatible retractions to restrict RGA updates to valid oracle and diffusion operators. Theoretically, we establish a local Riemannian $Î¼$-Polyak-Åojasiewicz (PL) inequality with $Î¼= \tfrac{1}{2}$, which yields a linear convergence rate of $1 - Îº^{-1}$ toward the global solution. Here, the condition number $Îº= L_{\mathrm{Rie}} / Î¼$, where $L_{\mathrm{Rie}}$ denotes the Riemannian Lipschitz constant of the gradient. Taking into account both the geometry of the unitary manifold and the special structure of the cost function, we show that $L_{\mathrm{Rie}} = O(\sqrt{N})$ for problem size $N = 2^n$. Consequently, the resulting iteration complexity is $O(\sqrt{N} \log(1/\varepsilon))$ for attaining an $\varepsilon$-accurate solution, which matches the quadratic speedup of $O(\sqrt{N})$ achieved by Grover's algorithm. These results demonstrate that an optimization-based viewpoint can offer fresh conceptual insights and lead to new advances in the design of quantum algorithms.",0,arxiv,Kuantum,CC-BY/arXiv,A Grover-compatible manifold optimization algorithm for quantum search
"Quantum teleportation is a fundamental quantum communications primitive that requires an entangled resource state. In the continuous-variable regime, non-Gaussian entangled resources have been shown theoretically to improve teleportation fidelity compared to Gaussian squeezed vacuum. We experimentally demonstrate a heralded two-mode resource state for non-Gaussian teleportation capable of real-time use. We characterize this state with two-mode homodyne tomography showing it has fidelity $F=0.973\pm 0.005$ with the expected resource state. Real-time use is enabled by a photon-subtraction orchestrator system performing live coincidence detection and outputting low-jitter and low-latency heralding signals. Live collection of real-time quadrature measurements of photon-subtracted states is enabled by the development of a synchronized homodyne detection server where the orchestrator system queries to collect the real-time quadrature samples corresponding to the heralded state. These results demonstrate significant advancement in enabling the use of heralded non-Gaussian states in quantum networking protocols, especially in the context of quantum repeaters, non-Gaussian quantum sensing and measurement-based quantum computing.",0,arxiv,Kuantum,CC-BY/arXiv,Real-time heralded non-Gaussian teleportation resource-state generator
"Long-lived spin-helix states facilitate the study of non-equilibrium dynamics in quantum magnets. We consider the decay of transverse spin-helices in antiferromagnetic spin-$S$ XXZ chains with single-ion anisostropy. The spin-helix decay is observable in the time evolution of the local magnetization that we calculate numerically for the system in the thermodynamic limit using infinite time-evolving block decimation simulations. Although the single-ion anisotropy prevents helix states from being eigenstates of the Hamiltonian, they still can be long-lived for appropriately chosen wave numbers. In case of an easy-axis exchange anisotropy the single-ion anisotropy may even stabilize the helices. Within a spin-wave approximation, we obtain a condition giving an estimate for the most stable wave number $Q$ that agrees qualitatively with our numerical results.",0,arxiv,Kuantum,CC-BY/arXiv,Decay of spin helices in XXZ quantum spin chains with single-ion anisotropy
"In this paper, we discuss a refinement of quantum data processing inequality for the sandwiched quasi-relative entropy $\mathcal{S}_2$ on a tracial von-Neumann algebra. The main result is a universal recoverability bound with the Petz recovery map, which was previously obtained in the finite dimensional setup.",0,arxiv,Kuantum,CC-BY/arXiv,Universal recoverability of quantum states in tracial von-Neumann algebras
"In a circuit QED architecture, we experimentally demonstrate a simple and hardware-efficient Single-Step Phase-Engineered (SSPE) pulse scheme for actively depopulating the readout cavity. The method appends a reset segment with tailored amplitude and phase to a normal square readout pulse. Within the linear-response regime, the optimal reset amplitude scales proportionally with the readout amplitude, while the optimal reset phase remains nearly invariant, significantly simplifying the calibration process. By characterizing the cavity photons dynamics, we show that the SSPE pulse accelerates photon depletion by up to a factor of six compared to passive free decay. We further quantify the qubit backaction induced by the readout pulse and find that the SSPE pulse yields the lowest excitation and relaxation rates compared to a Square and CLEAR pulses. Our results establish the SSPE scheme as a practical and scalable approach for achieving fast, smooth, low-backaction cavity reset in superconducting quantum circuits.",0,arxiv,Kuantum,CC-BY/arXiv,Single-Step Phase-Engineered Pulse for Active Readout Cavity Reset in Superconducting Circuits
"Demonstrating the practical utility of Noisy Intermediate-Scale Quantum (NISQ) hardware for recurrent tasks in Computer-Aided Drug Discovery is of paramount importance. We tackle this challenge by performing three-dimensional protein pockets hydration-site prediction on a quantum computer. Formulating the water placement problem as a Quadratic Unconstrained Binary Optimization (QUBO), we use a hybrid approach coupling a classical three-dimensional reference-interaction site model (3D-RISM) to an efficient quantum optimization solver, to run various hardware experiments up to 123 qubits. Matching the precision of classical approaches, our results reproduced experimental predictions on real-life protein-ligand complexes. Furthermore, through a detailed resource estimation analysis, we show that accuracy can be systematically improved with increasing number of qubits, indicating that full quantum utility is in reach. Finally, we provide evidence that advantageous situations could be found for systems where classical optimization struggles to provide optimal solutions. The method has potential for assisting simulations of protein-ligand complexes for drug lead optimization and setup of docking calculations.",0,arxiv,Kuantum,CC-BY/arXiv,Practical protein-pocket hydration-site prediction for drug discovery on a quantum computer
"We present a quantum algorithmic routine that extends the realm of Grover-based heuristics for tackling combinatorial optimization problems with arbitrary efficiently computable objective and constraint functions. Building on previously developed quantum methods that were primarily restricted to linear constraints, we generalize the approach to encompass a broader class of problems in discrete domains. To evaluate the potential of our algorithm, we assume the existence of sufficiently advanced logical quantum hardware. With this assumption, we demonstrate that our method has the potential to outperform state-of-the-art classical solvers and heuristics in terms of both runtime scaling and solution quality. The same may be true for more realistic implementations, as the logical quantum algorithm can achieve runtime savings of up to $10^2-10^3$.",0,arxiv,Kuantum,CC-BY/arXiv,Constraint-oriented biased quantum search for general constrained combinatorial optimization problems
"Quantum communication between remote chips is essential for realizing large-scale superconducting quantum computers. For such communication, itinerant microwave photons propagating through transmission lines offer a promising approach. However, demonstrations to date have relied on frequency-tunable circuit elements to compensate for fabrication-related parameter variations between sender and receiver devices, introducing control complexity and limiting scalability. In this work, we demonstrate deterministic quantum state transfer and remote entanglement generation between fixed-frequency superconducting qubits on separate chips. To compensate for the sender-receiver mismatch, we employ a frequency-tunable photon-generation technique which enables us to adjust the photon frequency without modifying circuit parameters. To enhance the frequency tunability, we implement broadband transfer resonators composed of two coupled coplanar-waveguide resonators, achieving a bandwidth of more than 100 MHz. This broadband design enables successful quantum communication across a 30-MHz range of photon frequencies between the remote qubits. Quantum process tomography reveals state transfer fidelities of around 78% and Bell-state fidelities of around 73% across the full frequency range. Our approach avoids the complexity of the control lines and noise channels, providing a flexible pathway toward scalable quantum networks.",0,arxiv,Kuantum,CC-BY/arXiv,Deterministic Quantum Communication Between Fixed-Frequency Superconducting Qubits via Broadband Resonators
"Machine learning is widely applied in modern society, but has yet to capitalise on the unique benefits offered by quantum resources. Boson sampling -- a quantum-interference based sampling protocol -- is a resource that is classically hard to simulate and can be implemented on current quantum hardware. Here, we present a quantum accelerator for classical machine learning, using boson sampling to provide a high-dimensional quantum fingerprint for reservoir computing. We show robust performance improvements under various conditions: imperfect photon sources down to complete distinguishability; scenarios with severe class imbalances, classifying both handwritten digits and biomedical images; and sparse data, maintaining model accuracy with twenty times less training data. Crucially, we demonstrate the acceleration and scalability of our scheme on a photonic quantum processing unit, providing the first experimental validation that boson-sampling-enhanced learning delivers real performance gains on actual quantum hardware.",0,arxiv,Kuantum,CC-BY/arXiv,Photonic Quantum-Accelerated Machine Learning
"Testing the correspondence principle in nonlinear quantum systems is a fundamental pursuit in quantum physics. In this paper, we employed mean field approximation theory to study the semiclassical dynamics in the Rabi-Stark model (RSM) and showed that the nonlinear Stark coupling significantly modulates the semiclassical phase space structure. By analyzing the linear entanglement entropy of coherent states prepared in the classical chaotic and regular regions of the semiclassical phase space, we demonstrate that quantum-classical correspondence can be achieved in the RSM with large atom-light frequency ratios. While this correspondence fails in the resonant Rabi model because its truncated photon number is insufficient to approach the large quantum number limit, we discovered that in the resonant RSM when the nonlinear Stark coupling $U \to \pm 1$, the time-averaged linear entanglement entropy correlates strongly with the semiclassical phase space. In particular, when $U \to -1$, the truncated photon number in the resonant RSM is very close to that in the resonant Rabi model, but the time-averaged linear entanglement entropy still corresponds well with the semiclassical phase space. This result demonstrates that quantum-classical correspondence can be realized in the few-body resonant RSM.",0,arxiv,Kuantum,CC-BY/arXiv,Quantum-classical correspondence in resonant and nonresonant Rabi-Stark model
"Major technological advances of the past century are rooted in our understanding of quantum physics in the non-interacting limit. A central challenge today is to understand the behavior of complex quantum many-body systems, where interactions play an essential role. About four decades ago, Richard Feynman proposed using controllable quantum systems to efficiently simulate complex physics and chemistry problems, envisioning quantum orreries, highly tunable quantum devices built to emulate less understood quantum systems. Here we ask whether quantum simulators have already uncovered new physical phenomena-and, if so, in which areas and with what impact. We find that, in several notable instances, they have advanced our understanding of many-body quantum dynamics. Although many of these insights could in principle have been obtained theoretically or numerically, they were nevertheless first achieved using quantum processors. While a broad landscape of problems beyond non-equilibrium dynamics still awaits exploration, it is encouraging that quantum simulators are already beginning to challenge and refine our conventional wisdom.",0,arxiv,Kuantum,CC-BY/arXiv,Discovering novel quantum dynamics with NISQ simulators
"Programmability is a unifying paradigm for enacting families of quantum transformations via fixed processors and program states, with a fundamental role and broad impact in quantum computation and control. While there has been a shift from viewing open systems solely as a source of error to treating them as a computational resource, their programmability remains largely unexplored. In this work, we develop a framework that characterizes and quantifies the programmability of Lindbladian semigroups by combining physically implementable retrieval maps with time varying program states. Within this framework, we identify quantum programmable classes enabled by symmetry and stochastic structure, including covariant semigroups and fully dissipative Pauli Lindbladians with finite program dimension. We further provide a necessary condition for physical programmability that rules out coherent generators and typical dissipators generating amplitude damping. For such nonphysically programmable cases, we construct explicit protocols with finite resources. Finally, we introduce an operational programming cost, defined via the number of samples required to program the Lindbladian, and establish its core structural properties, such as continuity and faithfulness. These results provide a notion of programming cost for Lindbladians, bridge programmable channel theory and open system dynamics, and yield symmetry driven compression schemes and actionable resource estimates for semigroup simulation and control in noisy quantum technologies.",0,arxiv,Kuantum,CC-BY/arXiv,Programmable Open Quantum Systems
"Quantum communication offers many applications, with teleportation and superdense coding being two of the most fundamental. In these protocols, pre-shared entanglement enables either the faithful transfer of quantum states or the transmission of more information than is possible classically. However, channel losses degrade the shared states, reducing teleportation fidelity and the information advantage in superdense coding. Here, we investigate how to mitigate these effects by optimising the measurements applied by the communicating parties. We formulate the problem as an optimisation over general positive operator-valued measurements (POVMs) and compare the results with physically realisable noiseless attenuation (NA) and noiseless linear amplification (NLA) circuits. For teleportation, NLA/NA and optimised POVMs improve the average fidelity by up to 78% while maintaining feasible success probabilities. For superdense coding, they enhance the quantum advantage over the classical channel capacity by more than 100% in some regimes and shift the break-even point, thereby extending the tolerable range of losses. Notably, the optimal POVMs effectively reduce to NA or NLA, showing that simple, experimentally accessible operations already capture the essential performance gains.",0,arxiv,Kuantum,CC-BY/arXiv,The utility of noiseless linear amplification and attenuation in single-rail discrete-variable quantum communications
"Scalability remains a major challenge in building practical fault-tolerant quantum computers. Currently, the largest number of qubits achieved across leading quantum platforms ranges from hundreds to thousands. In atom arrays, scalability is primarily constrained by the capacity to generate large numbers of optical tweezers, and conventional techniques using acousto-optic deflectors or spatial light modulators struggle to produce arrays much beyond $\sim 10,000$ tweezers. Moreover, these methods require additional microscope objectives to focus the light into micrometer-sized spots, which further complicates system integration and scalability. Here, we demonstrate the experimental generation of an optical tweezer array containing $280\times 280$ spots using a metasurface, nearly an order of magnitude more than most existing systems. The metasurface leverages a large number of subwavelength phase-control pixels to engineer the wavefront of the incident light, enabling both large-scale tweezer generation and direct focusing into micron-scale spots without the need for a microscope. This result shifts the scalability bottleneck for atom arrays from the tweezer generation hardware to the available laser power. Furthermore, the array shows excellent intensity uniformity exceeding $90\%$, making it suitable for homogeneous single-atom loading and paving the way for trapping arrays of more than $10,000$ atoms in the near future.",0,arxiv,Kuantum,CC-BY/arXiv,Direct Generation of an Array with 78400 Optical Tweezers Using a Single Metasurface
"In this work, we develop a protocol for learning a time-independent Lindblad model for operations that can be applied repeatedly on a quantum computer. The protocol is highly scalable for models with local interactions and is in principle insensitive to state-preparation errors. At its core, the protocol forms a linear system of equations for the model parameters in terms of a set of observable values and their gradients. The required gradient information is obtained by fitting time-series data with sums of exponentially damped sinusoids and differentiating those curves. We develop a robust curve-fitting procedure that finds the most parsimonious representation of the data up to shot noise. We demonstrate the approach by learning the Lindbladian for a full layer of gates on a 156-qubit superconducting quantum processor, providing the first learning experiment of this kind. We study the effects of state-preparation and measurement errors and limitations on the operations that can be learned. For improved performance under readout errors, we propose an optional fine-tuning strategy that improves the fit between the time-evolved model and the measured data.",0,arxiv,Kuantum,CC-BY/arXiv,Large-scale Lindblad learning from time-series data
"We theoretically investigated the readout process of a spin--qubit structure based on a gate-all-around (GAA) transistor. Our study focuses on a logical qubit composed of two physical qubits. Different spin configurations result in different charge distributions, which subsequently influence the electrostatic effects on the GAA transistor. Consequently, the current flowing through the GAA transistor depends on the qubit's state. We calculated the current-voltage characteristics of the three-dimensional configurations of the qubit and GAA structures, using technology computer-aided design (TCAD) simulations. Moreover, we performed circuit simulations using the Simulation Program with Integrated Circuit Emphasis (SPICE) to investigate whether a readout circuit made from complementary metal--oxide semiconductor (CMOS) transistors can amplify the weak signals generated by the qubits. Our findings indicate that, by dynamically controlling the applied voltage within a properly designed circuit, the readout can be detected effectively based on a conventional sense amplifier.",0,arxiv,Kuantum,CC-BY/arXiv,Device/circuit simulations of silicon spin qubits based on a gate-all-around transistor
"We study a coarse-graining map arising from incomplete and imperfect addressing of particles in a multipartite quantum system. In its simplest form, corresponding to a two-qubit state, the resulting channel produces a convex mixture of the two partial traces. We derive the probability density of obtaining a given coarse-grained state, using geometric arguments for two qubits coarse-grained to one, and random-matrix methods for larger systems. As the number of qubits increases, the probability density sharply concentrates around the maximally mixed state, making nearly pure coarse-grained states increasingly unlikely. For two qubits, we also compute the inverse state needed to characterize the effective dynamics under coarse-graining and find that the average preimage of the maximally mixed state contains a finite singlet component. Finally, we validate the analytical predictions by inferring the underlying probabilities from Monte-Carlo-generated coarse-grained statistics.",0,arxiv,Kuantum,CC-BY/arXiv,Detecting quantum many-body states with imperfect measuring devices
"Time, space and entanglement are the main characters in this work. Their nature is still a great mystery in physics and we study here the possibility that these three phenomena are closely connected, showing how entanglement can be at the basis of the emergence of time and space within closed quantum systems. We revisit and extend the Page and Wootters theory that was originally introduced in order to describe the emergence of time through entanglement between subsystems in a globally static, quantum Universe. In the book, after providing a complete review of the salient aspects of the theory, we establish a connection with recent research on the foundations of statistical mechanics and we propose a new understanding of the thermalization process. Furthermore, we generalize the framework in order describe the spatial degree of freedom and we provide a model of 3+1 dimensional, quantum spacetime emerging from entanglement among different subsystems in a globally ""timeless"" and ""positionless"" Universe. Finally, via the Page and Wootters theory, the evolution of quantum clocks within a gravitational field is treated and a time dilation effect is obtained in agreement with the Schwarzschild solution.",0,arxiv,Kuantum,CC-BY/arXiv,On the Emergence of Time and Space in Closed Quantum Systems
"We present a theoretical investigation of ion-induced Coulomb explosion imaging (CEI) of pyridazine molecules driven by energetic C$^{5+}$ projectiles, using time-dependent density-functional theory (TDDFT) with Ehrenfest nuclear dynamics. By systematically varying the projectile's impact point and orientation relative to the molecular plane, we compare orthogonal and in-plane trajectories and quantify their effects on fragment momenta, electron-density response, and atom-resolved ionization. Newton plots and time-resolved density snapshots show that trajectories avoiding direct atomic collisions yield the most faithful structural reconstructions, whereas direct impacts impart large, highly localized momenta that distort the recovered geometry. Planar trajectories generate substantially greater ionization and broader momentum distributions than orthogonal ones due to deeper traversal through the molecular electron cloud. Quantitative analysis of electron removal at 10~fs confirms that projectile proximity and orientation strongly modulate both local and global ionization. These findings clarify how impact geometry governs the fidelity of ion-induced CEI structural recovery and help explain the variability and noise observed in experimental CEI measurements. More broadly, the results highlight both the strengths and the intrinsic limitations of ion-induced CEI and identify key considerations for interpreting experiments.",0,arxiv,Kuantum,CC-BY/arXiv,Ab initio study of highly charged ion-induced Coulomb explosion imaging
"Long-range moire patterns in twisted WSe2 enable a built-in, moire-length-scale ferroelectric polarization that can be directly harnessed in electronic devices. Such a built-in ferroic landscape offers a compelling means to enable ultralow-voltage and non-volatile electronic functionality in two-dimensional materials; however, achieving stable polarization control without charge trapping has remained a persistent challenge. Here, we demonstrate a moire-engineered ferroelectric field-effect transistor (FeFET) utilizing twisted WSe2 bilayers that leverages atomically clean van der Waals interfaces to achieve efficient polarization-channel coupling and trap-suppressed, ultralow-voltage operation (subthreshold swing of 64 mV per decade). The device exhibits a stable non-volatile memory window of 0.10 V and high mobility, exceeding the performance of previously reported two-dimensional FeFET and matching that of advanced silicon-based devices. In addition, capacitance-voltage spectroscopy, corroborated by self-consistent Landau-Ginzburg-Devonshire modeling, indicates ultrafast ferroelectric switching (~0.5 microseconds). These results establish moire-engineered ferroelectricity as a practical and scalable route toward ultraclean, low-power, and non-volatile 2D electronics, bridging atomistic lattice engineering with functional device architectures for next-generation memory and logic technologies.",0,arxiv,Kuantum,CC-BY/arXiv,"Moire-Engineered Ferroelectric Transistors for Nearly Trap-free, Low-Power and Non-Volatile 2D Electronics"
"Feedback uses past detection outcomes to dynamically modify a quantum system and is central to quantum control. These outcomes can be stored in a memory, defined as a stochastic function of past measurements. In this work, we investigate the main properties of a general memory function subject to arbitrary feedback dynamics. We show that the memory can be treated as a classical system coupled to the monitored quantum system, and that their joint evolution is described by a hybrid bipartite state. This framework allows us to introduce information-theoretic measures that quantify the correlations between the system and the memory. Furthermore, we develop a general framework to characterize the statistics of the memory -- such as moments, cumulants, and correlation functions -- which can be applied both to general feedback-control protocols and to monitored systems without feedback. As an application, we analyze feedback schemes based on detection events in a two-level system coupled to a thermal bath, focusing on protocols that stabilize either the excited-state population or Rabi oscillations against thermal dissipation.",0,arxiv,Kuantum,CC-BY/arXiv,Deterministic Equations for Feedback Control of Open Quantum Systems II: Properties of the memory function
"Quantum correlations often defy an explanation in terms of fundamental notions of classical physics, such as causality, locality, and realism. While the mathematical theory underpinning quantum correlations between spacelike separated systems has been well-established since the 1930s, the mathematical theory for correlations between non-spacelike separated systems is much less developed. In this work, we develop the theory of what we refer to as ""local-density operators"", which we view as joint states for possibly non-spacelike separated quantum systems. Local-density operators are unit trace operators whose marginals are genuine density operators, which we show not only subsumes the notion of density operator, but also several extensions of the notion of density operator into the spatiotemporal domain, such as pseudo-density operators and quantum states over time. More importantly, we prove a result which establishes a one-to-one correspondence between local-density operators and what we refer to as ""Dirac measures"", which are complex-valued measures on the space of separable projectors associated with two quantum systems. In the case that one of the systems is the trivial quantum system with a one-dimensional Hilbert space, our result recovers the fundamental result known as Gleason's Theorem, which implies that the Born rule from quantum theory is the only way in which one may assign probabilities to the outcomes of measurements performed on quantum systems in a non-contextual manner. As such, our results establish a direct generalization of Gleason's Theorem to measurements performed on possibly non-spacelike separated systems, thus extending the mathematical theory of quantum correlations across space to quantum correlations across space and time.",0,arxiv,Kuantum,CC-BY/arXiv,On Dirac-type correlations
"State-of-the-art superconducting qubits rely on a limited set of thin-film materials. Expanding their materials palette can improve performance, extend operating regimes, and introduce new functionalities, but conventional thin-film fabrication hinders systematic exploration of new material combinations. Van der Waals (vdW) materials offer a highly modular crystalline platform that facilitates such exploration while enabling gate-tunability, higher-temperature operation, and compact qubit geometries. Yet it remains unknown whether a fully vdW superconducting qubit can support quantum coherence and what mechanisms dominate loss at both low and elevated temperatures in such a device. Here we demonstrate quantum-coherent merged-element transmons made entirely from vdW Josephson junctions. These first-generation, fully crystalline qubits achieve microsecond lifetimes in an ultra-compact footprint without external shunt capacitors. Energy relaxation measurements, together with microwave characterization of vdW capacitors, point to dielectric loss as the dominant relaxation channel up to hundreds of millikelvin. These results establish vdW materials as a viable platform for compact superconducting quantum devices.",0,arxiv,Kuantum,CC-BY/arXiv,Coherent and compact van der Waals transmon qubits
"Controlled quantum mechanical motion of trapped atomic ions can be used to simulate and explore collective quantum phenomena and to process quantum information. Groups of cold atomic ions in an externally applied trapping potential self-organize into ""Coulomb crystals"" due to their mutual electrostatic repulsion. The motion of the ions in these crystals is strongly coupled, and the eigenmodes of motion all involve multiple ions. While this enables studies of many-body physics, it limits the flexibility and tunability of the system as a quantum platform. Here, we demonstrate an array of trapped ions in individual trapping sites whose motional modes can be controllably coupled and decoupled by tuning the local applied confining potential for each ion. We show that a single motional quantum, or phonon, can be coherently shared among two or three ions confined at the vertices of an equilateral triangle 30 $Î¼$m on a side. We can adiabatically tune the ion participation in the motional modes around a closed contour in configuration space, observing that the single-phonon wavefunction acquires a topological Berry phase if the contour encircles a conical intersection of motional eigenvalue surfaces. We observe this phase by single-phonon interference and study its breakdown as the motional mode tuning becomes non-adiabiatic. Our results show that precise, individual quantum control of ion motion in a two-dimensional array can provide unique access to quantum multi-body effects.",0,arxiv,Kuantum,CC-BY/arXiv,Observation of a Topological Berry Phase with a Single Phonon in an Ion Microtrap Array
"Compiling shallow and accurate quantum circuits for Hamiltonian simulation remains challenging due to hardware constraints and the combinatorial complexity of minimizing gate count and circuit depth. Existing optimization method pipelines rely on hand-engineered classical heuristics, which cannot learn input-dependent structure and therefore miss substantial opportunities for circuit reduction.   We introduce F2, an offline reinforcement learning framework that exploits free-fermionic structure to efficiently compile Trotter-based Hamiltonian simulation circuits. F2 provides (i) a reinforcement-learning environment over classically simulatable free-fermionic subroutines, (ii) architectural and objective-level inductive biases that stabilize long-horizon value learning, and (iii) a reversible synthetic-trajectory generation mechanism that consistently yields abundant, guaranteed-successful offline data.   Across benchmarks spanning lattice models, protein fragments, and crystalline materials (12-222 qubits), F2 reduces gate count by 47% and depth by 38% on average relative to strong baselines (Qiskit, Cirq/OpenFermion) while maintaining average errors of 10^(-7). These results show that aligning deep reinforcement learning with the algebraic structure of quantum dynamics enables substantial improvements in circuit synthesis, suggesting a promising direction for scalable, learning-based quantum compilation",0,arxiv,Kuantum,CC-BY/arXiv,F2: Offline Reinforcement Learning for Hamiltonian Simulation via Free-Fermionic Subroutine Compilation
"We present a classical and quantum analysis of a particle confined in a three-dimensional paraboloidal cavity formed by two confocal paraboloids. Classically, the system is integrable and presents three independent constants of motion, namely, the energy, the $z$-component of the angular momentum, and a third dynamical constant associated with the paraboloidal geometry, which can be derived from the separability of the Hamilton--Jacobi equation. We derive closed-form analytical expressions for the actions, which allow us to determine the two conditions to get periodic closed trajectories. We classify these trajectories through the indices $(s,t,\ell)$. The caustic paraboloids that bound the motion provide a complete geometric characterization of admissible trajectories. Quantum mechanically, separability of the SchrÃ¶dinger equation in parabolic coordinates yields eigenmodes described by Whittaker functions. We determine the energy spectrum and identify degeneracies arising not only from azimuthal symmetry but also from specific cavity deformations. A direct correspondence between classical trajectories and quantum eigenstates reveals that probability densities concentrate in the classically allowed region with controlled penetration into forbidden zones.",0,arxiv,Kuantum,CC-BY/arXiv,Classical and quantum dynamics of a particle confined in a paraboloidal cavity
"We study trade-off relations in information extraction from quantum systems subject to null-result weak measurements, where the absence of a detected photon continuously updates the system state. We present a detailed analysis of qubit and qutrit systems and investigate a general framework for a multilevel quantum system. We develop a dynamical characterization of null-result weak measurements that quantifies the information extracted over time, revealing the amount of the obtained information and also the rate of the information accumulation. The characterizations are obtained by examining the time-dependent evolution of the information theoretic quantities. More specifically, we consider Shannon entropy, mutual information, fidelity, and relative entropy to characterize the weak measurement dynamics. Our results provide an information theoretic analysis of the weak measurement process and highlight the dynamical nature of information extraction and reversibility in the weak measurement processes.",0,arxiv,Kuantum,CC-BY/arXiv,Information-Theoretic Analysis of Weak Measurements and Their Reversal
"Present theoretical predictions for the entanglement entropy through topological defects are violated by numerical simulations. In order to resolve this, we introduce a paradigm shift in the preparation of reduced density matrices in the presence of topological defects, and emphasize the role of defect networks with which they can be dressed. We consider the cases of grouplike and duality defects in detail for the Ising model, and match all numerically found entanglement entropies. Since our construction functions at the level of reduced density matrices, it accounts for topological defects beyond the entanglement entropy to other entanglement measures.",0,arxiv,Kuantum,CC-BY/arXiv,Entanglement Through Topological Defects: Reconciling Theory with Numerics
"Mid-circuit measurements and feedback operations conditioned on the measurement outcomes are essential for implementing quantum error-correction on quantum hardware. When integrated in quantum many-body dynamics, they can give rise to novel non-equilibrium phase transitions both at the level of each individual quantum trajectory and the averaged quantum channel. Experimentally resolving both transitions on realistic devices has been challenging due to limitations on the fidelity and the significant latency for performing mid-circuit measurements and feedback operations in real time. Here, we develop a superconducting quantum processor that enables global mid-circuit measurement with an average quantum non-demolition (QND) fidelity of 98.7% and fast conditional feedback with a 200 ns real-time decision latency. Using this platform, we demonstrate the coexistence of an absorbing-state transition in the quantum channel and a measurement-induced entanglement transition at the level of individual quantum trajectories. For the absorbing-state transition, we experimentally extract a set of critical exponents at the transition point, which is in excellent agreement with the directed percolation universality class. Crucially, the two transitions occur at distinct values of the tuning parameter. Our results demonstrate that adaptive quantum circuits provide a powerful platform for exploring non-equilibrium quantum many-body dynamics.",0,arxiv,Kuantum,CC-BY/arXiv,Measurement-and Feedback-Driven Non-Equilibrium Phase Transitions on a Quantum Processor
"Compared to traditional semiconductor control electronics (TSCE) located at room temperature, cryogenic single flux quantum (SFQ) electronics can provide qubit measurement and control alternatives that address critical issues related to scalability of cryogenic quantum processors. Single-qubit control and readout have been demonstrated recently using SFQ circuits coupled to superconducting qubits. Experiments where the SFQ electronics are co-located with the qubit have suffered from excess decoherence and loss due to quasiparticle poisoning of the qubit. A previous experiment by our group showed that moving the control electronics to the 3 K stage of the dilution refrigerator avoided this source of decoherence in a high-coherence 3D transmon geometry. In this paper, we also generate the pulses at the 3 K stage but have optimized the qubit design and control lines for scalable 2D transmon devices. We directly compare the qubit lifetime $T_1$, coherence time $T_2^*$ and gate fidelity when the qubit is controlled by the Josephson pulse generator (JPG) circuit versus the TSCE setup. We find agreement to within the daily fluctuations for $T_1$ and $T_2^*$, and agreement to within 10% for randomized benchmarking. We also performed interleaved randomized benchmarking on individual JPG gates demonstrating an average error per gate of $0.46$% showing good agreement with what is expected based on the qubit coherence and higher-state leakage. These results are an order of magnitude improvement in gate fidelity over our previous work and demonstrate that a Josephson microwave source operated at 3 K is a promising component for scalable qubit control.",0,arxiv,Kuantum,CC-BY/arXiv,Coherence-limited digital control of a superconducting qubit using a Josephson pulse generator at 3 K
"Building upon the Covariant Derivative Expansion, we develop a method to compute effective actions that is able to capture non-perturbative effects induced by strong background fields. We demonstrate the method in scalar QED, by deriving the full second-derivative corrections to the scalar Heisenberg--Euler effective action. The corresponding result is interpreted as an effective field theory with three characteristic scales, two of which are large (mass and field strength) in comparison with the remaining one (derivatives of the field). As an application, we show that, at this order, the transseries structure of the Schwinger pair production rate is preserved, even if the involved coefficients are modified. Our analysis also helps clarify recent disagreements concerning the coefficients of this effective action.",0,arxiv,Kuantum,CC-BY/arXiv,Strong-field regime within effective field theory
"Photons are bosons, and yet, when prepared in specific entangled states, they can exhibit non-bosonic behaviour. While this phenomenon has so far been studied in two-photon systems, exchange symmetries and interference effects in multi-photon scenarios remain largely unexplored. In this work, we show that multi-photon states uncover a rich landscape of exchange symmetries. With three photons already, multiple pairwise combinations are possible, where each pair of photons can exhibit either bosonic, fermionic, or anyonic exchange symmetry. This gives rise to mixed symmetry systems that are not possible to achieve with two photon alone. We experimentally investigate how these symmetry configurations manifest themselves in the observed interference of three photons. We show that multi-photon interference can be effectively turned on and off by tuning the symmetry of the constituent pairs. The possibility of accessing and tuning new quantum statistics in a scalable photonic platform not only deepens our understanding of quantum systems, but is also highly relevant for quantum technologies that rely on quantum interference.",0,arxiv,Kuantum,CC-BY/arXiv,Exchange Symmetry in Multiphoton Quantum Interference
"We report the experimental observation of vortex leapfrogging in a two-dimensional fluid of light. By imprinting two vortex-antivortex pairs and tracking their real-time evolution through phase-resolved imaging, we observe a dynamics that is accurately described by a point-vortex model with an outward background flow. By precisely controlling the initial vortex separation, we identify configurations in which leapfrogging breaks down and determine the corresponding dissipation mechanisms. The first originates from phase-slip events occurring at large injected velocities. The second arises when the injection of multi-charged vortices leads to the formation of a dispersive shock wave which acts as a continuous source of phase slippage. These mechanisms advance our understanding of vortex dynamics and dissipation in superfluids.",0,arxiv,Kuantum,CC-BY/arXiv,Vortex leapfrogging and superfluid dissipation mechanisms in a fluid of light
"In quantum technologies, quantum channels are essential elements for the transmission of quantum states. The action of a quantum channel usually introduces noise in the quantum state and thereby reduces the information contained in it. Concatenating a quantum channel with another quantum channel makes it more noisy and degrades its information and resource preservability. These are mathematically described by completely positive trace-preserving linear maps that represent the generic evolution of quantum systems. These are special cases of Hermitian-preserving trace-preserving linear maps. In this work, we demonstrate a physically meaningful way to compare a pair of quantum channels using Hermitian-preserving trace-preserving linear maps. More precisely, given a pair of quantum channels and an arbitrary unknown input state, we show that if the output state of one quantum channel from the pair can be obtained from the output statistics of the other channel from the pair using some quantum measurement, then the latter channel from the pair can be obtained from the former channel by concatenating it with a Hermitian-preserving trace-preserving linear map. This relation between these two channels is a preorder, and we try to study its characterization in this work. We also illustrate the implications of our results for the incompatibility of quantum devices through an example.",0,arxiv,Kuantum,CC-BY/arXiv,Comparing quantum channels using Hermitian-preserving trace-preserving linear maps: A physically meaningful approach
"We introduce two classes of lightweight, adaptive calibration protocols for quantum computers that leverage fast feedback. The first enables shot-by-shot updates to device parameters using measurement outcomes from simple, indefinite-outcome quantum circuits. This low-latency approach supports rapid tuning of one or more parameters in real time to mitigate drift. The second protocol updates parameters after collecting measurements from definite-outcome circuits (e.g.~syndrome extraction circuits for quantum error correction), balancing efficiency with classical control overheads. We use numerical simulations to demonstrate that both methods can calibrate 1- and 2-qubit gates rapidly and accurately even in the presence of decoherence, state preparation and measurement (SPAM) errors, and parameter drift. We propose and demonstrate effective adaptive strategies for tuning the hyperparameters of both protocols. Finally, we demonstrate the feasibility of real-time in-situ calibration of qubits performing quantum error correction, using only syndrome data, via numerical simulations of syndrome extraction in the [[5,1,3]] code.",0,arxiv,Kuantum,CC-BY/arXiv,Fast-feedback protocols for calibration and drift control in quantum computers
"We examine a criterion for relativistic covariance of nonlinear quantum field theory recently proposed by GPT-5 and published in Physics Letters B. We show that this criterion inadvertently tests a different property -- locality of the Hamiltonian -- and is insensitive to whether the theory is nonlinear. We recall the correct criterion, identified by Gisin and Polchinski thirty-five years ago, and reformulate their result in field-theoretic language.",0,arxiv,Kuantum,CC-BY/arXiv,Nonlinear Quantum Mechanics and Artificial Intelligence
"Qubit readout is a critical operation in quantum computing systems, which maps the analog response of qubits into discrete classical states. Deep neural networks (DNNs) have recently emerged as a promising solution to improve readout accuracy . Prior hardware implementations of DNN-based readout are resource-intensive and suffer from high inference latency, limiting their practical use in low-latency decoding and quantum error correction (QEC) loops. This paper proposes LUNA, a fast and efficient superconducting qubit readout accelerator that combines low-cost integrator-based preprocessing with Look-Up Table (LUT) based neural networks for classification. The architecture uses simple integrators for dimensionality reduction with minimal hardware overhead, and employs LogicNets (DNNs synthesized into LUT logic) to drastically reduce resource usage while enabling ultra-low-latency inference. We integrate this with a differential evolution based exploration and optimization framework to identify high-quality design points. Our results show up to a 10.95x reduction in area and 30% lower latency with little to no loss in fidelity compared to the state-of-the-art. LUNA enables scalable, low-footprint, and high-speed qubit readout, supporting the development of larger and more reliable quantum computing systems.",0,arxiv,Kuantum,CC-BY/arXiv,LUNA: LUT-Based Neural Architecture for Fast and Low-Cost Qubit Readout
"We investigate a variational Monte Carlo framework for trapped one-dimensional mixture of spin-$\frac{1}{2}$ fermions using Kolmogorov-Arnold networks (KANs) to construct universal neural-network wavefunction ansÃ¤tze. The method can, in principle, achieve arbitrary accuracy, limited only by the Monte Carlo sampling and was checked against exact results at sub-percent precision. For attractive interactions, it captures pairing effects, and in the impurity case it agrees with known results. We present a method of systematic transfer learning in the number of network parameters, allowing for efficient training for a target precision. We vastly increase the efficiency of the method by incorporating the short-distance behavior of the wavefunction into the ansÃ¤tz without biasing the method.",0,arxiv,Kuantum,CC-BY/arXiv,Trapped Fermions Through Kolmogorov-Arnold Wavefunctions
"Hybrid quantum systems harness the distinct advantages of different physical platforms, yet their integration is not always trivial due to potential incompatibilities in operational principles. Here, we theoretically propose and demonstrate a scheme for generating non-Gaussian mechanical states using a strongly driven hybrid system that combines cavity quantum electrodynamics (QED) and cavity optomechanics. Our protocol prepares a non-Gaussian cavity state in the dispersive regime of cavity QED and subsequently transfers it to a mechanical oscillator using the optomechanical interaction enhanced by a coherent cavity drive. While non-Gaussian cavity state control in cavity QED is well established in the dispersive regime, its behavior under strong cavity drive, essential for cavity optomechanics, remains largely unexplored. To bridge this gap, we develop an efficient simulation framework to model cavity QED dynamics in the high-photon-number regime. We show that a strong cavity drive can coherently displace the cavity state with minimal perturbations, effectively decoupling it from the qubit. The resulting large coherent cavity field enhances the optomechanical coupling strength, enabling high-fidelity transfer of non-Gaussian cavity states to the mechanical mode. These results reveal new dynamical features of driven cavity QED and open a pathway toward realizing non-Gaussian mechanical quantum memories and sensors.",0,arxiv,Kuantum,CC-BY/arXiv,Strongly driven cavity quantum electrodynamical-optomechanical hybrid system
"We propose an experimental apparatus to reveal the quantum coherence manifested in downward quantum jumps of amplitude bistability. The underlying coherent superposition of macroscopic quantum states is translated into the statistical properties of the integrated charge deposited in the detector circuit of a mode-matched heterodyne/homodyne detection scheme. At first, the dynamical evolution of a signal transmitted from an auxiliary cavity is employed to pinpoint a macroscopic switching event in a bistable main cavity subject to direct photodetection. Once the decision is made on the occurrence of a downward switch, the main cavity mode is let to freely decay to the vacuum, monitored to the production of an integrated charge. In the long-time limit, the charge distribution over an identical collection of pure states generated during the jumps converges to the Q function (heterodyne detection) or marginals of the Wigner function (homodyne detection) dictated by the phase of the local oscillator. When fluctuations over the ensemble step in, we connect the statistical properties of several switching events and the ensuing production of current records, to the cavity field correlations associated with the breakdown of photon blockade.",0,arxiv,Kuantum,CC-BY/arXiv,Statistical properties of quantum jumps between macroscopic states of light: reading an operational coherence record
"We propose and analyze a trapped-ion quantum simulator of the Jackiw-Rebbi model, a paradigmatic quantum field theory in (1+1) dimensions where solitonic excitations of a scalar field can bind fermionic zero modes leading to fractionally charged excitations. In our approach, the scalar field is a coarse-grained description of the planar zigzag ion displacements in the vicinity of a structural phase transition. The internal electronic states of the ions encode spins with interactions mediated by the transverse phonons and in-plane spin-phonon couplings with a zigzag pattern, which together correspond to a Yukawa-coupled Dirac field. Instead of assuming a fixed soliton background, we study the effect of back-reaction and quantum fluctuations on the coupled dynamics of the full fermion-boson system. We start by applying a Born-Oppenheimer approximation to obtain an effective Peierls-Nabarro potential for the topological kink, unveiling how the fermionic back-reaction can lead to localization of the kink. Beyond this limit, a truncated Wigner approximation combined with fermionic Gaussian states captures the quantum spreading and localization of a kink and kink-antikink scattering. Our results reveal how back-reaction and quantum fluctuations modify the stability and real-time evolution of fractionalized fermions, predicting experimentally accessible signatures in current trapped-ion architectures.",0,arxiv,Kuantum,CC-BY/arXiv,Real-time collisions of fractional charges in a trapped-ion Jackiw-Rebbi field theory
"Fault-tolerant quantum computing will require error rates far below those achievable with physical qubits. Quantum error correction (QEC) bridges this gap, but depends on decoders being simultaneously fast, accurate, and scalable. This combination of requirements has not yet been met by a machine-learning decoder, nor by any decoder for promising resource-efficient codes such as the colour code. Here we introduce AlphaQubit 2, a neural-network decoder that achieves near-optimal logical error rates for both surface and colour codes at large scales under realistic noise. For the colour code, it is orders of magnitude faster than other high-accuracy decoders. For the surface code, we demonstrate real-time decoding faster than 1 microsecond per cycle up to distance 11 on current commercial accelerators with better accuracy than leading real-time decoders. These results support the practical application of a wider class of promising QEC codes, and establish a credible path towards high-accuracy, real-time neural decoding at the scales required for fault-tolerant quantum computation.",0,arxiv,Kuantum,CC-BY/arXiv,A scalable and real-time neural decoder for topological quantum codes
"We consider the complex SYK model in the double-scaling limit. We obtain the transfer matrix for the grand canonical ensemble and symmetrize it. In the (n,Q)- basis of chord states, the grand canonical transfer matrix is block diagonal, where each block is the canonical transfer matrix for the respective charge sector. We therefore conclude that the Krylov complexity for the grand canonical ensemble is given by the sum of the complexities in the charge sectors weighted by a probability function that depends on the chemical potential. Finally, we compute the Krylov complexity analytically in the limit of early and late time in the charge sector and numerically for both canonical and grand canonical ensemble.",0,arxiv,Kuantum,CC-BY/arXiv,Grand Canonical vs Canonical Krylov Complexity in Double-Scaled Complex SYK Model
"We present a historical review of the development and impact of spontaneous parametric down-conversion (SPDC) in Brazil, marking over three decades since the first twin-photon experiments were performed in the country. This article traces the pioneering efforts that initiated the field, highlighting key experiments, institutions, and researchers who contributed to its growth. We discuss seminal works that established SPDC as a fundamental tool in the Brazilian Quantum Optics community, including studies on spatial correlations, entanglement, and decoherence. By presenting a curated sequence of experiments, we offer an overview of how Brazilian research in twin-photon systems has explored profound concepts through fundamental demonstrations, leading to significant international impact. This review also highlights the formation of a strong scientific community and its ongoing efforts to turn fundamental knowledge into quantum applications.",0,arxiv,Kuantum,CC-BY/arXiv,Brazilian Twin Photons 32nd anniversary
"We propose a versatile control protocol based on modulated zero-pulse-area fields that dynamically suppresses Rydberg excitation while retaining Rydberg-Rydberg interactions as an entangling phase resource. This mechanism enables single-step, perfectly entangling phase gates for arbitrary blockade strengths, eliminating finite-blockade errors even when the Rabi frequency approaches or exceeds the interaction energy. The approach defines a new operational regime for Rydberg-blockade quantum logic in which speed, fidelity, and robustness are achieved simultaneously within a simple dynamical framework. Owing to its simplicity and generality, the technique is compatible with a wide range of neutral-atom architectures and offers a promising route toward scalable, high-fidelity quantum computation and simulation.",0,arxiv,Kuantum,CC-BY/arXiv,Single-Operation Rydberg Phase Gates via Dynamic Population Suppression
"Recently, the possibility of high-temperature superconductivity (SC) in flat-band (FB) systems has been the focus of a great deal of activity. This study reveals that unlike conventional intra-band SC for which disorder has a dramatic impact, that associated with FBs is surprisingly robust to disorder-induced fluctuations and quasi-particle localization. In particular, for weak off-diagonal disorder, the critical temperature decreases linearly with disorder amplitude for conventional SC, whereas it is only quadratic in the case of SC in FBs. Our findings could have a major impact on the research and development of new compounds whose high purity will no longer be a critical barrier to their synthesis.",0,arxiv,Kuantum,CC-BY/arXiv,Robustness of flat band superconductivity against disorder in a two-dimensional Lieb lattice model
"Spectrum tomography for the energy ($E$) of a ring-shaped Bose-Hubbard circuit is illustrated. There is an inter-particle interaction $U$ that controls superfluidity (SF) and the transition to the Mott Insulator (MI) regime. The circuit is coupled to an electromagnetic cavity mode of frequency $Ï‰_0$, and the coupling is characterized by a generalized fine-structure-constant $Î±$ that controls the emergence of superconductivity (SC). The ${(U,Î±,Ï‰_0,E)}$ diagram features SF and SC regions, a vast region of fragmented possibly chaotic states, and an MI regime for large $U$. The mesoscopic version of the Meissner effect and the Anderson-Higgs mechanism are discussed.",0,arxiv,Kuantum,CC-BY/arXiv,Mesoscopic superfluid to superconductor transition
"We propose a scheme to achieve a nonreciprocal quantum battery (QB) in the non-Hermitian (NH) system, which can overcome the intrinsic dissipation and reverse flow constraints. The design is based on a charger and a battery, which are coherently coupled and jointly interact with a bad cavity. By introducing the auxiliary bad cavity and exploiting the nonreciprocal condition, this model can harness the environmental dissipation to suppress the reverse energy transfer. Under resonant conditions, we have achieved a four ratio of the battery energy to the charger energy; in contrast, this ratio is significantly reduced under large detuning. Through damping optimization, high efficiency of the short-time charging power is attained. In comparison to the fully nonreciprocal scheme, the QB operating at the exceptional point (EP) exhibits greater resilience to parameter fluctuations. These findings highlight the potential of NH quantum engineering for advancing QB technology, particularly in regimes involving directional energy transfer, controlled dissipation, and entropy management in open quantum systems.",0,arxiv,Kuantum,CC-BY/arXiv,Enhanced charging power in nonreciprocal quantum battery by reservoir engineering
"The increasing complexity of advanced semiconductor packages, driven by chiplet architectures and 2.5D/3D integration, challenges conventional failure localization methods such as lock-in thermography (LIT) and complicates current Failure Analysis (FA) workflows. Dense redistribution layers and buried interconnects limit the ability of established techniques to understand failure mechanisms non-destructively. In this work, we validate quantum diamond microscopy (QDM) based on nitrogen-vacancy (NV) centers in diamond as a non-destructive localization method through magnetic current path imaging at the package level. Using commercial Integrated Fan-Out Package-on- Package (InFO-PoP) devices from iPhones, we showcase a complete FA workflow that includes QDM to localize a short-type failure at an Integrated Passive Device (IPD) at the package backside. We showcase that the QDM results provide invaluable information on top of conventional techniques and can significantly enhance root-cause identification in package-level FA workflows. This work demonstrates the potential of QDM for broader integration into semiconductor chip and package analysis workflows.",0,arxiv,Kuantum,CC-BY/arXiv,Quantum Diamond Microscopy for Non-Destructive Failure Analysis of an Integrated Fan-Out Package-on-Package iPhone Chip
"This paper proposes an approach to interpreting quantum expectation values that may help address the quantum measurement problem. Quantum expectation values are usually calculated via Hilbert space inner products and, thereby, differently from expectation values in classical mechanics, which are weighted phase-space integrals. It is shown that, by using Anti-Wick quantization to associate dynamical variables with self-adjoint linear operators, quantum expectation values can be interpreted as genuine weighted averages over phase space, paralleling their classical counterparts. This interpretation arises naturally in the Segal-Bargmann space, where creation and annihilation operators act as simple multiplication and differentiation operators. In this setting, the Husimi Q-function - the coherent-state representation of the quantum state - can be seen as a true probability density in phase space. Unlike Bohmian mechanics, the present approach retains the standard correspondence between dynamical variables and self-adjoint operators while paving the way for a classical-like probabilistic interpretation of quantum statistics.",0,arxiv,Kuantum,CC-BY/arXiv,Sharp values for all dynamical variables via Anti-Wick quantization
"High-fidelity fluid simulations are central to understanding transport phenomena, yet resolving large or geometrically complex systems remains computationally prohibitive with existing methods. Here we introduce a tensor-network formulation of the lattice Boltzmann method based on matrix product states (MPS), commonly known as a quantum-inspired approach, enabling compressed representations of structured flow fields with inherent error control. We demonstrate the generality of the method on flows through structured media and complex vascular geometries, establishing for the first time that tensor-network techniques can efficiently resolve fluid dynamics in complex, irregular domains. We show that in the presence of translational or approximate symmetries of the geometry, fluid states exhibit low effective complexity in MPS form, yielding compression ratios exceeding two orders of magnitude while preserving physical structure and dynamical fidelity. This reduction enables systematic numerical exploration of regimes that were previously intractable. Our results position tensor networks as a scalable paradigm for continuum mechanics.",0,arxiv,Kuantum,CC-BY/arXiv,Tensor Network Fluid Simulations in Structured Domains Using the Lattice Boltzmann Method
"Single-photon sources are crucial for quantum information technologies. Here, we demonstrate a microwave single-photon source fabricated using a tantalum-based thin film, whose favorable material properties enable high-quality and stable photon emission. The antibunching behavior of the emitted radiation is revealed by second-order correlation measurements. Furthermore, traveling-wave parametric amplifiers are used as the pre-amplifier in the detection chains, we substantially improve the signal-to-noise ratio and thereby greatly reduce the acquisition time required for second-order correlation measurements. These results demonstrate the viability of tantalum-based superconducting devices as reliable platforms for microwave quantum photonics.",0,arxiv,Kuantum,CC-BY/arXiv,On-Demand Microwave Single-Photon Source Based on Tantalum Thin Film
"We present a framework for efficient extraction of the viscosity solutions of nonlinear Hamilton-Jacobi equations with convex Hamiltonians. These viscosity solutions play a central role in areas such as front propagation, mean-field games, optimal control, machine learning, and a direct application to the forced Burgers' equation. Our method is based on an entropy penalisation method proposed by Gomes and Valdinoci, which generalises the Cole-Hopf transform from quadratic to general convex Hamiltonians, allowing a reformulation of viscous Hamilton-Jacobi dynamics by a discrete-time linear dynamics which approximates a linear heat-like parabolic equation, and can also extend to continuous-time dynamics. This makes the method suitable for quantum simulation. The validity of these results hold for arbitrary nonlinearity that correspond to convex Hamiltonians, and for arbitrarily long times, thus obviating a chief obstacle in most quantum algorithms for nonlinear partial differential equations. We provide quantum algorithms, both analog and digital, for extracting pointwise values, gradients, minima, and function evaluations at the minimiser of the viscosity solution, without requiring nonlinear updates or full state reconstruction.",0,arxiv,Kuantum,CC-BY/arXiv,Quantum algorithms for viscosity solutions to nonlinear Hamilton-Jacobi equations based on an entropy penalisation method
"Quantum computing offers the promise of speedups for scientific computations, but its application to reacting flows is hindered by nonlinear source terms and the challenges of time-dependent simulations. We present a quantum framework to address these issues. We employ a probability density function (PDF) formulation to transform the nonlinear reacting-flow governing equations into high-dimensional linear ones. The entire temporal evolution is then solved as a single large linear system using the history state method, which avoids the measurement bottleneck of conventional time-marching schemes and fully leverages the advantages of quantum linear system algorithms. To extract the quantity of interest from the resulting quantum state, we develop an efficient algorithm to measure the statistical moments of the PDF, bypassing the need for costly full-state tomography. A computational complexity analysis indicates the potential for a near-exponential speedup over classical algorithms. We validate the framework by simulating a perfectly stirred reactor, demonstrating its capability to capture the PDF evolution and statistics of a nonlinear reactive system. This work establishes a pathway for applying quantum computing to nonlinear reacting flows.",0,arxiv,Kuantum,CC-BY/arXiv,Quantum computing of nonlinear reacting flows via the probability density function method
"We show that a quantum system undergoing motion with uniform relativistic velocity through a thermal bath consisting of a massless scalar field is generically driven into a non-equilibrium steady-state (NESS) solely due to its motion, even in the absence of external driving or multiple baths. The relative motion between the system and the bath breaks detailed balance, preventing thermalization to a Gibbs state. We find that the resulting steady-states fall into two distinct classes: (i) NESSs with persistent probability currents, and (ii) current-free non-Gibbs steady states characterized by a frequency-dependent effective inverse temperature. We demonstrate, using a three-level system, that NESSs with probability current can function as noisy stochastic clock, while current-free non-Gibbs steady states possess non-zero non-equilibrium free energy, indicating their potential as a quantum battery for work extraction or storage.",0,arxiv,Kuantum,CC-BY/arXiv,Uniform relativistic motion through a thermal bath as a thermodynamic resource
"We investigate the quantum Mpemba effect in the relaxation of open quantum systems whose effective dynamics is described by Davies maps. We present a class of unitary transformations based on permutation matrices that, acting on the initial state of the system, (i) suppress the contribution of slowest decaying modes of the nonunitary dynamics; (ii) ensure that it is as distinguishable as possible from the steady state. The first requirement guarantees an exponentially accelerating convergence to the steady state, while the second implies that a quantum system initially farther from equilibrium approaches it more rapidly than an initial state closer to it. This protocol provides a genuine Mpemba effect, and its numerical simulation requires low computational effort. We prove that, for any initial state, there always exists a permutation matrix that maximizes its distance from the equilibrium for a given information-theoretic distinguishability measure. We illustrate our findings for the nonunitary dynamics of the transverse field Ising chain and XXZ chain, each weakly coupled to a bosonic thermal bath, showing the quantum Mpemba effect captured by the Hilbert-Schmidt distance, quantum relative entropy, and trace distance. Our results provide a universal and versatile framework to engineer the genuine quantum Mpemba effect in open quantum systems.",0,arxiv,Kuantum,CC-BY/arXiv,Exponentially accelerated relaxation and quantum Mpemba effect in open quantum systems
"The scalability of quantum photonic integrated circuits opens the path towards large-scale quantum computing and communication. To date, this scalability has been limited by the stochastic nature of the quantum light sources. Moreover, hybrid integration of different platforms will likely be necessary to combine state-of-the-art devices into a functioning architecture. Here, we demonstrate the active alignment and edge-coupling of arrays of ten site-controlled gallium arsenide quantum dots to an array of ten silicon nitride single-mode waveguides, at cryogenic temperatures. The coupling is facilitated by the fabrication of nanopillars, deterministically self-aligned around each quantum dot, leading to a high-yield and regular array of single-photon sources. An on-chip beamsplitter verifies the triggered emission of single photons into the silicon nitride chip. The low inhomogeneous broadening of the ensemble enables us to observe the spectral overlap of adjacent site-controlled emitters. Across the array of waveguides, the signal collected from each coupled quantum dot is consistently and reproducibly 0.17 relative to the free-space collection from the very same single-photon source. Comparing measurement with waveguide simulations, we infer that absolute coupling efficiencies of $\approx 5 \%$ are currently obtained between our quantum dots and the waveguides.",0,arxiv,Kuantum,CC-BY/arXiv,Site-controlled quantum dot arrays edge-coupled to integrated silicon nitride waveguides and devices
"Synchronization transmission describes the emergence of coherence between two uncoupled oscillators mediated by their mutual coupling to an intermediate one. In classical star networks, such mediated coupling gives rise to remote synchronization--where nonadjacent leaf nodes synchronize through a nonsynchronous hub--and to explosive synchronization, characterized by an abrupt collective transition to coherence. In the quantum regime, analogous effects can arise from the interplay between 1:1 phase locking and 2:1 phase-locking blockade in coupled spin-1 oscillators. In this work, we investigate a star network composed of spin-1 oscillators. For identical oscillators, symmetric and asymmetric dissipation lead to distinct transmission behaviors: remote synchronization and quasi-explosive synchronization appear in different coupling regimes, a phenomenon absent in classical counterparts. For nonidentical networks, we find that at large detuning remote synchronization emerges in the weak-coupling regime and evolves into quasi-explosive synchronization as the coupling increases, consistent with classical star-network dynamics. These findings reveal the rich dynamical characteristics of mediated quantum synchronization and point toward new possibilities for exploring synchronization transmission in larger and more complex quantum systems.",0,arxiv,Kuantum,CC-BY/arXiv,Mediated Transmission of Quantum Synchronization in Star Networks
"Non-Hermitian (NH) systems have attracted great attention due to their exotic phenomena beyond Hermitian domains. Here we study the wave-packet dynamics in general one-dimensional NH lattices and uncover several unexpected phenomena. The group velocity of a wave packet during the time evolution in such NH lattices is not only governed by the real part of the band structure but also by its imaginary part. The momentum also evolves due to the imaginary part of the band structure, which can lead to a self-induced Bloch oscillation in the absence of external fields. Furthermore, we discover the wave-packet dynamics can exhibit disorder-free NH jumps even when the energy spectra are entirely real. Finally, we show that the NH jumps can lead to both positive and negative temporal Goos--HÃ¤nchen shifts at the edge.",0,arxiv,Kuantum,CC-BY/arXiv,Anomalous Wave-Packet Dynamics in One-Dimensional Non-Hermitian Lattices
"Layered architectures for the Quantum Internet have been proposed, inspired by that of the classical Internet, which has demonstrated high maintainability even in large-scale systems. While lower layers in the Quantum Internet, such as entanglement generation and distribution, have been extensively studied, the application layer - responsible for translating user requests into executable quantum-network operations - remains largely unexplored. A significant challenge is translating application-level requests into the concrete instructions executable at lower layers. In this work, we introduce a RuleSet-based framework that explicitly incorporates the application layer into the layered architecture of the Quantum Internet. Our framework builds on a RuleSet-based protocol, clarifying communication procedures, organizing application request information, and introducing new Rules for application execution by embedding application specifications into RuleSets. To evaluate feasibility, we constructed state machines from the generated RuleSets. This approach enables a transparent integration from the application layer down to the physical layer, thereby lowering barriers to deploying new applications on the Quantum Internet.",0,arxiv,Kuantum,CC-BY/arXiv,RuleSet Generation Framework for Application Layer Integration in Quantum Internet
"We assess the possibilities offered by Hilbert space fundamentalism, an attitude towards quantum physics according to which all physical structures (e.g. subsystems, locality, spacetime, preferred observables) should emerge from minimal quantum ingredients (typically a Hilbert space, Hamiltonian, and state). As a case study, we first mainly focus on the specific question of whether the Hamiltonian can uniquely determine a tensor product structure, a crucial challenge in the growing field of quantum mereology. The present paper reviews, clarifies, and critically examines two apparently conflicting theorems by Cotler et al. and Stoica. We resolve the tension, show how the former has been widely misinterpreted and why the latter is correct only in some weaker version. We then propose a correct mathematical way to address the general problem of preferred structures in quantum theory, relative to the characterization of emergent objects by unitary-invariant properties. Finally, we apply this formalism in the particular case we started with, and show that a Hamiltonian and a state are enough structure to uniquely select a preferred tensor product structure.",0,arxiv,Kuantum,CC-BY/arXiv,On the emergence of preferred structures in quantum theory
"We propose a scheme to achieve efficient frequency conversion for a single photon propagating in a 1D conventional waveguide by exploiting the quantum interference induced by the scale of a V-type giant atom (GA) characterized by the distance between the two coupling points as well as single-photon transition pathways originated from the coupling between the GA and the resonator. The presence of photons in the resonator triggers the frequency conversion of photons. The scattering spectra and the conversion contrast are studied in both the Markovian and the non-Markovian regimes. The disappearance of frequency conversion is rooted in the complete suppression of the emission from the excited state to either of lower states in the $n+1$ subspace where $n$ is the photon number of the resonator, and the non-Markovicity-induced nonreciprocity is found under specific conditions. Altering the photon number $n$ induces the non-reciprocal transmission of single photons in the waveguide, hence, enhance the conversion probability.",0,arxiv,Kuantum,CC-BY/arXiv,Resonator-assisted single-photon frequency convertion in a conventional waveguide with a giant V-type atom
"Quantum optimisation is emerging as a promising approach alongside classical heuristics and specialised hardware, yet its performance is often difficult to assess fairly. Traditional benchmarking methods, rooted in digital complexity theory, do not directly capture the continuous dynamics, probabilistic outcomes, and workflow overheads of quantum and hybrid systems. This paper proposes principles and protocols for fair benchmarking of quantum optimisation, emphasising end-to-end workflows, transparency in tuning and reporting, problem diversity, and avoidance of speculative claims. By extending lessons from classical benchmarking and incorporating application-driven and energy-aware metrics, we outline a framework that enables practitioners to evaluate quantum methods responsibly, ensuring reproducibility, comparability, and trust in reported results.",0,arxiv,Kuantum,CC-BY/arXiv,Fair Benchmarking of Optimisation Applications
"Transition metal doping is commonly used for altering the properties of solid-state materials to suit applications in science and technology. Partially filled $d$-shells of transition metal atoms lead to electronic states with diverse spatial and spin symmetries. Chromium(III) cations have shown great potential for designing laser materials and, more recently, for developing spin qubits in quantum applications. They also represent an intriguing class of chemical systems with strongly correlated multi-reference excited states, due to the $d^3$ electron configuration. These states are difficult to describe accurately using single-reference quantum chemical methods such as density functional theory (DFT), the most commonly used method to study the electronic structures of solid-state systems. Recently, the periodic effective Hamiltonian of crystal field (pEHCF) method has been shown to overcome some limitations arising in the calculations of excited $d$-states. In this work, we assess the suitability of DFT and pEHCF to calculate the electronic structure and $d$-$d$ excitations of chromium(III) dopants in wide band gap host materials. The results will aid computational development of novel transition metal-doped materials and provide a deeper understanding of the complex nature of transition metal dopants in solids.",0,arxiv,Kuantum,CC-BY/arXiv,Multiplet structure of chromium(III) dopants in wide band gap materials
"Memory effects arise in many complex systems, from protein folding, to the spreading of epidemics and financial decisions. While so-called non-Markovian dynamics is common in larger systems with interacting components, observations in fundamental physical systems have been confined to specifically engineered cases. Here, we report the experimental observation of non-Markovian dynamics in an elemental material, crystalline cobalt. By driving this material with an intense terahertz electromagnetic field, we bring its magnetisation into a non-equilibrium state and follow its evolution. We measure the sample's low temperature magnetic response in the time domain which leads to an unexpectedly rich multi-peaked spectrum in the Fourier domain, that cannot be explained by established models. We use open quantum system theory, which predicts a non-Markovian memory kernel in the dynamical equations to capture the fundamental interaction between the spin system and the phonon bath. Simulations based on this theory produce a multi-peaked spectrum, which matches the measured one. Our non-Markovian approach is also able to reproduce the modification of the spectrum at higher temperatures. Our findings demonstrate that non-Markovian effects are observable at a much more fundamental level than previously thought, opening the door to their exploration and control in a broad range of condensed matter systems.",0,arxiv,Kuantum,CC-BY/arXiv,Intrinsic non-Markovian magnetisation dynamics
"Nitrogen-vacancy color centers in diamond have proven themselves as a good, sensitive element for the measurement of magnetic fields. While the mainstream of magnetometers based on NV centers uses so-called optically detected magnetic resonance, there has recently been a suggestion to use dispersive readout of a dielectric cavity to enhance the sensitivity of magnetometers. Here, we demonstrate that the dispersive readout approach can be significantly improved if a two-channel scheme is considered.",0,arxiv,Kuantum,CC-BY/arXiv,Dispersive readout with two orthogonal modes of a dielectric cavity
"This study explores the energy storage dynamics of a quantum battery (QB) modeled using a dipolar spin system with Dzyaloshinskii-Moriya (DM) interaction. We examine the performance of this system in terms of ergotropy, instantaneous power, capacity, and quantum coherence using a two-qubit model. By solving the system's time evolution under cyclic unitary processes, we analyze how external parameters such as temperature, magnetic field, and DM interaction influence the charging behavior and quantum resources of the battery. The findings demonstrate that quantum coherence and DM interaction significantly enhance the energy storage efficiency and power output of the quantum battery, offering promising strategies for designing high-performance quantum energy storage devices. Furthermore, we investigate the performance of quantum battery under the influence of a common dephasing environment, which limits the long-term work-extraction capability of dipolar quantum batteries.",0,arxiv,Kuantum,CC-BY/arXiv,Tunable Dynamics of a Dipolar Quantum Battery: Role of Spin-Spin Interactions and Coherence
"We present a high-speed continuous-variable quantum random number generator (QRNG) based on heterodyne detection of vacuum fluctuations. The scheme follows a source-device-independent (SDI) security model in which the entropy originates from quantum measurement uncertainty and no model of the source is required; security depends only on the trusted measurement device and the calibrated discretization, and thus remains valid even under adversarial state preparation. The optical field is split by a 90$^\circ$ optical hybrid and measured by two balanced photodiodes to obtain both quadratures of the vacuum state simultaneously. The analog outputs are digitized using a dual-channel 12-bit analog-to-digital converter operating at a sampling rate of 3.2 GS/s per channel, and processed in real time by an FPGA implementing Toeplitz hashing for randomness extraction. The quantum-to-classical noise ratio was verified through calibrated power spectral density measurements and cross-checked in the time domain, confirming vacuum-noise dominance within the 1.6 GHz detection bandwidth. After extraction, the system achieves a sustained generation rate of $R_{\rm net}= 33.92~\mathrm{Gbit/s}$ of uniformly distributed random bits, which pass all NIST and Dieharder statistical tests. The demonstrated platform provides a compact, FPGA-based realization of a practical heterodyne continuous-variable source-independent QRNG suitable for high-rate quantum communication and secure key distribution systems.",0,arxiv,Kuantum,CC-BY/arXiv,33 Gbit/s source-device-independent quantum random number generator based on heterodyne detection with real-time FPGA-integrated extraction
"We present a framework for effectively simulating the execution of quantum circuits originally designed to demonstrate quantum supremacy using accessible high-performance computing (HPC) infrastructure. Building on prior CPU-only approaches, our pipeline combines a single NVIDIA A100 GPU for quantum state construction, followed by N parallel CPU jobs that perform distributed measurement sampling. We validate the fidelity by simulating the 53-qubit, 14-cycle Sycamore circuit and achieving a linear cross-entropy benchmarking (XEB) score of 0.549, exceeding the published XEB score of 0.002 from Google's reference data. We then evaluate execution time performance with the more complex 53-qubit, 20-cycle circuit, completing the full 2.5 million-shot workload over 100 CPU jobs in 01:15:36, representing a 6.95 x 10^7 speedup compared to Google's original classical estimate. Further, we show that if 1,000 CPU jobs were employed, the estimated duration would be approximately 00:17:35, only 12 minutes slower than the time taken by the original QPU-based experiment. These results illustrate that 'quantum supremacy' is not fixed and continues to be a moving target. In addition, hybrid classical-quantum strategies may provide broader near-term quantum utility than once thought.",0,arxiv,Kuantum,CC-BY/arXiv,Revisiting Quantum Supremacy: Simulating Sycamore-Class Circuits Using Hybrid CPU/GPU HPC Workloads
"Identification of individual cells within heterogeneous populations is essential for biomedical research and clinical diagnostics. Conventional labeling-based sorting methods, such as fluorescence-activated cell sorting and magnetic-activated cell sorting, enable precise sorting when reliable markers are available. However, their applicability is limited in cells lacking defined markers or sensitive to labeling, as labeling can compromise cellular viability and function. We present a single-cell identification approach using quantum-enhanced NMR with diamond nitrogen-vacancy centers for label-free detection of intracellular proton ($^1$H) signals. Using this method, we distinguish two human tumor cell lines by their proton spin-lattice ($T_1$) relaxation times, which serve as a cell-intrinsic physicochemical signature. It lays the groundwork for label-free sorting applications in rare cell analysis, personalized medicine, and single-cell diagnostics.",0,arxiv,Kuantum,CC-BY/arXiv,Single-cell identification with quantum-enhanced nuclear magnetic resonance
"Stabilizer simulation of Clifford quantum circuits - error-correction circuits, Clifford subroutines, etc. - on classical computers has played a central role in our understanding of circuit performance. The stabilizer description, however, restricts the accessible noise one can incorporate into the simulation to Pauli-type noise. More general noise, including coherent errors, may have more severe impact on circuit performance than Pauli noise; yet, such general noise have been difficult to access, much less investigate fully, in numerical simulations. Here, through the use of stratified importance sampling, we show how general noise can be simulated within the stabilizer formalism in reasonable time, with non-unitary noise being nearly as cheap as Pauli noise. Unitary (or coherent) noise can require an order of magnitude more time for the simulation, but nevertheless completes in very reasonable times, a drastic improvement over past approaches that typically fail to converge altogether. Our work thus enables detailed beyond-Pauli understanding of circuit performance in the presence of real device noise, which is rarely Pauli in nature. Among other examples, we present direct simulation results for the performance of the popular rotated planar surface codes under circuit-level general noise, previously available only in limited situations and/or through mappings to efficiently simulatable physical models.",0,arxiv,Kuantum,CC-BY/arXiv,Simulating general noise nearly as cheaply as Pauli noise
"We explore the relation between quantum geometry in non-Hermitian systems and physically measurable phenomena. We highlight various situations in which the behavior of a non-Hermitian system is best understood in terms of quantum geometry, namely the notion of adiabatic potentials in non-Hermitian systems and the localization of Wannier states in periodic non-Hermitian systems. Further, we show that the non-Hermitian quantum metric appears in the response of the system upon time-periodic modulation, which one can use to experimentally measure the non-Hermitian quantum metric. We validate our results by providing numerical simulations of concrete exemplary systems.",0,arxiv,Kuantum,CC-BY/arXiv,Quantum geometrical effects in non-Hermitian systems
"Among all of the non-Hermitian large-tridiagonal-matrix quantum Hamiltonians we choose a subclass with the structure resembling the ``benchmark'' realistic Bose-Hubbard model. We demonstrate that this choice can be declared user-friendly in the sense that the underlying singular values can be specified via a ``Hermitized'' SchrÃ¶dinger-like equation. In particular, the related ``Hermitized'' Green's functions is shown given the two alternative compact and numerically efficient matrix continued fraction forms.",0,arxiv,Kuantum,CC-BY/arXiv,Non-Hermitian Bose-Hubbard-like quantum models
"Recent progress in open many-body quantum systems has highlighted the importance of the Markov length, the characteristic scale over which conditional correlations decay. It has been proposed that non-equilibrium phases of matter can be defined as equivalence classes of states connected by short-time evolution while maintaining a finite Markov length, a notion called local reversibility. A natural question is whether well-known classical models of non-equilibrium criticality fit within this framework. Here we investigate the Domany--Kinzel model -- which exhibits an active phase and an absorbing phase separated by a 1+1-D directed-percolation transition -- from this information-theoretic perspective. Using tensor network simulations, we provide evidence for local reversibility within the active phase. Notably, the Markov length diverges upon approaching the critical point, unlike classical equilibrium transitions where Markov length is zero due to their Gibbs character. Correspondingly, the conditional mutual information exhibits scaling consistent with directed percolation universality. Further, we analytically study the case of 1+1-D compact directed percolation, where the Markov length diverges throughout the phase diagram due to spontaneous breaking of domain-wall parity symmetry from strong to weak. Nevertheless, the conditional mutual information continues to faithfully detect the corresponding phase transition.",0,arxiv,Kuantum,CC-BY/arXiv,Local Reversibility and Divergent Markov Length in 1+1-D Directed Percolation
"Coherent Ising Machines (CIMs) have emerged as a hybrid form of quantum computing devices designed to solve NP-complete problems, offering an exciting opportunity for discovering optimal solutions. Despite challenges such as susceptibility to noise-induced local minima, we achieved notable advantages in improving the computational accuracy and stability of CIMs. We conducted a successful experimental demonstration of CIM via femto-second laser pumping that integrates optimization strategies across optical and structural dimensions, resulting in significant performance enhancements. The results are particularly promising. An average success rate of 55% was achieved to identify optimal solutions within a Mobius Ladder graph comprising 100 vertices. Compared with other alternatives, the femto-second pulse results in significantly higher peak power, leading to more pronounced quantum effects and lower pump power in optical fiber based CIMs. In addition, we have maintained an impressive success rate for a continuous period of 8 hours, emphasizing the practical applicability of CIMs in real-world scenarios. Furthermore, our research extends to the application of these principles in practical applications such as molecular docking and credit scoring. The results presented substantiate the theoretical promise of CIMs, paving the way for their integration into large-scale practical applications.",0,arxiv,Kuantum,CC-BY/arXiv,A versatile coherent Ising computing platform
"We investigate complex self-testing, a generalization of standard self-testing that accounts for quantum strategies whose statistics is indistinguishable from their complex conjugate's. We show that many structural results from standard self-testing extend to the complex setting, including lifting of common assumptions. Our main result is an operator-algebraic characterization: complex self-testing is equivalent to uniqueness of the real parts of higher moments, leading to a basis-independent formulation in terms of real C* algebras. This leads to a classification of non-local strategies, and a tight boundary where standard self-testing do not apply and complex self-testing is necessary. We further construct a strategy involving quaternions, establishing the first standard self-test for genuinely complex strategy. Our work clarifies the structure of complex self-testing and highlights the subtle role of complex numbers in bipartite Bell non-locality.",0,arxiv,Kuantum,CC-BY/arXiv,Beyond real: Investigating the role of complex numbers in self-testing
"Spin qubits in silicon quantum dot arrays are a promising quantum computation platform for long-term scalability due to their small qubit footprint and compatibility with advanced semiconductor manufacturing. However, spin qubit devices face a key architectural bottleneck: the large physical footprint of readout components relative to qubits prevents a dense layout where all qubits can be measured simultaneously, complicating the implementation of quantum error correction. This challenge is offset by the platform's unique rapid shuttling capability, which can be used to transport qubits to distant readout ports. In this work, we explore the design constraints and capabilities of spin qubits in silicon and propose the SNAQ (Shuttling-capable Narrow Array of spin Qubits) surface code architecture, which relaxes the 1:1 readout-to-qubit assumption by leveraging spin shuttling to time-multiplex ancilla qubit initialization and readout. Our analysis shows that, given sufficiently high (experimentally demonstrated) qubit coherence times, SNAQ delivers an orders-of-magnitude reduction in chip area per logical qubit. Additionally, by using a denser grid of physical qubits, SNAQ enables fast transversal logic for short-distance logical operations, achieving 4.0-22.3x improvement in local logical clock speed while still supporting global operations via lattice surgery. This translates to a 57-60% reduction in spacetime cost of 15-to-1 magic state distillation, a key fault-tolerant subroutine. Our work pinpoints critical hardware metrics and provides a compelling path toward high-performance fault-tolerant computation on near-term-manufacturable spin qubit arrays.",0,arxiv,Kuantum,CC-BY/arXiv,A manufacturable surface code architecture for spin qubits with fast transversal logic
"Quantum supremacy has been explored extensively in gate-model settings. Here, we introduce a quantum-supremacy framework for a hybrid digital-analog-digital quantum computing (DADQC) model. We consider a device that applies an initial layer of single-qubit gates, a single transverse-field Ising analog block, and a final single-qubit layer before $Z$-basis readout. The analog block approximates $Z$-diagonal Ising evolution, and we prove that the resulting output distribution is within constant total-variation (TV) distance of an Instantaneous Quantum Polynomial-time (IQP) circuit. Our bounds and constructions are established for fully connected as well as bounded-degree hardware graphs, matching a variety of architectures, including trapped-ion, neutral atom, and superconducting platforms. Assuming anticoncentration (which we prove for all-to-all hardware graphs and conjecture for bounded-degree hardware graphs) and an average-case hardness conjecture for the associated complex-temperature Ising partition functions, standard reductions imply that any efficient classical sampler achieving constant TV error collapses the polynomial hierarchy. Our results imply that quantum-supremacy tests are possible on today's quantum annealers, as well as other devices capable of hybrid digital-analog quantum evolution.",0,arxiv,Kuantum,CC-BY/arXiv,Digital-Analog-Digital Quantum Supremacy
"Quantum satellite networks offer a promising solution for achieving long-distance quantum communication by enabling entanglement distribution across global scales. This work formulates and solves the quantum satellite network scheduling problem by optimizing satellite-to-ground station pair assignments under realistic system and environmental constraints. Our framework accounts for limited satellite and ground station resources, fairness, entanglement fidelity thresholds, and real world non-idealities including atmospheric losses, weather and background noise. In addition, we incorporate the complexities of multi-satellite relays enabled via inter-satellite links. We propose an integer linear programming (ILP) based optimization framework that supports multiple scheduling objectives, allowing us to analyze tradeoffs between maximizing total entanglement distribution rate and ensuring fairness across ground station pairs. Our framework can also be used as a benchmark tool to measure the performance of other potential transmission scheduling policies.",0,arxiv,Kuantum,CC-BY/arXiv,Scheduling in Quantum Satellite Networks: Fairness and Performance Optimization
"This article suggests that thinking about the role of reference frames can provide new insight into Extended Wigner's Friend scenarios. This involves appealing to symmetries to make a principled distinction between properties of a system which are meaningful only relative to an external reference system and properties which are meaningful without further relativization. Thus we may propose that there are always well-defined facts about what observers have observed, but there are not necessarily well-defined facts about the relations between their reference frames, so there will not always exist a joint distribution over their outcomes which can meaningfully be compared to the predictions of quantum mechanics. In addition, this approach also offers a general argument against the idea that there should be a regress of relativization.",0,arxiv,Kuantum,CC-BY/arXiv,Wigner's Frame
"Microscopic inhomogeneity within superconducting films is a critical bottleneck hindering the performance and scalability of quantum circuits. All-nitride Josephson Junctions (JJs) have attracted substantial attention for their potential to provide enhanced coherence times and enable higher temperature operation. However, their performance is often limited by local variations caused by polymorphism, impurities, and interface quality. This work diagnoses atomic-scale limitations preventing superconducting NbN/AlN/NbN JJs from reaching their full potential. Electrical measurements reveal suppressed critical current density and soft onset of quasiparticle current. However, inverse proportionality between resistance and junction area confirms homogenous barrier thickness. This isolates structural and chemical variations in electrodes and barrier as the source of performance limitation. The observed characteristics are attributed to complex materials problems: NbN polymorphism, phase coexistence, and oxygen impurities. Using advanced microscopy and machine learning integrated approach, nanoscale inclusions of epsilon-Nb2N2 are found to coexist within dominant delta-NbN electrodes. DC performance of JJs may be affected by these defects, leading to unresolved supercurrent and soft transition to normal state. By identifying specific atomic scale defects, tracing its origin to initial film nucleation, and linking to its detrimental electrical signature, this work establishes a material-to-device correlation and provides targeted strategy for phase engineering towards reproducible, high coherence and scalable quantum devices.",0,arxiv,Kuantum,CC-BY/arXiv,Hidden Structural Variants in ALD NbN Superconducting Trilayers Revealed by Atomistic Analysis
"Typical stabilizer codes aim to solve the general problem of fault-tolerance without regard for the structure of a specific system. By incorporating a broader representation-theoretic perspective, we provide a generalized framework that allows the code designer to take this structure into account. For any representation of a finite group, we produce a quantum code with a code space invariant under the group action, providing passive error mitigation against errors belonging to the image of the representation. Furthermore, errors outside this scope are detected and diagnosed by performing a projective measurement onto the isotypic components corresponding to irreducible representations of the chosen group, effectively generalizing syndrome extraction to symmetry-resolved quantum measurements. We show that all stabilizer codes are a special case of this construction, including qudit stabilizer codes, and show that there is a natural one logical qubit code associated to the dihedral group. Thus we provide a unifying framework for existing codes while simultaneously facilitating symmetry-aware codes tailored to specific systems.",0,arxiv,Kuantum,CC-BY/arXiv,Symmetry-Based Quantum Codes Beyond the Pauli Group
"We propose a decoder for quantum low density parity check (LDPC) codes based on a beam search heuristic guided by belief propagation (BP). Our beam search decoder applies to all quantum LDPC codes and achieves different speed-accuracy tradeoffs by tuning its parameters such as the beam width. We perform numerical simulations under circuit level noise for the $[[144, 12, 12]]$ bivariate bicycle (BB) code at noise rate $p=10^{-3}$ to estimate the logical error rate and the 99.9 percentile runtime and we compare with the BP-OSD decoder which has been the default quantum LDPC decoder for the past six years. A variant of our beam search decoder with a beam width of 64 achieves a $17\times$ reduction in logical error rate. With a beam width of 8, we reach the same logical error rate as BP-OSD with a $26.2\times$ reduction in the 99.9 percentile runtime. We identify the beam search decoder with beam width of 32 as a promising candidate for trapped ion architectures because it achieves a $5.6\times$ reduction in logical error rate with a 99.9 percentile runtime per syndrome extraction round below 1ms at $p=5 \times10^{-4}$. Remarkably, this is achieved in software on a single core, without any parallelization or specialized hardware (FPGA, ASIC), suggesting one might only need three 32-core CPUs to decode a trapped ion quantum computer with 1000 logical qubits.",0,arxiv,Kuantum,CC-BY/arXiv,Beam search decoder for quantum LDPC codes
"We investigate the short-term temporal dynamics of superradiance in closely spaced quantum emitters. Building on Dicke's 1954 framework, we analyze the sequential emergence of coherence, superradiance, and entanglement, revealing a distinct temporal hierarchy in their extremal values: relative coherence develops first, followed by the peak of correlated emission, then minimal entanglement, and finally correlated dephasing. These findings suggest that enhanced relative coherence initiates correlated emission and when correlated dephasing is negligible, entanglement and correlated emission become tightly linked in time.",0,arxiv,Kuantum,CC-BY/arXiv,"Timing quantum emission: coherence, superradiance, and entanglement in order"
"Many problems in physical chemistry involve systems that are coupled to an environment, such as a molecule interacting with an adjacent surface, possibly resulting in meta-stable molecular states where electron density is transferred to the surface. Such systems can be described by non-Hermitian quantum mechanics (NHQM), where the Hamiltonian includes dissipative terms. Within NHQM, one can also formulate the Hartree-Fock (HF) and Kohn-Sham (KS) methods and, as in the conventional theory, an effective independent-particle picture is employed. The crucial observation of the present work is that even for systems that are not coupled to an environment, in the HF or KS equation a single electron is coupled to a bath of the remaining electrons which can act as an environment, opening up the possibility for the exchange of current density between the one-electron and the remaining N-1 electron system. The corresponding self-consistent states represent a new uncharted space of solutions to the HF and KS equations. We show that the additional solutions can have a physical interpretation and thus extend the range of problems HF and KS can be applied to. If open-system HF and KS calculations are performed, the new class of solutions is always encountered but this has also not been noted previously.",0,arxiv,Kuantum,CC-BY/arXiv,The uncharted space of non-Hermitian solutions to the Hartree-Fock and Kohn-Sham equations
"How classical chaos emerges from quantum mechanics remains a central open question, as the unitary evolution of isolated quantum systems forbids exponential sensitivity to initial conditions. A key insight is that this quantum-classical link is provided by measurement processes. In this work, we identify gain competition in a chaotic photon gas as an operational quantum measurement that selects single motional modes from an initial superposition through stochastic, nonlinear amplification. We show that this mechanism naturally gives rise to classical chaotic behavior, most notably sensitivity to initial conditions. Our results provide a concrete physical mechanism for the quantum-classical transition in a chaotic system and demonstrate that essential aspects of quantum measurement-state projection, Born-rule-like selection, and irreversibility-can naturally emerge from intrinsic gain dynamics.",0,arxiv,Kuantum,CC-BY/arXiv,From Quantum Chaos to Classical Chaos via Gain-Induced Measurement Dynamics in a Photon Gas
"The fine-structure constant alpha approximately 1/137 is traditionally regarded as a fundamental dimensionless parameter. I argue instead that alpha is a scaled quantity that arises only where the structural scales contributed by classical electromagnetism (e), quantum mechanics (h-bar), and special relativity (c) intersect. None of these theories, taken individually, supplies the independent scales required to define alpha. The constant first appears when relativistic corrections are added to the Schrodinger-Bohr description of hydrogen (Sommerfeld), and it becomes the structural coupling in quantum electrodynamics, where quantum and relativistic effects modify the classical electromagnetic interaction. Expressing the governing laws in canonical form reveals this dependence and eliminates representational artifacts that make alpha appear fundamental. The running of alpha in QED further demonstrates its status as a scale-dependent coupling rather than a universal constant. I conclude that alpha is a domain-specific structural ratio reflecting contingent relationships among independent physical scales.",0,arxiv,Kuantum,CC-BY/arXiv,The Fine-Structure Constant as a Scaled Quantity
"In this paper, we employ a semiperturbative theory to study the statistical structural properties of energy eigenfunctions (EFs) in many-body quantum chaotic systems consisting of a central system coupled to an environment. Under certain assumptions, we derive both the average shape and the statistical fluctuations of EFs on the basis formed by the direct product of the energy eigenbases of the system and the environment. Furthermore, we apply our results to two fundamental questions: (i) the properties of the reduced density matrix of the central system in an eigenstate, and (ii) the structure of the off-diagonal smooth function within the framework of the eigenstate thermalization hypothesis. Numerical results are also presented in support of our main findings.",0,arxiv,Kuantum,CC-BY/arXiv,Statistical structural properties of many-body chaotic eigenfunctions and applications
"de Broglie-Bohm theory (dBBT), treating quantum particles as point objects moving along well defined (Bohmian) trajectories, offers an appealing solution of the measurement problem in quantum mechanics; it has, however, problems relating to spin, relativity and lack of proper integration with the Hilbert space based framework. In this work, we present a consistent formalism which has the traditional state-observable framework integrated with the desirable features of dBBT. We adopt ensemble interpretation for the Schrodinger wave function $Ïˆ$. Given a Schrodinger wave function $Ïˆ$, we use its value $Ïˆ_0$ at some fixed time (say, $t = 0$) to define the probability measure $|Ïˆ_0|^2 {\rm d}x$ on the system configuration space $M$ ($=\mathbb{R}^n$). On the resulting probability space $\mathcal{M}_0$, we introduce a stochastic process $Î¾(t)$ corresponding to the Heisenberg position operator $X_H(t)$ such that, in the Heisenberg state $|Ïˆ_h\rangle$ corresponding to $Ïˆ_0$, the expectation value of $X_H(t)$ equals that of $Î¾(t)$ in $\mathcal{M}_0$. This condition leads to the de Broglie-Bohm guidance equation for the sample paths of the process $Î¾(t)$ which are, therefore, Bohmian trajectories supposedly representing time-evolutions of individual members of the $Ïˆ_0$-ensemble. Stochastic processes and Bohmian trajectories corresponding to observables with discrete eigenvalues (in particular spin) are treated by extending the configuration space to the spectral space of the commutative algebra obtained by adding appropriate discrete observables to the position observables. Pauli's equation is treated as an example. A straightforward derivation of von Neumann's projection rule employing the Schrodinger-Bohm evolution of individual systems along their Bohmian trajectories is given. Some comments on the potential application of the formalism developed here to quantum mechanics of the universe are included.",0,arxiv,Kuantum,CC-BY/arXiv,Bohmian Trajectories Within Hilbert Space Based Quantum Mechanics. Solution of the Measurement Problem
"We develop a theoretical framework for cooling a microwave cavity mode using a Poisson stream of internally correlated pairs of two-level systems and analyze its performance under realistic dissipation. Starting from a Lindblad model of a phonon-tethered cavity interacting with sequentially injected atom pairs, we derive closed-form expressions for the steady-state cavity occupation and effective temperature. Two coupling geometries are examined: a one-atom configuration, where only one member of each pair interacts with the cavity, and a two-atom configuration, where both atoms couple collectively. The single-atom model enables cooling below the phonon bath but not below the reservoir temperature, whereas the two-atom scheme exhibits enhanced refrigeration - pair correlations modify the cavity's upward and downward transition rates so that the steady-state temperature can fall well below that of the reservoir for weak phonon damping. We map the parameter space including detuning, coupling strength, damping, and intra-pair exchange, identifying cooling valleys near resonance and the crossover between reservoir- and phonon-dominated regimes. The two-atom configuration thus realizes a genuine quantum-enhanced cooling mechanism absent in the single-atom case. We further outline an experimental implementation using two superconducting qubits repeatedly prepared, coupled, and reset inside a 3D cavity. Realistic reset and flux-tuning protocols support MHz-rate interaction cycles, enabling engineered reservoirs to impose cavity temperatures of 50-120 mK even when the cryostat is at ~1 K, offering a pathway to autonomous, on-chip refrigeration of microwave modes in scalable quantum hardware.",0,arxiv,Kuantum,CC-BY/arXiv,Quantum Correlation Assisted Cooling of Microwave Cavities Below the Ambient Temperature
"It was argued [1] that there can be no extension of quantum mechanics with improved predictive power on a measurement freely chosen, independently of any event that is not in its future light cone. The assumption of measurement choice was criticized [2] to be too strong to be physically necessary and extensions of quantum mechanics were shown [3] to be possible under a more relaxed measurement assumption. Here I point out an error in the criticism and observe that the actual mistake of the no-go theorem lies in an unwarranted assumption implicitly made in the proof of [1]. Hence, quantum mechanics is guaranteed to have the maximal predictive power only in situations of complete certainty and complete uncertainty about measurement outcomes. I then show that the measurement assumption can be further relaxed without affecting the conclusion on the predictive power of quantum mechanics versus alternative theories. I further study the optimal predicative improvement over quantum mechanics of local spin measurements on a pair of entangled qubits by any alternative theory and conjecture a strict upper bound.",0,arxiv,Kuantum,CC-BY/arXiv,On possible extensions of quantum mechanics
"Spin qubit coherence is a fundamental resource for the realization of quantum technologies. For solid-state platforms, spin decoherence is dominated by the magneto-active environment in the lattice, limiting their applicability. While standard dynamical decoupling techniques, such as the Hahn echo, extend central spin coherence, they fail to suppress the fast noise arising from strong dipolar interactions within the bath. Here, we present a decoupling mechanism, Hybrid-LG, that suppresses intra-bath dipolar interactions -- thus, fast noise acting on spin qubits- and demonstrate its effectiveness in extending spin coherence through efficient in-house CCE simulations. Specifically, we investigate one of the most widely exploited solid-state quantum platforms: an ensemble of nitrogen-vacancy (NV) centers in diamond coupled to a large and dense bath of substitutional nitrogen paramagnetic impurities (P1 centers). Our results reveal at least a twofold enhancement in NV coherence time relative to standard techniques including P1 center driving, without requiring additional control power.",0,arxiv,Kuantum,CC-BY/arXiv,Suppressing Fast Dipolar Noise in Solid-State Spin Qubits
"A solution of the free SchrÃ¶dinger equation is investigated by means of Optimal transport. The curve of probability measures $Î¼_t$ this solution defines is shown to be an absolutely continuous curve in the Wasserstein space $W_2(\mathbb{R}^3)$. The optimal transport map from $Î¼_t$ to $Î¼_s$, the cost for this transport (i.e. the Wasserstein distance) and the value of the Fisher information along $Î¼_t$ are being calculated. It is finally shown that this solution of the free SchrÃ¶dinger equation can naturally be interpreted as a curve in so-called Shape space, which forgets any positioning in space but only describes properties of shapes. In Shape space, $Î¼_t$ continues to be a shortest path geodesic.",0,arxiv,Kuantum,CC-BY/arXiv,Optimal Transport of a Free Quantum Particle and its Shape Space Interpretation
"As quantum computing processors increase in size, there is growing interest in developing cryogenic electronics to overcome significant challenges to system scaling. Single flux-quantum (SFQ) circuits offer a promising alternative to remote, bulky, and power-hungry room temperature electronics. To meet the need for digital qubit control, readout, and co-processing, SFQ circuits must be adapted to operate at millikelvin temperatures near quantum processors. SEEQC's SFQuClass digital quantum management approach proximally places energy-efficient SFQ (ERSFQ) circuits and qubits in a multi-chip module. This enables extremely low power dissipation, compatible with a typical dilution cryostat's limited cooling power, while maintaining high processing speed and low error rates. We report on systematic testing from 4 K to 10 mK of a comprehensive set of ERSFQ cells, as well as more complex circuits such as programmable counters and demultiplexers used in digital qubit control. We compare the operating margins and error rates of these circuits and find that, at millikelvin, bias margins decrease and the center of the margins (i.e., the optimal bias current value) increases by ~15%, compared to 4.2 K. The margins can be restored by thermal annealing by reducing Josephson junction (JJ) critical current Ic. To provide guidance for how circuit parameters vary from 4.2 K to millikelvin, relevant analog process control monitors (PCMs) were tested in the temperature range of interest. The measured JJ critical current (of the PCM JJ arrays) increases by ~15% when decreasing temperature from 4.2 K to millikelvin, in good agreement with both theory and the empirically measured change in the center of bias margins for the tested digital circuits.",0,arxiv,Kuantum,CC-BY/arXiv,Single Flux Quantum Circuit Operation at Millikelvin Temperatures
"Quantum centric supercomputing (QCSC) framework, such as sample-based quantum diagonalization (SQD) holds immense promise toward achieving practical quantum utility to solve challenging problems. QCSC leverages quantum computers to perform the classically intractable task of sampling the dominant fermionic configurations from the Hilbert space that have substantial support to a target state, followed by Hamiltonian diagonalization on a classical processor. However, noisy quantum hardware produces erroneous samples upon measurements, making robust and efficient configuration-recovery strategies essential for a scalable QCSC pipeline. Toward this, in this work, we introduce PIGen-SQD, an efficiently designed QCSC workflow that utilizes the capability of generative machine learning (ML) along with physics-informed configuration screening via implicit low-rank tensor decompositions for accurate fermionic state reconstruction. The physics-informed pruning is based on a class of efficient perturbative measures that, in conjunction with hardware samples, provide a substantial overlap with the target state. This distribution induces an anchoring effect on the generative ML models to stochastically explore only the dominant sector of the Hilbert space for effective identification of additional important configurations in a self-consistent manner. Our numerical experiments performed on IBM Heron R2 quantum processors demonstrate this synergistic workflow produces compact, high-fidelity subspaces that substantially reduce diagonalization cost while maintaining chemical accuracy under strong electronic correlations. By embedding classical many body intuitions directly into the generative ML model, PIGen-SQD advances the robustness and scalability of QCSC algorithms, offering a promising pathway toward chemically reliable quantum simulations on utility-scale quantum hardware.",0,arxiv,Kuantum,CC-BY/arXiv,Physics Informed Generative Machine Learning for Accelerated Quantum-centric Supercomputing
"We investigate the emergence and control of multiple topological Anderson insulator (TAI) phases in a one-dimensional Su-Schrieffer-Heeger (SSH) waveguide lattice with generalized Bernoulli-type disorder introduced in the intradimer couplings. By systematically varying the disorder configuration -- including the values and probabilities of the multivariate distribution -- we demonstrate that both the number and width of TAI phases can be precisely engineered. Analytical determination of topological phase boundaries via the inverse localization length shows excellent agreement with numerical simulations. Our results reveal a rich landscape of disorder-induced topological phase transitions, including multiple reentrant TAI phases that arise as the disorder amplitude increases. Furthermore, we show that the mean chiral displacement serves as a sensitive probe for detecting these topological transitions, providing a practical route for experimental realization in photonic waveguide lattices. This work establishes a versatile framework for designing quantum and photonic materials with customizable topological properties driven by tailored disorder.",0,arxiv,Kuantum,CC-BY/arXiv,Controllable Emergence of Multiple Topological Anderson Insulator Phases in Photonic Su-Schrieffer-Heeger Lattices
"What physical mechanism enables quantum catalysis to boost quantum battery (QB) performance in open systems? We investigate an external-field-driven qubit QB coupled to a harmonic oscillator catalyst, revealing a key thermodynamic mechanism: the catalyst induces transient negative heat flow ($J(t)<0$, or energy backflow) into the battery. This backflow actively counters dephasing losses, rapidly pushing the qubit into non-passive states, and results in a drastic enhancement of extractable work (Ergotropy). Leveraging the quantum first law, we precisely quantify this causal link between negative heat flux and QB performance enhancement. Our work uncovers the fundamental role of transient thermodynamic backflow in quantum catalysis, offering a crucial blueprint for high-performance quantum energy storage devices.",0,arxiv,Kuantum,CC-BY/arXiv,Quantum catalysis-enhanced extract energy in qubit quantum battery
"We study probabilistic cellular automata (PCA) and quantum cellular automata (QCA) as frameworks for solving the Maximum Independent Set (MIS) problem. We first introduce a synchronous PCA whose dynamics drives the system toward the manifold of maximal independent sets. Numerical evidence shows that the MIS convergence probability increases significantly as the activation probability p tends to 1, and we characterize how the steps required to reach the absorbing state scale with system size and graph connectivity. Motivated by this behavior, we construct a QCA combining a pure dissipative phase with a constraint-preserving unitary evolution that redistributes probability within this manifold. Tensor Network simulations reveal that repeated dissipative--unitary cycles concentrate population on MIS configurations. We also provide an empirical estimate of how the convergence time scales with graph size, suggesting that QCA dynamics can provide an efficient alternative to adiabatic and variational quantum optimization methods based exclusively on local and translationally invariant rules.",0,arxiv,Kuantum,CC-BY/arXiv,Maximum Independent Set via Probabilistic and Quantum Cellular Automata
"The Mpemba effect, where a state prepared farther from equilibrium relaxes faster to equilibrium than one prepared closer, has a quantum counterpart where relaxation is resolved by conserved charge. However, the fate of the quantum Mpemba effect in systems with long-range interactions remains an open question. Here, we study the quantum Mpemba effect in long-ranged, U(1)-symmetric random unitary circuits. Using annealed RÃ©nyi-2 entanglement asymmetry computed via replica tensor networks and exact diagonalization, we track the symmetry restoration from three types of tilted product states: ferromagnetic, antiferromagnetic, and ferromagnetic with a central domain wall. The quantum Mpemba effect is present for tilted ferromagnetic states at all interaction ranges, but absent for tilted antiferromagnetic states, and occurs for the domain-wall state only in effectively short-ranged circuits, where the Mpemba time $t_{\rm M}$ is found to scale with the subsystem size $N_A$ as $t_{\rm M}\!\sim\!N_{A}^{\,z}$, with the dynamical exponent $z=\min(Î±-1,2)$. These results reveal how the quantum Mpemba effect is governed by the interplay between interaction range and initial-state charge bias in long-ranged chaotic systems.",0,arxiv,Kuantum,CC-BY/arXiv,Quantum Mpemba effect in long-ranged U(1)-symmetric random circuits
"Reliably simulating two-dimensional many-body quantum dynamics with projected entangled pair states (PEPS) has long been a difficult challenge. In this work, we overcome this barrier for low-energy quantum dynamics by developing a stable and efficient time-dependent variational Monte Carlo (tVMC) framework for PEPS. By analytically removing all gauge redundancies of the PEPS manifold and exploiting tensor locality, we obtain a numerically well-conditioned stochastic reconfiguration (SR) equation amenable to robust solution using the efficient Cholesky decomposition, enabling long-time evolution in previously inaccessible regimes. We demonstrate the power and generality of the method through four representative real-time problems in two dimensions: (I) chiral edge propagation in a free-fermion Chern insulator; (II) fractionalized charge transport in a fractional Chern insulator; (III) vison confinement dynamics in the Higgs phase of a Z2 lattice gauge theory; and (IV) superfluidity and critical velocity in interacting bosons. All simulations are performed on 12x12 or 13x13 lattices with evolution times T = 10 to 12 using modest computational resources (1 to 5 days on a single GPU card). Where exact benchmarks exist (case I), PEPS-tVMC matches free-fermion dynamics with high accuracy up to T = 12. These results establish PEPS-tVMC as a practical and versatile tool for real-time quantum dynamics in two dimensions. The method extends the reach of classical tensor-network simulations for studying elementary excitations in quantum many-body systems and provides a valuable computational counterpart to emerging quantum simulators.",0,arxiv,Kuantum,CC-BY/arXiv,Real-Time Dynamics in Two Dimensions with Tensor Network States via Time-Dependent Variational Monte Carlo
"Neutral atom systems are promising platforms for quantum simulation and computation, owing to their long coherence times. However, their intrinsically weak ground-state interactions pose a major limitation to the advancement of scalable quantum simulation and computation. To address this challenge, we propose an approach to enhancing the ground-state interaction strength of neutral atoms via Floquet modulation of a Rydberg atomic ensemble. Each Floquet period consists of ground-state coupling followed by a pulse driving the transition from the ground state to the Rydberg state. Theoretical analysis and numerical simulations demonstrate that after a defined evolution time, neutral atoms within Rydberg ensembles can collectively form a $W$ state in the ground-state manifold. Even when the Rydberg interaction strength is far below the blockade regime, the fidelity remains remarkably high. Finally, we analyze the application of this scheme in the preparation of single-photon sources. In general, our proposed mechanism offers an efficient and highly controllable method for quantum state preparation within the Rydberg atomic ensembles, significantly enhancing the accuracy and stability of quantum state engineering while providing a well-controlled quantum environment for single-photon generation.",0,arxiv,Kuantum,CC-BY/arXiv,Enhancing ground-state interaction strength of neutral atoms via Floquet stroboscopic dynamics
"Simon's problem admits an exponential quantum speedup, but current quantum devices support only qubits. This work introduces a general construction for simulating qudit versions of Simon's algorithm on qubit hardware by defining virtual qudits implemented through controlled permutations and qudit phase operations. We build a dimension lifted oracle that encodes the hidden shift in dimension d and show how to realize its action using only qubit gates. We mathematically verify that the lifted circuit reproduces the correct measurement statistics, analyze the depth overhead tradeoffs as a function of d, and provide numerical simulations in QuTiP for example values. Our approach demonstrates how higher-dimensional structures can be embedded into qubit devices and provides a general method for extending qudit algorithms to current hardware.",0,arxiv,Kuantum,CC-BY/arXiv,Virtual Qudits for Simon's Problem: Dimension-Lifted Algorithms on Qubit Hardware
"We address the multi-user quantum key distribution (QKD) problem under malicious quantum attacks, which is critical for realizing a large-scale quantum Internet. This paper maximizes the sum secret key rate (SKR) of a novel uplink non-orthogonal multiple access based continuous-variable QKD (NOMA-CVQKD) system under collective attacks. The proposed system uses Gaussian-modulated coherent states and a quantum successive interference cancellation based heterodyne receiver. We derive closed-form asymptotic bounds for the legitimate users' achievable key rates via the entropy power inequality and maximum entropy principle, as well as for the eavesdropper's intercepted information based on Holevo information. A successive convex approximation based power allocation algorithm is developed to maximize the asymptotic sum SKR of the NOMA-CVQKD system under collective attacks, with guaranteed convergence to a locally optimal Karush-Kuhn-Tucker solution. Simulation results show that the proposed NOMA-CVQKD system with the power allocation algorithm achieves up to 23% higher sum SKR than quantum-orthogonal multiple access, supports 16 users at excess noise variance 0.1, and remains robust under varying turbulence intensities and transmission distances.",0,arxiv,Kuantum,CC-BY/arXiv,Non-Orthogonal Multiple Access-Based Continuous-Variable Quantum Key Distribution: Secret Key Rate Analysis and Power Allocation
"Coherent states have been increasingly considered in optical quantum communications (OQCs). With the inherent non-orthogonality of coherent states, non-orthogonal multiple-access (NOMA) naturally lends itself to the implementation of multi-user OQC. However, this remains unexplored in the literature. This paper proposes a novel successive interference cancellation (SIC)-based Kennedy receiver for uplink NOMA-OQC systems, along with a new approach for power allocation of the coherent states emitted by users. The key idea is to rigorously derive the asymptotic sum-rate of the considered systems, taking into account the impact of atmospheric turbulence, background noise, and lossy photon channel. With the asymptotic sum-rate, we optimize the average number of photons (or powers) of the coherent states emitted by the users. Variable substitution and successive convex approximation (SCA) are employed to convexify and maximize the asymptotic sum-rate iteratively. A new coherent-state power allocation algorithm is developed for a small-to-medium number of users. We further develop its low-complexity variant using adaptive importance sampling, which is suitable for scenarios with a medium-to-large number of users. Simulations demonstrate that our algorithms significantly enhance the sum-rate of uplink NOMA-OQC systems using coherent states by over 20\%, compared to their alternatives.",0,arxiv,Kuantum,CC-BY/arXiv,Non-Orthogonal Multiple-Access for Coherent-State Optical Quantum Communications Under Lossy Photon Channels
"We investigate the quantum dynamics of the electron spin resonance of topological defects (edge state) in dimerized chains. These objects are discontinuities of the spin chain protected by the properties of the global system leading to a quantum many-body multiplet protected from the environment decoherence. Despite recent achievements in the realization of isolated and finite spin chains, the potential implementation in quantum devices needs the knowledge of the relaxation and decoherence sources. Our study reveals that electron spin lattice relaxation is governed at lowest temperatures by phonon-bottlenecked process and at high temperature by the chain dimerization gap. We show that the inter edge-state effective dipolar field is reduced by the intrachain exchange coupling leading to a longer coherence time than isolated ions at equivalent concentration. Ultimately, we demonstrate that the homogeneous broadening is governed by the intra-chain dipolar field, and we establish design principles for optimizing coherence in future materials.",0,arxiv,Kuantum,CC-BY/arXiv,Exploring electron spin dynamics in spin chains using defects as a quantum probe
"Entanglement plays a central role in quantum technologies, yet its characterization and control in materials remain challenging. Recent developments in spectrum-based entanglement witnesses have enabled new strategies for quantifying many-body entanglement in macroscopic materials. Here, we develop a protocol for detecting spin--orbital entanglement using experiment-accessible resonant inelastic x-ray scattering (RIXS). Central to our approach is the construction of a Hermitian generator from experimentally measurable spectra, which allows us to compute the quantum Fisher information (QFI) available in spin--orbital systems. The resulting QFI provides upper bounds for $k$-producible states and thus serves as a robust witness of spin--orbital entanglement. To account for realistic experimental limitations, we further extend our framework to include relaxed QFI bounds applicable to measurements lacking full polarization resolution.",0,arxiv,Kuantum,CC-BY/arXiv,Witnessing Spin-Orbital Entanglement using Resonant Inelastic X-Ray Scattering
"We demonstrate that if a quantum Markovian semigroup satisfies the standard quantum detailed balance condition, its generator admits a special representation that yields a vanishing entropy production rate. Conversely, if the generator admits a special representation adhering to the condition of thermodynamic consistency and leading to a vanishing entropy production rate, then the corresponding quantum Markovian semigroup must satisfy the standard quantum detailed balance condition. In this context, we adopt the definition of entropy production rate that is motivated by the physics literature and standard for thermodynamically consistent Lindbladians.",0,arxiv,Kuantum,CC-BY/arXiv,Interplay between Standard Quantum Detailed Balance and Thermodynamically Consistent Entropy Production
"Quantum generative models leverage quantum superposition and entanglement to enhance learning efficiency for both classical and quantum data. The quantum denoising diffusion probabilistic model (QuDDPM), inspired by its classical counterpart, has been proposed as a promising framework for quantum generative learning. QuDDPM is capable of efficiently learning and generating quantum data, and it demonstrates excellent performance in learning correlated quantum noise models, quantum many-body phases, and the topological structure of quantum data. However, we show that barren plateaus emerge in QuDDPMs due to the use of 2-design states as the input for the denoising process, which severely undermines the performance of QuDDPM. Through theoretical analysis and experimental validation, we confirm the presence of barren plateaus in the original QuDDPM. To address this issue, we introduce an improved QuDDPM that utilizes a distribution maintaining a certain distance from the Haar distribution, ensuring better trainability. Experimental results demonstrate that our approach effectively mitigates the barren plateau problem and generates samples with higher quality, paving the way for scalable and efficient quantum generative learning.",0,arxiv,Kuantum,CC-BY/arXiv,Mitigating Barren plateaus in quantum denoising diffusion probabilistic models
"Strange correlator is a powerful tool widely used in detecting symmetry-protected topological (SPT) phases. However, the result of strange correlator crucially relies on the adoption of the reference state. In this work, we report that an ill-chosen reference state can induce spurious long-range strange correlators in trivial SPT phases, leading to false positives in SPT diagnosis. Using matrix product state (MPS) representation, we trace the origin of these spurious signals in trivial SPT phases to the magnitude-degeneracy of the transfer matrix. We systematically classify three distinct mechanisms responsible for such degeneracy, each substantiated by concrete examples: (1) the presence of high-dimensional irreducible representations in the entanglement space; (2) a phase mismatch in symmetry representations between the target and reference states; and (3) long-range order arising from symmetry breaking. Our findings clarify the importance of the choice of proper reference states, providing a guideline to avoid pitfalls and correctly identify SPT order using strange correlators.",0,arxiv,Kuantum,CC-BY/arXiv,Spurious Strange Correlators in Symmetry-Protected Topological Phases
"Quantum network enables a variety of quantum information processing tasks, where multi-user quantum communication is one of the important objectives. Quantum cryptographic conferencing serves as an essential solution to establish secure keys to realize secure multi-user communications. However, existing QCC implementations have been fundamentally limited by the low probability of multi-user coincidence detection to measure or construct the Greenberger-Horne-Zeilinger (GHZ) entangled state. In this work, we report the experimental realization of QCC eliminating the need for coincidence detection, where the GHZ state is constructed by correlating detection events occurring within the coherence time, thereby greatly enhancing the success probability of GHZ-state measurement. Meanwhile, to establish and maintain high-visibility GHZ measurement among three independent users, we developed a three-party phase compensation scheme combined with precise temporal and polarization alignment within a time-bin-phase encoding framework. Furthermore, we designed an efficient pairing strategy to simplify subsequent data processing and enhance processing efficiency. Based on these techniques, we successfully performed QCC experiments over total channel losses of 66.3 dB, corresponding to 331.5 km of commercial fiber (0.2 dB/km), achieving secure key rates of 5.4 bit/s, whereas previous QCC experiments have been limited to 100 km. The results surpass the multi-user repeaterless bound in quantum networks, establishing a new regime of scalable, multi-user quantum communication and paving the way for metropolitan quantum networks.",0,arxiv,Kuantum,CC-BY/arXiv,Experimental demonstration of scalable quantum cryptographic conferencing
"Graph states are an important class of entangled states that serve as a key resource for distributed information processing and communication in quantum networks. In this work, we propose a protocol that utilizes a Bell sampling subroutine to characterize the diagonal elements in the graph basis of noisy graph states distributed across a network. Our approach offers significant advantages over direct diagonal estimation using unentangled single-qubit measurements in terms of scalability. Specifically, we prove that estimating the full vector of diagonal elements requires a sample complexity that scales linearly with the number of qubits ($\mathcal{O}(n)$), providing an exponential reduction in resource overhead compared to the best known $\mathcal{O}(2^n)$ scaling of direct estimation. Furthermore, we demonstrate that global properties, such as state fidelity, can be estimated with a sample complexity independent of the network size. Finally, we present numerical results indicating that the estimation in practice is more efficient than the derived theoretical bounds. Our work thus establishes a promising technique for efficiently estimating noisy graph states in large networks under realistic experimental conditions.",0,arxiv,Kuantum,CC-BY/arXiv,Efficient graph-diagonal characterization of noisy states distributed over quantum networks via Bell sampling
"Quantum machine learning offers a promising pathway for enhancing stock market prediction, particularly under complex, noisy, and highly dynamic financial environments. However, many classical forecasting models struggle with noisy input, regime shifts, and limited generalization capacity. To address these challenges, we propose a Quantum Temporal Convolutional Neural Network (QTCNN) that combines a classical temporal encoder with parameter-efficient quantum convolution circuits for cross-sectional equity return prediction. The temporal encoder extracts multi-scale patterns from sequential technical indicators, while the quantum processing leverages superposition and entanglement to enhance feature representation and suppress overfitting. We conduct a comprehensive benchmarking study on the JPX Tokyo Stock Exchange dataset and evaluate predictions through long-short portfolio construction using out-of-sample Sharpe ratio as the primary performance metric. QTCNN achieves a Sharpe ratio of 0.538, outperforming the best classical baseline by approximately 72\%. These results highlight the practical potential of quantum-enhanced forecasting model, QTCNN, for robust decision-making in quantitative finance.",0,arxiv,Kuantum,CC-BY/arXiv,Quantum Temporal Convolutional Neural Networks for Cross-Sectional Equity Return Prediction: A Comparative Benchmark Study
"Noise is an important factor that influences the reliability of information acquisition, transmission, processing, and storage. In order to suppress the inevitable noise effects, a fault-tolerant information processing approach via quantum weak measurement is proposed, where pairwise orthogonal postselected measurement bases with various tiny angles and optimal compositions of measured results are chosen as a decoding rule. The signal to be protected can be retrieved with a minimal distortion after having been transmitted through a noisy channel. Demonstrated by typical examples of encoding signal on two-level superposition state or Einstein-Podolsky-Rossen state transmitted through random telegraph noise and decoherence noises channel, the mean squared error distortion may be close to $0$ and the fault-tolerant capability could reach $1$ with finite quantum resources. To verify the availability of the proposed approach, classic coherent light and quantum coherent state are used for encoding information in the experiment. Potentially, the proposed approach may provide a solution for suppressing noise effects in long-distance quantum communication, high-sensitivity quantum sensing, and accurate quantum computation.",0,arxiv,Kuantum,CC-BY/arXiv,Fault-Tolerant Information Processing with Quantum Weak Measurement
"Geometric confinement is known to modify single-particle dynamics through effective potentials, yet its imprint on the interacting quantum vacuum remains largely unexplored. In this work, we investigate the Maxwell--Klein--Gordon system constrained to curved surfaces and demonstrate that the geometric potential $Î£_{\mathrm{geom}}(\mathbf{r})$ acts as a local renormalization environment. We show that extrinsic curvature modifies the scalar loop spectrum, entering the vacuum polarization as a position-dependent mass correction $M^2(\mathbf{r}) \to m^2 + Î£_{\mathrm{geom}}(\mathbf{r})$. This induces a finite, gauge-invariant ``geometry-induced running'' of the electromagnetic response. In the long-wavelength regime ($|{\bf Q}|R \ll 1$), we derive a closed-form expression for the relative frequency shift $Î”Ï‰/Ï‰$, governed by the overlap between the electric energy density and the geometric potential. Applying this formalism to Gaussian bumps, cylindrical shells, and tori, we identify distinct spectral signatures that distinguish these quantum loop corrections from classical geometric optics. Our results suggest that spatial curvature can serve as a tunable knob for ``vacuum engineering,'' offering measurable shifts in high-$Q$ cavities and plasmonic systems.",0,arxiv,Kuantum,CC-BY/arXiv,Geometry-Induced Vacuum Polarization and Mode Shifts in Maxwell-Klein-Gordon Theory
"We develop a theoretical framework for high-harmonic generation (HHG) driven by quantum states of light based on a temporal-mode expansion of the electromagnetic field. This approach extends previous single plane-wave mode treatments to realistic pulse configurations, resolving conceptual inconsistencies arising from non-normalizable infinite plane waves and establishing consistency between analytical and numerical methods. We derive a correction factor that quantifies deviations from the single-mode approximation and show that it remains below $10^{-4}$ for intensities typical of HHG ($\sim 10^{14}~$W/cm$^2$). This result confirms that free-space HHG driven by any quantum state of light is accurately described by averaging semi-classical calculations over the Husimi distribution, with no observable genuine quantum effects. The absence of such effects is attributed to the large photon numbers ($\sim 10^{11}$) required to reach HHG intensities in free space, which render quantum fluctuations negligible. We discuss nanophotonic environments with ultrasmall mode volumes as potential platforms where few-photon strong-field processes could exhibit genuine quantum signatures.",0,arxiv,Kuantum,CC-BY/arXiv,High-harmonic generation driven by temporal-mode quantum states of light
"To date, most integrated quantum photonics experiments rely on single-photon detectors operating at cryogenic temperatures coupled to photonic integrated circuits (PICs) through single-mode optical fibers. This approach presents significant challenges due to the detection complexity, as cryogenic conditions hinder the development of scalable systems. In addition, going towards fully-integrated devices or, at least, removing the optical fibers would be also advantageous to develop compact and cost-efficient solutions featuring a high number of optical modes. This work reports on the direct coupling of a PIC, fabricated by femtosecond laser writing (FLW), and a silicon single-photon avalanche diode (SPAD) array, fabricated in a custom planar technology and compatible with the operation at room temperature. The effectiveness of this solution is shown by achieving perfect coupling and a system detection efficiency as high as 41.0% at a wavelength of 561 nm, which is the highest value reported to date among both heterogeneous/hybrid integrated and directly coupled systems. We also show the robustness of the coupling to misalignments, demonstrating that costly alignment procedures are not needed. Finally, we exploit the SPAD array to characterize a reconfigurable Mach-Zehnder interferometer, i.e., the basic building block of multimode reconfigurable PICs. This solution provides a new avenue to the design and implementation of quantum photonics experiments, especially effective when compact and cost-efficient systems are needed.",0,arxiv,Kuantum,CC-BY/arXiv,Laser-written reconfigurable photonic integrated circuit directly coupled to a single-photon avalanche diode array
"The Travelling Salesman Problem (TSP) is a well-known NP-Hard combinatorial optimisation problem, with industrial use cases such as last-mile delivery. Although TSP has been studied extensively on quantum computers, it is rare to find quantum solutions of TSP network with more than a dozen locations. In this paper, we present high quality solutions in noise-free Qiskit simulations of networks with up to twelve locations using a hybrid penalty-free, circuit-model, Variational Quantum Algorithm (VQA). Noisy qubits are also simulated. To our knowledge, this is the first successful VQA simulation of a twelve-location TSP on circuit-model devices. Multiple encoding strategies, including factorial, non-factorial, and Gray encoding are evaluated. Our formulation scales as $\mathcal{O}(nlog_2(n))$ qubits, requiring only 29 qubits for twelve locations, compared with over 100 qubits for conventional approaches scaling as $\mathcal{O}(n^2)$. Computational time is further reduced by almost two orders of magnitude through the use of Simultaneous Perturbation Stochastic Approximation (SPSA) gradient estimation and cost-function caching. We also introduce a novel machine-learning model, and benchmark both quantum and classical approaches against a Monte Carlo baseline. The VQA outperforms the classical machine-learning approach, and performs similarly to Monte Carlo for the small networks simulated. Additionally, the results indicate a trend toward improved performance with problem size, outlining a pathway to solving larger TSP instances on quantum devices.",0,arxiv,Kuantum,CC-BY/arXiv,Solving larger Travelling Salesman Problem networks with a penalty-free Variational Quantum Algorithm
"Quantum algorithms offer an exponential advantage with respect to the number of dependent variables for solving certain nonlinear ordinary differential equations (ODEs). These algorithms typically begin by transforming the original nonlinear ODE into a higher-dimensional linear ODE using a linearization technique, most commonly Carleman linearization. Existing works restrict their analysis to ODEs where the nonlinearities are polynomial functions of the dependent variables, significantly limiting their applicability. In this work we construct an efficient quantum algorithm for solving ODEs with `Fourier' nonlinear terms expressible as $d{\bf u}/dt = G_0 + G_1 e^{i{\bf u}}$, where ${\bf u}$ denotes a vector of $n$ complex variables evolving with $t$, $G_0$ is an $n$-dimensional complex vector, $G_1$ is an $n \times n$ complex matrix and $e^{i{\bf u}}$ denotes the vector with entries $\{e^{iu_j}\}$. To tackle the Fourier nonlinear term, which is not expressible as a finite sum of polynomials of ${\bf u}$, our algorithm employs a generalization of the Carleman linearization technique known as Koopman linearization. We also make other methodological advances towards relaxing the stringent dissipativity condition required for efficient solution extraction and towards integrated readout of classical quantities from the solution state. Our results open avenues to the development of efficient quantum algorithms for a significantly wider class of high-dimensional nonlinear ODEs, thereby broadening the scope of their applications.",0,arxiv,Kuantum,CC-BY/arXiv,Efficient quantum algorithm for solving differential equations with Fourier nonlinearity via Koopman linearization
"Fault-tolerant quantum computation using surface codes relies on efficient scheduling of non-Clifford operations, realized via the injection of magic states produced through a probabilistic process that dominates spacetime costs. Existing scheduling approaches use dedicated bus qubits for routing and separate peripheral ancilla qubit factories for magic state preparation, leading to inefficient resource utilization. With the advent of magic state cultivation, preparation qubits can be placed anywhere within the surface code architecture. We introduce Pure Magic scheduling, which dynamically re-purposes magic state cultivation qubits for routing operations, eliminating dedicated bus infrastructure. By interrupting cultivation when qubits are needed for routing, Pure Magic naturally favors shorter cultivation times while ensuring no ancilla qubit remains idle. Our evaluation across 17 benchmark circuits improves scheduling efficiency by 19% to 223% compared to traditional bus routing and decreases average magic state preparation time by 2.6x to 9.7x. Benefits scale with circuit parallelism, making Pure Magic particularly valuable for highly parallel quantum algorithms. The Pure Magic architecture represents a paradigm shift from static to dynamic, demand-driven scheduling in fault-tolerant quantum architectures.",0,arxiv,Kuantum,CC-BY/arXiv,Scheduling Lattice Surgery with Magic State Cultivation
"Unconventional quantum antibunching, arising from quantum interference effects, represents a notable form of quantum correlation that has attracted significant attention for its ability to generate high-quality single-quantum sources. In this work, we propose a scheme to achieve and actively control strong photon blockade in a spinning microwave magnomechanical system by leveraging the combined nonlinear effects of Kerr-induced magnon interactions and an optical parametric amplifier. By exploiting the Sagnac-Fizeau shift, we establish nonreciprocal photon blockade and verify this effect through a combination of analytical modelling and numerical simulations. To gain intuitive insight into the underlying nonreciprocity, we approximate the equal-time second-order correlation function using the analytical solution of the SchrÃ¶dinger equation. This analytical result is then compared with the full numerical solution derived from the Lindblad master equation. The influences of thermal noise, the probe field amplitude, and the magnetic-dipole coupling strength are investigated within the constraints of the weak-coupling regime. The system's nonclassicality is characterized using the Mandel parameter, complemented by an analysis of the time evolution of the second-order correlation function. Our work provides a pathway for realizing nonreciprocal photon blockade in a nonlinear spinning microwave magnomechanical system.",0,arxiv,Kuantum,CC-BY/arXiv,Nonreciprocal photon blockade in a spinning microwave magnomechanical system through kerr-magnon and optical parametric amplifier
"We propose the use of motional states of two interacting atoms trapped in a potential stroboscopically engineered by an optical tweezer as a means to implement a qubit-oscillator system, in analogy to those implemented in circuit quantum electrodynamics and trapped ions. In our setting, the center of mass degree of freedom of the atoms plays the role of a photon or phonon mode, while the interacting, relative mode acts as a qubit. No internal state is involved in our system, which makes this motional qubit robust to spin-dependent noise. We show that a universal set of bosonic operations, including displacement, rotation, squeezing, and the corresponding set of gates controlled by the qubit, can be implemented through precise temporal modulation of the optical tweezers. We numerically check that these gates can be generated with high fidelity, and discuss possible schemes for initial state preparation and final state readout. While we restrict the discussion to a single qubit-oscillator module, scalability can be achieved by coupling arrays of atoms via dipolar or Rydberg-dressed interactions.",0,arxiv,Kuantum,CC-BY/arXiv,Hybrid qubit-oscillator module with motional states of two trapped interacting atoms
"Monogamy of entanglement essentially characterizes the entanglement distributions among the subsystems. Generally it is given by summation-form monogamy inequalities. In this paper, we present the product-form monogamy inequalities satisfied by the $Î½$-th ($Î½\geq2$) power of the concurrence. We show that they are tighter than the existing ones by detailed example. We then establish tighter product-form monogamy inequalities based on the negativity. We show that they are valid even for high dimensional states to which the well-known CKW inequality is violated.",0,arxiv,Kuantum,CC-BY/arXiv,Generalized product-form monogamy relations in multi-qubit systems
"Negatively charged silicon vacancy centers in diamond (SiV$^-$) are promising for quantum photonic technologies. However, when subject to resonant optical excitation, they can inadvertently transfer into a zero-spin optically dark state. We show that this unwanted change of charge state can be quickly reversed by the resonant laser itself in combination with static electric fields. By defining interdigitated metallic contacts on the diamond surface, we increase the steady-state SiV$^-$ photoluminescence under resonant excitation by a factor $\ge3$ for most emitters, making it practically constant for certain individual emitters. We electrically activate single \sivs near the positively biased electrode, which are entirely dark without applying local electric fields. Using time-resolved 3-color experiments, we show that the resonant laser not only excites the SiV$^-$, but also creates free holes that convert SiV$^{2-}$ to SiV$^-$ on a timescale of milliseconds. Through analysis of several individual emitters, our results show that the degree of electrical charge state controllability differs between individual emitters, indicating that their local environment plays a key role. Our proposed electric-field-based stabilization scheme enhances deterministic charge state control in group-IV color centers and improves its understanding, offering a scalable path toward quantum applications such as entanglement generation and quantum key distribution.",0,arxiv,Kuantum,CC-BY/arXiv,Mitigating the Transition of SiV$^-$ in Diamond to an Optically Dark State
"Bound states in the continuum (BICs) have been extensively exploited to enhance light--matter interactions in metamaterials, yet their emergence and utility in multi-atom waveguide platforms remain far less explored. Here we study atom--waveguide-dressed BICs in a one-dimensional coupled-resonator waveguide, where two spatially separated atomic arrays couple to distinct resonators with time-dependent strengths. We show that these BICs support a standing-wave photonic mode and enable the transfer of an arbitrary unknown quantum state between the two arrays with fidelities exceeding $99\%$. The protocol remains robust against both disorder and intrinsic dissipation. Our results establish BICs as long-lived resources for high-fidelity quantum information processing in waveguide-QED architectures.",0,arxiv,Kuantum,CC-BY/arXiv,Bound state in the continuum and multiple atom state transfer applications in a waveguide QED setup
"We propose tests of the weak equivalence principle (WEP) using a torsion balance, in which superposition of energy eigenstates are created in a controllable way for the test masses. After general considerations on the significance of tests of the WEP using quantum states and the need for considering inertial and gravitational masses as operators, we develop a model to derive the matrix elements of the free-fall operator, showing that the variance of the acceleration operator, in addition to its mean, enables estimation of violations of the WEP due to quantum coherence in a way that is robust with respect to shot-to-shot fluctuations. Building on this analysis, we demonstrate how the validity of the WEP may be tested in a torsion balance setup, by accessing the mean and variance of a torque operator we introduce and quantize. Due to the long acquisition times of the signal as compared to the timescale on which coherent superposition states may survive, we further propose a dynamical setting, where the torsion balance is subject to a time-dependent gravitational field, and measurements of angular acceleration encode possible violations of the WEP.",0,arxiv,Kuantum,CC-BY/arXiv,Testing the weak equivalence principle for nonclassical matter with torsion balances
"Non-Hermitian (NH) systems can display exotic topological phenomena without Hermitian counterparts, enabled by exceptional points (EPs). So far, investigations of NH topology have been restricted to EPs of the NH Hamiltonian, which governs the system dynamics conditional upon no quantum jumps occurring. The Liouvillian superoperator, which combines the effects of quantum jumps with NH Hamiltonian dynamics, possesses EPs (LEPs) that are significantly different from those of the corresponding NH Hamiltonian. We here study the topological features of the LEPs in the system consisting of a qubit coupled to a non-Markovian reservoir. We find that two distinct winding numbers can be simultaneously produced by executing a single closed path encircling the twofold LEP2, formed by two coinciding LEP2s, each involving a pair of coalescing eigenvectors of the extended Liouvillian superoperator. We experimentally demonstrate this purely non-Markovian phenomenon with a circuit, where a superconducting qubit is coupled to a decaying resonator which acts as a reservoir with memory effects. The results push the exploration of exceptional topology from the Markovian to non-Markovian regime.",0,arxiv,Kuantum,CC-BY/arXiv,Exploring the topology induced by non-Markovian Liouvillian exceptional points
"By defining a graded global R-operator $\mathbb{R}_{ab}^{(2D,2S)}$ that couples free-fermion structures and incorporates anisotropic Hubbard interactions while satisfying the Yang--Baxter equation, we construct a strictly solvable two-dimensional lattice model. We then build the layer-to-layer transfer matrix through a bidirectional-monodromy construction and prove the model's integrability via the associated global RTT relations. Using the nested algebraic Bethe ansatz, we obtain the exact eigenvalues of the transfer matrix and derive the corresponding first- and second-level Bethe equations. Finally, by taking the logarithmic derivative of the transfer matrix at the regular point, we recover explicitly a local Hamiltonian that features anisotropic hopping, an on-site Hubbard interaction, and orbital-coupling contributions.",0,arxiv,Kuantum,CC-BY/arXiv,Integrable construction of a two-dimensional lattice model with anisotropic Hubbard couplings
"We utilize Kolmogorov-Arnold Networks to design an interpretable model capable of detecting quantum entanglement within a set of nine-parameter two-qubit states. This network serves as an entanglement witness, achieving an accuracy of $94\%$ in distinguishing entangled states. Additionally, by analyzing the output functions of the KAN models, we explore the significance of each parameter (feature) in identifying the presence of entanglement. This analysis enables us to rank the features and eliminate the less significant ones, leading to the development of new entanglement witness functions that rely on fewer number of features, and hence do not require complete state tomography for their evaluation.",0,arxiv,Kuantum,CC-BY/arXiv,Entanglement Witness Derived By Using Kolmogorov-Arnold Networks
"In this study, we compare the Wigner function $W$, its modulus, and the Husimi distribution $H$ in a one-dimensional quantum system exhibiting a transition from a single-well to a double-well configuration, using the quasi-exactly solvable sextic oscillator as a representative example. High-accuracy variational wavefunctions for the lowest states are used to compute two-dimensional phase-space structures, one-dimensional marginals, and the corresponding Shannon entropies, mutual information, and Cumulative Residual Jeffreys divergences. The analysis shows that the Wigner representation is uniquely responsive to interference effects and displays clear, nonmonotonic entropic behavior as the wells separate, whereas the modulus-Wigner and Husimi distributions account only for geometric splitting or coarse-grained delocalization. These findings establish a quantitative hierarchy in the ability of $W$, $|W|$, and $H$ to resolve structural changes in a quantum state and provide a general framework for assessing the descriptive power of different phase-space representations in systems with emerging bimodality or tunneling.",0,arxiv,Kuantum,CC-BY/arXiv,Wigner-Husimi phase-space structure of quasi-exactly solvable sextic potential
"Solid-state quantum light sources offer a scalable pathway for interfacing stationary spin qubits with flying photonic qubits, forming the backbone of future quantum networks. Telecom-band spin-photonic qubits, operating in the 1260-1675 nm wavelength range, are particularly well-suited for long-distance quantum communication due to minimal loss in standard optical fibers. Achieving scalability, however, hinges on fulfilling several stringent criteria: coherent spin-state control, deterministic and indistinguishable single-photon emission, and integration with nanophotonic structures that enhance radiative properties, such as lifetime, coherence, and photon indistinguishability. This study explores the state-of-the-art spin-photonic qubits across solid-state platforms, including diamond color centers, silicon carbide defect centers, quantum dots, and two-dimensional materials. Special attention is given to silicon-based emitters, particularly G, T, C- and Ci-centers, which promise monolithic integration with complementary metal-oxide-semiconductor (CMOS) technology and telecom-band operation. We classify these systems based on spin-photon interface availability, CMOS process compatibility, and emitter scalability. We also discuss recent advances in cavity quantum electrodynamics (cQED), including Purcell enhancement and quality factor engineering in integrated photonic (circuits) environments. The work highlights emerging demonstrations of quantum networking over metropolitan scales and outlines the trajectory toward chip-scale quantum photonic integrated circuits (QPICs). It combines deterministic emitter creation, coherent spin manipulation, and quantum information processing. These developments pave the way for global quantum networks, enabling secure communication, distributed quantum computing, and quantum-enhanced sensing.",0,arxiv,Kuantum,CC-BY/arXiv,Spin-photon Qubits for Scalable Quantum Network
"Quantum sensors based on electronic spins have emerged as powerful probes of microwave-frequency fields. Among other solid-state platforms, spins in molecular crystals offer a range of advantages, from high spin density to functionalization via chemical tunability. Here, we demonstrate microwave vector magnetometry using the photoexcited spin triplet of pentacene molecules, operating at zero external magnetic field and room temperature. We achieve full three-dimensional microwave field reconstruction by detecting the Rabi frequencies of anisotropic spin-triplet transitions associated with two crystallographic orientations of pentacene in deuterated naphthalene crystals. We further introduce a phase alternated protocol that extends the rotating-frame coherence time by an order of magnitude and enables sensitivities of approximately $1~Î¼\mathrm{T}/\sqrt{\mathrm{Hz}}$ with sub-micrometer spatial resolution. These results establish pentacene-based molecular spins as a practical and high-performance platform for microwave quantum sensing in addition to demonstrating control techniques broadly applicable to other molecular and solid-state spin systems.",0,arxiv,Kuantum,CC-BY/arXiv,Robust AC vector sensing at zero magnetic field with pentacene
"We investigate electron transport in one dimension from the quantum-acoustic perspective, where the coherent-state representation of lattice vibrations results in a time-dependent deformation potential whose rate is set by the sound speed, fluctuation spectrum is set by the temperature, and overall amplitude is set by the electron-lattice coupling strength. We introduce an acceleration-based adiabatic criterion, consistent with the adiabatic theorem and Landau-Zener theory, that separates adiabatic and diabatic dynamics across the $(T,v)$ plane. The discrete classification agrees with a continuous mean-squared acceleration scale and correlates with a coherence measure given by the ratio of coherence length to the initial packet width $L_Ï†(t)/Ïƒ_0$. We identify a broad Planckian domain in which the dimensionless diffusivity $Î±\!=\!Dm/\hbar$ is of order unity and only weakly depends on the parameters. This domain is more prevalent in diabatic regions and in areas of reduced phase coherence, indicating a dephasing driven crossover from Anderson localization to Planckian diffusion. Using the Einstein relation together with nearly constant $Î±$, we directly obtain a low temperature tendency $1/Ï„_{\rm tr}\propto T$, offering a insight to $T$-linear resistivity in strange metals. These results provide a unified picture that links adiabaticity, dephasing, and Planckian diffusion in dynamically disordered quantum-acoustics.",0,arxiv,Kuantum,CC-BY/arXiv,Adiabaticity Crossover: From Anderson Localization to Planckian Diffusion
"The linear combination of unitaries (LCU) algorithm is a building block of many quantum algorithms. However, because LCU generally requires an ancillary system and complex controlled unitary operators, it is not regarded as a hardware-efficient routine. Recently, a randomized LCU implementation with many applications to early FTQC algorithms has been proposed that computes the same expectation values as the original LCU algorithm using a shallower quantum circuit with a single ancilla qubit, at the cost of a quadratically larger sampling overhead. In this work, we propose a quantum algorithm intermediate between the original and randomized LCU that manages the tradeoff between sampling cost and the circuit size. Our algorithm divides the set of unitary operators into several groups and then randomly samples LCU circuits from these groups to evaluate the target expectation value. Notably, we analytically prove an underlying monotonicity: larger group sizes entail smaller sampling overhead, by introducing a quantity called the reduction factor, which determines the sampling overhead across all grouping strategies. Our hybrid algorithm not only enables substantial reductions in circuit depth and ancilla-qubit usage while nearly maintaining the sampling overhead of LCU-based non-Hermitian dynamics simulators, but also achieves intermediate scaling between virtual and coherent quantum linear system solvers. It further provides a virtual ground-state preparation scheme that requires only a resettable single-ancilla qubit and asymptotically shows advantages in both virtual and coherent LCU methods. Finally, by viewing quantum error detection as an LCU process, our approach clarifies when conventional and virtual detection should be applied selectively, thereby balancing sampling and hardware overhead.",0,arxiv,Kuantum,CC-BY/arXiv,Tradeoffs between quantum and classical resources in linear combination of unitaries
"The growing demand for solving large-scale, data-intensive linear and conic optimization problems, particularly in applications such as artificial intelligence and machine learning, has highlighted the limitations of classical interior point methods (IPMs). Despite their favorable polynomial-time convergence, conventional IPMs often suffer from high per-iteration computational costs, especially for dense problem instances. Recent advances in quantum computing, particularly quantum linear system solvers, offer promising avenues to accelerate the most computationally intensive steps of IPMs. However, practical challenges such as quantum error, hardware noise, and sensitivity to poorly conditioned systems remain significant obstacles. In response, a series of Quantum IPMs (QIPMs) has been developed to address these challenges, incorporating techniques such as feasibility maintenance, iterative refinement, and preconditioning. In this work, we review this line of research with a focus on our recent contributions, including an almost-exact QIPM framework. This hybrid quantum-classical approach constructs and solves the Newton system entirely on a quantum computer, while performing solution updates classically. Crucially, all matrix-vector operations are executed on quantum hardware, enabling the method to achieve an optimal worst-case scalability w.r.t dimension, surpassing the scalability of existing classical and quantum IPMs.",0,arxiv,Kuantum,CC-BY/arXiv,Quantum Interior Point Methods: A Review of Developments and An Optimally Scaling Framework
"We propose using even and odd SchÃ¶dinger cat states formed from coherent states of U(3) of an ensemble of qutrits with a symmetrical V-configuration (a qubit-disguised qutrit) to encode a logical qubit. These carefully engineered logical qubit states are parameter independent stationary states of the effective master equation governing the evolution of the ensemble and, consequently, constitute dark states and are invulnerable to dissipation and correlated collective dephasing. In particular, the logical qubit states are immune to single qutrit decay (the analogous of single photon loss process for qutrits) and simultaneous decay and driving of two qutrits (the analogous two-photon loss and driving processes for qutrits). In addition, we show how to implement the single-qubit quantum NOT gate and the Hadamard gate followed by either the phase gate or the phase and $Z$ gates. We study analytically the case of two qutrits and conclude that the logical qubit states exhibit parity-sensitive inhomogeneous broadening and local correlated dephasing: the even logical state is completely immune to these processes, while odd one is vulnerable. Nevertheless, in the presence of these interactions one can also define another odd state with mixed permutation symmetry that is immune to both inhomogeneous broadening and local correlated dephasing. We suggest that these results can be extrapolated to an arbitrary number of qutrits. The effective master equation is deduced from a physical system composed of two parametrically coupled cavities with one of them interacting dispersively with an ensemble of three-level atoms (the qutrits). In principle this physical system can be implemented by means of two coplanar waveguide resonators, a SQUID parametrically coupling them, and a cloud of alkali atoms close to one of the resonators.",0,arxiv,Kuantum,CC-BY/arXiv,Highly robust logical qubit encoding in an ensemble of V-symmetrical qutrits
"We revisit the Pauli-Clifford connection to introduce a real, grade-preserving algebraic framework for $N$-qubit quantum computation based on the tensor product structure $C\ell_{2,0}(\mathbb{R})^{\otimes N}$. In this setting the bivector $J = e_{12}$ satisfies $J^{2} = -1$ and supplies the complex structure on a minimal left ideal via right-multiplication, while Pauli operations arise as left actions of suitable Clifford elements. Adopting a canonical stabilizer mapping, the $N$-qubit computational basis state $|0\cdots 0\rangle$ is represented natively by a tensor product of real algebraic idempotents. This structural choice leads to a State-Operator Clifford Compatibility law that is stable under the geometric product for $N$ qubits and aligns symbolic Clifford multiplication with unitary evolution on the Hilbert space.",0,arxiv,Kuantum,CC-BY/arXiv,The State-Operator Clifford Compatibility: A Real Algebraic Framework for Quantum Information
"Mm-wave and THz superconducting circuits find numerous applications in areas ranging from quantum information and sensing to high-energy physics. Planar THz transmission lines and resonators are fabrication-friendly, compact, and scalable, and they can be efficiently interfaced with external signals and controls. However, planar circuits radiate strongly at high frequencies, which precludes their use in loss-sensitive applications. Here, we present the design and characterization of planar dispersion-engineered transmission lines that effectively suppress radiation leakage in desired mm-wave bands. We extend this concept to design planar resonators with extremely low radiation leakage, resulting in radiation Q-factors above 106 at 553 GHz. Low-loss planar THz circuitry will impact many application domains, including broadband communications, quantum information, radio astronomy, and cosmology.",0,arxiv,Kuantum,CC-BY/arXiv,Dispersion Engineering of Planar Sub-millimeter Wave Waveguides and Resonators with Low Radiation Loss
"High-dimensional photonic systems access large Hilbert spaces for quantum information processing. They offer proven advantages in quantum computation, communication, and sensing. However, implementing scalable, low-loss unitary gates across many modes remains a central challenge. Here we propose a deterministic, universal, and fully programmable high-dimensional quantum gate based on a cavity-assisted sum-frequency-generation process, achieving near-unity fidelity. The device implements an M-by-N truncated unitary transformation (with 1 <= M < N), or a full unitary when M = N, on frequency-bin modes. With current technology, the attainable dimensionality reaches M-by-N on the order of ten to the power of four, with N up to about one thousand, and can be further increased using multiple pulse shapers. Combined with compatible SPDC sources, high-efficiency detection, and fast feed-forward, this approach provides a scalable, fiber-compatible platform for high-dimensional frequency-bin quantum processing.",0,arxiv,Kuantum,CC-BY/arXiv,Deterministic and Universal Frequency-Bin Gate for High-Dimensional Quantum Technologies
"This article reviews recent theoretical developments in the ab initio study of polarons in materials. The polaron is an emergent quasiparticle that arises from the interaction between electrons and phonons in solids, and consists of an electron or a hole accompanied by a distortion of the crystal lattice. Recent advances in experiments, theory, and computation have made it possible to investigate these quasiparticles with unprecedented detail, reigniting the interest in this classic problem of condensed matter physics. Recent theoretical and computational advances include ab initio calculations of polaron spectral functions, wavefunctions, lattice distortions, and transport and optical properties. These developments provide new insight into polaron physics, but they have evolved somewhat independently from the earlier effective Hamiltonian approaches that laid the foundation of the field. This article aims to bridge these complementary perspectives by placing them within a single unified conceptual framework. To this end, we start by reviewing effective Hamiltonians of historical significance in polaron theory, ab initio techniques based on density functional theory, and many-body first-principles approaches to polarons. After this survey, we outline a general field-theoretic framework that bridges between these diverse approaches to polaron physics. For completeness, we also review recent progress in the study of exciton polarons and self-trapped excitons and their relations to polarons. Beyond the methodology, we discuss recent applications to several classes of materials that attracted attention in the context of polaron physics.",0,arxiv,Kuantum,CC-BY/arXiv,Polarons from first principles
"A major challenge in light-matter simulations is bridging the disparate time and length scales of electrodynamics and molecular dynamics. Current computational approaches often rely on heuristic approximations of either the electromagnetic (EM) or material component, hindering the exploration of complex light-matter systems. Herein, MaxwellLink -- a modular, open-source Python framework -- is developed for the massively parallel, self-consistent propagation of classical EM fields interacting with a large heterogeneous molecular ensemble. The package utilizes a robust TCP/UNIX socket interface to couple EM solvers with a wide range of external molecular drivers. This decoupled architecture allows users to seamlessly switch between levels of theory of either the EM solver or molecules without modifying the counterpart. Crucially, MaxwellLink supports EM solvers spanning from single-mode cavities to full-feature three-dimensional finite-difference time-domain (FDTD) engines, and molecules described by multilevel open quantum systems, force-field and first-principles molecular dynamics, and nonadiabatic real-time Ehrenfest dynamics. Benefiting from the socket-based design, the EM engine and molecular drivers scale independently across multiple high-performance computing (HPC) nodes, facilitating large-scale simulations previously inaccessible to existing numerical schemes. The versatility and accuracy of this code are demonstrated through applications including superradiance, radiative energy transfer, vibrational strong coupling in Bragg resonators, and plasmonic heating of molecular gases. By providing a unified, extensible engine, MaxwellLink potentially offers a powerful platform for exploring emerging phenomena across the research fronts of spectroscopy, quantum optics, plasmonics, and polaritonics.",0,arxiv,Kuantum,CC-BY/arXiv,MaxwellLink: A unified framework for self-consistent light-matter simulations
"We show that collective three-body interactions (3BIs), implementable with $N$ atoms loaded inside an optical cavity, offer a significant advantage for preparing complex multipartite entangled states. Firstly, they enable a speedup of order $\sim N$ in preparing generalized Greenberger-Horne-Zeilinger (GHZ) states, outperforming conventional methods based on all-to-all two-body Ising interactions. Secondly, they saturate the Heisenberg bound in phase estimation tasks using a time-reversal protocol realized through simple rotations and followed by experimentally accessible collective spin measurements. Lastly, compared with two-body interactions (2BIs), in the presence of cavity losses and single particle decoherence, 3BIs feature a high gain in sensitivity for moderate atom numbers and in large ensembles a fast entanglement generation despite constraints in parameter regimes where they are implementable.",0,arxiv,Kuantum,CC-BY/arXiv,Collective three-body interactions enable a robust quantum speedup
"We present a comprehensive first-principles investigation of carbon self-interstitial defects in diamond, ranging from mono- to hexa-interstitial complexes. By quantum mechanical density functional theory, empowered by interatomic potential models, we efficiently sample the complex configurational landscape and identify both known and previously unreported defect geometries. Our results reveal a pronounced energetic driving force for aggregation: the formation energy per interstitial decreases systematically from isolated split interstitials to compact multi-interstitial clusters, with the tetra-interstitial platelet emerging as a particularly stable structural motif. Additionally, charge analysis indicates that the predominantly covalent bonding in diamond becomes more polar within the defect centers. Analysis of defect energy levels shows that only the investigated mono-, di-, penta-, and hexa-interstitial complexes introduce in-gap electronic states, whereas the tri- and tetra-interstitial clusters are electronically inert. Vibrational spectroscopies further reveal that self-interstitials generate characteristic signatures. Short carbon-carbon bonds inside the defect cores give rise to high-frequency vibrational modes between 1375 and 1925 cm$^{-1}$, which are strongly IR-active but exhibit weak Raman activity. Taken together, these findings provide a coherent picture of the structural, electronic, and vibrational characteristics of carbon self-interstitials and establish a robust framework for their experimental identification.",0,arxiv,Kuantum,CC-BY/arXiv,From Mono- to Hexa-Interstitials: Computational Insights into Carbon Defects in Diamond
"Accurately predicting protein-ligand binding free energies (BFEs) remains a central challenge in drug discovery, particularly because the most reliable methods, such as free energy perturbation (FEP), are computationally intensive and difficult to scale. Here, we introduce a hybrid quantum-classical framework that combines Mining Minima sampling with quantum mechanically refined ligand partial charges, QM/MM interaction evaluation, and variational quantum eigensolver (VQE)-based electronic energy correction. This design enables explicit treatment of polarization, charge redistribution, and electronic correlation effects that are often underestimated in purely classical scoring schemes, while retaining computational efficiency. Across 23 protein targets and 543 ligands, the method achieves a mean absolute error of about 1.10 kcal/mol with strong rank-order fidelity (Pearson R = 0.75, Spearman rho = 0.76, Kendall tau = 0.57), consistent with the performance of contemporary FEP protocols. Notably, the workflow requires only about 25 minutes per ligand on standard compute resources, resulting in an approximate 20-fold reduction in computational cost relative to alchemical free energy approaches. This level of accuracy and efficiency makes the method well-suited for high-throughput lead optimization and iterative design cycles in pharmaceutical discovery. The framework also provides a natural foundation for future integration with machine learning models to enable predictive, large-scale, and adaptive screening strategies.",0,arxiv,Kuantum,CC-BY/arXiv,"Synergistic Computational Approaches for Accelerated Drug Discovery: Integrating Quantum Mechanics, Statistical Thermodynamics, and Quantum Computing"
"Semiconductor quantum dots embedded in circular Bragg gratings (CBGs) are among the most efficient integrated single-photon sources. However, the fully etched rings of conventional CBGs restrict the implementation of charge and Stark tuning via electrical contacts. To overcome this limitation, a labyrinth CBG geometry with four bridges has been proposed, yet the added bridges significantly degraded optical performance. In this work, we numerically demonstrate that a periodic labyrinth CBG design preserves both high coupling efficiency and strong Purcell enhancement while enabling electrical integration if optimized after introducing the bridges. We show three optimized designs at emission wavelengths of 780 nm, 930 nm, and 1550 nm, because these wavelengths are among the most relevant for quantum dots and show the general applicability of our approach. At all three wavelengths collection efficiencies exceeding 90% into a numerical aperture of 0.7 and Purcell factors greater than 25 are achieved. Furthermore, we propose a device layout incorporating a barrier layer that separates p- and n-doped semiconductor regions, which is incorporated to prevent tunneling of one of the charge carriers for selective charging. Also this design can be reoptimized to retain the performance of a device without tunnel barrier. These results establish labyrinth CBGs as a platform for electrically tunable quantum dot single-photon sources with high efficiency and scalability.",0,arxiv,Kuantum,CC-BY/arXiv,High-Performance Labyrinth Circular Bragg Grating Design for Charge and Stark-Tunable Quantum Light Sources Spanning Visible to Telecom Wavelengths
"The evolution of a system coupled to baths is commonly described by a master equation that, in the long-time limit, yields a steady-state density matrix. However, when the same evolution is unraveled into quantum trajectories, it is possible to observe a transition in the scaling of entanglement within the system as the system-bath coupling increases - a phenomenon that is invisible in the trajectory-averaged reduced density matrix of the system. Here, we go beyond the paradigm of trajectories from master equations and explore whether a qualitatively analogous entanglement-scaling transition emerges in the unitary evolution of the combined system-bath setup. We investigate the scaling of entanglement in a unitary quantum setup composed of a 2D lattice of free fermions, where each site is coupled to a fermionic bath. Varying the system-bath coupling reveals a transition from logarithmic-law to area-law scaling, visible in the logarithmic fermionic negativity, mutual information, and also in the correlations. This occurs while the system's steady-state properties are trivial, highlighting that the signatures of these different scalings are within the bath-bath correlations.",0,arxiv,Kuantum,CC-BY/arXiv,Entanglement transition in unitary system-bath dynamics
"We develop a finite-dimensional formulation of the recently introduced notion of ``timelike entanglement'', defined in terms of two-point functions between operators supported on different Cauchy slices. Using a local orthonormal operator basis, we recast this construction in terms of a generalized response tensor. Building on this, we introduce a generalized spacetime density kernel (GSDK) corresponding to higher-point correlation functions, including time-ordered as well as out-of-time-ordered correlators. We show that the Haar-averaged $(2N)$-point function yields the $(2N)$-th moment of the spectral form factor (SFF), evaluated at an $N$-enhanced effective temperature. The correlation functions of the GSDK operators also yield the SFF, with an effective $(1/N)$-reduction of the physical time-scales. The GSDK places both scrambling diagnostics and spectral statistics on a similar footing and clarifies how higher-point correlators and non-trivial time ordering capture fine-grained dynamical information of a quantum system.",0,arxiv,Kuantum,CC-BY/arXiv,Temporal correlations and chaos from spacetime kernel
"Vacuum fluctuations in quantum field theory impose fundamental limitations on our ability to measure time in short scales. To investigate the impact of universal quantum field theory effects on observer-dependent time measurements, we introduce a clock model based on the vacuum decay probability of a finite-sized quantum system. Using this model, we study a microscopic twin paradox scenario and find that, in the smallest scales, time is not only dependent on the trajectory connecting two events, but also on how vacuum fluctuations interact with the microscopic details of the clocks.",0,arxiv,Kuantum,CC-BY/arXiv,The Twin Paradox in Quantum Field Theory
"A critical step in developing circuits for quantum simulation is to synthesize a desired unitary operator using the circuit building blocks. Studying unitaries and their generators from the Lie algebraic perspective has given rise to several algorithms for synthesis based on a Cartan decomposition of the dynamical Lie algebra. For unitaries of the form $e^{-itH}$, such as time-independent Hamiltonian simulation, the resulting circuits have depth that does not depend on simulation time $t$. However, finding such circuits has a large classical overhead in the cost function evaluation and the high dimensional optimization problem. In this work, by further partitioning the dynamical Lie algebra, we break down the optimization problem into smaller independent subproblems. Moreover, the resulting algebraic structure allows us to easily shift the evaluation of the cost function to the quantum computer, further cutting the classical overhead of the algorithm. As an application of the new hybrid algorithm, we synthesize the time evolution unitary for the 4-site transverse field Ising model on several IBM devices and Quantinuum's H1-1 quantum computer.",0,arxiv,Kuantum,CC-BY/arXiv,RedCarD: A Quantum Assisted Algorithm for Fixed-Depth Unitary Synthesis via Cartan Decomposition
"The study of dynamic systems at the nanometer scale can benefit from the loss and background resilience offered by quantum two-photon interference. However, fast measurements with the required resolution are difficult to realize. As a solution, we introduce extreme energy entanglement between the photons undergoing interference. Using a flux probing analysis technique, we recover vibrational signals with frequencies as high as 21 kHz. Along with validating nanometer-scale precision and accuracy, we observe a significant quantum advantage when measuring in the presence of loss and background.",0,arxiv,Kuantum,CC-BY/arXiv,Entanglement-Enhanced Quantum Nano-Vibrometry
"Defects in solid-state materials play a central role in determining coherence, stability, and performance in quantum technologies. Although narrowband techniques can probe specific resonances with high precision, a broadband spectroscopic approach captures the full spectrum of defect properties and dynamics. Two-level system (TLS) defects in amorphous dielectrics are a particularly important example because they are major sources of decoherence and energy loss in superconducting quantum devices. However, accessing and characterizing their collective dynamics remains far more challenging than probing individual TLS defects. Building on our previously developed Broadband Cryogenic Transient Dielectric Spectroscopy (BCTDS) technique, we study the coherent control and time-resolved dynamics of TLS defect ensembles over a wide frequency range of 3-5 GHz without requiring full device fabrication, revealing quantum interference effects, memory-dependent dynamics, and dressed-state evolution within the TLS defect bath. The spectral response reveals distinct V-shaped structures corresponding to the bare eigenmode frequencies. Using these features, we extract a TLS defect spectral density of 84 GHz^-1 for a silicon sample, across a 4.1-4.6 GHz span. Furthermore, we systematically investigate amplitude- and phase-controlled interference fringes for multiple temperatures and inter-pulse delays, providing direct evidence of coherent dynamics and control. A driven minimal spin model with dipole-dipole interactions that qualitatively capture the observed behavior is presented. Our results establish BCTDS as a versatile platform for broadband defect spectroscopy, offering new capabilities for diagnosing and mitigating sources of decoherence, engineering many-body dynamics, and exploring non-equilibrium phenomena in disordered quantum systems.",0,arxiv,Kuantum,CC-BY/arXiv,Spectroscopy and Coherent Control of Two-Level System Defect Ensembles Using a Broadband 3D Waveguide
"We introduce an alternative receiver architecture for deep-space optical communication, in which a single large aperture is replaced by an array of smaller ones with outputs combined coherently, employing phase stabilization based on photon counting events. We show that it allows to increase the signal to noise ratio, thus potentially attaining higher information transmission rates in the regime of large noise, typical for daytime communication. We analyze its practical performance by simulating pulse position modulation-based communication from the recently launched Psyche mission. Under nighttime conditions the achieved performance is comparable to that offered by a single large aperture, whereas in daytime conditions the single photon coherent beam combination architecture provides an advantage in the information transmission rate.",0,arxiv,Kuantum,CC-BY/arXiv,Deep-Space Optical Communication Receiver Based on Single Photon Coherent Beam Combination
"Continuous measurements are central to quantum control and sensing, yet lack a model-independent operational description that can be applied to arbitrary non-Markovian processes without specifying a microscopic measurement model. Existing multi-time frameworks, such as process matrices, allow for an arbitrary sequence of operations to be applied on a general process, but are restricted to interventions at discrete times and cannot represent measurements of finite duration. We introduce a continuous-time extension of multi-time quantum processes based on process and operation functionals, which generalize the Feynman-Vernon influence functional and yield a continuous Born rule that cleanly separates processes from operations. This framework provides a consistent representation of non-Markovian dynamics under continuous monitoring and leads to a natural definition of Markovianity in continuous time. We illustrate the formalism by analyzing continuous measurements in a generalized Caldeira-Leggett model, demonstrating its applicability to realistic non-Markovian scenarios.",0,arxiv,Kuantum,CC-BY/arXiv,Continuous operations on non-Markovian processes
"Large superconducting quantum circuits have a number of important applications in quantum computing. Accurately predicting the performance of these devices from first principles is challenging, as it requires solving the many-body SchrÃ¶dinger equation. This work introduces a new, general ab-initio method for analyzing large quantum circuits based on lattice field theory, a tool commonly applied in nuclear and particle physics. This method is competitive with state-of-the-art techniques such as tensor networks, but avoids introducing systematic errors due to truncation of the infinite-dimensional Hilbert space associated with superconducting phases. The approach is applied to fluxonium, a specific many-component superconducting qubit with favorable qualities for quantum computation. A systematic study of the influence of impedance on fluxonium is conducted that parallels previous experimental studies, and ground capacitance effects are explored. The qubit frequency and charge noise dephasing rate are extracted from statistical analyses of charge noise, where thousands of instantiations of charge disorder in the Josephson junction array of a fixed fluxonium qubit are explicitly averaged over at the microscopic level. This is difficult to achieve with any other existing method.",0,arxiv,Kuantum,CC-BY/arXiv,Lattice field theory for superconducting circuits
"We develop a microscopic theory of magnon-exciton drag effect in a bilayer van der Waals antiferromagnetic semiconductor CrSBr. Effective exciton-magnon coupling arises from an orbital mechanism: Magnons tilt the layer magnetizations, enabling charge carrier tunneling that mixes intra- and interlayer excitons and thereby modulate the exciton energy. We derive the effective Hamiltonian of exciton-magnon coupling, based on our calculation of the magnon spectrum taking into account short-range exchange interaction between Cr-ion spins, single-ion anisotropy, and long-range dipole-dipole interactions. The latter produces a negative group velocity of magnons at small wavevectors. We show that despite rather small renormalization of exciton's energy and effective mass by the exciton-magnon interaction, the three key two-magnon processes: exciton-magnon scattering, two-magnon absorption by exciton, and two-magnon emission are highly efficient. By solving the Boltzmann kinetic equation, we evaluate short exciton-magnon scattering time which is in the sub-ps range and strongly decreases with the increase of magnon population. Hence, exciton-magnon scattering is likely to be dominant over other scattering processes related to the exciton-phonon and exciton-disorder interactions. We demonstrate that magnons can efficiently drag excitons, resulting in a large and nearly isotropic exciton propagation that can significantly exceed the intrinsic anisotropic diffusion. Our results provide a theoretical basis for recent observations of anomalous exciton transport in CrSBr [F. Dirnberger, et al., Nat. Nano. (2025)] and establish magnon-exciton drag as a powerful mechanism for controlling exciton propagation in magnetic systems.",0,arxiv,Kuantum,CC-BY/arXiv,Boltzmann transport theory of magnon-exciton drag
"Quantum algorithms have been identified as a potential means to accelerate computational fluid dynamics (CFD) simulations, with the lattice Boltzmann method (LBM) being a promising candidate for realizing quantum speedups. Here, we extend the recent quantum algorithm for the incompressible LBM to account for realistic fluid dynamics setups by incorporating walls, inlets, outlets, and external forcing. We analyze the associated complexity cost and show that these modifications preserve the asymptotic scaling, and potential quantum advantage, of the original algorithm. Moreover, to support our theoretical analysis, we provide a classical numerical study illustrating the accuracy, complexity, and convergence of the algorithm for representative incompressible-flow cases, including the driven Taylor-Green vortex, the lid-driven cavity flow, and the flow past a cylinder. Our results provide a pathway to accurate quantum simulation of nonlinear fluid dynamics, and a framework for extending quantum LBM to more challenging flow configurations.",0,arxiv,Kuantum,CC-BY/arXiv,Simulating non-trivial incompressible flows with a quantum lattice Boltzmann algorithm
"We investigate the asymptotic stability and ergodic properties of quantum trajectories under imperfect measurement, extending previous results established for the ideal case of perfect measurement. We establish a necessary and sufficient condition ensuring the convergence of the estimated trajectory, initialized from an estimated state, to the true trajectory. This result is obtained assuming that the associated quantum channel is irreducible. Building on this, we prove the uniqueness of the invariant measure and demonstrate convergence toward this measure.",0,arxiv,Kuantum,CC-BY/arXiv,Asymptotic stability and ergodic properties of quantum trajectories under imperfect measurement
"Erasing memory is a fundamental operational task in quantum information processing, governed by Landauer's principle, which links information loss to thermodynamic work. We introduce and analyze assisted quantum erasure, where correlations with a remote system reduce the energetic cost of resetting a memory. We identify exclusive control of erasure as the central operational requirement: only a designated party should be able to achieve the minimal cost, while any adversary necessarily fails. In the device-dependent regime, we show that entanglement of formation exactly characterizes exclusivity, establishing entanglement as the decisive thermodynamic resource. Moving to a one-sided device-independent scenario, in which only the memory holder's device is trusted, we develop an operational erasure protocol based on random dephasing and conditional operations. These results elevate quantum erasure from a thermodynamic constraint to an operational primitive: the erasure work cost quantifies secure, exclusive control over quantum memory, guaranteeing that an unauthorized agent cannot fully erase information under bounded work.",0,arxiv,Kuantum,CC-BY/arXiv,Exclusive Control of Quantum Memory Erasure
"Quantum coherence serves as a fundamental resource for generating intrinsic randomness, yet the quantification of randomness in quantum random number generators (QRNGs) based on spontaneous emission has remained largely phenomenological. Existing randomness analysis lacks rigorous adversarial models and a clear characterization of the role of quantum coherence in these systems. In this work, we develop a comprehensive quantum information-theoretic framework for randomness generation in spontaneous emission processes. We characterize two distinct eavesdropping strategies: one where the adversary directly accesses the atom ensemble, and the other where the adversary accesses only its purification. Our analysis reveals that when randomness is generated through single-photon detection and temporal mode measurements, the QRNG is vulnerable to the first adversary scenario, though it still guarantees a lower bound on intrinsic randomness against the second adversary scenario even under maximal information leakage from the atoms. In contrast, QRNGs based on spatial mode detection and phase fluctuations demonstrate security against both types of adversaries, providing robust randomness generation. Furthermore, we provide a quantitative calculation of intrinsic randomness for these spontaneous-emission-based QRNG schemes.",0,arxiv,Kuantum,CC-BY/arXiv,Randomness quantification in spontaneous emission
"We investigate the ground states and rich dynamics of vortices in spin-orbit coupled Bose-Einstein condensates (BEC) subject to position-dependent detuning. Such a detuning plays the role of an effective rotational frequency, causing the generation of a synthetic magnetic field. Through scanning the detuning gradient, we numerically obtain static vortex lattice structures containing 1 to 6 vortices using the coupled Gross-Pitaevskii equations. When quenching detuning gradient below its initial value, the vortex lattices exhibit interesting periodic rotation motion, and their dynamical stability can persist for up to 1000ms. In particular, depending on the detuning gradient, the twin vortices exhibit either a scissors-like rotational oscillation or a clockwise periodic rotation, reflecting the response to the magnetic field gradient experienced by the condensates. We fit the numerical results to quantitatively analyze the relation between rotation dynamics and magnetic field gradients. When quenching the detuning gradient beyond its initial value, additional vortices appear. Our findings may motivate further experimental studies of vortex dynamics in synthetic magnetic fields and offer insights for engineering a BEC-based magnetic field gradiometer.",0,arxiv,Kuantum,CC-BY/arXiv,Quenching dynamics of vortex in spin-orbit coupled Bose-Einstein condensates
"We report the results and unique instrument configuration of a novel experiment in which we successfully transitioned a DPPH sample from its natural paramagnetic state and essentially a non-magnetic material to a ferromagnetic state at room temperature. This was achieved using a specifically applied helical flux magnetic field. The DPPH sample (2,2-diphenyl-1-picrylhydrazyl) remained ferromagnetic for at least one hour after the experiment, indicating that a transformation in the material was induced by the external field rather than being merely a temporary magnetic phase transition observed only during the experiment. The external magnetic field used had a helical pitch angle of approximately $54.7Â°$, known mathematically as the Magic Angle, relative to the +z-axis, which is aligned with the normal S to N external field's magnetic moment vector. Based on the phenomenology of the experiment, we infer that this specific magic angle corresponding to the known quantization precession spin angle of free electrons under a homogeneous straight flux magnetic field potentially enhances the percentage of unpaired valence electrons within the DPPH material, allowing them to align in parallel with the applied external field. Typically, in paramagnetic materials, the distribution of unpaired electrons' quantum spins relative to an external field is nearly random, showing roughly a 50% chance of either parallel or antiparallel alignment. Only a slight majority preference exists in one alignment direction due to the Boltzmann thermal distribution, which contributes to the paramagnetic nature of these materials. In our measurements, we found that the induced ferromagnetism of the DPPH sample resulted in an abnormal thousand-fold decimal value increase in relative magnetic permeability at $Î¼{\approx}1.4$, compared to its typical paramagnetic value of $1.0001$ for this material.",0,arxiv,Kuantum,CC-BY/arXiv,Ferromagnetic Phase Transition of DPPH Induced by a Magic Angle Helical Magnetic Field
"Quantum technologies -- spanning communication, sensing, computing, and cryptography -- are rapidly emerging as critical paths of geopolitical competition and strategic defence innovation. Unlike traditional technological advances, quantum introduces novel capabilities that fundamentally disrupt established norms of security, intelligence, and diplomatic engagement. This strategic analysis explores the evolving quantum landscape through the dual lenses of diplomacy and geopolitics, with specific implications for defence leaders, policymakers, and industry stakeholders. The benefits and challenges of quantum technologies are examined from a diplomatic and geopolitical perspective to help leaders make informed strategic decisions. Leading powers now recognise quantum as a domain where technological leadership directly translates to geopolitical influence, compelling an intense race for dominance alongside new forms of multilateral diplomacy aimed at managing both risks and opportunities. Quantum technologies do not all have the same operational maturity, but technological progress is accelerating. Post-quantum cryptography demands immediate action -- every encrypted communication created today may be harvested and decrypted within the decade by adversaries equipped with quantum capabilities.",0,arxiv,Kuantum,CC-BY/arXiv,"Quantum, Diplomacy, and Geopolitics"
"Cavity quantum electrodynamics with atomic ensembles is typically associated with collective spin phenomena, such as superradiance and spin squeezing, in which the atoms evolve collectively as a macroscopic spin ($S\sim N/2$) on the Bloch sphere. Surprisingly, we show that the tendency toward a collective spin description need not imply collective spin phenomena; rather, it can be exploited to generate new forms of strongly correlated quantum matter. The key idea is to use uniform cavity-mediated interactions to energetically project the system into the total-spin singlet sector ($S=0$) - a highly entangled subspace where the physics is governed entirely by cavity fluctuations. Focusing on Rydberg atom arrays coupled to a single-mode cavity, we show that global cavity fluctuations can effectively squeeze classical antiferromagnets into quantum spin liquids, characterized by non-local entanglement, fractionalized excitations, and emergent gauge fields. This work suggests that cavity QED can be a surprising resource for inducing strongly correlated phenomena, which could be explored in the new generation of hybrid tweezer-cavity platforms.",0,arxiv,Kuantum,CC-BY/arXiv,Squeezing Classical Antiferromagnets into Quantum Spin Liquids via Global Cavity Fluctuations
"We investigate mixed eigenstates in systems with sharply-divided phase space, using different piecewise-linear maps whose regular-chaotic boundaries are formed by marginally unstable periodic orbits (MUPOs) or by quasi-periodic orbits. With the overlap index and the entropy localization length, we classify mixed eigenstates and show that the contribution from dynamical tunneling scales as $\sim \hbar\, \exp(-b/\hbar)$, with $b>0$ associated with the relative size of the regular region. The dominant fraction of states that remain sticky to the boundaries, referred to as sticky eigenstates, scales as $\hbar^{1/2}$ in the MUPO case and oscillates around this algebraic behavior in the quasi-periodic case. This behavior generalizes established predictions for hierarchical states in KAM systems, which scale as $\hbar^{1 - 1/Î³}$, with $Î³$ set by the corresponding classical stickiness reflected in the algebraic decay of cumulative RTDs $t^{-Î³}$. For the piecewise-linear maps studied here, $Î³= 2$. These results reveal a clear quantum signature of classical stickiness in non-KAM systems.",0,arxiv,Kuantum,CC-BY/arXiv,Sticky eigenstates in systems with sharply-divided phase space
"Phase space quasi-probability functions provide powerful representations of quantum states and operators, as well as criteria for assessing quantum computational resources. In discrete, odd-dimensional systems (qudits), protocols involving only non-negative phase space distributions can be efficiently classically simulated. For bosonic systems, defined in continuous variables, phase space negativities are likewise necessary to prevent efficient classical simulation of the underlying physical processes. However, when quantum information is encoded in bosonic systems, this connection becomes subtler: as negativity is only a necessary property for potential quantum advantage, encoding (i.e., physical) states may exhibit large negativities while still corresponding to architectures that remain classically simulable. Several frameworks have attempted to relate non-negativity of states and gates in the computational phase space to non-negativity of processes in the physical bosonic phase space, but a consistent correspondence remains elusive. Here, we introduce a general framework that connects the physical phase space structure of bosonic systems to their encoded computational representations across arbitrary dimensions and encodings. This framework highlights the key role of the reference frame-equivalently, the choice of vacuum-in defining the computational basis and linking its phase space simulability properties to those of the physical system. Finally, we provide computational and physical interpretations of the planar (quadrature-like) phase space limit, where genuinely quantum features may gradually vanish, yielding classically simulable behavior.",0,arxiv,Kuantum,CC-BY/arXiv,"Heisenberg-Weyl bosonic phase spaces: emergence, constraints and quantum informational resources"
"This paper is concerned with a quantum memory system for storing quantum information in the form of its initial dynamic variables in the presence of environmental noise. In order to compensate for the deviation from the initial conditions, the classical parameters of the system Hamiltonian are affected by the actuator output of a measurement-based classical controller. The latter uses an observation process produced by a measuring apparatus from the quantum output field of the memory system. The underlying system is modelled as an open quantum harmonic oscillator whose Heisenberg evolution is governed by linear Hudson-Parthasarathy quantum stochastic differential equations. The controller is organised as a classical linear time-varying system, so that the resulting closed-loop system has quantum and classical dynamic variables. We apply linear-quadratic-Gaussian control and fixed-point smoothing at the level of the first two moments and consider controllers with a separation structure which involve a continuously updated estimate for the initial quantum variables. The initial-point smoother is used for actuator signal formation so as to minimise the sum of a mean-square deviation of the quantum memory system variables at a given time horizon from their initial values and an integral quadratic penalty on the control signal.",0,arxiv,Kuantum,CC-BY/arXiv,Measurement-based Initial Point Smoothing and Control Approach to Quantum Memory Systems
"This paper systematically investigates the geometry of fundamental quantum cones, the separable cone ($\mathscr{P}_+$) and the Positive Partial Transpose (PPT) cone ($\mathcal{P}_{\mathrm{PPT}}$), under generalized non-commutative convexity. We demonstrate a sharp stability dichotomy analyzing $C^*$-convex hulls of these cones: while $\mathscr{P}_+$ remains stable under local $C^*$-convex combinations, its global $C^*$-convex hull collapses entirely to the cone of all positive semidefinite matrices, $\operatorname{MCL}(\mathscr{P}_+) = \mathscr{P}_0$. To gain finer control and classify intermediate structures, we introduce the concept of ``$k$-$C^*$-convexity'', by using the operator Schmidt rank of $C^*$-coefficients. This constraint defines a new hierarchy of nested intermediate cones, $\operatorname{MCL}_k(\cdot)$. We prove that this hierarchy precisely recovers the known Schmidt number cones for the separable case, establishing a generalized convexity characterization: $\operatorname{MCL}_k(\mathscr{P}_+) = \mathcal{T}_k$. Applied to the PPT cone, this framework generates a family of conjectured non-trivial intermediate cones, $\mathcal{C}_{\mathrm{PPT}, k}$.",0,arxiv,Kuantum,CC-BY/arXiv,A Hierarchy of Entanglement Cones via Rank-Constrained $C^*$-Convex Hulls
"We present a simple and direct method for measuring laser chirp rate, i.e., group delay dispersion (GDD) of ultrashort laser pulses at power levels compatible with single-quantum-emitter excitation. Traditional pulse characterization techniques rely on nonlinear optical processes that require high peak powers, making them unsuitable for the attojoule-to-femtojoule regime relevant to quantum photonics. Our approach utilizes a wavelength-to-time mapping method in which the arrival times of spectrally filtered components of a broadband pulse are recorded using a superconducting nanowire single-photon detector and correlated via a high-resolution time-tagging system. The resulting linear relationship between wavelength and arrival time directly yields the dispersion parameter and, subsequently, the GDD. Beyond single-emitter excitation, this technique can be applied in areas such as single-photon spectroscopy, ultralow-power optical communications, and time-domain quantum control, where linear and non-destructive dispersion characterization is essential.",0,arxiv,Kuantum,CC-BY/arXiv,How to measure laser chirp rate at single-emitter excitation energies
"We investigate two senders and one receiver multiparty communication scenario. Following Phys.Rev.A83, 062112 and arXiv : 2506.07699, we study multiparty communication bounded by dimension and distinguishability. We provide an explicit characterization of the classical correlations achievable under these constraints. We then demonstrate that quantum communication systematically exceeds these classical limits, even in the absence of preshared entanglement and without any input choice for the receiver. Furthermore, we implement semidefinite hierarchy tools tailored to the two-sender, one-receiver setting for both types of constraints considered. Our results reveal a clear quantum advantage in multiparty communication under those restrictions.",0,arxiv,Kuantum,CC-BY/arXiv,Quantum advantages in multiparty communication
"Quantum key distribution (QKD) enables information-theoretically secure communication against eavesdropping. However, phase instability remains a challenge across many QKD applications, particularly in schemes such as twin-field QKD and measurement-device-independent QKD. The most dominant source of phase fluctuation arises from the frequency offset between independent lasers. Here we propose a method to address this issue by employing a classical photodiode to compensate for the laser frequency difference. As an application of this method, we implement this technique in a mode-pairing QKD system, achieving an error rate approaching the theoretical limit and surpassing the linear key-rate bound over a fiber distance of 296.8 km. This approach provides a practical solution for frequency matching between independent lasers and can be extended to other fields requiring precise phase stabilization.",0,arxiv,Kuantum,CC-BY/arXiv,Frequency-matching quantum key distribution
"Quantum-centric supercomputing (QCSC) workflows often involve hybrid classical-quantum algorithms that are inherently probabilistic and executed on remote quantum hardware, making them difficult to interpret and limiting the ability to monitor runtime performance and behavior. The high cost of quantum circuit execution and large-scale high-performance computing (HPC) infrastructure further restricts the number of feasible trials, making comprehensive evaluation of execution results essential for iterative development. We propose an observability architecture tailored for QCSC workflows that decouples telemetry collection from workload execution, enabling persistent monitoring across system and algorithmic layers and retaining detailed execution data for reproducible and retrospective analysis, eliminating redundant runs. Applied to a representative workflow involving sample-based quantum diagonalization, our system reveals solver behavior across multiple iterations. This approach enhances transparency and reproducibility in QCSC environments, supporting infrastructure-aware algorithm design and systematic experimentation.",0,arxiv,Kuantum,CC-BY/arXiv,Observability Architecture for Quantum-Centric Supercomputing Workflows
"Quantum geometry is a differential geometry based on quantum mechanics. It is related to various transport and optical properties in condensed matter physics. The Zeeman quantum geometry is a generalization of quantum geometry including the spin degrees of freedom. It is related to electromagnetic cross responses. Quantum geometry is generalized to non-Hermitian systems and density matrices. Especially, the latter is quantum information geometry, where the quantum Fisher information naturally arises as quantum metric. We apply these results to the $X$-wave magnets, which include $d$-wave, $g$-wave and $i$-wave altermagnets as well as $p$-wave and $f$-wave magnets. They have universal physics for anomalous Hall conductivity, tunneling magneto-resistance and planar Hall effect. We obtain various analytic formulas based on the two-band Hamiltonian.",0,arxiv,Kuantum,CC-BY/arXiv,"Quantum geometry and $X$-wave magnets with $X=p,d,f,g,i$"
"In hierarchal order of molecular geometry, we compare the performances of Geometric Quantum Machine Learning models. Two molecular datasets are considered: the simplistic linear shaped LiH-molecule and the trigonal pyramidal molecule NH3. Both accuracy and generalizability metrics are considered. A classical equivariant model is used as a baseline for the performance comparison. The comparative performance of Quantum Machine Learning models with no symmetry equivariance, rotational and permutational equivariance, and graph embedded permutational equivariance is investigated. The performance differentials and the molecular geometry in question reveals the criteria for choice of models for generalizability. Graph embedding of features is shown to be an effective pathway to greater trainability for geometric datasets. Permutational symmetric embedding is found to be the most generalizable quantum Machine Learning model for geometric learning.",0,arxiv,Kuantum,CC-BY/arXiv,PERM EQ x GRAPH EQ: Equivariant Neural Networks for Quantum Molecular Learning
"State transfer between light and microwaves is a key challenge in quantum networks. Promising transducers use a mechanical intermediary that couples to both fields via radiation pressure. Such electro-optomechanical devices have achieved high efficiencies, yet require resolved-sideband cavities, and generally compromise in scalability and noise performance. Here, we relax this constraint by extending the protocol of Navarathna et al. that transfers optical quantum information onto a mechanical resonator using a broadband, sideband-unresolved cavity and feedback. Combining this with parametric mechanical-to-microwave conversion, we show that continuous optical-to-microwave quantum state transfer is possible using measurement-based feedback, while all-optical coherent feedback enables bidirectional transfer. To assess the transfer, we introduce the quantum transfer witness $\mathcal{W}_T$, which -- though similar to the input-referred added noise -- also identifies whether a channel is capable of both preserving Gaussian entanglement and outperforming classical transduction schemes. Finally, we show that quantum-compatible noise performance is within reach of current experimental capabilities. Our results unlock a new design space for electro-optomechanical transducers and strengthens their candidacy as scalable quantum links between distant nodes.",0,arxiv,Kuantum,CC-BY/arXiv,Mechanically mediated optical-microwave quantum state transfer by feedback
"We study single-copy shadow tomography in the adversarial robust setting, where the goal is to learn the expectation values of $M$ observables $O_1, \ldots, O_M$ with $\varepsilon$ accuracy, but $Î³$-fraction of the outcomes can be arbitrarily corrupted by an adversary. We show that all non-adaptive shadow tomography algorithms must incur an error of $\varepsilon=\tildeÎ©(Î³\min\{\sqrt{M}, \sqrt{d}\})$ for some choice of observables, even with unlimited copies. Unfortunately, the classical shadows algorithm by [HKP20] and naive algorithms that directly measure each observable suffer even more. We design an algorithm that achieves an error of $\varepsilon=\tilde{O}(Î³\max_{i\in[M]}\|O_i\|_{HS})$, which nearly matches our worst-case error lower bound for $M\ge d$ and guarantees better accuracy when the observables have stronger structure. Remarkably, the algorithm only needs $n=\frac{1}{Î³^2}\log(M/Î´)$ copies to achieve that error with probability at least $1-Î´$, matching the sample complexity of the classical shadows algorithm that achieves the same error without corrupted measurement outcomes. Our algorithm is conceptually simple and easy to implement. Classical simulation for fidelity estimation shows that our algorithm enjoys much stronger robustness than [HKP20] under adversarial noise. Finally, based on a reduction from full-state tomography to shadow tomography, we prove that for rank $r$ states, both the near-optimal asymptotic error of $\varepsilon=\tilde{O}(Î³\sqrt{r})$ and copy complexity $\tilde{O}(dr^2/\varepsilon^2)=\tilde{O}(dr/Î³^2)$ can be achieved for adversarially robust state tomography, closing the large gap in [ABCL25] where optimal error can only be achieved using pseudo-polynomial number of copies in $d$.",0,arxiv,Kuantum,CC-BY/arXiv,Shadow Tomography Against Adversaries
"Monte Carlo methods are widely used to estimate observables in many-body quantum systems. However, conventional sampling schemes often require a large number of samples to achieve sufficient accuracy. In this work we propose the concentrated Monte Carlo sampling approach, which builds on the idea that in systems with only short range correlations, to obtain accurate expectation values for local observables, one would favor detailed information in the surroundings of this observable compared to far away from it. In this approach we consider all possible configurations in the surroundings of a local observable, and unique samples from the remaining of the setup drawn using Markov chain Monte Carlo. We have tested the performance of this approach for ground states of the spin-1/2 tilted Ising model in different phases, and also for thermal states in the a spin-1 bilinear-biquadratic model. Our results demonstrate that CMCS yields higher accuracy for local observables in short-range correlated states while requiring substantially fewer samples, showcasing in which regimes one can obtain acceleration for the evaluation of expectation values.",0,arxiv,Kuantum,CC-BY/arXiv,Concentrated Monte Carlo sampling for local observables in quantum spin chains
"The acceleration-induced transparency (AIT) effect has been suggested recently to amply the transition probability of the two-level detctor and offers a potential avenue for the experimental detection of the Unruh effect. In this paper, we explore the influence of the AIT effect on quantum entanglement between two detectors accelerated in a thermal field background, since the thermal backgound field cannot be avoided completely in any experiments. Interestingly, we find that although the backgound thermal field generally degrade the entanglement between the detectors, the AIT effect can effectively protect it.",0,arxiv,Kuantum,CC-BY/arXiv,Entanglement is protected by acceleration-induced transparency in thermal field
"We examine the classical limit of a fairly general nonlinear semiclassical hybrid system within a MaxEnt framework. The consistency of the hybrid dynamics requires algebraic constraints on quantum operators and smoothness conditions for the classical variables. Analytically, we demonstrate that the classical limit is characterized by a pure density matrix representing a single state, which reproduces the dynamics of its classical analogue. To illustrate the methodology, we revisit and synthesize two previously studied examples.",0,arxiv,Kuantum,CC-BY/arXiv,Nonlinear Classical Dynamics described by a Density Matrix in the Classical Limit
"On-demand authentication is critical for scalable quantum systems, yet current approaches require the signer to initiate communication, creating unnecessary overhead. We introduce a new method where the verifier can request authentication only when needed, improving efficiency for quantum networks and blockchain applications. Our approach adapts the concept of zero-knowledge proofs widely used in classical cryptography to quantum settings, ensuring that verification reveals nothing about secret keys. We develop a general framework that converts any suitable quantum proof into a verifier-driven signature protocol and present a concrete implementation based on quantum measurements. The protocol achieves strong security guarantees, including resistance to forgery and privacy against curious verifiers, without relying on computational hardness assumptions and with qubit technologies. This work delivers the first general verifier-initiated quantum signature scheme with formal security, paving the way for scalable, secure authentication in future quantum infrastructures and decentralized systems.",0,arxiv,Kuantum,CC-BY/arXiv,Verifier-initiated quantum message-authentication via quantum zero-knowledge proofs
"Microwave (MW) field sensing is foundational to modern technology, yet its evolution, reliant on classical antennas, is constrained by fundamental physical limits on field, temporal, and spatial resolutions. Here, we demonstrate an MW electrometry that simultaneously surpasses these constraints by using individual Rydberg atoms in an optical tweezer array as coherent sensors. This approach achieves a field sensitivity within 13% of the standard quantum limit, a response time that exceeds the Chu limit by more than 11 orders of magnitude, and in-situ near-field mapping with Î»/3000 spatial resolution. This work establishes Rydberg-atom arrays as a powerful platform that unites quantum-limited sensitivity, nanosecond-scale response time, and sub-micrometer resolution, opening new avenues in quantum metrology and precision electromagnetic field imaging.",0,arxiv,Kuantum,CC-BY/arXiv,Microwave electrometry with quantum-limited resolutions in a Rydberg atom array
"Superconducting transmon qubits are commonly made with thin-film Nb wiring, but recent studies have shown increased performance with Ta wiring. In this work, we compare the resonator-induced single photon, millikelvin dielectric loss for pentoxides of Nb (Nb2O5) and Ta (Ta2O5) in order to further understand limiting losses in qubits. Nb and Ta pentoxides of three thicknesses are deposited via pulsed laser deposition onto identical coplanar waveguide resonators. The two-level system (TLS) loss in Nb2O5 is determined to be about 30% higher than that of Ta2O5. This work indicates that qubits with Nb wiring are affected by higher loss arising from the native pentoxide itself, likely in addition to the presence of suboxides, which are largely absent in Ta.",0,arxiv,Kuantum,CC-BY/arXiv,Comparison of Nb and Ta Pentoxide Loss Tangents for Superconducting Quantum Devices
"High-fidelity quantum gates are a cornerstone of any quantum computing and communications architecture. Realizing such control in the presence of realistic errors at the level required for beyond-threshold quantum error correction is a long-standing challenge for all quantum hardware platforms. Here we theoretically develop and experimentally demonstrate error-protected quantum gates in a solid-state quantum network node. Our work combines room-temperature randomized benchmarking with a new class of composite pulses that are simultaneously robust to frequency and amplitude, affecting random and systematic errors. We introduce Power-Unaffected, Doubly-Detuning-Insensitive Gates (PUDDINGs) -- a theoretical framework for constructing conditional gates with immunity to both amplitude and frequency errors. For single-qubit and two-qubit CNOT gate demonstrations in a solid-state nitrogen-vacancy (NV) center in diamond, we systematically measure an improvement in the error per gate up to a factor of 9. By projecting the application of PUDDING to cryogenic temperatures we show a record two-qubit error per gate of $1.2 \times 10^{-5}$, corresponding to a fidelity of $99.9988\%$, far below the thresholds required by surface and color code error correction. These results present viable building blocks for a new class of fault-tolerant quantum networks and represent the first experimental realization of error-protected conditional gates in solid-state systems.",0,arxiv,Kuantum,CC-BY/arXiv,"Highly resilient, error-protected quantum gates in a solid-state quantum network node"
"Photoelectrical detection of magnetic resonance (PDMR) offers a scalable alternative to optical readout of spin defects in semiconductors and is particularly promising for near-infrared (NIR) emitters, where photodetection is often challenging. Here, we demonstrate room-temperature coherent PDMR of PL3 (divacancy), PL5, PL6, and PL7 spins. PL7 and PL5 exhibit notably stronger PDMR than PL6 as opposed to optical detection, indicating higher ionization efficiency and suitability for electrical readout. Rabi oscillation and two-frequency spectroscopy reveal a previously undiscovered secondary resonance of PL7. We determine the zero-field splitting parameters of PL7 and assign the recently reported PL3a defect to PL7. The demonstrated PDMR of these NIR defects constitutes a key advancement toward quantum electronic devices. Also, the clarified spin parameters and ionization characteristics provide a solid foundation for advancing quantum technologies utilizing these defects regardless of the detection schemes.",0,arxiv,Kuantum,CC-BY/arXiv,Photoelectrical detection and characterization of divacancy and PL5-PL7 spins in silicon carbide
"We develop a non-equilibrium quantum field theory of the free-electron laser based on the Preparata model, using the real-time Keldysh formalism. Starting from a microscopic Lagrangian for a relativistic electron beam coupled to a single radiation mode, we construct a Keldysh functional integral, perform the large-N rescaling, and integrate out the electronic degrees of freedom. This yields an effective action for the FEL mode in which dispersion, gain, and noise are all generated by a single electronic self-energy built from the current correlations of the beam. For a stationary Gaussian beam, we obtain closed analytic expressions for the retarded and Keldysh components of the self-energy, which directly encode frequency pulling, gain reduction due to energy spread, and the noise spectrum experienced by the field. At low frequency, the theory reduces to a Landau-Ginzburg-Keldysh description of a single complex mode with a mass, growth rate, nonlinearity, and noise strength fully determined by beam current, energy spread, and detuning. In this framework, the FEL threshold appears as a continuous non-equilibrium phase transition in the laser universality class: the coherent field amplitude plays the role of an order parameter, while the amplitude of critical fluctuations is fixed by the microscopic noise kernel. The result is a minimal open quantum field theory analog of Vlasov-Maxwell FEL theory, in which gain, dispersion, and noise arise from a unified self-energy framework rather than from separate phenomenological ingredients.",0,arxiv,Kuantum,CC-BY/arXiv,Non-equilibrium quantum field theory of the free-electron laser in Keldysh formalism
"Quantum entanglement, in the form of spin squeezing, is known to improve the sensitivity of atomic sensors to static or slowly varying fields. Sensing transient events presents a distinct challenge, requires different analysis tools, and has not been shown to benefit from entanglement in practically important scenarios such as spin-precession magnetometry. To address this, we apply concepts from continuous quantum measurements and estimation theory to optical atomic magnetometers, aiming to accurately model these devices, interpret their measurement data, control their dynamics, and achieve optimal sensitivity. Quantifying this optimal performance requires determining a fundamental quantum limit on sensitivity. We derive this limit, imposed by noise, and show that it scales at best linearly with sensing time and atom number N, ruling out any super-classical scaling. This limit is independent of the initial state, measurement, estimator, and measurement-based feedback, and depends only on the decoherence model and the strength of field fluctuations. Thus, finding an estimator that attains this bound proves the sensing strategy optimal. To approach this limit, we develop a quantum dynamical model scalable with N, based on a co-moving Gaussian approximation of the stochastic master equation, which includes measurement backaction and decoherence. This enables a real-time estimation and control architecture integrating an extended Kalman filter with a linear quadratic regulator. Simulating the magnetometer with our model and EKF+LQR strategy shows that quantum-limited tracking of constant and fluctuating fields is within reach of current atomic magnetometers. Our sensing strategy can also track biologically relevant signals, such as heartbeat-like waveforms, and drive the atomic ensemble into an entangled state, even when the measurement record is used for feedback but later discarded.",0,arxiv,Kuantum,CC-BY/arXiv,Real-time optimal quantum control for atomic magnetometers with decoherence
"Quantum correlations that typically develop between a quantum battery and its charger reduce the amount of work extractable from the battery. We show that by coupling the system with an additional environment that can be continuously monitored, one can weaken these correlations and enhance work extraction beyond what is achievable in the ideal (closed system) limit. This general mechanism is illustrated using both a cavity-mediated spin-spin and Dicke quantum battery models.",0,arxiv,Kuantum,CC-BY/arXiv,Boosting Work Extraction in Quantum Batteries via Continuous Environment Monitoring
"Quantum algorithms for partial differential equations (PDEs) face severe practical constraints on near-term hardware: limited qubit counts restrict spatial resolution to coarse grids, while circuit depth limitations prevent accurate long-time integration. These hardware bottlenecks confine quantum PDE solvers to low-fidelity regimes despite their theoretical potential for computational speedup. We introduce a multifidelity learning framework that corrects coarse quantum solutions to high-fidelity accuracy using sparse classical training data, facilitating the path toward practical quantum utility for scientific computing. The approach trains a low-fidelity surrogate on abundant quantum solver outputs, then learns correction mappings through a multifidelity neural architecture that balances linear and nonlinear transformations. Demonstrated on benchmark nonlinear PDEs including viscous Burgers equation and incompressible Navier-Stokes flows via quantum lattice Boltzmann methods, the framework successfully corrects coarse quantum predictions and achieves temporal extrapolation well beyond the classical training window. This strategy illustrates how one can reduce expensive high-fidelity simulation requirements while producing predictions that are competitive with classical accuracy. By bridging the gap between hardware-limited quantum simulations and application requirements, this work establishes a pathway for extracting computational value from current quantum devices in real-world scientific applications, advancing both algorithm development and practical deployment of near-term quantum computing for computational physics.",0,arxiv,Kuantum,CC-BY/arXiv,Bridging quantum and classical computing for partial differential equations through multifidelity machine learning
"The simulation of large-scale quantum systems is one of the most sought-after applications of quantum computers. Of particular interest for near-term demonstrations of quantum computational advantage are analog quantum simulations, which employ analog controls instead of digitized gates. Most analog quantum simulations to date, however, have been performed using qubit-based processors, despite the fact that many physical systems are more naturally represented in terms of qudits (i.e., $d$-level systems). Motivated by this, we present an experimental realization of the Lipkin-Meshkov-Glick (LMG) model using an analog simulator based on a single superconducting transmon qudit with up to $d = 9$ levels. This is accomplished by moving to a rotated frame in which evolution under any time-dependent local field and one-axis twisting can be realized by the application of multiple simultaneous drives. Combining this analog drive scheme with universal control and single-shot readout of the qudit state, we provide a detailed study of five finite-size precursors of quantum criticality in the LMG model: dynamical phase transitions, closing of the energy gap, Kibble-Zurek-like dynamics, statistics of the order parameter, and excited-state phase transitions. For each experiment we devise a protocol for extracting the relevant properties which does not require any prior knowledge of the system eigenstates, and can therefore be readily extended to higher dimensions or more complicated models. Our results cement high-dimensional transmon qudits as an exciting path towards simulating many-body physics.",0,arxiv,Kuantum,CC-BY/arXiv,Analog quantum simulation of the Lipkin-Meshkov-Glick model in a transmon qudit
"Simulations of energy loss and hadronization are essential for understanding a range of phenomena in non-equilibrium strongly-interacting matter. We establish a framework for performing such simulations on a quantum computer and apply it to a heavy quark moving across a modest-sized 1+1D SU(2) lattice of light quarks. Conceptual advances with regard to simulations of non-Abelian versus Abelian theories are developed, allowing for the evolution of the energy in light quarks, of their local non-Abelian charge densities, and of their multi-partite entanglement to be computed. The non-trivial action of non-Abelian charge operators on arbitrary states suggests mapping the heavy quarks to qubits alongside the light quarks, and limits the heavy-quark motion to discrete steps among spatial lattice sites. Further, the color entanglement among the heavy quarks and light quarks is implemented using hadronic operators, and Domain Decomposition is shown to be effective in quantum state preparation. Scalable quantum circuits that account for the heterogeneity of non-Abelian charge sectors across the lattice are used to prepare the interacting ground-state wavefunction in the presence of heavy quarks. The discrete motion of heavy quarks between adjacent spatial sites is implemented using fermionic SWAP operations. Quantum simulations of the dynamics of a system on $L=3$ spatial sites are performed using IBM's ${\tt ibm\_pittsburgh}$ quantum computer using 18 qubits, for which the circuits for state preparation, motion, and one second-order Trotter step of time evolution have a two-qubit depth of 398. A suite of error mitigation techniques are used to extract the observables from the simulations, providing results that are in good agreement with classical simulations. The framework presented here generalizes straightforwardly to other non-Abelian groups, including SU(3) for quantum chromodynamics.",0,arxiv,Kuantum,CC-BY/arXiv,A Framework for Quantum Simulations of Energy-Loss and Hadronization in Non-Abelian Gauge Theories: SU(2) Lattice Gauge Theory in 1+1D
"In this paper, we extend a previously presented Grover-based heuristic to tackle general combinatorial optimization problems with linear constraints. We further describe the introduced method as a framework that enables performance improvements through circuit optimization and machine learning techniques. Comparisons with state-of-the-art classical solvers further demonstrate the algorithm's potential to achieve a quantum advantage in terms of speed, given appropriate quantum hardware.",0,arxiv,Kuantum,CC-BY/arXiv,Constraint-oriented biased quantum search for linear constrained combinatorial optimization problems
"Continuous-variables (CV) quantum optics is a natural formalism for neural networks (NNs) due to its ability to reproduce the information processing of such trainable interconnected systems. In quantum optics, Gaussian operators induce affine mappings on the quadratures of optical modes while non-Gaussian resources -- the challenging piece for physical implementation -- originate the nonlinear effects, unlocking quantum analogs of an artificial neuron. This work presents a novel experimentally-feasible framework for continuous-variable quantum optical neural networks (QONNs) developed with available photonic components: coherent states as input encoding, a general Gaussian transformation followed by multi-mode photon subtractions as the processing layer, and homodyne detection as outputs readout. The closed-form expressions of such architecture are derived demonstrating the family of adaptive activations and the quantum-optical neurons that emerge from the amount of photon-subtracted modes, proving that the proposed design satisfies the Universal Approximation Theorem within a single layer. To classically simulate the QONN training, the high-performance QuaNNTO library has been developed based on Wick--Isserlis expansion and Bogoliubov transformations, allowing multi-layer exact expectation values of non-Gaussian states without truncating the infinite-dimensional Hilbert space. Experiments on supervised learning and state-preparation tasks show balanced-resource efficiency with strong expressivity and generalization capabilities, illustrating the potential of the architecture for scalable photonic quantum machine learning and for quantum applications such as complex non-Gaussian gate synthesis.",0,arxiv,Kuantum,CC-BY/arXiv,Hardware-inspired Continuous Variables Quantum Optical Neural Networks
"In this work, we detail different approaches to treat multi-mode photonic environments within non-relativistic quantum electrodynamics in the long-wavelength approximation efficiently. Specifically we show that for equilibrium properties of coupled light-matter systems, we can approximately capture the effects of multi-mode photonic environments on matter systems by either only keeping the polarization part of the electric field in the length-gauge formulation or by a few effective modes. We present a comprehensive set of approximation methods designed to accurately capture equilibrium phenomena in quantum light-matter systems across a range of complex photonic environments, from weak to strong coupling. These methods are applied to atomic and molecular models as well as to a two-dimensional quantum ring, demonstrating the versatility of our approach and laying the groundwork for first-principles simulations of real materials in cavity quantum electrodynamics.",0,arxiv,Kuantum,CC-BY/arXiv,Multimode equilibrium approximations in light-matter systems from weak to strong coupling
"Efficient encoding of classical data into quantum circuits is a critical challenge that directly impacts the scalability of quantum algorithms. In this work, we present an automated compilation framework for resource-aware quantum data loading tailored to a given input vector and target error tolerance. By explicitly exploiting the trade-off between exact and approximate state preparation, our approach systematically partitions the total error budget between precision and approximation errors, thereby minimizing quantum resource costs. The framework supports a comprehensive suite of state-of-the-art methods, including multiplexer-based loaders, quantum read-only memory (QROM) constructions, sparse encodings, matrix product states (MPS), Fourier series loaders (FSL), and Walsh transform-based diagonal operators. We demonstrate the effectiveness of our framework across several applications, where it consistently uncovers non-obvious, resource-efficient strategies enabled by controlled approximation. In particular, we analyze a computational fluid dynamics workflow where the automated selection of MPS state preparation and Walsh transform-based encoding, combined with a novel Walsh-based measurement technique, leads to resource reductions of over four orders of magnitude compared to previous approaches. We also introduce two independent advances developed through the framework: a more efficient circuit for d-diagonal matrices, and an optimized block encoding for kinetic energy operators. Our results underscore the indispensable role of automated, approximation-aware compilation in making large-scale quantum algorithms feasible on resource-constrained hardware.",0,arxiv,Kuantum,CC-BY/arXiv,Quantum compilation framework for data loading
"Building on recent advances in quantum algorithms which measure and reuse qubits and in efficient classical simulation leveraging projective measurements, we extend these frameworks to real-time dynamics of quantum many-body systems undergoing discrete-time and continuous-time Hamiltonian evolution, and find improvements that significantly reduce sampling overhead. The approach exploits causal light-cone structure by interleaving time and space evolution and applying projective measurements as soon as local subsystems reach the target physical time, suppressing entanglement growth. Comparing to time-evolving block decimation, the method reaches longer times per sample for the same resources. We also gain the ability to study dynamics of entanglement that would be occurring on quantum hardware when following similar protocols, such as the holographic quantum dynamics simulation framework. We show how to efficiently obtain local observables as well as equal-time and time-dependent correlation functions. Our findings show how optimizations for quantum hardware can benefit classical tensor network simulations and how such classical methods can yield insights into the utility of quantum simulations.",0,arxiv,Kuantum,CC-BY/arXiv,Investigating a Quantum-Inspired Method for Quantum Dynamics
"We study the emergence of chaos in multilevel atoms with all-to-all interactions, inspired by cavity QED. Focusing on a 3-level Tavis-Cummings model in a far detuned limit, we detail its deep Hilbert space structure -- i.e. we enumerate all distinct dynamical sectors, beyond the totally symmetric subspace -- by using the Schur-Weyl duality, which is applicable thanks to the permutation symmetry in the all-to-all Hamiltonian. Strong Hilbert space fragmentation ensues from the non-abelian nature of the symmetry, with some sectors displaying regular dynamics and others being chaotic. We uncover that many permutation symmetry sectors contribute to the dynamics in the classical limit, in addition to the commonly studied totally symmetric subspace. To elucidate the dynamical responses in each of the symmetry sectors, we propose a semiclassical description in terms of spin coherent states, which is also able to explain the origin of chaotic or regular dynamics with a simple geometrical argument. Our work contributes to the study of the quantum-classical correspondence in chaotic systems, and uncovers a rich structure in multilevel all-to-all interacting models.",0,arxiv,Kuantum,CC-BY/arXiv,The deep Hilbert space of all-to-all interacting SU(3) atoms: from quantum to classical
"We implement a decoy-state quantum key distribution scheme using a telecom C-band single-emitter source. The decoy states are created by varying the optical excitation of the quantum emitter to modulate the photon number distribution. We provide an analysis of our scheme based on existing security proofs, allowing the calculation of secret key rates including finite key effects. This enables us to demonstrate, with a realistic single-photon source, positive secret key rates using our scheme over 227 km of optical fiber, equivalent to a loss tolerance one order of magnitude greater than non-decoy schemes. This work broadens the scope of single-photon sources in future quantum networks by enabling long-distance QKD with realistic levels of single-photon purity.",0,arxiv,Kuantum,CC-BY/arXiv,Decoy-state quantum key distribution over 227 km with a frequency-converted telecom single-photon source
"Distributed quantum computing involves superconducting computation nodes operating at microwave frequencies, which are connected by long-distance transmission lines that transmit photons at optical frequencies. Quantum transduction, which coherently converts between microwave and optical (M-O) photons, is a critical component of such an architecture. Current approaches are hindered by the unavoidable problem of device heating due to the optical pump. In this work, we propose a pump-free scheme based on color centers that generates time-bin encoded M-O Bell pairs. Our scheme first creates spin-photon entanglement and then converts the spin state into a time-bin-encoded microwave photon using a strongly coupled Purcell-enhanced resonator. In our protocol, the microwave retrieval is heralded by detecting the microwave signal with a three-level transmon. We have analyzed the resulting Bell state fidelity and generation probability of this protocol. Our simulation shows that by combining a state-of-the-art spin-optical interface with our proposed strongly-coupled spin-microwave design, the pump-free scheme can generate M-O Bell pairs at a heralding rate exceeding one kilohertz with near-unity fidelity, which establishes the scheme as a promising source for M-O Bell pairs.",0,arxiv,Kuantum,CC-BY/arXiv,Pump Free Microwave-Optical Quantum Transduction
"We introduce a multimode superconducting inductor architecture that enables radio-frequency reflectometry at multiple discrete frequencies up to 2 GHz, addressing limitations of conventional single-mode designs. The spiral inductor's distributed inter-turn capacitance yields distinct resonant modes with varied impedance-matching conditions. By probing a quantum dot across several modes, we extract tunneling rates over a broad frequency range and identify signatures of nearby charge defects. Using one of the higher-order modes, we demonstrate single-shot spin readout via a radio-frequency single-electron transistor (RF-SET), achieving singlet-triplet readout with an integration time of 8 us and a readout fidelity of 98%. These results establish multimode inductance as a scalable and flexible component for fast spin-qubit readout and device-quality characterization.",0,arxiv,Kuantum,CC-BY/arXiv,Multimode RF Reflectometry for Spin Qubit Readout and Device Characterization
"Distributed quantum computing (DQC) enables scalable quantum computations by distributing large quantum circuits on multiple quantum processing units (QPUs) in the quantum cloud. In DQC, after partitioning quantum circuits, they must be scheduled and executed on heterogenous QPUs while balancing latency, overhead, QPU communication resource limits. However, since fully functioning quantum communication networks have not been realized yet, near-term quantum clouds will only rely on local operations and classical communication settings between QPUs, without entangled quantum links. Additionally, existing DQC scheduling frameworks do not account for user-defined execution deadlines and adopt inefficient wire cutting techniques. Accordingly, in this work, a deadline aware DQC scheduling framework with efficient wire cutting for near-term quantum cloud is proposed. The proposed framework schedules partitioned quantum subcircuits while accounting for circuit deadlines and QPU capacity limits. It also captures dependencies between partitioned subcircuits and distributes the execution of the sampling shots on different QPUs to have efficient wire cutting and faster execution. In this regard, a deadline-aware circuit scheduling optimization problem is formulated, and solved using simulated annealing. Simulation results show a marked improvement over existing shot-agnostic frameworks under urgent deadlines, reaching a 12.8% increase in requests served before their deadlines. Additionally, the proposed framework serves 8.16% more requests, on average, compared to state-of-the-art dependency-agnostic baseline frameworks, and by 9.60% versus the dependency-and-shot-agnostic baseline, all while achieving a smaller makespan of the DQC execution. Moreover, the proposed framework serves 23.7%, 24.5%, and 25.38% more requests compared to greedy, list scheduling, and random schedulers, respectively.",0,arxiv,Kuantum,CC-BY/arXiv,Deadline-Aware Scheduling of Distributed Quantum Circuits in Near-Term Quantum Cloud
"We study finite-time driving across second-order dissipative quantum phase transitions described by Lindblad dynamics. We show that the nonadiabatic entropy production, which quantifies deviations from the instantaneous nonequilibrium steady state, exhibits universal power-law scaling with the ramp duration in analogy to the Kibble-Zurek mechanism for closed systems. This establishes the universality of irreversible dissipation induced by driving an open quantum system near criticality. Furthermore, in systems described by bosonic Gaussian states, our scaling laws predict that the nonadiabatic entropy production is independent of driving speed to leading order, revealing a distinctive feature of Gaussian dissipative quantum phase transitions. We validate these analytical predictions in the thermodynamic limit of the driven-dissipative Dicke model and via finite-size scaling in the open Kerr model. Our results establish a general framework for understanding universal nonequilibrium response and thermodynamic irreversibility in critical open quantum systems.",0,arxiv,Kuantum,CC-BY/arXiv,Thermodynamic universality across dissipative quantum phase transitions
"Unsupervised anomaly-based intrusion detection requires models that can generalize to attack patterns not observed during training. This work presents the first large-scale evaluation of hybrid quantum-classical (HQC) autoencoders for this task. We construct a unified experimental framework that iterates over key quantum design choices, including quantum-layer placement, measurement approach, variational and non-variational formulations, and latent-space regularization. Experiments across three benchmark NIDS datasets show that HQC autoencoders can match or exceed classical performance in their best configurations, although they exhibit higher sensitivity to architectural decisions. Under zero-day evaluation, well-configured HQC models provide stronger and more stable generalization than classical and supervised baselines. Simulated gate-noise experiments reveal early performance degradation, indicating the need for noise-aware HQC designs. These results provide the first data-driven characterization of HQC autoencoder behavior for network intrusion detection and outline key factors that govern their practical viability. All experiment code and configurations are available at https://github.com/arasyi/hqcae-network-intrusion-detection.",0,arxiv,Kuantum,CC-BY/arXiv,Hybrid Quantum-Classical Autoencoders for Unsupervised Network Intrusion Detection
"The Quantum Approximate Optimization Algorithm (QAOA) is a leading approach for solving combinatorial optimization problems on near-term quantum processors. However, finding good variational parameters remains a significant challenge due to the non-convex energy landscape, often resulting in slow convergence and poor solution quality. In this work, we propose a quantum meta-learning framework that trains advanced quantum sequence models to generate effective parameter initialization policies. We investigate four classical or quantum sequence models, including the Quantum Kernel-based Long Short-Term Memory (QK-LSTM), as learned optimizers in a ""learning to learn"" paradigm. Our numerical experiments on the Max-Cut problem demonstrate that the QK-LSTM optimizer achieves superior performance, obtaining the highest approximation ratios and exhibiting the fastest convergence rate across all tested problem sizes (n=10 to 13). Crucially, the QK-LSTM model achieves perfect parameter transferability by synthesizing a single, fixed set of near-optimal parameters, leading to a remarkable sustained acceleration of convergence even when generalizing to larger problems. This capability, enabled by the compact and expressive power of the quantum kernel architecture, underscores its effectiveness. The QK-LSTM, with only 43 trainable parameters, substantially outperforms the classical LSTM (56 parameters) and other quantum sequence models, establishing a robust pathway toward highly efficient parameter initialization for variational quantum algorithms in the NISQ era.",0,arxiv,Kuantum,CC-BY/arXiv,Meta-Learning for Quantum Optimization via Quantum Sequence Model
"We demonstrate high-fidelity single qubit control in a natural Si-MOS quantum dot fabricated in an industrial 300 mm wafer process on a silicon on insulator (SOI) wafer using electron spin resonance. A relatively high optimal Rabi frequency of 5 MHz is achieved, dynamically decoupling the electron spin from its 29-Si environment. Tracking the qubit frequency reduces the impact of low frequency noise in the qubit frequency and improves the $T^{Rabi}$ from 7 to 11 $Î¼$s at a Rabi frequency of 5 MHz, resulting in Q-factors exceeding 50. Randomized benchmarking returns an average single gate control fidelity of 99.5 $\pm$ 0.3%. As a result of pulse-area calibration, this fidelity is limited by the Rabi Q-factor. These results show that a fast Rabi frequency, low charge noise, and a feedback protocol enable high fidelity in these Si-MOS devices, despite the low-frequency magnetic noise.",0,arxiv,Kuantum,CC-BY/arXiv,High Fidelity Qubit Control in a Natural Si-MOS Quantum Dot using a 300 mm Silicon on Insulator Wafer
"Long short-term memory (LSTM) models are a particular type of recurrent neural networks (RNNs) that are central to sequential modeling tasks in domains such as urban telecommunication forecasting, where temporal correlations and nonlinear dependencies dominate. However, conventional LSTMs suffer from high parameter redundancy and limited nonlinear expressivity. In this work, we propose the Quantum-inspired Kolmogorov-Arnold Long Short-Term Memory (QKAN-LSTM), which integrates Data Re-Uploading Activation (DARUAN) modules into the gating structure of LSTMs. Each DARUAN acts as a quantum variational activation function (QVAF), enhancing frequency adaptability and enabling an exponentially enriched spectral representation without multi-qubit entanglement. The resulting architecture preserves quantum-level expressivity while remaining fully executable on classical hardware. Empirical evaluations on three datasets, Damped Simple Harmonic Motion, Bessel Function, and Urban Telecommunication, demonstrate that QKAN-LSTM achieves superior predictive accuracy and generalization with a 79% reduction in trainable parameters compared to classical LSTMs. We extend the framework to the Jiang-Huang-Chen-Goan Network (JHCG Net), which generalizes KAN to encoder-decoder structures, and then further use QKAN to realize the latent KAN, thereby creating a Hybrid QKAN (HQKAN) for hierarchical representation learning. The proposed HQKAN-LSTM thus provides a scalable and interpretable pathway toward quantum-inspired sequential modeling in real-world data environments.",0,arxiv,Kuantum,CC-BY/arXiv,QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory
"We present a native realization of iSWAP and parameterized exchange gates for neutral atom quantum processing units. Our approach leverages strong dipole-dipole interactions between two dipole-coupled Rydberg states, and employs optimal control techniques to design time-efficient, high-fidelity gate protocols. To minimize experimental complexity, we utilize global driving terms acting identically on all atoms. We implement a noise-aware pulse selection strategy to identify candidate protocols with reduced susceptibility to certain noise sources, then analyze their performance under realistic noise sources -- including atomic motion, Rydberg decay, and experimentally motivated laser phase and intensity noise. For a $^{88}$Sr-based architecture, we demonstrate fast iSWAP gate protocols which exceed fidelities of $99.9\%$ under realistic experimental conditions. These results pave the way for expanding the neutral atom gate set beyond typical Rydberg blockade-based entangling gates.",0,arxiv,Kuantum,CC-BY/arXiv,Expanding the Neutral Atom Gate Set: Native iSWAP and Exchange Gates from Dipolar Rydberg Interactions
"This is a re-editing, which takes quantum mechanics into account, of Wittgenstein's famous Tractatus. The operation has a playful side in the form, but is a serious attempt to capture possible philosophical implications of the Relational Interpretation of Quantum Mechanics, and formalize the naturalistic third-way between realism and instrumentalism explored by this interpretation.",0,arxiv,Kuantum,CC-BY/arXiv,Tractatus Quanticum
"We construct a class of wormhole geometries supported by the non-local gravitational self-energy that regularizes the particle and black-hole sectors of spacetime. Using this framework, inspired by T-duality, we show that two entangled particles (or particle-black-hole pairs) naturally source an Einstein-Rosen-type geometry in which the required violation of the strong energy condition arises from intrinsic quantum-gravity effects rather than from ad hoc exotic matter, which is matter that violates the null energy condition. We classify the resulting wormholes, analyze their horizons, throat structure and embedding properties, and we identify the exotic energy needed at the minimal surface. Imposing the ER=EPR requirement of non-traversability and the absence of a macroscopic throat, we find that only the zero-throat geometry is compatible with an entanglement-induced Einstein-Rosen bridge, providing a concrete realization of ER=EPR within a fully regular spacetime. Finally, we briefly discuss possible implications for microscopic ER networks from vacuum fluctuations, replica-wormhole interpretations of Hawking radiation, and possible links to entanglement-driven dark-energy scenarios.",0,arxiv,Kuantum,CC-BY/arXiv,Emergence of ER=EPR from non-local gravitational energy
"We investigate the Magnus expansion of the $N$-operator in relativistic quantum field theory, which is related to the $S$-matrix via $S = e^{iN}$. We develop direct methods to compute matrix elements of the $N$-operator, which we refer to as Magnus amplitudes, bypassing scattering amplitudes entirely. At tree level, Magnus amplitudes are expressed in terms of retarded and advanced propagators, with each diagram weighted by factors that we identify as Murua coefficients. At loop level this structure is augmented by the Hadamard cut function, and we establish remarkable relations between loop- and tree-level Magnus amplitudes. Among these, we find that $n$-point one-loop Magnus amplitudes are entirely determined by phase-space integrals of forward limits of $(n{+}2)$-point tree-level amplitudes, and hence related to Murua coefficients, and we generalise this to a class of higher-loop contributions. Furthermore, in the case of heavy particles interacting via massless mediators, we conjecture that Magnus diagrams that contribute to the classical limit are always given by forward limits of trees, and we show this explicitly in a one-loop example. We derive these results studying theories of scalar fields with cubic interactions, but our methods are applicable to general theories as well as to integral functions appearing in gravitational-wave computations. Given that Magnus amplitudes are free of hyper-classical terms, and the known relations between Magnus amplitudes and the radial action, our results lay the groundwork for systematic and efficient calculations of classical observables from quantum field theory.",0,arxiv,Kuantum,CC-BY/arXiv,The Magnus expansion in relativistic quantum field theory
"We demonstrate that an anisotropic and rotated Fermi surface can generate a finite Hall-like transverse response in electron transport, even in the absence of a magnetic field or Berry curvature. Using a two-dimensional continuum model, we show that broken $k_y \to -k_y$ symmetry inherent to anistropic band structures leads to a nonzero transverse conductivity. We construct a lattice model with direction-dependent nearest- and next-nearest-neighbor hoppings that faithfully reproduces the continuum dispersion and allows controlled rotation of the Fermi contour. Employing a multiterminal geometry and the BÃ¼ttiker-probe method, we compute the resulting Hall voltage and establish its direct correspondence with the continuum transverse response. The effect increases with the degree of anisotropy and vanishes at rotation angles where mirror symmetry is restored. Unlike the quantum Hall effect, the Hall response predicted here is not quantized but varies continuously with the band-structure parameters. Our results provide a symmetry-based route to engineer Hall-like signals in low-symmetry materials without magnetic fields or topological effects.",0,arxiv,Kuantum,CC-BY/arXiv,Hall-like response from anisotropic Fermi surfaces
"We present a unified quantum open system framework for lossy plasmonic cavities in which coherent dynamics, relaxation, dephasing, and irreversible absorption are treated on equal footing. The Dyson equation for the cavity photon propagator in the random-phase approximation yields a complex self-energy S that accounts for both the renormalization and the damping of hybrid plasmon-photon modes (polaritons, in a quasi-particle description). Tracing out the electronic and photonic environments leads to a Liouvillian for the upper (UP) and lower (LP) polaritonic branches, incorporating leakage through the imaginary part of the self-energy, internal UP-LP scattering rates, and dephasing. Time evolution equations for polariton populations, interbranch coherence, and driven amplitudes in closed form also provide analytic expressions for their steady-state values, the quench rate of UP-LP oscillations and polaritonic lineshapes, valid in the limit of low polaritonic density, but covering light-matter ultrastrong coupling. The theory establishes a self-consistent description of dissipative polariton dynamics in plasmonic and nanophotonic cavities, directly applicable to response spectra, time-domain measurements, and dissipation engineering.",0,arxiv,Kuantum,CC-BY/arXiv,Quantum open system description of a hybrid plasmonic cavity
"Quantum control refers to our ability to manipulate quantum systems. This tutorial-style chapter focuses on the use of classical electromagnetic fields to steer the system dynamics. In this approach, the quantum nature of the control stems solely from the underlying dynamics, through the exploitation of destructive and constructive interference to reach the control target. We first discuss two basic control principles -- coherent control which uses manipulation in frequency or time to design these interferences, and adiabatic following where access to the control target is enabled by tracking the time-dependent ground state. For complex control targets and system dynamics that exceed the scope of these basic principles, optimal control theory provides a powerful suite of tools to design the necessary protocols. A key consideration for the successful application of optimal control theory is a proper choice of the optimization functional. All concepts are illustrated using recent work from my research group, with a focus on controlling atoms and superconducting qubits. The chapter concludes with an outlook on integrating coherent control with engineered dissipation and a discussion of open questions in the field.",0,arxiv,Kuantum,CC-BY/arXiv,Introduction to quantum control: From basic concepts to applications in quantum technologies
"In this paper, we develop an operator-based framework for laser--plasma wakefield acceleration (LPWA) in capillary discharges, providing a compact and systematic description of the coupled dynamics of laser fields and plasma response. The formalism employs key operators: the transverse modal operator $\hat{K}$, the nonlinear plasma operator $\hat{N}[Î¨]$, the plasma oscillation operator $\hatÎ©_p^{\,2}$, and the ponderomotive source operator $\hatÎ±$, which together describe mode coupling, plasma oscillations, and nonlinear feedback induced by the ponderomotive force. In the linear regime, the system is characterized by invariant subspaces associated with stable modal structures, while nonlinear interactions break these invariances, leading to mode mixing and complex dynamics. The approach establishes a direct connection between LPWA and Hilbert-space operator theory, including the invariant subspace, providing a formal mathematical interpretation of energy transfer and wakefield formation. Furthermore, the operator formalism integrates with neural operator methods, allowing efficient approximation of $\hat{N}$ and $\hatÎ±$ for reduced-order modeling and predictive control. This hybrid physics--AI framework offers a robust foundation for modeling, analysis, and optimization of high-intensity laser--plasma interactions in next-generation accelerator experiments.",0,arxiv,Kuantum,CC-BY/arXiv,Operator Formalism for Laser-Plasma Wakefield Acceleration
"Sample-based quantum diagonalization (SQD) is an algorithm for hybrid quantum-classical molecular simulation that has been of broad interest for application with noisy intermediate scale quantum (NISQ) devices. However, SQD does not always converge on a practical timescale. Here, we explore scaling of the algorithm for a variable-length molecule made up of 2 to 6 copper oxide plaquettes with a minimal molecular orbital basis. The results demonstrate that enabling all-to-all connectivity, instituting a higher expansion order for the SQD algorithm, and adopting a non-Hartree-Fock molecular orbital basis can all play significant roles in overcoming sampling bottlenecks, though with tradeoffs that need to be weighed against the capabilities of quantum and classical hardware. Additionally, we find that noise on a real quantum computer, the Quantinuum H2 trapped ion device, can improve energy convergence beyond expectations based on noise-free statevector simulations.",0,arxiv,Kuantum,CC-BY/arXiv,Convergence of sample-based quantum diagonalization on a variable-length cuprate chain
"Previous demonstrations of quantum acoustic systems have been limited to isolated devices, with limited capability to route phonons and interconnect multi-port acoustic elements for further extension. Here, we demonstrate a scalable architecture for circuit quantum acoustodynamics (cQAD) by integrating superconducting qubits with suspension-free phononic integrated circuits (PnICs). Coherent coupling between tunable transmon qubits and waveguide-integrated phononic cavities, including Fabry-Perot cavities via monolithic integration and microring cavities via flip-chip assembly, has been achieved, producing a pronounced enhancement of phonon emission with a Purcell factor of ~19. These devices represent elementary building blocks for scalable phononic circuits, establishing the foundation for phonon-based quantum information processors and the testbed for novel quantum acoustic phenomena.",0,arxiv,Kuantum,CC-BY/arXiv,Circuit Quantum Acoustodynamics in a Scalable Phononic Integrated Circuit Architecture
"We study spin-phonon coupled dynamics in the vicinity of a sloped conical intersection created by laser coupling the electronic (spin) and vibrational degrees of freedom of a pair of trapped Rydberg ions. We show that the shape of the potential energy surfaces can be engineered and controlled by exploiting the sideband transitions of the crystal vibration and dipole-dipole interactions between Rydberg ions in the Lamb-Dicke regime. Using the sideband transition, we realize a sloped conical intersection whose cone axis is only tilted along one spatial axis. When the phonon wavepacket is located in the potential minimum of the lower potential surface, the spin and phonon dynamics are largely frozen owing to the geometric phase effect. When starting from the upper potential surface, the electronic and phonon states tunnel to the lower potential surface, leading to a partial revival of the initial state. In contrast, the dynamics drastically change when the initial wavepackets are away from the conical intersection. The initial state is revived, and is almost entirely irrelevant to whether it is from the lower or upper potential surface. Complete Rabi oscillations of the adiabatic states are found when the wavepacket is initialized on the upper potential surface. The dynamics occur on the microsecond and nanometer scales, implying that Rydberg ions provide a platform for simulating nonadiabatic processes in the vicinity of a sloped conical intersection.",0,arxiv,Kuantum,CC-BY/arXiv,Exploring vibronic dynamics near a sloped conical intersection with trapped Rydberg ions
"In [Ann. Henri PoincarÃ©, {\bf 22} (2021), 3199-3234], De Palma and Trevisan described a one-to-one correspondence between quantum couplings and quantum channels realizing transport between states. The aim of this short note is to demonstrate that taking the Petz recovery map for a given channel and initial state is precisely the counterpart of the swap transpose operation on couplings. That is, the swap transpose of the coupling $Î _Î¦$ corresponding to the channel $Î¦$ and initial state $Ï$ is the coupling $Î _{rec}$ corresponding to the Petz recovery map $Î¦_{rec}.$",0,arxiv,Kuantum,CC-BY/arXiv,The swap transpose on couplings translates to Petz' recovery map on quantum channels
"Transmitting information about quantum states over classical noisy channels is an important problem with applications to science, computing, and sensing. This task, however, poses fundamental challenges due to the exponential scaling of state space with system size. We introduce shadow tomography-based transmission with unequal error protection (STT-UEP), a novel communication protocol that enables efficient transmission of properties of quantum states, allowing decoder-side estimation of arbitrary observables. Unlike conventional approaches requiring the transmission of a number of bits that is exponential in the number of qubits, STT-UEP achieves communication complexity that scales logarithmically with the number of observables, depending on the observable weight. The protocol exploits classical shadow tomography for measurement efficiency, and applies unequal error protection by encoding measurement bases with stronger channel codes than measurement outcomes. We provide theoretical guarantees on estimation accuracy as a function of the bit error probability of the classical channel, and validate the approach against several benchmarks via numerical results.",0,arxiv,Kuantum,CC-BY/arXiv,Communicating Properties of Quantum States over Classical Noisy Channels
"Variational Quantum Linear Solvers (VQLS) are a promising method for solving linear systems on near-term quantum devices. However, their performance is often limited by barren plateaus and inefficient parameter initialization, which significantly hinder trainability as the system size increases. In this work, we introduce PVLS, a learning-based parameter prediction framework that uses Graph Neural Networks (GNNs) to generate high-quality initial parameters for VQLS circuits. By leveraging structural information from the coefficient matrix, PVLS predicts expressive and scalable initializations that improve convergence and reduce optimization difficulty. Extensive experiments on matrix sizes ranging from 16 to 1024 show that PVLS provides up to a 2.6x speedup in optimization and requires fewer iterations while maintaining comparable solution accuracy. These results demonstrate the potential of machine-learning-guided initialization strategies for improving the practicality of hybrid quantum-classical algorithms in the NISQ era.",0,arxiv,Kuantum,CC-BY/arXiv,PVLS: A Learning-based Parameter Prediction Technique for Variational Quantum Linear Solvers
"In the study of quantum-mechanical tunneling processes, numerous approaches have been developed to determine the decay rate of states initially confined within a metastable potential region. Virtually all analytical treatments, however, fall into one of two superficially unrelated conceptual frameworks: the resonant-state approach and the instanton method. Whereas the concept of resonant states and their associated decay widths is grounded in physical reasoning by capturing the regime of uniform probability decay, the instanton method lacks a comparably clear physical interpretation. We demonstrate the equivalence of the two approaches, revealing that the contour-deformation prescription in the functional integral put forward by Callan and Coleman directly corresponds to the outgoing Gamow--Siegert boundary conditions defining resonant states.",0,arxiv,Kuantum,CC-BY/arXiv,Instantons meet resonances: Unifying two seemingly distinct approaches to quantum tunneling
"We propose a protocol that characterizes the pairwise overlaps of the internal modes of single photons more efficiently than pairwise Hong-Ou-Mandel characterization experiments. This protocol exploits multiphoton interference. We experimentally implement this protocol to characterize three photons. We show that our implementation of the characterization protocol outperforms the pairwise Hong-Ou-Mandel characterization, even if the Hong-Ou-Mandel characterization would have been performed in a noiseless, perfect experiment. We demonstrate this via the Fisher information matrix.",0,arxiv,Kuantum,CC-BY/arXiv,Multiphoton interference outperforms pairwise overlaps for distinguishability characterization
"This work investigates Bayesian stepwise estimation (Se) for measuring the two parameters of a unitary qubit rotation. While asymptotic analysis predicts a precision advantage for SE over joint estimation (JE) in regimes where the quantum Fisher information matrix is near-singular (""sloppy"" models), we demonstrate that this advantage is mitigated within a practical Bayesian framework with limited resources. We experimentally implement a SE protocol using polarisation qubits, achieving uncertainties close to the classical Van Trees bounds. However, comparing the total error to the ultimate quantum Van Trees bound for JE reveals that averaging over prior distributions erases the asymptotic SE advantage. Nevertheless, the stepwise strategy retains a significant practical benefit as it operates effectively with simple, fixed measurements, whereas saturating the JE bound typically requires complex, parameter-dependent operations.",0,arxiv,Kuantum,CC-BY/arXiv,Bayesian stepwise estimation of qubit rotations
"We propose methods to mitigate single- and two-qubit control errors due to residual exchange coupling in systems of exchange-coupled resonant singlet-triplet qubits. Commensurate driving, where the pulse length is an integer multiple of the drive period, can mitigate errors from residual intra-qubit exchange, including effects from counter rotating terms and off-axis rotations, as well as leakage errors during two-qubit operations. Residual inter-qubit exchange creates crosstalk errors that reduce single-qubit control fidelities. We show that using a single-spin coupler between two resonant singlet-triplet qubits can reduce this crosstalk error by an order of magnitude. Assuming perfect coupler state preparation and realistic charge and hyperfine noise, we predict that coupler-assisted two-qubit gate errors can be below $3\times10^{-3}$ for gate times as short as $66~\text{ns}$, even in the presence of residual exchange levels exceeding several hundred kHz. Our results suggest the potential of utilizing coupler-based architectures for large scale fault-tolerant spin qubit processors based on resonant singlet-triplet qubits.",0,arxiv,Kuantum,CC-BY/arXiv,Mitigating Residual Exchange Coupling in Resonant Singlet-Triplet Qubits
"We propose a hybrid variational quantum algorithm that has variational parameters used by both the quantum circuit and the subsequent classical optimization. Similar to the Variational Quantum Eigensolver (VQE), this algorithm applies a parameterized unitary operator to the qubit register. We generate this operator using diabatic state preparation. The quantum measurement results then inform the classical optimization procedure used by the Cascaded Variational Quantum Eigensolver (CVQE). We demonstrate the algorithm on a system of interacting electrons and show how it can be used on long-term error-corrected as well as short-term intermediate-scale quantum computers. Our simulations performed on IBM Brisbane produced energies well within chemical accuracy.",0,arxiv,Kuantum,CC-BY/arXiv,Hybrid VQE-CVQE algorithm using diabatic state preparation
"The quantum principle of relativity (QPR) puts forward an ambitious idea: extend special relativity with a formally superluminal branch of Lorentz-type maps, and treat the resulting consistency constraints as hints about why quantum theory has the structure it does [1]. The discussion that followed has emphasized a basic point: writing down coordinate maps is not the same thing as providing a physical theory. In particular, quantum superposition is not operationally defined by drawing multiple paths on paper: it is defined by what happens when alternatives recombine in an interference loop [2, 3]. In parallel, careful 1+1 analyses have clarified how sign conventions and time-orientation choices enter the superluminal formulas [4]. Finally, tachyonic QFT proposals suggest a possible mathematical bridge via an enlarged (twin) Hilbert space [5], although this proposal remains contested (e.g., on commutator covariance and microcausality grounds) [6]. The aim of this short note is organizational. We keep three layers separate: (K) kinematics (which maps exist and what they preserve), (O) operational content (what an experiment must actually reproduce, especially closed-loop interference), and (D/B) dynamics and bridges (how amplitudes and probabilities are generated, and how subluminal and superluminal sectors might be linked). The goal is not relativity derives quantum theory, but a clear checklist of what must be added for that ambition to become a well-posed programme.",0,arxiv,Kuantum,CC-BY/arXiv,From Kinematics to Interference: Operational Requirements for the Quantum Principle of Relativity
"In this paper we investigate the (Kohn-Sham) density-to-potential map in the case of spinless fermions in one spatial dimension, whose existence has been rigorously established by the first author in [arXiv:2504.05501 (2025)]. Here, we focus on the regularity of this map as a function of the density and the coupling constant in front of the interaction term. More precisely, we first prove a quantitative version of the Hohenberg-Kohn theorem, thereby showing that this map is Lipschitz continuous with respect to the natural Sobolev norms in the space of densities and potentials. In particular, this implies that the inverse (Kohn-Sham) problem is not only well-posed but also Lipschitz stable. Using this result, we then show that the density-to-potential map is in fact real analytic with respect to both the density and the interaction strength. As a consequence, we obtain a holomorphic extension of the universal constrained-search functional to a suitable subset of complex-valued densities. This partially extends the DFT framework to non-self-adjoint SchrÃ¶dinger operators. As further applications of these results, we also establish the existence of an exchange-only part of the exchange-correlation potential, and justify the GÃ¶rling-Levy perturbation expansion for the correlation energy.",0,arxiv,Kuantum,CC-BY/arXiv,A quantitative Hohenberg-Kohn theorem and the unexpected regularity of density functional theory in one spatial dimension
"Imaginary-time evolution has been shown to be a promising framework for tackling combinatorial optimization problems on quantum hardware. In this work, we propose a classical quantum-inspired strategy for solving combinatorial optimization problems with integer-valued decision variables by encoding decision variables into multi-level quantum states known as qudits. This method results in a reduced number of decision variables compared to binary formulations while inherently incorporating single-association constraints. Efficient classical simulation is enabled by constraining the system to remain in a product state throughout optimization. The qudit states are optimized by applying a sequence of unitary operators that iteratively approximate the dynamics of imaginary time evolution. Unlike previous studies, we propose a gradient-based method of adaptively choosing the Hermitian operators used to generate the state evolution at each optimization step, as a means to improve the convergence properties of the algorithm. The proposed algorithm demonstrates promising results on Min-d-Cut problem with constraints, outperforming Gurobi on penalized constraint formulation, particularly for larger values of d.",0,arxiv,Kuantum,CC-BY/arXiv,Quantum-Inspired Optimization through Qudit-Based Imaginary Time Evolution
"The study of the conversion of ultracold atoms into molecules has long remained a hot topic in atomic, molecular, and optical physics. However, most prior research has focused on diatomic molecules, with relatively scarce exploration of polyatomic molecules. Here we propose a two-step strategy for the formation of stable ultracold tetratomic molecules. We first suggest a generalized nonlinear stimulated Raman exact passage (STIREP) technique for the coherent conversion of ultracold atoms to tetratomic molecules, which is subsequently followed by a chainwise-STIREP technique to transfer the resulting molecules into a sufficiently stable ground state. Through systematic numerical analysis, we demonstrate that the proposed two-step strategy holds great potential for the robust, fast, and efficient formation of stable ultracold tetratomic molecules.",0,arxiv,Kuantum,CC-BY/arXiv,"Robust, fast, and efficient formation of stable tetratomic molecules from ultracold atoms via generalized stimulated Raman exact passage"
"Recently proposed metastability-induced quantum batteries have shown particular promise for coherent microwave generation. However, achieving high-power coherent microwave generation in quantum batteries remains fundamentally challenging due to quantum correlations, aging, and self-discharging processes. For the cavity-quantum-electrodynamics (CQED)-based quantum batteries, a further trade-off arises between strong spin-photon coupling for energy storage and sufficient output coupling for power delivery. To overcome these constraints, we introduce dissipation engineering as a dynamic control strategy that temporally separates energy storage and release. By suppressing emission during charging and rapidly enhancing the output coupling during discharging, we realize nanosecond microwave bursts with watt-level peak power. By optimizing three dissipation schemes, we improve work extraction efficiency of the quantum battery by over two orders of magnitude and achieve high power compression factors outperforming the state-of-the-art techniques, establishing dissipation engineering as a pathway toward room-temperature, high-power coherent microwave sources.",0,arxiv,Kuantum,CC-BY/arXiv,Watt-level coherent microwave emission from dissipation engineered solid-state quantum batteries
"We introduce fermionic neural Gibbs states (fNGS), a variational framework for modeling finite-temperature properties of strongly interacting fermions. fNGS starts from a reference mean-field thermofield-double state and uses neural-network transformations together with imaginary-time evolution to systematically build strong correlations. Applied to the doped Fermi-Hubbard model, a minimal lattice model capturing essential features of strong electronic correlations, fNGS accurately reproduces thermal energies over a broad range of temperatures, interaction strengths, even at large dopings, for system sizes beyond the reach of exact methods. These results demonstrate a scalable route to studying finite-temperature properties of strongly correlated fermionic systems beyond one dimension with neural-network representations of quantum states.",0,arxiv,Kuantum,CC-BY/arXiv,Fermionic neural Gibbs states
"Strongly interacting many-body systems often show collective properties that are non-trivially related to the microscopic degrees of freedom. Collectivity is responsible for intriguing ground state properties, for example, in superconductors. However, collective effects may also govern the non-equilibrium response of quantum systems, not only in condensed matter physics but also in quantum field theories modeling the properties of our universe. Understanding emergent collective dynamics from first principles, in particular in non-perturbative regimes, is therefore one of the central challenges in quantum many-body physics. Here we report on the observation of collective cluster nucleation in 2D quantum Ising systems realized in an atomic Rydberg array. We observe a confined regime in which the steady-state cluster size is energy-dependent and a deconfined regime characterized by kinetically constrained dynamics of cluster nucleation. Our results mark a qualitative leap for quantum simulations with Rydberg arrays and shed light on highly collective non-equilibrium processes in one of the most important textbook models of condensed matter physics with relevance from quantum magnets and the kinetics of glass formers to cosmology.",0,arxiv,Kuantum,CC-BY/arXiv,Collective cluster nucleation dynamics in 2D Ising quantum magnets
"The hallmark of two-dimensional chiral topological phases is the existence of anomalous gapless modes at the spatial boundary. Yet, the manifestation of this edge anomaly within the bulk ground-state wavefunction itself remains only partially understood. In this work, we introduce a family of multipartite entanglement measures that probe chirality directly from the bulk wavefunction. Our construction involves applying different permutations between replicas of the ground state wavefunction in neighboring spatial regions, creating ""permutation defects"" at the boundaries between these regions. We provide general arguments for the robustness of these measures and develop a field-theoretical framework to compute them systematically. While the standard topological field theory prescription misses the chiral contribution, our method correctly identifies it as the chiral conformal field theory partition function on high-genus Riemann surfaces. This feature is a consequence of the bulk-edge correspondence, which dictates that any regularization of the theory at the permutation defects must introduce gapless boundary modes. We numerically verify our results with both free-fermion and strongly-interacting chiral topological states and find excellent agreement. Our results enable the extraction of the chiral central charge and the Hall conductance using a finite number of wavefunction replicas, making these quantities accessible to Monte-Carlo numerical techniques and noisy intermediate-scale quantum devices.",0,arxiv,Kuantum,CC-BY/arXiv,Probing chiral topological states with permutation defects
"In quantum field theory (QFT), the ""vacuum"" is not just empty space but the lowest-energy state of a quantum field. If the energy landscape has multiple local minima, the local ground states are the false vacuum (FV) which can tunnel towards the global ground state (true vacuum, TV). This process exhibits signature akin to classical supercooled gas transitions and many-body tunneling in discrete quantum systems. Here, we study the FV decay and bubble nucleation in a Rydberg atom ring. The long-range van-der-Waals interactions and individual-site addressability allow us to explore physics beyond the standard Ising model. We observe that the FV decay rate decreases exponentially with the inverse of the symmetry-breaking field, directly mirroring QFT predictions. Moreover, we demonstrate that even minor deviations from the ideal metastable state can cause a stark departure from this universal scaling law. Extending beyond short-time decay dynamics, we also examine resonant bubble nucleation, a feature distinctive to systems with discrete energy spectra. Our findings and methods open avenues for future studies of many-body tunneling in higher dimensions or more complex geometries.",0,arxiv,Kuantum,CC-BY/arXiv,Probing false vacuum decay and bubble nucleation in a Rydberg atom array
"The variational quantum eigen solver (VQE), has been widely used to find the ground state energy of different Hamiltonians with no analytical solutions and are classically difficult to compute. In our work, we have used VQE to identify the phase transition boundary for an infinite order phase transition. We use long-range XXZ (LRXXZ) chain for our study. In order to probe infinite order phase transition, we propose to utilise the ground state energy obtained from VQE. The idea rests on the argument that VQE requires an ansatz circuit; therefore, the accuracy of the VQE will rely on this ansatz circuit. We have designed this circuit such that the estimated ground state energy is sensitive to the phase it is evaluated in. It is achieved by applying the constraint that the net spin remains constant throughout the optimisation process. Consequently, the ansatz works in a certain phase where it gives relatively small random error, as it should, when compared to the error in ground state energy calculations of the other phases, where the ansatz fails. By identifying these changes in the behaviour of the error in ground state energy using VQE, we were able to determine the phase boundaries. Using exact diagonalisation, we also compare the behaviour of the energy gradient and energy gap across both the phase transition boundaries for this model. Further, by increasing the depth of the optimisation circuit, we also accurately evaluate the ground energy of the LRXXZ chain for the value of coupling constant, J equal to -1",0,arxiv,Kuantum,CC-BY/arXiv,Ground state energy and phase transitions of Long-range XXZ using VQE
"The performance of quantum key distribution (QKD) is heavily dependent on the physical properties of the channel over which it is executed. Propagation losses and perturbations in the encoded photons' degrees of freedom, such as polarisation or phase, limit both the QKD range and key rate. The maintenance of phase coherence over optical fibres has lately received considerable attention as it enables QKD over long distances, e.g., through phase-based protocols like Twin-Field (TF) QKD. While optical single mode fibres (SMFs) are the current standard type of fibre, recent hollow core fibres (HCFs) could become a superior alternative in the future. Whereas the co-existence of quantum and classical signals in HCF has already been demonstrated, the phase noise resilience required for phase-based QKD protocols is yet to be established. This work explores the behaviour of HCF with respect to phase noise for the purpose of TF-QKD-like protocols. To achieve this, two experiments are performed. The first, is a set of concurrent measurements on 2 km of HCF and SMF in a double asymmetric Mach-Zehnder interferometer configuration. The second, uses a TF-QKD interferometer consisting of HCF and SMF channels. These initial results indicate that HCF is suitable for use in TF-QKD and other phase-based QKD protocols.",0,arxiv,Kuantum,CC-BY/arXiv,Phase noise characterisation of a 2-km Hollow-Core Nested Antiresonant Nodeless Fibre for Twin-Field Quantum Key Distribution
"Accurate computation of non-covalent, intermolecular interaction energies is important to understand various chemical phenomena, and quantum computers are anticipated to accelerate it. Although the state-of-the-art quantum computers are still noisy and intermediate-scale ones, development of theoretical frameworks those are expected to work on a fault-tolerant quantum computer is an urgent issue. In this work, we explore resource-efficient implementation of the quantum phase estimation-based complete active space configuration interaction (QPE-CASCI) calculations, with the aid of the second-order MÃ¸ller--Plesset perturbation theory (MP2)-based active space selection with Boys localized orbitals. We performed numerical simulations of QPE for the supramolecular approach-based intermolecular interaction energy calculations of the hydrogen-bonded water dimer, using 6 system and 6 ancilla qubits. With the aid of algorithmic error mitigation, the QPE-CASCI simulations achieved interaction energy predictions with an error of 0.02 kcal mol$^{-1}$ relative to the CASCI result, demonstrating the accuracy and efficiency of the proposed methodology. Preliminary results on quantum circuit compression for QPE are also presented to reduce the number of two-qubit gates and depth.",0,arxiv,Kuantum,CC-BY/arXiv,Supramolecular approach-based intermolecular interaction energy calculations using quantum phase estimation algorithm
"We introduce a qudit-native framework for engineering robust discrete time crystals (DTCs) by leveraging their internal multilevel structure. Our approach confines the periodic drive to specified on-site subspaces, creating an embedded kick that suppresses heating by preventing population leakage to inactive levels. We underpin DTC stability with a normal-form analysis that decomposes the effective dynamics into distinct components: the carrier locks the subharmonic frequency, neutral terms govern the slow decay and dephasing of the subharmonic response, and charged terms scatter spectral weight away from the locked modes. This framework's predictive power is demonstrated across various qudit platforms: in spin-1 chains, we enhance the stability of DTC by confining the drive to a subspace; in spin-3/2 systems, we show that robustness is dictated by the symmetry of the subspace partition; and in spin-2 platforms, we realize concurrent 2T and 3T DTCs under a unified drive. These findings establish a systematic, hardware-efficient methodology for designing stable and multifunctional Floquet phases of matter on modern qudit-based quantum processors.",0,arxiv,Kuantum,CC-BY/arXiv,A Qudit-native Framework for Discrete Time Crystals
"The structural characterization of high-dimensional mutually unbiased bases (MUBs) by classifying MUBs subsets remains a major open problem. The existing methods not only fail to conclude on the exact classification, but also are severely limited by computational resources and suffer from the numerical precision problem. Here we introduce an operational approach to identify the inequivalence of MUBs subsets, which has less time complexity and entirely avoids the computational precision issues. For arbitrary MUBs subsets of $k$ elements in any prime dimension, this method yields a universal analytical upper bound for the amount of MUBs equivalence classes. By applying this method through simple iterations, we further obtain tighter classification upper bounds for any prime dimension $d\leq 37$. Crucially, the comparison of these upper bounds with existing lower bounds successfully determines the exact classification for all MUBs subsets in any dimension $d \leq 17$. We further extend this method to the case that the dimension is a power of prime number. This general and scalable framework for the classification of MUBs subsets sheds new light on related applications.",0,arxiv,Kuantum,CC-BY/arXiv,Efficient Identification the Inequivalence of Mutually Unbiased Bases via Finite Operators
"Extending optical nonlinearity into the extremely weak light regime is at the heart of quantum optics, since it enables the efficient generation of photonic entanglement and implementation of photonic quantum logic gate. Here, we demonstrate the capability for continuously tunable single-photon level nonlinearity, enabled by precise control of Rydberg interaction over two orders of magnitude, through the use of microwave-assisted wave-function engineering. To characterize this nonlinearity, light storage and retrieval protocol utilizing Rydberg electromagnetically induced transparency is employed, and the quantum statistics of the retrieved photons are analyzed. As a first application, we demonstrate our protocol can speed up the preparation of single photons in low-lying Rydberg states by a factor of up to ~ 40. Our work holds the potential to accelerate quantum operations and to improve the circuit depth and connectivity in Rydberg systems, representing a crucial step towards scalable quantum information processing with Rydberg atoms.",0,arxiv,Kuantum,CC-BY/arXiv,Continuously tunable single-photon level nonlinearity with Rydberg state wave-function engineering
"In recent years, various limitations of conventional supervised learning have been highlighted, leading to the emergence of reinforcement learning -- and, further, quantum reinforcement learning that exploits quantum resources such as entanglement and superposition -- as promising alternatives. Among the various reinforcement learning methodologies, gradient-based approaches, particularly policy gradient methods, are considered to have many benefits. Moreover, in the quantum regime, they also have a profit in that they can be readily implemented through parameterized quantum circuits (PQCs). From the perspective of learning, two indicators can be regarded as most crucial: expressivity and, for gradient-based methods, trainability. While a number of attempts have been made to quantify the expressivity and trainability of PQCs, clear efforts in the context of reinforcement learning have so far been lacking. Therefore, in this study, we newly define the notion of expressivity suited to reinforcement learning and demonstrate that the mutual information between action distribution and reward-signal distribution can, in certain respects, indicate information about both expressivity and trainability. Such research is valuable in that it provides an easy criterion for choosing among various PQCs employed in reinforcement learning, and further, enables the indirect estimation of learning progress even in black-box settings where the agent's achievement aligned with the episodes cannot be explicitly evaluated.",0,arxiv,Kuantum,CC-BY/arXiv,A Mutual Information-based Metric for Temporal Expressivity and Trainability Estimation in Quantum Policy Gradient Pipelines
"The emergence of huge-scale, data-intensive linear optimization (LO) problems in applications such as machine learning has driven the need for more computationally efficient interior point methods (IPMs). While conventional IPMs are polynomial-time algorithms with rapid convergence, their per-iteration cost can be prohibitively high for dense large-scale LO problems. Quantum linear system solvers have shown potential in accelerating the solution of linear systems arising in IPMs. In this work, we introduce a novel almost-exact quantum IPM, where the Newton system is constructed and solved on a quantum computer, while solution updates occur on a classical machine. Additionally, all matrix-vector products are performed on the quantum hardware. This hybrid quantum-classical framework achieves an optimal worst-case scaling of $\mathcal{O}(n^2)$ for fully dense LO problems. To ensure high precision, despite the limited accuracy of quantum operations, we incorporate iterative refinement techniques both within and outside the proposed IPM iterations. The proposed algorithm has a quantum complexity of $\mathcal{O}(n^{1.5} Îº_A \log(\frac{1}Îµ))$ queries to QRAM and $\mathcal{O}(n^2 \log(\frac{1}Îµ))$ classical arithmetic operations. Our method outperforms the worst-case complexity of prior classical and quantum IPMs, offering a significant improvement in scalability and computational efficiency.",0,arxiv,Kuantum,CC-BY/arXiv,Optimal Scaling Quantum Interior Point Method for Linear Optimization
"We present QReach, the first reachability analysis tool for quantum Markov chains based on decision diagrams CFLOBDD (presented at CAV 2023). QReach provides a novel framework for finding reachable subspaces, as well as a series of model-checking subprocedures like image computation. Experiments indicate its practicality in verification of quantum circuits and algorithms. QReach is expected to play a central role in future quantum model checkers.",0,arxiv,Kuantum,CC-BY/arXiv,QReach: A Reachability Analysis Tool for Quantum Markov Chains
"Although the control of non-Hermitian quantum systems has a growing interest for their nonunitary feature in the time evolution, the existing discussions are not more than two or three dimensions and heavily influenced by the singularity of the energy spectrum. We here develop a general theory to control an arbitrary number of bosonic modes governed by the time-dependent non-Hermitian Hamiltonian. It takes advantage of the gauge potential in the instantaneous frame rather than the energy spectrum of Hamiltonian. In particular, the dynamics of a general non-Hermitian continuous-variable system is analyzed in the instantaneous frame associated with time-dependent ancillary operators that are superpositions of the laboratory-frame operators and irrelevant to the original Hamiltonian. The gauge potential is determined by the unitary transformation between the time-dependent and stationary ancillary frames. The upper triangularization condition for the Hamiltonian's coefficient matrix in the stationary ancillary frame enables two of the time-dependent ancillary operators to be nonadiabatic Heisenberg passages of the non-Hermitian system. The probability conservation of the system wavefunction can be restored at the end of these passages without artificial normalization. Our theory is exemplified with the perfect and nonreciprocal state transfers in a cavity magnonic system. The former holds for arbitrary initial states and is irrelevant to the parity-time symmetry of the Hamiltonian and the exceptional point of the spectra; and the latter is consistent with the unidirectional perfect absorbtion. Our work essentially extends the universal quantum control (UQC) theory to the non-Hermitian continuous-variable systems, providing a promising approach for their coherent control.",0,arxiv,Kuantum,CC-BY/arXiv,Universal quantum control over non-Hermitian continuous-variable systems
"This study explores the use of subspace methods in combination with counterdiabatic driving in a Rydberg atom system to solve the Maximum Independent Set (MIS) problem. Although exact counterdiabatic driving offers excellent performance, it comes at an unscalable computational cost. In this work, we demonstrate that counterdiabatic driving can be significantly improved by restricting the analysis to a relevant subspace of the system. We first show that both direct diagonalization and the Krylov method for obtaining the counterdiabatic matrix can be accelerated through the use of subspace techniques, while still maintaining strong performance. We then demonstrate that the cost function used in the standard Krylov method can be further optimized by employing a subspace-based cost function. These findings open up new possibilities for applying counterdiabatic driving in a practical and efficient manner to a variety of quantum systems.",0,arxiv,Kuantum,CC-BY/arXiv,Less is more: subspace reduction for counterdiabatic driving of Rydberg atom arrays
"This article investigates a simple kinematical model of a disc (Disc B) rolling on the edge of a fixed disc (Disc A) to study the geometric nature of rotation. The total rotation angle $Î”$ of Disc B after one cycle is decomposed into a dynamical phase $Î”_d$ and a geometric phase $Î”_g$. The paper's main contribution is to demonstrate that this geometric phase can be essentially described as the $U(1)$ holonomy of the Hopf fibration with the canonical connection. By using a Gauss map to represent the disc's motion as a curve on a two-sphere ($S^2$), the work connects the physical rotation to the underlying geometry of the Hopf fiber bundle $S^3 \to S^2$ and clarifies the origin of the geometric phase.",0,arxiv,Kuantum,CC-BY/arXiv,Rotation angles of a rotating disc as the holonomy of the Hopf fibration
"We investigate a proposal of Kitaev for a microscopic construction of a Hamiltonian intended to describe the edge dynamics of a quantum Hall system. We show that the construction works in the setting of translation-invariant free-fermion Hamiltonians. In this case, the resulting edge Hamiltonian exhibits only edge modes, and these modes have the correct chirality.",0,arxiv,Kuantum,CC-BY/arXiv,Edge Hamiltonian for Free Fermion Quantum Hall Models
"Recent advances in quantum-secure communication have highlighted the value of hybrid schemes that combine Quantum Key Distribution (QKD) with Post-Quantum Cryptography (PQC). Yet most existing hybrid designs omit realistic finite-key effects on QKD key rates and do not specify how to maintain security when both QKD and PQC primitives leak information through side-channels. These gaps limit the applicability of hybrid systems in practical, deployed networks. In this work, we advance a recently proposed hybrid QKD-PQC system by integrating tight finite-key security to the QKD primitive and improving the design for better scalability. This hybrid system employs an information-theoretically secure instruction sequence that determines the configurations of different primitives and thus ensures message confidentiality even when both the QKD and the PQC primitives are compromised. The novelty in our work lies in the implementation of the tightest finite-key security to date for the BBM92 protocol and the design improvements in the primitives of the hybrid system that ensure the processing time scales linearly with the size of secret instructions.",0,arxiv,Kuantum,CC-BY/arXiv,Combined Quantum and Post-Quantum Security Performance Under Finite Keys
"Security of quantum key distribution (QKD) relies on certifying that observed correlations arise from genuine quantum entanglement rather than eavesdropper manipulation. Theoretical security proofs assume idealized conditions, practical certification must contend with adaptive adversaries who optimize their attack strategies against detection systems. Established fundamental adversarial limits for quantum certification using Eve GAN, a generative adversarial network trained to produce classical correlations indistinguishable from quantum. Our central finding: when Eve interpolates her classical correlations with quantum data at mixing parameter, all tested detection methods achieve ROC AUC = 0.50, equivalent to random guessing. This means an eavesdropper needs only 5% classical admixture to completely evade detection. Critically, we discover that same distribution calibration a common practice in prior certification studies inflates detection performance by 44 percentage points compared to proper cross distribution evaluation, revealing a systematic flaw that may have led to overestimated security claims. Analysis of Popescu Rohrlich (PR Box) regime identifies a sharp phase transition at CHSH S = 2.05: below this value, no statistical method distinguishes classical from quantum correlations; above it, detection probability increases monotonically. Hardware validation on IBM Quantum demonstrates that Eve-GAN achieves CHSH = 2.736, remarkably exceeding real quantum hardware performance (CHSH = 2.691), illustrating that classical adversaries can outperform noisy quantum systems on standard certification metrics. These results have immediate implications for QKD security: adversaries maintaining 95% quantum fidelity evade all tested detection methods. We provide corrected methodology using cross-distribution calibration and recommend mandatory adversarial testing for quantum security claims.",0,arxiv,Kuantum,CC-BY/arXiv,Adversarial Limits of Quantum Certification: When Eve Defeats Detection
"Studies of the 3D quantum Hall effect (QHE) have primarily emphasized transport features that mimic the well-established 2D QHE. In this work, we show that qualitatively new features arise when an in-plane magnetic field is applied to a 3D Weyl semimetal in the quantum Hall regime. An unexpected Hall quantum oscillation, distinct from the Weyl-orbit oscillation, coexists with the QHE, along with an unquantized two-terminal magnetoresistance. Moreover, unconventional antichiral transmission enables a peculiar disorder-robust negative longitudinal resistance. Quantization tunable by the lead configuration is further found in this transport geometry. A unique type of nonlocal quantum backscattering channels underlies these phenomena. Our work demonstrates a breakdown of the topological characterization of transport even with 3D Chern numbers and reveals hidden 3D QHE transport properties. It opens a new class of transport measurements and phenomena.",0,arxiv,Kuantum,CC-BY/arXiv,In-plane anomalous features in the 3D quantum Hall regime
"The operator layer cake theorem provides an integral representation for the directional derivative of the operator logarithm in terms of a family of projections [arXiv:2507.06232]. Recently, the related work [arXiv:2507.07065] showed that the theorem gives an alternative proof to Frenkel's integral formula for Umegaki's relative entropy [Quantum, 7:1102 (2023)]. In this short note, we find a converse implication, demonstrating that the operator layer cake theorem is equivalent to Frenkel's integral formula.",0,arxiv,Kuantum,CC-BY/arXiv,The operator layer cake theorem is equivalent to Frenkel's integral formula
"Quantum transduction, which enables the coherent conversion of quantum information between disparate physical platforms, is a cornerstone for realizing scalable and interoperable quantum networks. Among various approaches, parametric frequency mixing processes such as four-wave mixing (FWM) offer a promising pathway toward efficient and low-noise transduction. In this work, we demonstrate the feasibility of coherent quantum state transfer by indirectly verifying high-fidelity wavefunction's phase mapping (>99%) from the input field to the generated output field wave. Using a gas-filled hollow-core capillary fiber, we systematically investigate spectral phase evolution across a broad range, including infrared (IR) to ultraviolet (UV) transitions, as well as conversions from telecom-band (1550 nm) to visible (516 nm) and deep-UV (308 nm) wavelengths. Our results reveal that strong phase coherence can be maintained throughout these diverse conversion regimes. Because quantum properties such as coherence and entanglement are intrinsically encoded in both the amplitude and phase of a photonic wavefunction, preserving spectral phase is essential for faithful quantum information transfer. We further show that efficient and phase-preserving transduction can be achieved by tuning system parameters, offering valuable insights into nonlinear coupling dynamics. These findings establish a promising foundation for advancing FWM-based quantum transduction schemes and open new avenues for integrating heterogeneous quantum systems across wide spectral domains within future quantum communication networks.",0,arxiv,Kuantum,CC-BY/arXiv,Universal Quantum Interconnects via Phase-Coherent Four-Wave Mixing
"We experimentally demonstrate enhanced sensitivity of an atom-based Rydberg radio frequency (RF) receiver integrated with a gradient refractive index (GRIN) Luneburg-type metamaterial lens. By analyzing the electromagnetically induced transparency (EIT) effect in Cesium vapor, we compare receiver performance with and without the GRIN lens under a 2.2~GHz and a 3.6~GHz far-field excitation. Our measurements reveal a significant amplification of the EIT transparency window when the lens is introduced, consistent with the theoretical prediction that the local E-field enhancement at the vapor cell reduces the minimum detectable electric field and increases the signal-to-noise ratio (SNR) of the Rydberg RF receiver. This experimental validation highlights the potential of metamaterial-assisted quantum sensing to overcome the inherent bandwidth and sensitivity limitations of bare Rydberg receivers for a variety of applications, such as electromagnetic compatibility (EMC) testing, quantum radar, and wireless communications.",0,arxiv,Kuantum,CC-BY/arXiv,Experimental Sensitivity Enhancement of a Quantum Rydberg Atom-Based RF Receiver with a Metamaterial GRIN Lens
"Temporal modes of quantum light pulses is a promising resource for modern quantum technologies, driving advancements in quantum computing, communication, and metrology. Precise control and manipulation of these modes remain critical challenges, particularly in systems where nonlinear multimode dynamics interact with dispersion effects. In this work, we focus on the role of group velocity dispersion (GVD) within optical cavities - a phenomenon traditionally viewed as detrimental but increasingly recognized as a versatile tool for quantum light manipulation. We present a perturbation-theory-based approach to analyze GVD effects in a synchronously pumped dispersive cavity. By comparing perturbative solutions to rigorous steady-state results, we establish the validity region of the perturbative approach and assess its limitations in multimode systems. Our study identifies key parameters governing the breakdown of perturbation theory, such as mode order, dispersion strength, and cavity decay rates.",0,arxiv,Kuantum,CC-BY/arXiv,Limits of Perturbation Theory for Multimode Light Propagation in Dispersive Optical Cavities
"Unspeakable coherence is a key feature separating quantum and classical physics. Modelled as asymmetry with respect to a continuous transformation generated by a physically relevant observable, such as the Hamiltonian or angular moment, unspeakable coherence has been shown to be the relevant notion of coherence for achieving quantum advantage in the tasks of metrology, reference frame alignment and work extraction, among others. A question of both practical and foundational value is: Given some copies of a state with low coherence, can we prepare a more coherent state via coherence non-increasing operations? Here, we study this question in the minimal limiting case: Given two uncorrelated copies of a coherent state, can one, via globally coherence non-increasing unitaries, increase the coherence in a subsystem? We fully solve this problem for qubits, identifying the optimal unitaries and revealing the existence of bound coherence. This is then used to create a completely constructive multi-qubit coherence enhancement protocol, where only effective-qubit unitaries are used. Unexpectedly, in this protocol, we show that there exists states for which the ratio of the input-output coherence can be amplified unboundedly. Extending beyond qubits, we derive two fundamental upper bounds on the amount of local coherence that can be increased and prove a no-go theorem showing that certain global correlations cannot be converted to local coherence.",0,arxiv,Kuantum,CC-BY/arXiv,Unspeakable Coherence Concentration
"We consider the problem of the stability (with sharp exponent) of the Lieb--Solovej inequality for symmetric $SU(N)$ coherent states, which was obtained only recently by the authors. Here, we propose an elementary proof of this result, based on reformulating the Wehrl-type entropy as a function defined on the unit sphere in $\mathbb{C}^d$, for some suitable $d$, and on some explicit (and somewhat surprising) computations.",0,arxiv,Kuantum,CC-BY/arXiv,An elementary approach to Wehrl-type entropy bounds in quantitative form
"Quantum circuit simulation remains essential for developing and validating quantum algorithms, especially as current quantum hardware is limited in scale and quality. However, the growing diversity of simulation methods and software tools creates a high barrier to selecting the most suitable backend for a given circuit. We introduce Maestro, a unified interface for quantum circuit simulation that integrates multiple simulation paradigms - state vector, MPS, tensor network, stabilizer, GPU-accelerated, and p-block methods - under a single API. Maestro includes a predictive runtime model that automatically selects the optimal simulator based on circuit structure and available hardware, and applies backend-specific optimizations such as multiprocessing, GPU execution, and improved sampling. Benchmarks across heterogeneous workloads demonstrate that Maestro outperforms individual simulators in both single-circuit and large batched settings, particularly in high-performance computing environments. Maestro provides a scalable, extensible platform for quantum algorithm research, hybrid quantum-classical workflows, and emerging distributed quantum computing architectures.",0,arxiv,Kuantum,CC-BY/arXiv,Maestro: Intelligent Execution for Quantum Circuit Simulation
"Quantum networks are expected to be heterogeneous systems, combining distinct qubit platforms, photon wavelengths, and device timescales to achieve scalable, multiuser connectivity. Building and iterating on such systems is costly and slow, motivating hardware-faithful simulations to explore architecture design space and justify implementation decisions. This paper presents a framework for simulating heterogeneous quantum networks based on SeQUeNCe, a discrete-event simulator of quantum networks. We introduce faithful device models for two representative platforms - Ytterbium atoms and superconducting qubits. On top of these models, we implement entanglement generation and entanglement swapping protocols for time-bin encoded photons that account for disparate clock rates and quantum frequency conversion and transducer losses/noise brought by the heterogeneity. Using extensive simulations, we map the rate-fidelity trade space and identify the dominant bottlenecks unique to heterogeneous systems. The models are open source and extensible, enabling reproducible evaluation of future heterogeneous designs and protocols.",0,arxiv,Kuantum,CC-BY/arXiv,Simulation of a Heterogeneous Quantum Network
"We investigate the monitored dynamics of many-body quantum systems in which projective measurements of extensive operators are alternated with unitary evolution. Focusing on mean-field models characterized by all-to-all interactions, we develop a general framework that captures the thermodynamic limit, where a semiclassical description naturally emerges. Remarkably, we uncover novel stationary states, distinct from the conventional infinite-temperature state, that arise upon taking the infinite-volume limit. Counterintuitively, this phenomenon is not linked to the closing of the Lindbladian gap in that limit. We provide analytical explanation for this unexpected behavior.",0,arxiv,Kuantum,CC-BY/arXiv,Phase Transitions without gap closing in monitored quantum mean-field systems
"The electromagnetic duality symmetry of Maxwell's equations in vacuum implies that the circular polarization $Q$ of classical electromagnetic waves is conserved. In quantum field theory, the normal-ordered operator $\hat Q$ represents the difference between the number operators of right- and left-handed photons. Previous studies have shown that its expectation value is not conserved for observers propagating in a gravitational field. Here, we show that this Noether symmetry can also be realized in empty waveguides with duality-preserving boundary conditions, and we quantize the source-free Maxwell theory inside a long, cylindrical waveguide undergoing both linear and rotational acceleration from rest. In the vacuum $|0\rangle$ associated to inertial observers, we find that the expectation value $\langle 0| \hat Q |0\rangle $ fails to be conserved for observers co-moving with the waveguide. In particular, frame-dragging effects induce a spectral asymmetry between the right- and left-handed field modes at late times. As a consequence, accelerated detectors co-moving with the rotating waveguide can detect photon-pair excitations from the quantum vacuum, exhibiting an imbalance between opposite helicity modes. This is a relativistic quantum effect, which shows that the classical conservation law associated with duality symmetry is broken in the quantum theory even in flat spacetime, provided we work with non-inertial systems. Our analysis provides a concrete proof of concept for testing this effect in analogue gravity platforms.",0,arxiv,Kuantum,CC-BY/arXiv,Asymmetric excitation of left- vs right-handed photons in accelerating waveguides
"There is mounting evidence that entanglement dynamics in chaotic many-body quantum systems in the limit of large subsystems and long times is described by an entanglement membrane effective theory. In this paper, we derive the membrane description in a solvable chaotic large-$N$ model, the Brownian SYK chain. This model has a collective field description in terms of fermion bilinears connecting different folds of the multifold Schwinger-Keldysh path integral used to compute RÃ©nyi entropies. The entanglement membrane is a traveling wave solution of the saddle point equations governing these collective fields. The entanglement membrane is characterised by a velocity $v$ and a membrane tension ${\cal E}(v)$ that we calculate. We find that the membrane has finite width for $v<v_B$ (the butterfly velocity), however for $v > v_B$, the membrane splits into two wave fronts, each moving with the butterfly velocity. Our results provide a new viewpoint on the entanglement membrane and uncover new connections between quantum information dynamics and scrambling.",0,arxiv,Kuantum,CC-BY/arXiv,Entanglement membrane in the Brownian SYK chain
"Low-density parity check (LDPC) codes are a well known class of Pauli stabiliser Hamiltonians that furnish fixed-point realisations of nontrivial gapped phases such as symmetry breaking and topologically ordered (including fracton) phases. In this work, we propose symmetry-preserving deformations of these models, in the presence of a transverse field, and identify special points along the deformations with interesting features: (i) the special point is frustration-free, (ii) its ground states include a product state and the code space of the underlying code, and (iii) it remains gapped in the thermodynamic (infinite volume) limit. So the special point realises a first-order transition between (or the coexistence of) the trivial gapped phase and the nontrivial gapped phase associated with the code. In addition, if the original model has a non-invertible duality symmetry, then so does the deformed model. In this case, the duality symmetry is spontaneously broken at the special point, consistent with the associated anomaly.   A key step in proving the gap is a coarse-graining/blocking procedure on the Tanner graph of the code that allows us to apply the martingale method successfully. Our model, therefore, provides the first application of the martingale method to a frustration-free model, that is not commuting projector, defined on an arbitrary Tanner graph.   We also discuss several familiar examples on Euclidean spatial lattice. Of particular interest is the 2+1d transverse field Ising model: while there is no non-invertible duality symmetry in this case, our results, together with known numerical results, suggest the existence of a tricritical point in the phase diagram.",0,arxiv,Kuantum,CC-BY/arXiv,Deformed LDPC codes with spontaneously broken non-invertible duality symmetries
"Fault-tolerant quantum computers rely on Quantum Error-Correcting Codes (QECCs) to protect information from noise. However, no single error-correcting code supports a fully transversal and therefore fault-tolerant implementation of all gates required for universal quantum computation. Code switching addresses this limitation by moving quantum information between different codes that, together, support a universal gate set. Unfortunately, each switch is costly-adding time and space overhead and increasing the logical error rate. Minimizing the number of switching operations is, therefore, essential for quantum computations using code switching. In this work, we study the problem of minimizing the number of code switches required to run a given quantum circuit. We show that this problem can be solved efficiently in polynomial time by reducing it to a minimum-cut instance on a graph derived from the circuit. Our formulation is flexible and can incorporate additional considerations, such as reducing depth overhead by preferring switches during idle periods or biasing the compilation to favor one code over another. To the best of our knowledge, this is the first automated approach for compiling and optimizing code-switching-based quantum computations at the logical level.",0,arxiv,Kuantum,CC-BY/arXiv,Minimizing the Number of Code Switching Operations in Fault-Tolerant Quantum Circuits
"We present a universal quantum computing architecture which combines the measurement-driven aspect of MBQC with the circuit model's algorithm dependent generation of qubit entanglement. Our architecture, which we call QGATE, is tailored for discrete-variable photonic quantum computers with deterministic photon sources capable of generating 1D entangled photonic states. QGATE achieves universal quantum computing on a logical data qubit register via the implementation of Clifford operations, QGATE ancilla, and arbitrary angle single-qubit measurements. We realise unitary evolutions defined by multi-qubit Pauli strings via the generation of entanglement between a sub-set of logical qubits and a mutual QGATE ancilla qubit. Measurement of the QGATE ancilla in the appropriate basis then implements a given term of the desired unitary operation. This enables QGATE to both directly perform Hamiltonian evolutions in terms of a series of multi-qubit Pauli operators, in terms of projectors for an arbitrary sparse Hamiltonian, or realise multi-controlled gates enabling direct translation of circuit models to QGATE. We consider examples inspired by quantum chemistry and computational fluid dynamics. We propose an example photonic implementation of QGATE and calculate thresholds of $10.36\pm0.02\%$ or $25.98\pm0.28\%$ on the photonic loss for logical qubits constructed from foliated rotated surface codes, dependent on the deployment of intra-layer or inter-layer fusion respectively.",0,arxiv,Kuantum,CC-BY/arXiv,A Quantum Gate Architecture via Teleportation and Entanglement
"The task of conclusive exclusion for a set of quantum states is to find a measurement such that for each state in the set, there is an outcome that allows one to conclude with certainty that the state in question was not prepared. Defining classicality of statistics as realizability by a generalized-noncontextual ontological model, we show that there is a quantum-over-classical advantage for how well one can achieve conclusive exclusion. This is achieved in an experimental scenario motivated by the construction appearing in the Pusey-Barrett-Rudolph theorem. We derive noise-robust noncontextuality inequalities bounding the conclusiveness of exclusion, and describe a quantum violation of these. Finally, we show that this bound also constitutes a classical causal compatibility inequality within the bilocality scenario, and that its violation in quantum theory yields a novel possibilistic proof of a quantum-classical gap in that scenario.",0,arxiv,Kuantum,CC-BY/arXiv,A contextual advantage for conclusive exclusion: repurposing the Pusey-Barrett-Rudolph construction
"Lattice surgery with two-dimensional quantum error correcting codes is among the leading schemes for fault-tolerant quantum computation, motivated by superconducting hardware architectures. In conventional lattice surgery compilation schemes, logical circuits are compiled following a place-and-route paradigm, where logical qubits remain statically fixed in space throughout the computation. In this work, we introduce a paradigm shift by exploiting movable logical qubits via teleportation during the logical lattice surgery CNOT gate. Focusing on lattice surgery with the color code, we propose a proof-of-concept compilation scheme that leverages this capability. Numerical simulations show that the proposed approach can substantially reduce the routed circuit depth compared to standard place-and-route compilation techniques. Our results demonstrate that optimizations based on movable logical qubits are not limited to architectures with physically movable qubits, such as neutral atoms or trapped ions - they are also readily applicable to superconducting quantum hardware. An open-source implementation of our method is available on GitHub https://github.com/munich-quantum-toolkit/qecc.",0,arxiv,Kuantum,CC-BY/arXiv,Exploiting Movable Logical Qubits for Lattice Surgery Compilation
"Exactly solvable dissipative models provide an analytical tool for studying the relaxation dynamics in open quantum systems. In this work, we study an exactly solvable model based on an anisotropic variant of the Yao-Lee spin-orbital model, with dissipation acting in the spin sector. We map Liouvillian dynamics to fermions hopping in a doubled Hilbert space under a non-Hermitian Hamiltonian and demonstrate the model's exact solvability. We analyze the model's strong and weak symmetries, which protect an exponentially large manifold of non-equilibrium steady states, establishing the system as a physically feasible dissipative spin liquid. Furthermore, we analyze the transient dynamics in a translationally invariant sector and discover that the single-particle Liouvillian spectrum hosts an exceptional ring in momentum space. We map out a characteristic $\mathcal{PT}$ symmetry breaking transition driven by the dissipation strength, which governs the crossover from oscillatory to decaying relaxation of physical observables. Our work provides a physically motivated, solvable setting for exploring the coexistence of dissipative spin liquid physics and Liouvillian spectral singularities.",0,arxiv,Kuantum,CC-BY/arXiv,Dissipative Yao-Lee Spin-Orbital Model: Exact Solvability and $\mathcal{PT}$ Symmetry Breaking
"We investigate quantum Markov semigroups on bosonic Fock space and identify a broad class of infinite-dimensional dissipative evolutions that exhibit instantaneous Sobolev-regularization. Motivated by stability problems in quantum computation, we show that for certain Lindblad operators that are polynomials of creation and annihilation operators, the resulting dynamics immediately transform any initial state into one with finite expectation in all powers of the number operator. A key application is in the bosonic cat code, where we obtain explicit estimates in the trace norm for the speed of convergence. These estimates sharpen existing perturbative bounds at both short and long times, offering new analytic tools for assessing stability and error suppression in bosonic quantum information processing. For example, we improve the strong exponential convergence of the (shifted) $2$-photon dissipation to its fixed point to the uniform topology.",0,arxiv,Kuantum,CC-BY/arXiv,Instantaneous Sobolev Regularization for Dissipative Bosonic Dynamics
"The discovery of Bell that there exist quantum correlations that cannot be reproduced classically is one of the most important in the foundations of quantum mechanics, as well as having practical implications. Bell's result was originally proven in a simple bipartite causal structure, but analogous results have also been shown in further causal structures. Here we study the only causal structure with six or fewer nodes in which the question of whether or not there exist quantum correlations that cannot be achieved classically was open. In this causal structure we show that such quantum correlations exist using a method that involves imposing additional restrictions on the correlations. This hence completes the picture of which causal structures of up to six nodes support non-classical quantum correlations. We also provide further illustrations of our method using other causal structures.",0,arxiv,Kuantum,CC-BY/arXiv,Closing the problem of which causal structures of up to six total nodes have a classical-quantum gap
"We introduce a finite-time protocol that thermalizes a quantum harmonic oscillator, initially in its ground state, without requiring a macroscopic bath. The method uses a second oscillator as an effective environment and implements sudden quenches of the oscillator frequencies and coupling. Owing to the Gaussian nature of the dynamics, the thermalization condition reduces to three solvable equations, yielding exact analytic solutions for a dense discrete set of temperatures and numerical solutions in all other cases. Any target temperature can be approximated with arbitrary precision, with a trade-off between speed and accuracy. The simplicity of the protocol makes it a promising tool for rapid, controlled thermalization in quantum thermodynamics experiments and state preparation.",0,arxiv,Kuantum,CC-BY/arXiv,Thermalization from quenching in coupled oscillators
"Quantum key distribution (QKD) security fundamentally relies on the ability to distinguish genuine quantum correlations from classical eavesdropper simulations, yet existing certification methods lack rigorous statistical guarantees under finite-sample conditions and adversarial scenarios. We introduce TARA (Test by Adaptive Ranks), a novel framework combining conformal prediction with sequential martingale testing for quantum anomaly detection that provides distribution-free validity guarantees. TARA offers two complementary approaches. TARA k, based on Kolmogorov Smirnov calibration against local hidden variable (LHV) null distributions, achieving ROC AUC = 0.96 for quantum-classical discrimination. And TARA-m, employing betting martingales for streaming detection with anytime valid type I error control that enables real time monitoring of quantum channels. We establish theoretical guarantees proving that under (context conditional) exchangeability, conformal p-values remain uniformly distributed even for strongly contextual quantum data, confirming that quantum contextuality does not break conformal prediction validity a result with implications beyond quantum certification to any application of distribution-free methods to nonclassical data. Extensive validation on both IBM Torino (superconducting, CHSH = 2.725) and IonQ Forte Enterprise (trapped ion, CHSH = 2.716) quantum processors demonstrates cross-platform robustness, achieving 36% security margins above the classical CHSH bound of 2. Critically, our framework reveals a methodological concern affecting quantum certification more broadly: same-distribution calibration can inflate detection performance by up to 44 percentage points compared to proper cross-distribution calibration, suggesting that prior quantum certification studies using standard train test splits may have systematically overestimated adversarial robustness.",0,arxiv,Kuantum,CC-BY/arXiv,TARA Test-by-Adaptive-Ranks for Quantum Anomaly Detection with Conformal Prediction Guarantees
"Quantum high-harmonic generation (HHG) is a growing field of research with capabilities of providing high photon-number entangled states of light. However, there is an open debate regarding the theory level required for correctly describing the quantum aspects of HHG emission, such as squeezing or entanglement. Previous approaches have employed non-interacting classical ensembles of trajectories, or perturbation theory utilizing the classical trajectories as a starting point, missing out key entanglement features. In this Letter, we develop a full quantum theory for entanglement measures in HHG solving exactly the light-matter interaction Hamiltonian and employ it for evaluating the entanglement between emitted photons of different harmonics. For the first time, we reach qualitative agreement of theory with recent experiments showing that the R entanglement parameter decreases with increasing laser power for below-threshold harmonics. Our results indicate that fine-tuning the laser power could enhance HHG entanglement features, which are observed to oscillate with the driving power and exhibit local non-classical maxima structures. Similarly, our theory predicts that the oscillatory behavior of entanglement observed for below-threshold harmonics also appears for entanglement involving above-threshold harmonics. We also show that the long-range behavior of driven electronic trajectories can qualitatively change the resulting entanglement. Lastly, we show that focal averaging over classical degrees of freedom, which has thus far been ignored in quantum HHG theories, plays a key role in entanglement measures and can change the qualitative behavior of observables. Our work establishes the state-of-the art in exploring entanglement features in HHG, and paves way for analysis and engineering of 'truly-quantum' multi-photon states in the XUV and ultrafast regime for more complex matter systems.",0,arxiv,Kuantum,CC-BY/arXiv,Fully quantum theory of strong-field driven tunable entangled multi-photon states in HHG
"The accurate and efficient detection of quantum entanglement remains a central challenge in quantum information science. In this work, we study the detection of entanglement of polarized photons for measurement devices that are solely specified by rotational symmetry. We derive explicit positive operator valued measures (POVMs) showing that from a quantum information perspective any such setting is classified by one real measurable parameter r. In Particular, we give a POVM formulation of the Klein--Nishina formula for Compton scattering of polarized photons. We provide an SDP-based entanglement certification method that operates on the full measured statistics and gives tight bounds, also considering semi-device independent scenarios. Furthermore, we show that, while Bell violations are impossible with rotationally covariant measurements, EPR steering can still be certified under one-sided symmetry constraints. Finally, we present a rotationally covariant showcase experiment, analyzing the scattering of polarized optical light in a selection of soft drinks. Our results suggest that lemonade-based detectors are suitable for entanglement detection.",0,arxiv,Kuantum,CC-BY/arXiv,Entanglement Detection with Rotationally Covariant Measurements - From Compton Scattering to Lemonade
"Amid the International Year of Quantum Science and Technology 2025 (IYQ 2025), a significant portion of global funding has been dedicated to various quantum initiatives, with over 30 countries announcing their respective quantum strategies. Within the Southeast Asia context, Singapore, Thailand, and the Philippines have launched their respective quantum strategies and roadmaps. Meanwhile, six out of eleven Southeast Asia countries have expressed interest in formulating a regional quantum ecosystem to pursue a set of common goals. Quantum technologies, though still in their infancy within the second quantum revolution, have advanced rapidly in recent years. Due to their dual-use nature, quantum technologies are considered emerging and disruptive, often raising concerns from the cybersecurity perspective. While several discussions regarding Malaysia's quantum initiative and strategy are ongoing, it is vital to broaden the conversation and position Malaysia within the regional ecosystem. This paper provides an overview of Malaysia's quantum landscape and a summary of the regional initiatives since the establishment of Southeast Asia Quantum Network. We then analyse Malaysia's strengths in quantum research and provide four recommendations to strengthen the regional ecosystem.",0,arxiv,Kuantum,CC-BY/arXiv,Quantum Diplomacy within the Southeast Asia Quantum Ecosystem
"Boron vacancies ($V_B^-$) in hexagonal boron nitride (hBN) have emerged as a promising platform for two-dimensional quantum sensors capable of operating at atomic-scale proximity. However, the mechanisms responsible for photoluminescence quenching in thin hBN sensing layers when placed in contact with absorptive materials remain largely unexplored. In this Letter, we investigate non-radiative FÃ¶rster resonance energy transfer (FRET) between $V_B^-$ centers and either monolayer graphene or 2D semiconductors. Strikingly, we find that the FRET rate is negligible for hBN sensing layers thicker than 3 nm, highlighting the potential of $V_B^-$ centers for integration into ultra-thin quantum sensors within van der Waals heterostructures. Furthermore, we experimentally extract the intrinsic radiative decay rate of $V_B^-$ defects.",0,arxiv,Kuantum,CC-BY/arXiv,Non-radiative energy transfer between boron vacancies in hexagonal boron nitride and other 2D materials
"We develop an image theory for the recently proposed single-bounce quantum gravimeter. Free fall and quantum bounce of a matter wave-packet are described through decompositions over a basis of continuous energies. This leads to a much clearer interpretation of the origin of quantum interferences, associated to semi-classical estimations. We then give new tools to explore the space of parameters, and discuss the expected accuracy of the free-fall acceleration measurement.",0,arxiv,Kuantum,CC-BY/arXiv,Image Theory for the Single Bounce Quantum Gravimeter
"For a subclass of a general $\mathcal{PT}-$symmetric Hamiltonian obeying anti-commutation relation with its conjugate, a Hermitian basis is found that spans the bi-orthonormal energy eigenvectors. Using the modified projectors constructed from these eigenvectors, the generalized density matrix of the $\mathcal{PT}-$symmetric evolution is calculated, and subsequently, ergotropy for a closed system is obtained. The $\mathcal{PT}-$symmetric system, in an open system scenario, is studied to understand ergotropy under different regimes of non-Hermiticity of the Hamiltonian. The consistency of the three laws of thermodynamics for the $\mathcal{PT}-$symmetric system in an open system scenario is also analyzed.",0,arxiv,Kuantum,CC-BY/arXiv,Thermodynamics of an Open $\mathcal{PT-}$Symmetric Quantum System
"The theoretical description of broadband, multimode quantum pulses undergoing a second-order $Ï‡^{(2)}$-nonlinear interaction can be quite intricate, due to the large dimensionality of the underlying phase space. However, in many cases only a few broadband (temporal) modes are relevant before and after the nonlinear interaction. Here we present an efficient framework to calculate the relation between the quantum states at the input and output of a nonlinear element in their respective relevant modes. Since the number of relevant input and output modes may differ, resulting in an open quantum system, we introduce the generalized Bloch-Messiah decomposition (GBMD), reducing the description to an equal number of input and output modes. The GBMD enables us to calculate the multimode Wigner function of the output state by convolving the rescaled Wigner function of the reduced input quantum pulse with a multivariate Gaussian phase-space function. We expand on this result by considering two examples input states: A Fock state in a single broadband mode and a two-mode squeezed vacuum, both in the THz-frequency regime, up-converted to a single output broadband mode of optical frequencies. We investigate the effect, the convolution and thermalization due to entanglement breakage have on the output Wigner function by calculating the von Neumann entropy of the output Wigner function. The methods presented here can be used to optimize the amplification or frequency conversion of broadband quantum states, opening an avenue to the generation and characterization of optical quantum states on ultrafast time scales.",0,arxiv,Kuantum,CC-BY/arXiv,Phase-space open-systems dynamics of second-order nonlinear interactions with pulsed quantum light
"We propose a formulation of quantum measurement within a modified framework of frames, in which a quantum system - a single qubit - is directly coupled to a classical measurement bit. The qubit is represented as a positive probability distribution over two classical bits, a and a', denoted by p(aa'). The measurement apparatus is described by a classical bit $Î±= \pm 1$, initialized in the pure distribution $p(Î±) = \frac{1}{2}(1 + Î±)$. The measurement interaction is modeled by a quasi-bistochastic process $ S(bb'Î²\mid aa'Î±)$ - a bistochastic map that may include negative transition probabilities, while acting on an entirely positive state space. When this process acts on the joint initial state $p(aa')p(Î±)$, it produces a collapsed state $p(bb'\midÎ²)$, yielding the measurement outcome $Î²$ with the correct quantum-mechanical probability $p(Î²)$. This approach bypasses the von Neumann chain of infinite couplings by treating the measurement register classically, while capturing the nonclassical nature of measurement through the quasi-bistochastic structure of the interaction.",0,arxiv,Kuantum,CC-BY/arXiv,Rethinking Collapse: Coupling Quantum States to Classical Bits with quasi-probabilities
"Uncertainty is fundamental in modern power systems, where renewable generation and fluctuating demand make stochastic optimization indispensable. The chance constrained unit commitment problem (UCP) captures this uncertainty but rapidly becomes computationally challenging as the number of scenarios grows. Quantum computing has been proposed as a potential route to overcome such scaling barriers. In this work, we evaluate the applicability of quantum annealing platforms to the chance constrained UCP. Focusing on a scenario approximation, we reformulated the problem as a mixed integer linear program and solved it using DWave hybrid quantum classical solver alongside Gurobi. The hybrid solver proved competitive under strict runtime limits for large scenario sets (15,000 in our experiments), while Gurobi remained superior on smaller cases. QUBO reformulations were also tested, but current annealers cannot accommodate stochastic UCPs due to hardware limits, and deterministic cases suffered from embedding overhead. Our study delineates where chance constrained UCPs can already be addressed with hybrid quantum classical methods, and where current quantum annealers remain fundamentally limited.",0,arxiv,Kuantum,CC-BY/arXiv,Towards Quantum Stochastic Optimization for Energy Systems under Uncertainty: Joint Chance Constraints with Quantum Annealing
"Quantum information protocols offer significant advantages in properties such as security, anonymity, and privacy for communication and computing tasks. An application where guaranteeing the highest possible security and privacy is critical for democratic societies is electronic voting. As computational power continues to evolve, classical voting schemes may become increasingly vulnerable to information leakage. In this work, we present the experimental demonstration of an information-theoretically secure and efficient electronic voting protocol that, crucially, does not rely on election authorities, leveraging the unique properties of quantum states. Our experiment is based on a high-performance source of Greenberger-Horne-Zeilinger (GHZ) states and realizes a proof-of-principle implementation of the protocol in two scenarios: a configuration with four voters and two candidates employing privacy enhancement techniques and an election scenario supporting up to eight voters and sixteen candidates. The latter is particularly well-suited for secure board-level elections within organizations or small-scale governmental contexts.",0,arxiv,Kuantum,CC-BY/arXiv,Experimental Quantum Electronic Voting
"The extended Hubbard model on a two-dimensional lattice captures key physical phenomena, but is challenging to simulate due to the presence of long-range interactions. In this work, we present an efficient quantum algorithm for simulating the time evolution of this model. Our approach, inspired by the fast multipole method, approximates pairwise interactions by interactions between hierarchical levels of coarse-graining boxes. We discuss how to leverage recent advances in two-dimensional neutral atom quantum computing, supporting non-local operations such as long-range gates and shuttling. The resulting circuit depth for a single Trotter step scales polylogarithmically with system size.",0,arxiv,Kuantum,CC-BY/arXiv,Polylogarithmic-Depth Quantum Algorithm for Simulating the Extended Hubbard Model on a Two-Dimensional Lattice Using the Fast Multipole Method
"We study how electrons move across a graphene sheet when it encounters two magnetic barriers with a region in between that is continuously driven by laser light. Rather than acting as a static obstacle, this illuminated middle section becomes a Floquet cavity that opens new transport channels through controlled photon absorption and emission. By combining Floquet theory with the transfer matrix method, we track electron transmission through both the main energy band and the emerging photon-assisted sidebands. We find that the laser does more than modify the potential--it reshapes how electrons interact between the magnetic barriers, enabling a switch from ordinary transmission to transport dominated by photon exchange. Because the magnetic field and the optical drive are applied to separate sections of the device, the system supports interference between cyclotron-filtered motion and discrete photon-pumping channels, producing Fano resonances and angle-dependent transmission zeros that cannot appear in double magnetic or double laser barrier systems alone. Under well-defined conditions, the distance between the magnetic barriers controls the coupling between Floquet channels, allowing highly tunable resonances and even perfect transmission, despite strong magnetic confinement. We also observe that low-energy carriers are efficiently blocked by the magnetic regions, while conductance steadily rises with energy until it reaches a clear saturation plateau. This hybrid design provides a versatile way to steer graphene electrons by balancing optical pumping and magnetic momentum filtering.",0,arxiv,Kuantum,CC-BY/arXiv,Laser-induced modulation of conductance in graphene with magnetic barriers
"Extensive theoretical and experimental work has established high-fidelity electron shuttling in Si/SiGe systems, whereas demonstrations in Si/SiO2 (SiMOS) remain at an early stage. To help address this, we perform full 3D simulations of conveyor-belt charge shuttling in a realistic SiMOS device, building on earlier 2D modelling. We solve the Poisson and time-dependent Schrodinger equations for varying shuttling speeds and gate voltages, focusing on potential pitfalls of typical SiMOS devices such as oxide-interface roughness, gate fabrication imperfections, and charge defects along the transport path. The simulations reveal that for low clavier-gate voltages, the additional oxide screening in multi-layer gate architectures causes conveyor-belt shuttling to collapse to the bucket-brigade mode, inducing considerable orbital excitation in the process. Increasing the confinement restores conveyor-belt operation, which we find to be robust against interface roughness, gate misalignment, and charge defects buried in the oxide. However, our results indicate that defects located at the Si/SiO2-interface can induce considerable orbital excitation. For lower conveyor gate biases, positive defects in the transport channel can even capture passing electrons. Hence we identify key challenges and find operating regimes for reliable charge transport in SiMOS architectures.",0,arxiv,Kuantum,CC-BY/arXiv,Modelling the Impact of Device Imperfections on Electron Shuttling in SiMOS devices
"We provide a brief overview of approaches for calculating the density of states of quantum systems and random matrix Hamiltonians using the tools of free probability theory. For a given Hamiltonian of a quantum system or a generic random matrix Hamiltonian, which can be written as a sum of two non-commutating operators, one can obtain an expression for the density of states of the Hamiltonian from the known density of states of the two component operators by assuming that these operators are mutually free and by using the free additive convolution. In many examples of interacting quantum systems and random matrix models, this procedure is known to provide a reasonably accurate approximation to the exact numerical density of states. We review some of the examples that are known in the literature where this procedure works very well, and also discuss some of the limitations of this method in situations where the free probability approximation fails to provide a sufficiently accurate description of the exact density of states. Subsequently, we describe a perturbation scheme that can be developed from the subordination formulas for the Cauchy transform of the density of states and use it to obtain approximate analytical expressions for the density of states in various models, such as the Rosenzweig-Porter random matrix ensemble and the Anderson model with on-site disorder.",0,arxiv,Kuantum,CC-BY/arXiv,Density of states of quantum systems from free probability theory: a brief overview
"Conventional classical solvers are commonly used for solving matrix equation systems resulting from the discretization of SIEs in computational electromagnetics (CEM). However, the memory requirement would become a bottleneck for classical computing as the electromagentic problems become much larger. As an alternative, quantum computing has a natural ""parallelization"" advantage with much lower storage complexity due to the superposition and entanglement in quantum mechanics. Even though several quantum algorithms have been applied for the SIEs-based methods in the literature, the size of the matrix equation systems solvable using them is still limited. In this work, we use a hybrid quantum-classical scheme to solve the EFIE for analyzing electromagentic scattering from three-dimensional (3D) perfect electrically conducting objects with arbitrary shapes in CEM for the first time. Instead of directly solving the original EFIE matrix equation system using the quantum algorithms, the hybrid scheme first designs the preconditioned linear system and then uses a double-layer iterative strategy for its solution, where the external iteration layer builds subspace matrix equation systems with smaller dimension and the internal iteration layer solves the smaller systems using the quantum algorithms. Two representative quantum algorithms, HHL and VQLS, are considered in this work, which are executed on the quantum simulator and quantum computer platforms. We present the theoretical time complexity analysis of the hybrid quantum-classical scheme and perform numerical experiments to investigate the accuracy and efficiency of the hybrid scheme. The results show that the computational complexity of the hybrid VQLS-classical scheme is lower than the conventional fast solvers in classical computing, which indicates the hybrid scheme is more promising for analyzing large-scale electromagnetic problems.",0,arxiv,Kuantum,CC-BY/arXiv,Solution of the Electric Field Integral Equation Using a Hybrid Quantum-Classical Scheme: Investigation of Accuracy and Efficiency
"In the paper, we consider the problem of searching for the Largest empty rectangle in a 2D map, and the one-dimensional version of the problem is the problem of searching for the largest empty segment. We present a quantum algorithm for the Largest Empty Square problem and the Largest Empty Rectangle of a fixed width $d$ for $n\times n$-rectangular map. Query complexity of the algorithm is $\tilde{O}(n^{1.5})$ for the square case, and $\tilde{O}(n\sqrt{d})$ for the rectangle with a fixed width $d$ case, respectively. At the same time, the lower bounds for the classical case are $Î©(n^2)$, and $Î©(nd)$, respectively. The Quantum algorithm for the one-dimensional version of the problem has $O(\sqrt{n}\log n\log\log n)$ query complexity. The quantum lower bound for the problem is $Î©(\sqrt{n})$ which is almost equal to the upper bound up to a log factor. The classical lower bound is $Î©(n)$. So, we obtain the quadratic speed-up for the problem.",0,arxiv,Kuantum,CC-BY/arXiv,Quantum Algorithm for Searching for the Longest Segment and the Largest Empty Rectangle
"Quantum computing offers powerful new approaches for modeling complex social phenomena. Here, we propose and demonstrate quantum simulations of opinion dynamics, leveraging quantum superposition, measurement-induced state collapse, and entanglement to model realistic psychological and social processes. Specifically, we develop quantum models of opinion dynamics, solving exactly and simulating on IBM Quantum hardware. Our results, based on quantum devices and validated with practical quantum circuits, illustrate how quantum effects can enhance understanding of consensus formation, polarization, and collective decision-making. These findings pave the way for further exploration into quantum-enhanced social modeling, highlighting the potential of near-term quantum computers for simulating collective behavior in complex systems.",0,arxiv,Kuantum,CC-BY/arXiv,Quantum Simulations of Opinion Dynamics
"Cubic phase states provide the essential non-Gaussian resource for continuous-variable quantum computing. We show that they also offer significant potential for quantum metrology, surpassing the phase-sensing sensitivity of all Gaussian states at equal average photon number. Optimal sensitivity requires only moderate initial squeezing, and the non-Gaussian advantage remains robust against loss and detection noise. We identify optimal measurement strategies and show that several experimentally relevant preparation schemes surpass Gaussian limits, in some cases reaching the sensitivity of cubic phase states. Our results establish cubic phase states as a promising resource for quantum-enhanced precision measurements beyond Gaussian limits.",0,arxiv,Kuantum,CC-BY/arXiv,Metrological Sensitivity beyond Gaussian Limits with Cubic Phase States
"Computational fluid dynamics (CFD) is a cornerstone of classical scientific computing, and there is growing interest in whether quantum computers can accelerate such simulations. To date, the existing proposals for fault-tolerant quantum algorithms for CFD have almost exclusively been based on the Carleman embedding method, used to encode nonlinearities on a quantum computer. In this work, we begin by showing that these proposals suffer from a range of severe bottlenecks that negate conjectured quantum advantages: lack of convergence of the Carleman method, prohibitive time-stepping requirements, unfavorable condition number scaling, and inefficient data extraction. With these roadblocks clearly identified, we develop a novel algorithm for the incompressible lattice Boltzmann equation that circumvents these obstacles, and then provide a detailed analysis of our algorithm, including all potential sources of algorithmic complexity, as well as gate count estimates. We find that for an end-to-end problem, a modest quantum advantage may be preserved for selected observables in the high-error-tolerance regime. We lower bound the Reynolds number scaling of our quantum algorithm in dimension $D$ at Kolmogorov microscale resolution with $O(\mathrm{Re}^{\frac{3}{4}(1+\frac{D}{2})} \times q_M)$, where $q_M$ is a multiplicative overhead for data extraction with $q_M = O(\mathrm{Re}^{\frac{3}{8}})$ for the drag force. This upper bounds the scaling improvement over classical algorithms by $O(\mathrm{Re}^{\frac{3D}{8}})$. However, our numerical investigations suggest a lower speedup, with a scaling estimate of $O(\mathrm{Re}^{1.936} \times q_M)$ for $D=2$. Our results give robust evidence that small, but nontrivial, quantum advantages can be achieved in the context of CFD, and motivate the need for additional rigorous end-to-end quantum algorithm development.",0,arxiv,Kuantum,CC-BY/arXiv,An end-to-end quantum algorithm for nonlinear fluid dynamics with bounded quantum advantage
"Many spintronic, magnetic-memory, and neuromorphic devices rely on spatially varying magnetic fields. Quantitatively imaging these fields with full vector information over extended areas remains a major challenge. Existing probes either offer nanoscale resolution at the cost of slow scanning, or widefield imaging with limited vector sensitivity or material constraints. Quantum sensing with nitrogen-vacancy (NV) centers in diamond promises to bridge this gap, but a practical camera-based vector magnetometry implementation on relevant microstructures has not been demonstrated. Here we adapt a commercial widefield microscope to implement a camera-compatible pulsed optically detected magnetic resonance protocol to reconstruct stray-field vectors from microscale devices. By resolving the Zeeman shifts of the four NV orientations, we reconstruct the stray-field vector generated by microfabricated permalloy structures that host multiple stable remanent states. Our implementation achieves a spatial resolution of $\approx 0.52 ~Î¼\mathrm{m}$ across an $83~Î¼\mathrm{m} \times 83~Î¼\mathrm{m}$ field of view and a peak sensitivity of $ (828 \pm 142)~\mathrm{nT\,Hz^{-1}}$, with acquisition times of only a few minutes. These results establish pulsed widefield NV magnetometry on standard microscopes as a practical and scalable tool for routine vector-resolved imaging of complex magnetic devices.",0,arxiv,Kuantum,CC-BY/arXiv,Widefield Quantum Sensor for Vector Magnetic Field Imaging of Micromagnetic Structures
"The fundamental metrological limits of temperature sensing in open quantum systems remain largely unresolved, particularly regarding the role of non-Gaussian quantum resources. In this letter, we establish analytic bounds on the quantum Fisher information (QFI) for temperature estimation using non-Gaussian states undergoing dissipative bosonic evolution. By focusing on the short-time regime governed by a time-local master equation, we derive precise scaling laws that elucidate when and how non-Gaussian probes decisively outperform Gaussian states under identical energy constraints. Our analysis uncovers a distinct linear-in-time QFI enhancement unique to Fock states, in contrast to the inherently weaker, quadratic scaling of Gaussian probes. These theoretical insights are substantiated through exact numerical simulations and mapped onto experimentally accessible platforms such as circuit QED. Our results not only clarify the quantum thermometric advantage of non-Gaussianity but also chart a realistic pathway toward harnessing it in noisy quantum technologies.",0,arxiv,Kuantum,CC-BY/arXiv,Non-Gaussian Dissipative Quantum Thermometry Beyond Gaussian Bounds
"Quantifiers of stationarity, classicality, purity and vorticity are derived from phase-space differential geometrical structures within the Weyl-Wigner framework, after which they are related to the hyperbolic stability of classical and quantum-modified Hamiltonian (non-linear) equations of motion. By examining the equilibrium regime produced by such an autonomous system of ordinary differential equations, a correspondence between Wigner flow properties and hyperbolic stability boundaries in the phase-space is identified. Explicit analytical expressions for equilibrium-stability parameters are obtained for quantum Gaussian ensembles, wherein information quantifiers driven by Wigner currents are identified. Illustrated by an application to a Harper-like system, the results provide a self-contained analysis for identifying the influence of quantum fluctuations associated to the emergence of phase-space vorticity in order to quantify equilibrium and stability properties of Hamiltonian non-linear dynamics.",0,arxiv,Kuantum,CC-BY/arXiv,Geometrical structure of the Wigner flow information quantifiers and hyperbolic stability in the phase-space framework
"Optomechanical cooling of levitated nanoparticles has become an essential topic in modern quantum physics, providing a platform for exploring macroscopic quantum phenomena and high-precision sensing. However, conventional cavity-assisted cooling is fundamentally constrained by cavity dissipation and environmental noise, limiting the attainable minimum temperature. In this work, we propose a non-Hermitian optomechanical cooling scheme through nonreciprocal coupling between two levitated nanoparticles, where one particle is directly cooled by an optical cavity and the other is cooled indirectly through a non-Hermitian interaction. Both analytical solutions and numerical simulations reveal that increasing nonreciprocity enhances directional energy transfer, enabling the target particle to reach a lower phonon occupation than is achievable in conventional cavity cooling. This study demonstrates a new cooling mechanism driven by non-Hermitian interactions, offering theoretical guidance for realizing controllable energy flow and deep cooling in levitated optomechanical systems, and paving the way for future developments in quantum control and sensing technologies.",0,arxiv,Kuantum,CC-BY/arXiv,Sympathetic Cooling of Levitated Optomechanics through Nonreciprocal Coupling
"Cold-atom experiments based on alkali-like atoms provide us with a tool to experimentally realize Hubbard models with a large number $N$ of components. The value of $N$ can be seen as a new handle to tune the properties of the system, leading to new physics both in the case of fully SU($N$) symmetric systems, or in the presence of controlled symmetry breaking.   We focus on the Mott transition at global half filling and we characterize local correlations between particles complementing conventional estimates with the inter-flavor mutual information. We prove that these correlations have classical nature and, using Dynamical Mean-Field Theory, we show that the SU(4) system has significantly smaller correlations than the SU(2) counterpart. In the atomic limit we prove that increasing $N$ further decreases the strength of the correlations. This suggests that a controlled reduction of the symmetry, reducing the number of effective components, can be used to enhance the degree of correlation.   We confirm this scenario solving the model for $N=4$ and gradually breaking the symmetry via a Raman field, revealing an evolution from the SU(4) to the SU(2) Mott transition as the symmetry-breaking term increases, with a sudden recovery of the large correlations of the SU(2) model at weak Raman coupling in the Mott state. By further exploring the interplay between energy repulsion and the Raman field, we obtain a rich phase diagram with three different phases -- a metal, a band insulator, and a Mott insulator -- all coexisting at a single tricritical point.",0,arxiv,Kuantum,CC-BY/arXiv,More is uncorrelated: Tuning the local correlations of SU($N$) Fermi-Hubbard systems via controlled symmetry breaking
"Much recent work on distributed quantum computing have focused on the use of entangled pairs and distributed two qubit gates. But there has also been work on efficient schemes for achieving multipartite entanglement between nodes in a single shot, removing the need to generate multipartite entangled states using many entangled pairs. This paper looks at how multipartite entanglement resources (e.g., GHZ states) can be useful for distributed fan-out operations; we also consider the use of qudits of dimension four for distributed quantum circuit compression. In particular, we consider how such fan-out operations and qudits can be used to implement circuits which are challenging for distributed quantum computation, involving pairwise qubit interactions, i.e., what has been called global gates (a.k.a. global MÃ¸lmer-SÃ¸rensen gates). Such gates have been explored to possibly yield more efficient computations via reduced circuit depth, and can be carried out efficiently in some types of quantum hardware (e.g., trapped-ion quantum computers); we consider this as an exploration of an ``extreme'' case for distribution given the global qubit-qubit interactions. We also conclude with some implications for future work on quantum circuit compilation and quantum data centre design.",0,arxiv,Kuantum,CC-BY/arXiv,Distributed Quantum Computing with Fan-Out Operations and Qudits: the Case of Distributed Global Gates (a Preliminary Study)
"Continuous time quantum walks on exponentially large, sparse graphs form a powerful paradigm for quantum computing: On the one hand, they can be efficiently simulated on a quantum computer. On the other hand, they are themselves BQP-complete, providing an alternative framework for thinking about quantum computing -- a perspective which has indeed led to a number of novel algorithms and oracle problems. Recently, simulating the dynamics of a system of harmonic oscillators (that is, masses and springs) was set forth as another BQP-complete problem defined on exponentially large, sparse graphs. In this work, we establish a direct and transparent mapping between these two classes of problems. As compared to linking the two classes of problems via their BQP-completeness, our mapping has several desirable features: It is transparent, in that it respects the structure of the problem, including the geometry of the underlying graph, initialization, read-out, and efficient oracle access, resulting in low overhead in terms of both space and time; it allows to map also between restricted subsets of instances of both problems which are not BQP-complete; it provides a recipe to directly translate any quantum algorithm designed in the quantum walk paradigm to harmonic oscillators (and vice versa); and finally, it provides an alternative, transparent way to prove BQP-completeness of the harmonic oscillator problem by mapping it to BQP-completeness construction for the quantum walk problem (or vice versa).",0,arxiv,Kuantum,CC-BY/arXiv,Direct Equivalence between Dynamics of Quantum Walks and Coupled Classical Oscillators
"Quantum communication protocols seek to leverage the unique properties of quantum systems for coordination or communication tasks, usually with guarantees of security or anonymity that exceed what is possible classically. One promising domain of application is elections, where strong such guarantees are essential to ensure legitimacy. We experimentally implement a recently proposed election protocol from Centrone et al. such that no one, including a potential central authority, can know the preferred candidate of any voter other than themself. We conduct a four-party election, generating and distributing four-partite GHZ states with $\approx 89\%$ fidelity and successfully recording voters' intentions $\approx 87\%$ of the time.",0,arxiv,Kuantum,CC-BY/arXiv,Experimental quantum voting using photonic GHZ states
"Ultracold neutral atoms in optical lattices are a promising platform for simulating the behavior of complex materials and implementing quantum gates. We optimize collision gates for fermionic Lithium atoms confined in a double-well potential, controlling the laser amplitude and keeping its relative phase constant. We obtain high-fidelity gates based on a one-dimensional confinement simulation. Our approach extends beyond earlier Fermi-Hubbard simulations by capturing a momentum dependence in the interaction energy. This leads to a higher interaction strength when atoms begin in separate subwells compared to the same subwell. This momentum dependence might limit the gate fidelity under realistic experimental conditions, but also enables tailored applications in quantum chemistry and quantum simulation by optimizing gates for each of these cases separately.",0,arxiv,Kuantum,CC-BY/arXiv,Optimizing two-qubit gates for ultracold fermions in optical lattices
"Recent advances in coherent conveyor-mode spin qubit shuttling are paving the way for large-scale quantum computing platforms with qubit connectivity achieved by spin qubit shuttles. We developed a simulation tool to investigate numerically the impact of device imperfections on the spin-coherence of conveyor-mode shuttling in Si/SiGe. We simulate the quantum evolution of a mobile electron spin-qubit under the influence of sparse and singly charged point defects placed in the Si/SiGe heterostructure in close proximity to the shuttle lane. We consider different locations of a single charge defect with respect to the center of the shuttle lane, multiple orbital states of the electron in the shuttle with $g$-factor differences between the orbital levels, and orbital relaxation induced by electron-phonon interaction. With this simulation framework, we identify the critical defect density of charged point defects in the heterostructure for conveyor-mode spin qubit shuttle devices and quantify the impact of a single defect on the coherence of a qubit.",0,arxiv,Kuantum,CC-BY/arXiv,Numerical simulation of coherent spin-shuttling in a QuBus with charged defects
"The performance of Gottesman-Kitaev-Preskill (GKP) codes, an approach to hardware-efficient quantum error correction, is limited by the finite squeezing capabilities of current experimental platforms. To circumvent this hardware demand, we introduce Energy-Scaled Zero-Noise Extrapolation (ES-ZNE), a quantum error mitigation protocol that uses the mean photon number of the GKP code as a tunable effective noise parameter. The protocol measures logical observables at a series of accessible finite energies and extrapolates the results to the ideal, infinite-energy limit using an ansatz based on the code's asymptotic error scaling. Through simulating a GKP qubit under a pure-loss channel, we demonstrate that ES-ZNE successfully mitigates finite-energy errors, recovering the ideal expectation values (within numerical uncertainty) in the shallow-noise regime. Furthermore, by computationally removing artifacts arising from the finite-energy encoding, our method characterizes the intrinsic performance of the ideal GKP code, revealing a sharp error threshold beyond which the code's corrective power diminishes. These results establish ES-ZNE as a practical, software-based strategy for enhancing the performance of near-term bosonic quantum processors, trading sampling overhead for demanding physical resources like high squeezing.",0,arxiv,Kuantum,CC-BY/arXiv,Energy-Scaled Zero-Noise Extrapolation for Gottesman-Kitaev-Preskill Code
"We present Quantum Graph Hash (QGH-256), a novel quantum spectral hashing algorithm that generates high-entropy fingerprints from message-induced graphs. Each input message is mapped to a weighted graph via a discrete random walk on an n X n toroidal grid, where the walk dynamics determine the edge weights. Quantum Phase Estimation (QPE) is then used to extract the phase spectrum of the graph Laplacian. Unlike standard QPE settings, the phase estimation is performed with respect to a superposition state (a uniform superposition over all node basis states) rather than an eigenvector, ensuring that all eigencomponents contribute to the resulting spectrum. This yields spectral features that distinguish even co-spectral but non-isomorphic message-induced graphs. The final spectral fingerprint is converted into a 256-bit digest, producing a compact representation of the input. As the fingerprint encodes both spectral and dynamical properties of the message-induced graph, the resulting hash exhibits strong sensitivity to input perturbations and provides a structurally rich foundation for post-quantum hashing. To demonstrate the feasibility of the approach, we implement QGH-256 on a 4 X 4 toroidal grid, chosen empirically: smaller grids exhibit collisions, whereas larger grids significantly increase execution time. The entire pipeline is implemented in Qiskit, and we use a seeded statevector simulator to obtain stable, noise-free results.",0,arxiv,Kuantum,CC-BY/arXiv,Quantum Hash Function Based on Spectral Properties of Graphs and Discrete Walker Dynamics
"Edge bit in an average symmetry protected topological (ASPT) mixed state is studied. The state is protected by one strong $Z_2$ and one weak (average) $Z_2$ symmetries. As analogous objects of pure symmetry protected topological (SPT) states, the ASPT possesses edge bits. In particular, the analogous operator response exists, that is, symmetry fractionalization. The fractionalization preserves the presence of the ASPT in the bulk, and the fractionalized edge operators acting on the edge bits of the ASPT. %analogous to the ones in the pure SPTs. In this work, based on the cluster model and by employing Choi mapping, we discuss generic features of the edge bits and numerically clarify the behavior of the edge bits and their robustness for varying decoherence and perturbative interactions. By using an operator-space mutual information (OSMI), we track the flow of quantum correlations between the two edges. Remarkably, even in the ASPT regime, a finite portion of the initial edge-to-edge correlation survives.",0,arxiv,Kuantum,CC-BY/arXiv,Edge bits in average symmetry protected topological mixed state
"The success of a quantum annealing algorithm requires a polynomial scaling of the energy gap. Recently it was shown that a two-dimensional transverse-field Ising model on a square lattice with nearest-neighbor $\pm J$ random coupling has a polynomial energy gap in the symmetric subspace of the parity operator [Nature 631, 749-754 (2024)], indicating the efficient preparation of its ground states by quantum annealing. However, it is not clear if this result can be generalized to other spin glass models with continuous or biased randomness. Here we prove that under general independent and identical distributions (i.i.d.) of the exchange energies, the energy gap of a one-dimensional random transverse-field Ising model follows a stretched exponential scaling even in the parity-restricted subspace. We discuss the implication of this result to quantum annealing problems.",0,arxiv,Kuantum,CC-BY/arXiv,Stretched Exponential Scaling of Parity-Restricted Energy Gaps in a Random Transverse-Field Ising Model
"We develop a complex-entropy framework for Wigner negativity and apply it to avoided crossings in an oval quantum billiard. For a real Wigner function the Gibbs--Shannon functional becomes complex; its imaginary part, proportional to the Wigner-negative volume, serves as an entropy-like measure of phase-space nonclassicality. A sign-resolved decomposition separates the total negative weight from its phase-space distribution and defines a negative-channel Fisher information that quantifies how sensitively the negative lobe reshapes as a control parameter is varied. This structure yields a Cauchy--Schwarz bound that limits how rapidly the imaginary entropy, and hence the Wigner negativity, can change with the parameter. In the oval billiard, avoided crossings display enhanced negativity and an amplified negative-channel Fisher response, providing a clear phase-space signature of mode hybridization. The construction is generic and extends to other wave-chaotic and mesoscopic systems with phase-space representations.",0,arxiv,Kuantum,CC-BY/arXiv,Complex Wigner entropy and Fisher control of negativity in an oval quantum billiard
"We establish new theoretical results demonstrating the efficiency and robustness of system bath interaction models for quantum thermal and ground state preparation. Unlike existing analyses, which relies on the weak coupling Lindblad limit and require $O(Îµ)$ coupling strengths for $Îµ$ accuracy, leading to slow mixing, we rigorously show that accurate state preparation remains possible far beyond this regime. In particular, even when the cumulative coupling strength remains constant rather than vanishing, the induced quantum channel still approximately fixes the target state. Our proof introduces new techniques for controlling all orders of the Dyson expansion and for analyzing the associated multidimensional operator Fourier transforms. These bounds substantially improve upon prior results, and numerical simulations on the TFIM and Hubbard models further confirm the robustness of the system bath interaction framework across both weak and strong coupling regimes.",0,arxiv,Kuantum,CC-BY/arXiv,Beyond Lindblad Dynamics: Rigorous Guarantees for Thermal and Ground State Preservation under System Bath Interactions
"Encrypted control has been extensively studied to ensure the confidentiality of system states and control inputs for networked control systems. This paper presents a computationally efficient encrypted control framework for networked systems enabled by quantum communication. A quantum channel between sensors and actuators is used to generate identical secret keys, whose security is further enhanced through quantum key distribution. These keys enable lightweight encryption and decryption while preserving confidentiality and control accuracy. We develop a novel encryption-decryption architecture for state-feedback control of linear systems based on quantum keys, and characterize the impact of quantum state errors on closed-loop stability. In particular, we establish the existence of a critical threshold on intrinsic quantum noise below which stability is guaranteed. In contrast to classical encrypted control schemes, which may collapse under a single key-bit error, the proposed quantum encrypted control exhibits strong robustness to key imperfections. We further adopt quantization techniques to address the scenarios with limited communication bits in practical situations, and implement privacy protection for quantum keys based on a stochastic quantizer. These results demonstrate that integrating quantum technologies into control systems in a nontrivial and principled manner, even at their current level of maturity, can yield substantial performance gains in reducing computational complexity and improving resilience to key errors while ensuring security against multiple eavesdropping sources.",0,arxiv,Kuantum,CC-BY/arXiv,Quantum Encrypted Control of Networked Systems
"Non-trivial dispersion relations engineered in photonic waveguide for the precise control of atomic dynamics has recently attracted considerable attention. Here, we study a system in which atoms are coupled to one-dimensional coupled-resonator waveguides with long-range hoppings. By carefully engineering the jth-order nearest neighbor (JNN) hoppings between resonators, we construct linear dispersion relations with the chiral characteristic. To quantify the degree of linearity, we analyze the propagation fidelities of Gaussian wave packets in these waveguides. Furthermore, we demonstrate that such coupled-resonator waveguides can serve as versatile platforms for enabling directional atomic radiation and absorption. Beyond linear dispersion relations, more general forms, including quadratic and cubic relations, can also be achieved through tailored JNN-hoppings. Our study thus provides a unified framework for simulating atom-environment couplings with arbitrary dispersion relations.",0,arxiv,Kuantum,CC-BY/arXiv,Engineering photonic dispersion relation and atomic dynamics in waveguide QED setup via long-range hoppings
"Temporal single-photon (TSP-) LiDAR presents a promising solution for imaging-free target recognition over long distances with reduced size, cost, and power consumption. However, existing TSP-LiDAR approaches are ineffective in handling open-set scenarios where unknown targets emerge, and they suffer significant performance degradation under low signal-to-noise ratio (SNR) and short acquisition times (fewer photons). Here, inspired by semantic communication, we propose a semantic TSP-LiDAR based on a self-updating semantic knowledge base (SKB), in which the target recognition processing of TSP-LiDAR is formulated as a semantic communication. The results, both simulation and experiment, demonstrate that our approach surpasses conventional methods, particularly under challenging conditions of low SNR and limited acquisition time. More importantly, our self-updating SKB mechanism can dynamically update the semantic features of newly encountered targets in the SKB, enabling continuous adaptation without the need for extensive retraining of the neural network. In fact, a recognition accuracy of 89% is achieved on nine types of unknown targets in real-world experiments, compared to 66% without the updating mechanism. These findings highlight the potential of our framework for adaptive and robust target recognition in complex and dynamic environments.",0,arxiv,Kuantum,CC-BY/arXiv,Semantic Temporal Single-photon LiDAR
"We develop a phenomenological Ginzburg-Landau (GL) framework for high-$T_c$ cuprates in which a short-range modulation of the electronic charge density couples to a $d$-wave superconducting condensate. The resulting modulated electron lattice (MEL) state is distinct from long-range static charge density wave order: it is short range, partially phase coherent, and linked to superconducting coherence. A preferred wave vector $q^{\ast} \approx 0.3$ reciprocal lattice units along the Cu-O bond direction emerges from the interplay between a momentum-dependent susceptibility and bond-stretching phonons, consistent with neutron and x-ray data on YBa$_2$Cu$_3$O$_{7-Î´}$ and related cuprates. The GL free energy contains coupled $d$-wave superconducting and charge sectors with parameters constrained by optimally doped YBa$_2$Cu$_3$O$_{7-Î´}$. We identify an MEL enhancement window in doping, temperature, MEL correlation length, and disorder where a coherence linked modulation enhances the superfluid stiffness. Classical Monte Carlo simulations yield an in-plane stiffness enhancement of order ten percent, which we treat as a qualitative prediction to be tested by self-consistent Bogoliubov de Gennes calculations. The MEL framework yields falsifiable experimental signatures. For scanning tunneling spectroscopy in Bi-based cuprates we highlight two predictions: the Fourier-transformed local density of states should exhibit a $q^{\ast} \approx 0.3$ peak whose spectral weight sharpens as superconducting phase coherence develops below $T_c$, in contrast to static charge scenarios, and the local gap magnitude $Î”(r)$ should correlate positively with the local MEL amplitude. The framework implies correlations between MEL correlation length, superfluid stiffness, disorder, and vortex pinning, and organizes cuprate observations into testable STM/STS predictions.",0,arxiv,Kuantum,CC-BY/arXiv,Short-Range Modulated Electron Lattice and d-Wave Superconductivity in Cuprates: A Phenomenological Ginzburg-Landau Framework
"Federated learning (FL) deployed over quantum enabled and heterogeneous classical networks faces significant performance degradation due to uneven client quality, stochastic teleportation fidelity, device instability, and geometric mismatch between local and global models. Classical aggregation rules assume euclidean topology and uniform communication reliability, limiting their suitability for emerging quantum federated systems. This paper introduces A2G (Adaptive Aggregation with Two Gains), a dual gain framework that jointly regulates geometric blending through a geometry gain and modulates client importance using a QoS gain derived from teleportation fidelity, latency, and instability. We develop the A2G update rule, establish convergence guarantees under smoothness and bounded variance assumptions, and show that A2G recovers FedAvg, QoS aware averaging, and manifold based aggregation as special cases. Experiments on a quantum classical hybrid testbed demonstrate improved stability and higher accuracy under heterogeneous and noisy conditions.",0,arxiv,Kuantum,CC-BY/arXiv,A2G-QFL: Adaptive Aggregation with Two Gains in Quantum Federated learning
"Parametric amplification is essential for quantum measurement, enabling the amplification of weak microwave signals with minimal added noise. While Josephson-junction-based amplifiers have become standard in superconducting quantum circuits, their magnetic sensitivity, limited saturation power, and sub-kelvin operating requirements motivate the development of alternative nonlinear platforms. Here we demonstrate a two-mode kinetic-inductance parametric amplifier based on a pair of capacitively coupled Kerr-nonlinear resonators fabricated from NbTiN and NbN thin films. The distributed Kerr nonlinearity of these materials enables nondegenerate four-wave-mixing amplification with gains approaching 40 dB, gain-bandwidth products up to 6.9 MHz, and 1-dB compression powers two to three orders of magnitude higher than those of state-of-the-art Josephson amplifiers. A coupled-mode theoretical model accurately captures the pump-induced modification of the hybridized modes and quantitatively reproduces the observed signal and idler responses. The NbN device exhibits a significantly larger Kerr coefficient and superior gain-bandwidth performance, highlighting the advantages of high-kinetic-inductance materials. Our results establish coupled kinetic-inductance resonators as a robust platform for broadband, high-power, and magnetically resilient quantum-limited amplification, offering a scalable route for advanced readout in superconducting qubits, spin ensembles, quantum dots, and other microwave-quantum technologies.",0,arxiv,Kuantum,CC-BY/arXiv,Hybridized-Mode Parametric Amplifier in Kinetic-Inductance Circuits
"We investigate quench dynamics in the quantum $S=1/2$ XXZ antiferromagnetic chain with staggered and anisotropic interactions in the flat-band limit. Our quench protocol interchanges the odd- and even-bond strengths of a fully dimerized chain, enabling us to derive exact time-dependent states for arbitrary even system sizes by working in the Bell basis. We obtain closed-form, size-independent expressions for the von Neumann and second-order RÃ©nyi entanglement entropies. We further calculate exact Loschmidt echoes and the corresponding return rate functions across various anisotropies and system sizes, and identify Loschmidt zeros in finite chains. Our analysis reveals the precise conditions on the anisotropy parameter that govern the periodicity of the dynamical observables. In addition to the analytic study, we perform two types of numerical experiments on IBM-Q quantum devices. First, we use the Hadamard test to estimate the Bell-basis expansion coefficients and reconstruct the dynamical states, achieving accurate entanglement entropies and the Loschmidt echo for small systems. Second, we implement Trotter-error-free time-evolution circuits combined with randomized Pauli measurements. Post-processing via statistical correlations and classical shadows yields reliable estimates of the second-order RÃ©nyi entanglement entropy and the Loschmidt echo, showing satisfactory agreement with exact results.",0,arxiv,Kuantum,CC-BY/arXiv,Quench dynamics of the quantum XXZ chain with staggered interactions: Exact results and simulations on digital quantum computers
"Many years have passed since the conception of the quintessential method of shortcut to adiabaticity known as counterdiabatic driving (or transitionless quantum driving). Yet, this method appears to be energetically cost-free and thus continually challenges the task of quantifying the amount of energy it demands to be accomplished. This paper proposes that the energy cost of controlling a closed quantum system using the counterdiabatic method can also be assessed using the instantaneous excess work during the process and related quantities, as the time-averaged excess work. Starting from the Mandelstam-Tamm bound for driven dynamics, we have shown that the speed-up of counterdiabatic driving is linked with the spreading of energy between the eigenstates of the total Hamiltonian, which is necessarily accompanied by transitions between these eigenstates. Nonetheless, although excess work can be used to quantify energetically these transitions, it is well known that the excess work is zero throughout the entire process under counterdiabatic driving. To recover the excess work as an energetic cost quantifier for counterdiabatic driving, we will propose a different interpretation of the parameters of the counterdiabatic Hamiltonian, leading to an excess work different from zero. We have illustrated our findings with the Landau-Zener model.",0,arxiv,Kuantum,CC-BY/arXiv,Excess work in counterdiabatic driving
"An excitonic approach to the ultrafast optical response of confined semiconductors at elevated densities below the Mott transition is presented. The theory is valid from the coherent regime, where coherent excitonic transitions and biexcitons dominate, to the incoherent regime, where excitonic occupations dominate. Numerical simulations of the $1s$ exciton dynamics during intense circularly polarized pump pulses in two different Coulomb-interaction regimes are performed for two-dimensional semiconductors: Moderate Coulomb interaction is compared with dominating Coulomb interaction with respect to the light-matter interaction strength. The different many-body contributions are disentangled and it is found, that excitonic Rabi oscillations in the Coulomb-dominated regime are considerably less strong. By also comparing circular and linear excitation in a MoSe$_2$ monolayer, it is found, that linear excitation creates a regime, where excitonic Rabi oscillations are almost completely suppressed.",0,arxiv,Kuantum,CC-BY/arXiv,Excitonic Theory of the Ultrafast Optical Response of 2D-Quantum-Confined Semiconductors at Elevated Densities
"Analog quantum simulators can directly emulate time-dependent Hamiltonian dynamics, enabling the exploration of diverse physical phenomena such as phase transitions, quench dynamics, and non-equilibrium processes. Realizing accurate analog simulations requires high-fidelity time-dependent pulse control, yet existing calibration schemes are tailored to digital gate characterization and cannot be readily extended to learn continuous pulse trajectories. We present a characterization algorithm for in situ learning of pulse trajectories by extending the Quantum Signal Processing (QSP) framework to analyze time-dependent pulses. By combining QSP with a logical-level analog-digital mapping paradigm, our method reconstructs a smooth pulse directly from queries of the time-ordered propagator, without requiring mid-circuit measurements or additional evolution. Unlike conventional Trotterization-based methods, our approach avoids unscalable performance degradation arising from accumulated local truncation errors as the logical-level segmentation increases. Through rigorous theoretical analysis and extensive numerical simulations, we demonstrate that our method achieves high accuracy with strong efficiency and robustness against SPAM as well as depolarizing errors, providing a lightweight and optimal validation protocol for analog quantum simulators capable of detecting major hardware faults.",0,arxiv,Kuantum,CC-BY/arXiv,In Situ Quantum Analog Pulse Characterization via Structured Signal Processing
"We investigate the quantum resource requirements of a dataset generated from simulations of two-dimensional, periodic, incompressible shear flow, aimed at training machine learning models. By measuring entanglement and non-stabilizerness on MPS-encoded functions, we estimate the computational complexity encountered by a stabilizer or a tensor network solver applied to Computational Fluid Dynamics (CFD) simulations across different flow regimes. Our analysis reveals that, under specific initial conditions, the shear width identifies a transition between resource-efficient and resource-intensive regimes for non-trivial evolution. Furthermore, we find that the two resources qualitatively track each other in time, and that the mesh resolution along with the sign structure play a crucial role in determining the resource content of the encoded state. These findings offer useful guidelines for the development of scalable, quantum-inspired approaches to fluid dynamics.",0,arxiv,Kuantum,CC-BY/arXiv,Magic of the Well: assessing quantum resources of fluid dynamics data
"Quantum annealers are emerging as programmable, dynamical experimental platforms for probing strongly correlated spin systems. Yet key thermal assumptions, chiefly a Gibbs-distributed output ensemble, remain unverified in the large-scale regime. Here, we experimentally and quantitatively assess Gibbs sampling fidelity across system sizes spanning over three orders of magnitude. We explore a wide parameter space of coupling strengths, system sizes, annealing times, and D-wave hardware architectures. We find that the naively assumed scaling law for the effective temperature requires a non-negligible, coupling-independent offset that is robust across machines and parameter regimes, quantifying residual non-thermal effects that still conform to an effective Gibbs description. These non-idealities are further reflected in a systematic discrepancy between the physical temperature inferred from the sampled ensemble and the nominal cryogenic temperature of the device. Our results systematically assess the viability of quantum annealers as experimental platforms for probing classical thermodynamics, correct previous assumptions, and provide a physically grounded thermometry framework to benchmark these machines for future thermodynamic experiments.",0,arxiv,Kuantum,CC-BY/arXiv,Classical Thermometry of Quantum Annealers
"Scaling quantum computers to large sizes requires the implementation of many parallel qubit readouts. Here we present an ultrastable superconducting-qubit readout method using the multi-tone self-phase-referenced Pound-Drever-Hall (PDH) technique, originally developed for use with optical cavities. In this work, we benchmark PDH readout of a single transmon qubit, using room-temperature heterodyne detection of all tones to reconstruct the PDH signal. We demonstrate that PDH qubit readout is insensitive to microwave phase drift, displaying $0.73^\circ$ phase stability over 2 hours, and capable of single-shot readout in the presence of phase errors exceeding the phase shift induced by the qubit state. We show that the PDH sideband tones do not cause unwanted measurement-induced state transitions for a transmon qubit, leading to a potential signal enhancement of at least $14$~dB over traditional heterodyne readout.",0,arxiv,Kuantum,CC-BY/arXiv,The Pound-Drever-Hall Method for Superconducting-Qubit Readout
"The topological classification of insulators and superconductors, the ""ten-fold way"", is grounded on fermionic many-body symmetries and has had a dramatic impact on many fields of physics. Therefore, it seems equally important to investigate a similar approach for bosons as tightly analogous to the fermionic prototype as possible. There are, however, several obstacles coming from the fundamental physical differences between fermions and bosons. Here, we propose a potentially optimal way forward: a theory of free boson topology (topological classification and bulk-boundary correspondence) protected by bosonic many-body symmetry operations, namely, squeezing transformations, particle number, and bosonic time reversal. We identify two symmetry classes that are topologically non-trivial in one dimension. They include key models like the bosonic Kitaev chain, protected by a squeezing symmetry within our framework, and the celebrated bosonic SSH model, protected by a squeezing symmetry and particle number. To provide a robust experimental platform for testing our theory, we introduce a new quantum meta-material: photo-magnonic crystals. They consist of arrays of interconnected photo-magnonic cavities. They are remarkable for their experimental flexibility and natural affinity for displaying band topological physics at microwave frequencies. We engineer a many-body symmetry-protected topological photo-magnonic chain with boundary modes mandated by a Pfaffian invariant. Using an electromagnetic finite-element modelling, we simulate its reflection and transmission and identify experimental signatures of its boundary modes. The experimental tuning of the crystal to its symmetry-protected topological phase is also addressed. Our modelling of the photo-magnonic chain provides a thorough blueprint for its experimental realisation and the unambiguous observation of its exotic physics.",0,arxiv,Kuantum,CC-BY/arXiv,Many-body symmetry-protected zero boundary modes of synthetic photo-magnonic crystals
"Measurement-based quantum computing relies on the generation of large entangled cluster states that act as a universal resource on which logical circuits can be imprinted and executed through local measurements. A number of strategies for constructing sufficiently large photonic cluster states propose fusing many smaller resource states generated by a series of quantum emitters. However, the fusion process is inherently probabilistic with a 50% success probability in standard guise. A recent proposal has shown that, in the limit of low loss, the probability of achieving successful fusion may be boosted to near unity by redundantly encoding the vertices of linear graph states using Greenberger-Horne-Zeilinger states [Quantum 7, 992 (2023)]. Here we present a protocol for deterministically generating redundantly encoded photonic resource states using single quantum emitters, and study the impact of protocol errors and photonic losses on the generated resource states and type-II photonic fusion. Our work provides a route for efficiently constructing complex entangled photonic qubit states for photonic quantum computing and quantum repeaters.",0,arxiv,Kuantum,CC-BY/arXiv,Generating redundantly encoded resource states for photonic quantum computing
"In a periodic lattice system an entangled antipodal pair state, otherwise known as a crosscap state, is a simple two site product state in which spins at antipodal sites are prepared in Bell pairs. Such states have maximal bipartite entanglement and serve as a useful platform for studying the quench dynamics of systems which have large initial entanglement. In this paper, we study a generalization of these states which we dub entangled mutipodal states. These states, which are defined for fermionic systems, generalize the crosscap states by having correlations among more than two sites, specifically, those which sit at the vertices of regular polygons. By construction, the states are Gaussian and translationally invariant allowing many of their properties to be understood. We study the bipartite entanglement entropy of these states both in and out of equilibrium. In equilibrium, the entanglement profile as a function of subsystem size exhibits two distinct regimes, a volume-law growth followed by a saturation to a constant value, thus generalizing the Page-curve profile of the crosscap state. In the non-equilibrium setting, we study quenches from these initial states to the free-fermion chain, whose ensuing dynamics displays a far richer structure compared to the crosscap case. We interpret our results in terms of the quasiparticle picture, which requires multiplets of quasiparticles to be excited non-locally around the system. This scenario is confirmed by the appearance of a post-quench, negative tripartite information.",0,arxiv,Kuantum,CC-BY/arXiv,Entanglement evolution from entangled multipodal states
"We investigate the steady-state phases of generic $\mathbb{Z}_2$-symmetric monitored, open quantum dynamics. We describe the phases systematically in terms of both information-theoretic diagnostics and spontaneous breaking of strong and weak symmetries of the dynamics. We find a completely broken phase where information is retained by the quantum system, a strong-to-weak broken phase where information is leaked to the environment, and an unbroken phase where information is learned by the observer. We find that weak measurement and dephasing alone constitute a minimal model for generic open systems with $\mathbb{Z}_2$ symmetry, but we also explore perturbations by unitary gates. For a 1d set of qubits, we examine information-theoretic and symmetry-breaking observables in the path integral of the doubled state. This path integral reduces to the standard classical 2d random-bond Ising model in certain limits but generically involves negative weights, enabling a special self-dual random-bond Ising model at the critical point when only measurements are present. We obtain numerical evidence for the steady-state phases using efficient tensor network simulations of the doubled state.",0,arxiv,Kuantum,CC-BY/arXiv,Information dynamics and symmetry breaking in generic monitored $\mathbb{Z}_2$-symmetric open quantum systems
"We investigate the emergence of quantum chaos and unitary T-design behavior in derandomized Clifford+T circuits using causal cover architectures. Motivated by the need for deterministic constructions that can exhibit chaotic behavior across diverse quantum hardware platforms, we explore deterministic Clifford circuit architectures (random Clifford circuits with causal cover, bitonic sorting networks, and permutation-based routing circuits) to drive quantum circuits toward Wigner-Dyson (WD) entanglement spectrum statistics and OTOC decay.Our experiments demonstrate that causal connectivity, not circuit depth or randomness, is a critical feature that drives circuits to chaos. We show that initializing with n T-states and adding a second T-layer after a causally covered Clifford evolution yields consistent OTOC decay and WD statistics. This also enables deeper understanding of the circuit structures that generate complex entanglement behavior. Notably, our work suggests polylogarithmic-depth deterministic circuits suffice to approximate chaotic behavior, highlighting that causal connectivity is sufficient for operator spreading to induce Wigner-Dyson entanglement statistics and OTOC decay.",0,arxiv,Kuantum,CC-BY/arXiv,Structured Clifford+T Circuits for Efficient Generation of Quantum Chaos
"Quantum entanglement is a fundamental resource for emerging quantum technologies, enabling secure communication and enhanced sensing. For decades, generating polarization entangled states has been mainly achieved using bulk crystals with spontaneous parametric down conversion (SPDC), preventing scalability and on-chip integration. Miniaturizing the quantum source provides access to more versatility and tunability while enabling an easier integration to other devices, notably necessary for satellite-based quantum communication, and eventually reducing fabrication costs. This challenging task can be achieved with Zinc Blende GaAs nanowires. They already have shown an efficient photon pairs generation via SPDC at 1550 nm. Here we demonstrate that a pair of orthogonal GaAs nanowires constitutes a new nanoscale platform to control the quantum state at telecommunication wavelength, enabling a transition from polarization entangled to separable states as a function of the pump polarization, with fidelities reaching 90%",0,arxiv,Kuantum,CC-BY/arXiv,Tunable polarization-entangled near-infrared photons from orthogonal GaAs nanowires
"Multipartite quantum states saturating the Heisenberg limit of sensitivity typically require full-body correlators to be prepared. On the other hand, experimentally practical Hamiltonians often involve few-body correlators only. Here, we study the metrological performances under this constraint, using tools derived from the quantum Fisher information. Our work applies to any encoding generator, also including a dependence on the parameter. We find that typical random symmetric ground states of $k$-body permutation-invariant Hamiltonians exhibit Heisenberg scaling. Finally, we establish a tradeoff between the Hamiltonian's gap, which quantifies preparation hardness, and the quantum Fisher information of the corresponding ground state.",0,arxiv,Kuantum,CC-BY/arXiv,Many-body $k$-local ground states as probes for unitary quantum metrology
"We report universal skyrmionic spin textures in the cores of optical and acoustic vortex beams, described within the framework of Laguerre-Gaussian modes. We analytically demonstrate nondiffractive propagating spin merons, independent of whether the field is transverse or longitudinal, with their sign controlled by the wavefront helicity. Experimental confirmation is provided in acoustics through full three-dimensional measurements of the velocity vector field. Although these phenomena are intrinsic to vortex cores, we also show that the claimed universality breaks down for higher-order topological charges, depending on the carrier mode, here exemplified using the Bessel framework.",0,arxiv,Kuantum,CC-BY/arXiv,Universal nondiffractive topological spin textures in vortex cores of light and sound
"Self-testing constitutes one of the most powerful forms of device certification, enabling a complete and device-independent characterization of a quantum apparatus solely from the observed correlations. In recent work by the authors [23], a general framework was introduced for constructing Bell inequalities that self-test entire families of Clifford generators. In this manuscript, we develop an alternative and complementary self-testing criterion based on symmetric spanning sets. This formulation provides an explicit and constructive route to designing self-testing Bell inequalities in arbitrary dimensions.",0,arxiv,Kuantum,CC-BY/arXiv,Systematic construction of ROCN Bell-inequalities
"Quantum hypergraph states emerged in the literature as a generalization of graph states, and since then, considerable progress has been made toward implementing this class of genuine multipartite entangled states for quantum information and computation. Here, we review the definition of hypergraph states and their main applications so far, both in discrete-variable and continuous-variable quantum information.",0,arxiv,Kuantum,CC-BY/arXiv,Quantum hypergraph states: a review
"We propose a novel heuristic quantum algorithm for the Minimum Vertex Cover (MVC) problem based on continuous-time quantum walks (CTQWs). In this framework, the coherent propagation of a quantum walker over a graph encodes its structural properties into state amplitudes, enabling the identification of highly influential vertices through their transition probabilities. To enhance stability and solution quality, we introduce a dynamic decoupling (``freezing'') mechanism that isolates vertices already selected for the cover, preventing their interference in subsequent iterations of the algorithm. The method employs a compact binary encoding, requiring only $\lceil \log_2 (V)\rceil$ qubits to represent a graph with $V$ vertices, resulting in an exponential reduction of quantum resources compared to conventional vertex-based encodings. We benchmark the proposed heuristic against exact solutions obtained via Mixed-Integer Linear Programming (MILP) and against established classical heuristics, including Simulated Annealing, FastVC, and the 2-Approximation algorithm, across ErdÅ‘s--RÃ©nyi, BarabÃ¡si--Albert and regular random graph ensembles. Our results demonstrate that the CTQW-based heuristic consistently achieves superior approximation ratios and exhibits remarkable robustness with respect to network topology, outperforming classical approaches in both heterogeneous and homogeneous structures. These findings indicate that continuous-time quantum walks, when combined with topology-independent decoupling strategies, provide a powerful paradigm for large-scale combinatorial optimization and complex network control, with potential applications spanning infrastructure resilience, epidemic containment, sensor network optimization, and biological systems analysis.",0,arxiv,Kuantum,CC-BY/arXiv,Scalable Quantum Walk-Based Heuristics for the Minimum Vertex Cover Problem
"We study the quantum dynamics generated by the repeated action of a non-unitary evolution operator on a system of qubits. Breaking unitarity can lead to the purification of mixed initial states, which corresponds to the loss of sensitivity to initial conditions, and hence the absence of a key signature of dynamical chaos. However, the scrambling of quantum information can delay purification to times that are exponential in system size. Here we study purification in systems whose evolution operators are fixed in time, where all aspects of the dynamics are in principle encoded in spectral properties of the evolution operator for a single time step. The operators that we study consist of global Haar random unitary operators and non-unitary single-qubit operations. We show that exponentially slow purification arises from a distribution of eigenvalues in the complex plane that forms a ring with sharp edges at large radii, with the eigenvalue density exponentially large near these edges. We argue that the sharp edges of the eigenvalue distribution arise from level attraction along the radial direction in the complex plane. By calculating the spectral form factor we also show that there is level repulsion around the azimuthal direction, even close to the outer edge of the ring of eigenvalues. Our results connect this spectral signature of quantum chaos to the sensitivity of the system to its initial conditions.",0,arxiv,Kuantum,CC-BY/arXiv,Stability of quantum chaos against weak non-unitarity
"Quantum machine learning algorithms have very recently attracted significant attention in photonic platforms. In particular, reconfigurable integrated photonic circuits offer a promising route, thanks to the possibility of implementing adaptive feedback loops, which is an essential ingredient for achieving the necessary nonlinear behavior characteristic of neural networks. Here, we implement a quantum reservoir computing protocol in which information is processed through a reconfigurable linear optical integrated photonic circuit and measured using single-photon detectors. We exploit a multiphoton-based setup for time-series forecasting tasks in a variety of scenarios, where the input signal is encoded in one of the circuit's optical phases, thus modulating the quantum reservoir state. The resulting output probabilities are used to set the feedback phases and, at the end of the computation, are fed to a classical digital layer trained via linear regression to perform predictions. We then focus on the investigation of the role of input photon indistinguishability in the reservoir's capabilities of predicting time-series. We experimentally demonstrate that two-photon indistinguishable input states lead to significantly better performance compared to distinguishable ones. This enhancement arises from the quantum correlations present in indistinguishable states, which enable the system to approximate higher-order nonlinear functions when using comparable physical resources, highlighting the importance of quantum interference and indistinguishability as a resource in photonic quantum reservoir computing.",0,arxiv,Kuantum,CC-BY/arXiv,Time-series forecasting with multiphoton quantum states and integrated photonics
"Inspired by the remarkable ability of plasmons to boost radiative emission rates, we propose leveraging acoustic graphene plasmons (AGPs) to realize tunable, giant Purcell enhancements for single-photon, entangled-photon, and multipolar quantum emitters. These AGPs are localized inside a cavity defined by a graphene sheet and a metallic nanocube and filled with a dielectric of thickness of a few nanometers and consisting of stacked layers of 2D materials, containing impurities or defects that act as quantum light emitters. Through finite-difference time domain (FDTD) calculations, we show that this geometry can achieve giant Purcell enhancement factors over a large portion of the infrared (IR) spectrum, up to 6 orders of magnitude in the mid-IR and up to 4 orders of magnitude at telecommunications wavelengths, reaching quantum efficiencies of 95\% and 89\%, respectively, with high-mobility graphene. We obtain Purcell enhancement factors for single-photon electric dipole (E1), electric quadrupole (E2), and electric octupole (E3) transitions and two-photon spontaneous emission (2PSE) transitions, of the orders of $10^{4}$, $10^{7}$, $10^{9}$, and $10^9$, respectively, and a quantum efficiency of 79\% for entangled-photon emission with high-mobility graphene at a wavelength of $Î»=1.55$ $Î¼$m. Importantly, AGP mode frequencies depend on the graphene Fermi energy, which can be tuned via electrostatic gating to modulate fluorescence enhancement in real time. As an example, we consider the Purcell enhancement of spontaneous single- and two-photon emissions from an erbium atom inside single-layer (SL) WS$_2$. Our results could be useful for electrically tunable quantum emitter devices with applications in quantum communication and quantum information processing.",0,arxiv,Kuantum,CC-BY/arXiv,Tunable giant Purcell enhancement of quantum light emitters by means of acoustic graphene plasmons
"We propose Shuttling-based Distributed Quantum Computing (SDQC), a hybrid architecture that combines the strengths of physical qubit shuttling and distributed quantum computing to enable scalable trapped-ion quantum computing. SDQC performs non-local quantum operations by distributing entangled ion qubits via deterministic shuttling, combining the high-fidelity and deterministic operations of shuttling-based architectures with the parallelism and pipelining advantages of distributed quantum computing. We present (1) a practical architecture incorporating quantum error correction (QEC), (2) pipelining strategies to exploit parallelism in entanglement distribution and measurement, and (3) a performance evaluation in terms of logical error rate and clock speed. For a 256-bit elliptic-curve discrete logarithm problem (ECDLP) instance, which requires 2,871 logical qubits at code distance 13, SDQC achieves a logical error rate which is $1.20^{+0.94}_{-0.45}\times10^{-8}$ of Photonic DQC error rate and $3.79^{+5.09}_{-2.84}\times10^{-3}$ of Quantum Charge-Coupled Device (QCCD) error rate, while providing 2.82 times faster logical clock speed than QCCD.",0,arxiv,Kuantum,CC-BY/arXiv,SDQC: Distributed Quantum Computing Architecture Utilizing Entangled Ion Qubit Shuttling
"We study the problem of data transmission under the influence of a jammer, which is typical for wireless systems and commonly modeled as an arbitrarily varying channel (AVC) in information theory. AVC fulfilling a certain set of linear equations are called symmetrizable and are known to be prone to denial of service attacks. Recent work has shown that deciding if a given AVC is symmetrizable or not is a non-Turing computable problem. By relaxing the formulation of symmetrizability, we show the existence of a polynomial-time algorithm that determines whether a given AVC is non-symmetrizable, but displays a critical dependence on the number of jammer input states. We then show how imposing an energy constraint on the jammer allows the same algorithm to efficiently identify large classes of AVCs which are non-symmetrizable.",0,arxiv,Kuantum,CC-BY/arXiv,Detecting Symmetrizability in Physical Systems
"The transition from the quantum to the classical realm remains one of the most profound open questions in physics. While quantum theory predicts the existence of macroscopic superpositions, their apparent absence in the everyday world is attributed either to environmental decoherence or to an intrinsic mechanism for wave-function collapse. This work presents a quantitative and experimentally grounded framework for distinguishing these possibilities. We propose a levitated optomechanical platform capable of generating controllable Schrodinger-cat states in the center of mass motion of a dielectric nanosphere. A comprehensive master equation incorporates gas collisions, black-body radiation, and photon-recoil noise, establishing a calibrated environmental baseline. The Continuous Spontaneous Localization (CSL) model is embedded within the same framework, predicting a characteristic saturation of the decoherence rate with superposition size and a quadratic scaling with mass. A Bayesian inference protocol is outlined to discriminate collapse induced excess decoherence from environmental noise. Together these elements provide a concrete experimental blueprint for a decisive test of quantum linearity, either revealing new physics beyond standard quantum mechanics or setting the most stringent bounds to date on objective-collapse parameters.",0,arxiv,Kuantum,CC-BY/arXiv,Experimental Blueprint for Distinguishing Decoherence from Objective Collapse
"The Phononic Casimir effect between planar objects is investigated by deriving a formalism from the quantum partition function of the system following multiscattering approach. This fluctuation-induced coupling is mediated by phonons modeled as an effective elastic medium. We find that excitations with three types of polarizations arise from the resolved boundary conditions, however the coupling is dominated by only one of these degrees of freedom due to exponential suppression effects in the other two. The obtained scaling laws and dependence on materials properties and temperature suggest effective pathways of interaction control. Scenarios of materials combinations are envisioned where the Phononic Casimir effect is of similar order as the standard Casimir interaction mediated by electromagnetic fluctuations.",0,arxiv,Kuantum,CC-BY/arXiv,Phononic Casimir Effect in Planar Materials
"Quantum voting, inspired by quantum game theory, provides a framework in which the quantum majority rule (QMR) constitution of Bao and Yunger Halpern [Phys. Rev. A 95, 062306 (2017)] violates the quantum analogue of Arrow's impossibility theorem. We evaluate this QMR constitution analytically on classical profile data and implement its final measurement stage as a quantum circuit, running on both noiseless simulators and noisy IBM quantum hardware to map how realistic noise deforms the resulting societal ranking distribution. Moderate-high single-qubit noise does not change the qualitative behavior of QMR, whereas strong noise shifts the distribution toward other dominant winners than the classical one. We quantify this behavior using winner-agreement rates, Condorcet-winner flip rates, and Jensen-Shannon divergence between societal ranking distributions. In a second, exploratory component, we demonstrate an explicitly entanglement-based variant of the QMR constitution that serves as a testbed for multi-voter quantum correlations under noise, which we refer to as the QMR2-inspired variant. There, GHZ-type and separable superpositions over opposite rankings have the same expectation values but respond very differently to noise. Taken together, these two components connect the abstract QMR constitution to concrete implementations on noisy intermediate-scale quantum (NISQ) devices and highlight design considerations for future quantum voting protocols.",0,arxiv,Kuantum,CC-BY/arXiv,Implementation and Analysis of Quantum Majority Rules under Noisy Conditions
"We study the robustness of topological ground state degeneracy to long-range interactions in quantum many-body systems. We focus on slowly decaying two-body interactions that scale like a power-law $1/r^Î±$ where $Î±$ is smaller than the spatial dimension; such interactions are beyond the reach of known stability theorems which only apply to short-range or rapidly decaying long-range perturbations. Our main result is a computation of the ground state splitting of several toy models, which are variants of the 1D Ising model $H = -\sum_i Ïƒ^z_i Ïƒ^z_{i+1} + Î»\sum_{ij} |i-j|^{-Î±} Ïƒ^x_i Ïƒ^x_j$ with $Î»> 0$ and $Î±< 1$. These models are also closely connected to the Kitaev p-wave wire model with power-law density-density interactions. In these examples, we find that the splitting $Î´$ scales like a stretched exponential $Î´\sim \exp(-C L^{\frac{1+Î±}{2}})$ where $L$ is the system size. Our computations are based on path integral techniques similar to the instanton method introduced by Coleman. We also study another toy model with long-range interactions that can be analyzed without path integral techniques and that shows similar behavior.",0,arxiv,Kuantum,CC-BY/arXiv,Effect of slowly decaying long-range interactions on topological qubits
"Single-photon sources based on neutral or charged excitons in a semiconductor quantum dot are attractive resources for photonic quantum computers and simulators. To obtain indistinguishable photons, the source is pumped on resonance with polarized laser pulses, and the output is collected in orthogonal polarization. However, for sources featuring vertical emission of light, 50% of the emitted photons are unavoidably lost in this way. Here, we theoretically study the quantum dynamics of an exciton embedded in an asymmetric vertical cavity that favors emission in a specific polarization. We identify the configuration for optimal state initialization and demonstrate a path toward near-unity polarized efficiency. We also derive simple analytical formulas for the photon output in each polarization as a function of the Purcell-enhanced emission rates, which shed light on the physical mechanism behind our results.",0,arxiv,Kuantum,CC-BY/arXiv,Theory of single-photon emission from neutral and charged excitons in a polarization-selective cavity
"Dirac semimetals, with their protected Dirac points, present an ideal platform for realizing intrinsic topological superconductivity. In this work, we investigate superconductivity in a two-dimensional, square-lattice nonsymmorphic Dirac semimetal. In the normal state near half-filling, the Fermi surface consists of two distinct pockets, each enclosing a Dirac point at a time-reversal invariant momentum ($\textbf{X}=(Ï€,0)$ and $\textbf{Y}=(0,Ï€)$). Considering an on-site repulsive and nearest-neighbor attractive interaction, we use self-consistent mean-field theory to determine the ground-state pairing symmetry. We find that an even-parity, spin-singlet $d_{x^{2}-y^{2}}$-wave pairing is favored as it gives rise to a fully gapped superconducting state. Since the pairing amplitude has opposite signs on the two Dirac Fermi pockets, the superconducting state is identified as a second-order topological superconductor. The hallmark of this topological phase is the emergence of Majorana zero modes at the system's boundaries. Notably, the positions of these Majorana modes are highly controllable and can be manipulated simply by tailoring the boundary sublattice terminations. Our results highlight the promise of nonsymmorphic Dirac semimetals for realizing and manipulating Majorana modes.",0,arxiv,Kuantum,CC-BY/arXiv,Intrinsic Second-Order Topological Superconductors with Tunable Majorana Zero Modes
"Fault-tolerant quantum computation traditionally incurs substantial resource overhead, with both qubit and time overheads scaling polylogarithmically with the size of the computation. While prior work by Gottesman showed that constant qubit overhead is achievable under stochastic noise using quantum low-density parity-check (QLDPC) codes, it has remained an open question whether similar guarantees hold under more general, non-stochastic noise models. In this work, we address this question by considering a general circuit-level noise model defined via the diamond norm, which captures both stochastic and non-stochastic noise, including coherent and amplitude damping noise. We prove that constant qubit overhead fault-tolerant quantum computation is achievable in this general setting, using QLDPC codes with constant rate and linear minimum distance. To establish our result, we develop a fault-tolerant error correction scheme and a method for implementing logic gates under general circuit noise. These results extend the theoretical foundations of fault-tolerant quantum computation and offer new directions for fault-tolerant architectures under realistic noise models.",0,arxiv,Kuantum,CC-BY/arXiv,Fault-tolerant quantum computation with constant overhead for general noise
"The Sachdev-Ye-Kitaev (SYK) model has attracted widespread attention due to its relevance to diverse areas of physics, such as high temperature superconductivity, black holes, and quantum chaos. The model is, however, extremely challenging to realize experimentally. In this work, we show how a particular form of Floquet engineering, termed ``kinetic driving'', effectively eliminates single-particle processes and creates quasi-random all-to-all interactions when applied to models of Hubbard type. For the specific case of the Bose-Hubbard model, we explicitly verify that the driven system indeed reproduces SYK physics by direct comparison of the spectral form factor and out-of-time ordered correlation functions (OTOCs). Our findings indicate that a cold-atom realization of kinetic driving -- achieved through modulation of hopping amplitudes in an optical lattice -- offers a practical and accurate platform for quantum simulation of the SYK model.",0,arxiv,Kuantum,CC-BY/arXiv,Reaching Sachdev-Ye-Kitaev physics by shaking the Hubbard model
"Non-Hermitian quantum mechanics with parity-time (PT) symmetry offers a powerful framework for exploring the complex interplay of dissipation and coherent interactions in open quantum systems. While PT-symmetry breaking has been studied in various physical systems, its observation on a quantum many-body level remains elusive. Here, we experimentally realize a non-Hermitian XY model in a strongly-interacting Rydberg-atom array. By measuring the Loschmidt Echo of a fully polarized state, we observe distinct dynamical signatures of a PT-symmetry-breaking phase transition. Dipole interactions are found to play a crucial role, not only determining the transition point but also triggering a non-Hermitian many-body blockade effect that protects the Loschmidt Echo from decay with a non-monotonic dependence on the system size. Our results reveal intricate interaction-induced effects on PT-symmetry breaking and open the door for exploring non-Hermitian many-body dynamics beyond single-particle and mean-field paradigms.",0,arxiv,Kuantum,CC-BY/arXiv,Observation of non-Hermitian many-body phase transition in a Rydberg-atom array
"Understanding and classifying multipartite entanglement is fundamental to quantum information processing. A useful measure of multipartite entanglement is the minimal decomposition entropy, defined as the minimum of the RÃ©nyi entropy $ S_q $ associated with the state's decomposition over all local product bases. This quantity identifies the product bases in which the state is maximally localized, thereby yielding optimal representations for analyzing local-unitary equivalence and structural properties of multipartite states.   We investigate the minimal decomposition entropy for absolutely maximally entangled (AME) states, a class of highly entangled states characterized by their maximal entanglement across any bipartitions. We present a numerical algorithm for computing the minimal decomposition entropy for finite $ q>1 $. Entropy distributions for AME and Haar random states are obtained for $ q=2 $ and $ q=\infty $ in qubit, qutrit, and ququad systems. For $ q=2 $, AME states of four qutrits and four ququads exhibit smaller minimal decomposition entropy than Haar random states, indicating more localized optimal representations. For $ q=\infty $, corresponding to the geometric measure of entanglement, AME states display higher entanglement than Haar random states. The algorithm additionally produces simpler and sparser decompositions of known AME states, aiding in distinguishing genuinely quantum AME states from those associated with classical combinatorial designs.",0,arxiv,Kuantum,CC-BY/arXiv,Minimal decomposition entropy and optimal representations of absolutely maximally entangled states
"Evidence of non-hermitian behavior has been recently demonstrated in cavity magnonics, including the emergence of mode level attraction and exceptional points in spectroscopic measurements. This work demonstrates experimental evidence of time-domain dynamics of magnon-photon systems that are coupled through a long-range interaction (i.e. remote coupling) exhibiting level attraction mediated by an auxiliary mode. We directly observe the temporal evolution of dissipatively coupled cavity-magnon modes, where heavily damped transmission line modes mediate the interaction. Our frequency-domain measurements confirm the predicted level attraction, while time-domain ring-down measurements reveal the characteristic signatures of dissipative coupling dynamics. Our approach offers in situ tunability over the dissipative coupling strength, including complete suppression, without requiring physical modifications to the experimental setup, providing a versatile platform for exploring tunable, non-Hermitian physics.",0,arxiv,Kuantum,CC-BY/arXiv,Dynamic Modulation of Long Range Photon Magnon Coupling
"Born-rule generative modeling, a central task in quantum machine learning, seeks to learn probability distributions that can be efficiently sampled by measuring complex quantum states. One hope is for quantum models to efficiently capture probability distributions that are difficult to learn and simulate by classical means alone. Quantum Boltzmann machines were proposed about one decade ago for this purpose, yet efficient training methods have remained elusive. In this paper, I overcome this obstacle by proposing a practical solution that trains quantum Boltzmann machines for Born-rule generative modeling. Two key ingredients in the proposal are the Donsker-Varadhan variational representation of the classical relative entropy and the quantum Boltzmann gradient estimator of [Patel et al., arXiv:2410.12935]. I present the main result for a more general ansatz known as an evolved quantum Boltzmann machine [Minervini et al., arXiv:2501.03367], which combines parameterized real- and imaginary-time evolution. I also show how to extend the findings to other distinguishability measures beyond relative entropy. Finally, I present four different hybrid quantum-classical algorithms for the minimax optimization underlying training, and I discuss their theoretical convergence guarantees.",0,arxiv,Kuantum,CC-BY/arXiv,Generative modeling using evolved quantum Boltzmann machines
"The Mpemba effect refers to the anomalous relaxation of a quantum state that, despite being initially farther from equilibrium, relaxes faster than a closer counterpart. Detecting such a quantum Mpemba effect typically requires full knowledge of the quantum state during its time evolution, which is an experimentally challenging task since state tomography becomes exponentially difficult as system size increases. This poses a significant obstacle in studying Mpemba effect in complex many-body systems. In this work, we demonstrate that this limitation can be overcome by identifying suitable observables that signal rapid relaxation. Moreover, as long as the system equilibrates to a known unique steady-state, it is possible to fully detect the occurrence of quantum Mpemba effect just by measuring the observable for known state preparations. Our approach thus significantly reduces experimental complexity and offers a practical route for observing the quantum Mpemba effect in complex and extended multi-qubit setups.",0,arxiv,Kuantum,CC-BY/arXiv,Detection of Mpemba effect through good observables in open quantum systems
"The lecture notes cover the basics of quantum computing methods for quantum field theory applications. No detailed knowledge of either quantum computing or quantum field theory is assumed and we have attempted to keep the material at a pedagogical level. We review the anharmonic oscillator, using which we develop a hands-on treatment of certain interesting QFTs in $1+1D$: $Ï†^4$ theory, Ising field theory, and the Schwinger model. We review quantum computing essentials as well as tensor network techniques. The latter form an essential part for quantum computing benchmarking. Some error modelling on QISKIT is also done in the hope of anticipating runs on NISQ devices.   These lecture notes are the expanded version of a one semester course taught by AS during August-November 2025 at the Indian Institute of Science and TA-ed by UB. The programs written for this course are available in a GitHub repository.",0,arxiv,Kuantum,CC-BY/arXiv,Lectures on Quantum Field Theory on a Quantum Computer
"Quantum key distribution (QKD) offers unconditional information security by allowing two distant users to establish a common encryption key resilient to hacking. Resultingly, QKD networks interconnecting critical infrastructure and enabling the secure exchange of classified information, can provide a solution to the increasing number of successful cyberattacks. To efficiently deploy quantum networks, the technology must be integrated over existing communication infrastructure, such as optical fibre links. Yet, QKD poses stringent requirements on the conditions of the network over which it is deployed. This work demonstrates the first quantum communication network in Cyprus via the deployment of a multi-node quantum network, exploiting existing commercial underground optical fibre. The network employs bidirectional occupation of fibres and wavelength multiplexing in a ring architecture to achieve, with minimal use of dark fibres, high-rate QKD. Results obtained reveal consistent key generation rates across all nodes, confirming reliable operation in a real-world environment. This deployment highlights the feasibility of leveraging existing telecom infrastructure for quantum-secured communication, marking a significant step toward scalable and cost-effective quantum networks suited for critical applications.",0,arxiv,Kuantum,CC-BY/arXiv,Multi-node quantum key distribution network using existing underground optical fibre infrastructure
"As a class of electron-rich materials, electrides demonstrate promising applications in many fields. However, the required high pressure restricts the practical applications to some extent. This study reveals that the unique feature of electride, i.e., the localization of interstitial electrons, can be greatly enhanced and tuned by self-defective doping, applying tensile/compressive stress, or shear stress. Moreover, the requirement of orbital orthogonality between the valence and core electron wave functions, as well as the Pauli exclusion principle, should be the driven force for the electron interstitial localization; and the exertion of external pressure modifies the available space to accommodate the electronic wave functions, thus enhances the interstitial localization. These discoveries lay down the ground for searching for promising electrides that are practicable at ambient conditions.",0,arxiv,Kuantum,CC-BY/arXiv,Rational regulation strategies of interstitial localized electrons in electride: A density functional theory study
"We investigate tensor network simulations of the two-dimensional Hubbard model by mapping the lattice onto a one-dimensional chain using space-filling curves. In particular, we focus on the Hilbert curve, whose locality-preserving structure minimizes the range of effective interactions in the mapped model. This enables a more compact matrix product state (MPS) representation compared to conventional snake mapping. Through systematic benchmarks, we show that the Hilbert curve consistently yields lower ground-state energies at fixed bond dimension, with the advantage increasing for larger system sizes and in physically relevant interaction regimes. Our implementation reaches clusters up to $32\times32$ sites with open and periodic boundary conditions, delivering reliable ground-state energies and correlation functions in agreement with established results, but at significantly reduced computational cost. These findings establish space-filling curve mappings, particularly the Hilbert curve, as a powerful tool for extending tensor-network studies of strongly correlated two-dimensional quantum systems beyond the limits accessible with standard approaches.",0,arxiv,Kuantum,CC-BY/arXiv,Efficient Simulation of the 2D Hubbard Model via Hilbert Space-Filling Curve Mapping
"Trapped ions are among the most promising platforms for realizing a large-scale quantum information processor. Current progress focuses on integrating optical and electronic components into microfabricated ion traps to allow scaling to large numbers of ion qubits. Most available fabrication strategies for such integrated processors employ monolithic integration of all processor components and rely heavily on CMOS-compatible semiconductor fabrication technologies that are not optimized for the requirements of a trapped-ion quantum processor. In this work, we present a modular approach in which the processor modules, called chiplets, have specific functions and are fabricated separately. The individual chiplets are then combined using heterogeneous integration techniques. This strategy opens up the possibility of choosing the optimal materials and fabrication technology for each of the chiplets, with a minimum amount of fabrication limitations compared to the monolithic approach. Chiplet technology furthermore enables novel processor functionalities to be added in a cost-effective, modular fashion by adding or modifying only a subset of the chiplets. We describe the design concept of a chiplet-based trapped-ion quantum processor and demonstrate the technology with an example of an integrated individual-ion addressing system for a ten-ion crystal. The addressing system emphasizes the modularity of the chiplet approach, combining a surface ion trap manufactured on a glass substrate with a silicon substrate carrying integrated waveguides and a stack of 3D-printed micro-optics, achieving diffraction-limited focal spots at the ion positions.",0,arxiv,Kuantum,CC-BY/arXiv,Chiplet technology for large-scale trapped-ion quantum processors
"The generation of high harmonics is a strongly nonlinear effect that allows to probe properties of the target and to study electron dynamics in matter. It has been investigated in many different kinds of targets, including molecular gases, liquids and solids. Recently, high-harmonic generation was studied in organic molecular crystals by Wiechmann et al. [Nat. Commun. 16, 9890 (2025)]. It was found that the laser-polarization-dependent harmonic yield is sensitive to the weak couplings between nearest- and next-nearest-neighbor molecules. In this paper, the impact of the laser polarization angle and the intermolecular interaction on the harmonic yield is examined in detail using a simple but insightful two-dimensional tight-binding system that models a molecular dimer, i.e. two weakly coupled molecules. We find that the intensities of lower harmonic orders tend to maximize for a laser polarization direction aligning with the molecular axes, whereas higher harmonic orders rather show the strongest yield for a polarization direction along the intermolecular axis. We further demonstrate that the harmonic order at which the maximum flips from the molecular to the intermolecular direction strongly depends on the intermolecular coupling strength. To gain a deeper insight into the origins of the findings, we include a detailed adiabatic analysis, showing that the flipping of the maximum yield towards the intermolecular direction is already contained qualitatively in the adiabatically following states.",0,arxiv,Kuantum,CC-BY/arXiv,High-harmonic generation from two weakly coupled molecules: a simple tight-binding model
"We present a quantum computing approach to analyzing Large Language Model (LLM) embeddings, leveraging complex-valued representations and modeling semantic relationships using quantum mechanical principles. By establishing a direct mapping between LLM semantic spaces and quantum circuits, we demonstrate the feasibility of estimating semantic similarity using quantum hardware. One of the key results is the experimental calculation of cosine similarity between Google Sentence Transformer embeddings using a real quantum computer, providing a tangible demonstration of a quantum approach to semantic analysis. This work reveals a connection between LLMs and quantum mechanics, suggesting that these principles can offer new perspectives on semantic representation and processing, and paving the way for future development of quantum algorithms for natural language processing.",0,arxiv,Kuantum,CC-BY/arXiv,Quantum LLMs Using Quantum Computing to Analyze and Process Semantic Information
"Quantum technologies, encompassing communication, computation, and metrology, rely on the generation and control of non-Gaussian states of light. These states enable secure quantum communication, fault-tolerant quantum computation, and precision sensing beyond classical limits, yet their practical realisation remains a major challenge due to reliance on high-photon-number Fock states or strong non-linearities. Here we introduce a unified optical framework that removes this constraint, using only Gaussian inputs, optical parametric amplification, and heralded photon detection. Within a single architecture, we demonstrate the generation of photon-added squeezed states with near unit fidelity, cubic-phase-like states with strong non-linearities and fidelities above 98.5%, and squeezed-cat states exceeding 99% fidelity that can be iteratively bred into GKP grid states surpassing the 9.75 dB fault-tolerance threshold. Operating entirely below 3 dB of input squeezing, the approach provides a scalable, experimentally accessible platform that unites the state resources required for quantum communication, metrology, and computation within one coherent optical framework.",0,arxiv,Kuantum,CC-BY/arXiv,A unified optical platform for non-Gaussian and fault-tolerant Gottesman-Kitaev-Preskill states
"Fault-tolerant quantum computing (FTQC) requires fast and accurate decoding of Quantum Error Correction (QEC) syndromes. However, in large-scale systems, the number of available decoders is much smaller than the number of logical qubits, leading to a fundamental resource shortage. To address this limitation, Virtualized Quantum Decoder (VQD) architectures have been proposed to share a limited pool of decoders across multiple qubits. While the Minimize Longest Undecoded Sequence (MLS) heuristic has been introduced as an effective scheduling policy within the VQD framework, its locally greedy decision-making structure limits its ability to consider global circuit structure, causing inefficiencies in resource balancing and limited scalability. In this work, we propose Constraint-Optimal Driven Allocation (CODA), an optimization-based scheduling algorithm that leverages global circuit structure to minimize the longest undecoded sequence length. Across 19 benchmark circuits, CODA achieves an average 74\% reduction in the longest undecoded sequence length. Crucially, while the theoretical search space scales exponentially with circuit size, CODA effectively bypasses this combinatorial explosion. Our evaluation confirms that the scheduling time scales linearly with the number of qubits, determined by physical resource constraints rather than the combinatorial search space, ensuring robust scalability for large-scale FTQC systems. These results demonstrate that CODA provides a global optimization-based, scalable scheduling solution that enables efficient decoder virtualization in large-scale FTQC systems.",0,arxiv,Kuantum,CC-BY/arXiv,Constraint-Optimal Driven Allocation for Scalable QEC Decoder Scheduling
